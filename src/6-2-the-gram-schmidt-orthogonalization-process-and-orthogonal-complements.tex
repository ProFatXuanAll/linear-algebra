\section{The Gram-Schmidt Orthogonalization Process and Orthogonal Complements}\label{sec:6.2}

\begin{defn}\label{6.2.1}
  Let \(\V\) be an inner product space over \(\F\).
  A subset of \(\V\) is an \textbf{orthonormal basis} for \(\V\) over \(\F\) if it is an ordered basis that is orthonormal.
\end{defn}

\begin{eg}\label{6.2.2}
  The standard ordered basis for \(\vs{F}^n\) over \(\F\) is an orthonormal basis for \(\vs{F}^n\) over \(\F\).
\end{eg}

\begin{thm}\label{6.3}
  Let \(\V\) be an inner product space over \(\F\) and \(S = \set{\seq{v}{1,,k}}\) be an orthogonal subset of \(\V\) consisting of nonzero vectors.
  If \(y \in \spn{S}\), then
  \[
    y = \sum_{i = 1}^k \frac{\inn{y, v_i}}{\norm{v_i}^2} v_i.
  \]
\end{thm}

\begin{proof}[\pf{6.3}]
  Write \(y = \sum_{i = 1}^k a_i v_i\), where \(\seq{a}{1,,k} \in \F\).
  Then, for \(j \in \set{1, \dots, k}\), we have
  \begin{align*}
    \inn{y, v_j} & = \inn{\sum_{i = 1}^k a_i v_i, v_j}                                     \\
                 & = \sum_{i = 1}^k a_i \inn{v_i, v_j} &  & \text{(by \cref{6.1.1}(a)(b))} \\
                 & = a_j \inn{v_j, v_j}                &  & \text{(by \cref{6.1.12})}      \\
                 & = a_j \norm{v_j}^2.                 &  & \text{(by \cref{6.1.9})}
  \end{align*}
  So \(a_j = \frac{\inn{y, v_j}}{\norm{v_j}^2}\), and the result follows.
\end{proof}

\begin{cor}\label{6.2.3}
  If, in addition to the hypotheses of \cref{6.3}, \(S\) is orthonormal and \(y \in \spn{S}\), then
  \[
    y = \sum_{i = 1}^k \inn{y, v_i} v_i.
  \]
\end{cor}

\begin{proof}[\pf{6.2.3}]
  We have
  \begin{align*}
    y & = \sum_{i = 1}^k \frac{\inn{y, v_i}}{\norm{v_i}^2} v_i &  & \text{(by \cref{6.3})}    \\
      & = \sum_{i = 1}^k \inn{y, v_i} v_i.                     &  & \text{(by \cref{6.1.12})}
  \end{align*}
\end{proof}

\begin{note}
  If \(\V\) possesses a finite orthonormal basis, then \cref{6.2.3} allows us to compute the coefficients in a linear combination very easily.
\end{note}

\begin{cor}\label{6.2.4}
  Let \(\V\) be an inner product space over \(\F\), and let \(S\) be an orthogonal subset of \(\V\) consisting of nonzero vectors.
  Then \(S\) is linearly independent.
\end{cor}

\begin{proof}[\pf{6.2.4}]
  Suppose that \(\seq{v}{1,,k} \in \S\) and
  \[
    \sum_{i = 1}^k a_i v_i = \zv.
  \]
  As in the proof of \cref{6.3} with \(y = \zv\), we have \(a_j = \inn{\zv, v_j} / \norm{v_j}^2 = 0\) for all \(j \in \set{1, \dots, k}\).
  So \(S\) is linearly independent.
\end{proof}

\begin{note}
  \cref{6.2.4} tells us that the vector space \(\vs{H}\) in \cref{6.1.13} contains an infinite linearly independent set, and hence \(\vs{H}\) is not a finite-dimensional vector space.
\end{note}

\begin{thm}\label{6.4}
  Let \(\V\) be an inner product space over \(\F\) and \(S = \set{\seq{w}{1,,n}}\) be a linearly independent subset of \(\V\).
  Define \(S' = \set{\seq{v}{1,,n}}\), where \(v_1 = w_1\) and
  \begin{equation}\label{eq:6.2.1}
    v_k = w_k - \sum_{j = 1}^{k - 1} \frac{\inn{w_k, v_j}}{\norm{v_j}^2} v_j \quad \text{for } k \in \set{2, \dots, n}.
  \end{equation}
  Then \(S'\) is an orthogonal set of nonzero vectors such that \(\spn{S'} = \spn{S}\).
  The construction of \(\set{\seq{v}{1,,n}}\) is called the \textbf{Gram--Schmidt process}.
\end{thm}

\begin{proof}[\pf{6.4}]
  The proof is by mathematical induction on \(n\), the number of vectors in \(S\).
  For \(k \in \set{1, \dots, n}\), let \(S_k = \set{\seq{w}{1,,k}}\).
  If \(n = 1\), then the theorem is proved by taking \(S_1' = S_1\);
  i.e., \(v_1 = w_1 \neq \zv\).
  Assume then that the set \(S_n' = \set{\seq{v}{1,,n}}\) with the desired properties has been constructed by the repeated use of \cref{eq:6.2.1}.
  We show that the set \(S_{n + 1}' = \set{\seq{v}{1,,n+1}}\) also has the desired properties, where \(v_{n + 1}\) is obtained from \(S_n'\) by \cref{eq:6.2.1}.
  If \(v_{n + 1} = \zv\), then \cref{eq:6.2.1} implies that \(w_{n + 1} \in \spn{S_n'} = \spn{S_n}\), which contradicts the assumption that \(S_{n + 1}\) is linearly independent.
  For \(i \in \set{1, \dots, n}\), it follows from \cref{eq:6.2.1} that
  \begin{align*}
    \inn{v_{n + 1}, v_i} & = \inn{w_{n + 1}, v_i} - \sum_{j = 1}^n \frac{\inn{w_{n + 1}, v_j}}{\norm{v_j}^2} \inn{v_j, v_i} &  & \text{(by \cref{6.1.1}(a)(b))}   \\
                         & = \inn{w_{n + 1}, v_i} - \frac{\inn{w_{n + 1}, v_i}}{\norm{v_i}^2} \norm{v_i}^2                  &  & \text{(by induction hypotheses)} \\
                         & = 0,
  \end{align*}
  since \(\inn{v_j, v_i} = 0\) if \(i \neq j\) by the induction assumption that \(S_n'\) is orthogonal.
  Hence \(S_{n + 1}'\) is an orthogonal set of nonzero vectors.
  Now, by \cref{eq:6.2.1}, we have that \(\spn{S_{n + 1}'} \subseteq \spn{S_{n + 1}}\).
  But by \cref{6.2.4}, \(S_{n + 1}'\) is linearly independent;
  so \(\dim(\spn{S_{n + 1}'}) = \dim(\spn{S_{n + 1}}) = n + 1\).
  Therefore by \cref{1.11} we have \(\spn{S_{n + 1}'} = \spn{S_{n + 1}}\) and this closes the induction.
\end{proof}

\begin{defn}\label{6.2.5}
  If we continue applying the Gram--Schmidt orthogonalization process to the basis \(\set{1, x, x^2, \dots}\) for \(\ps{\R}\), we obtain an orthogonal basis whose elements are called the \emph{Legendre polynomials}.
\end{defn}

\begin{thm}\label{6.5}
  Let \(\V\) be a nonzero finite-dimensional inner product space over \(\F\).
  Then \(\V\) has an orthonormal basis \(\beta\).
  Furthermore, if \(\beta = \set{\seq{v}{1,,n}}\) and \(x \in \V\), then
  \[
    x = \sum_{i = 1}^n \inn{x, v_i} v_i.
  \]
\end{thm}

\begin{proof}[\pf{6.5}]
  Let \(\beta_0\) be an ordered basis for \(\V\) over \(\F\).
  Apply \cref{6.4} to obtain an orthogonal set \(\beta'\) of nonzero vectors with \(\spn{\beta'} = \spn{\beta_0} = \V\).
  By normalizing each vector in \(\beta'\), we obtain an orthonormal set \(\beta\) that generates \(\V\).
  By \cref{6.2.4}, \(\beta\) is linearly independent;
  therefore \(\beta\) is an orthonormal basis for \(\V\) over \(\F\).
  The remainder of the theorem follows from \cref{6.2.3}.
\end{proof}

\begin{cor}\label{6.2.6}
  Let \(\V\) be a finite-dimensional inner product space over \(\F\) with an orthonormal basis \(\beta = \set{\seq{v}{1,,n}}\).
  Let \(\T \in \ls(\V)\), and let \(A = [\T]_{\beta}\).
  Then for any \(i, j \in \set{1, \dots, n}\), \(A_{i j} = \inn{\T(v_j), v_i}\).
\end{cor}

\begin{proof}[\pf{6.2.6}]
  From \cref{6.5}, we have
  \[
    \T(v_j) = \sum_{i = 1}^n \inn{\T(v_j), v_i} v_i.
  \]
  Hence by \cref{2.2.4} \(A_{i j} = \inn{\T(v_j), v_i}\).
\end{proof}

\begin{defn}\label{6.2.7}
  Let \(\beta\) be an orthonormal subset (possibly infinite) of an inner product space \(\V\) over \(\F\), and let \(x \in \V\).
  We define the \textbf{Fourier coefficients} of \(x\) relative to \(\beta\) to be the scalars \(\inn{x, y}\), where \(y \in \beta\).

  In the first half of the 19th century, the French mathematician Jean Baptiste Fourier was associated with the study of the scalars
  \[
    \int_0^{2\pi} f(t) \sin(nt) \; dt \quad \text{and} \quad \int_0^{2\pi} f(t) \cos(nt) \; dt,
  \]
  or more generally,
  \[
    c_n = \frac{1}{2\pi} \int_0^{2\pi} f(t) e^{-int} \; dt,
  \]
  for a function \(f\).
  In the context of \cref{6.1.8}, we see that \(c_n = \inn{f, f_n}\), where \(f_n(t) = e^{int}\);
  that is, \(c_n\) is the \(n\)th Fourier coefficient for a continuous function \(f \in \V\) relative to \(S\).
  These coefficients are the ``classical'' Fourier coefficients of a function, and the literature concerning the behavior of these coefficients is extensive.
\end{defn}

\begin{eg}\label{6.2.8}
  Let \(S = \set{e^{int} : n \text{ is an integer}}\).
  In \cref{6.1.13}, \(S\) was shown to be an orthonormal set in \(\vs{H}\).
  We compute the Fourier coefficients of \(f(t) = t\) relative to \(S\).
  Using integration by parts, we have, for \(n \neq 0\),
  \begin{align*}
    \inn{f, f_n} & = \frac{1}{2\pi} \int_0^{2\pi} t \conj{e^{int}} \; dt                                         \\
                 & = \frac{1}{2\pi} \int_0^{2\pi} t e^{-int} \; dt                                               \\
                 & = \frac{1}{2\pi} \pa{\eval{\frac{-1}{in} t e^{-int}}_0^{2\pi} - \int_0^{2\pi} e^{-int} \; dt} \\
                 & = \frac{1}{2\pi} \pa{\frac{-2\pi}{in} - \frac{-1}{in} \eval{e^{-int}}_0^{2\pi}}               \\
                 & = \frac{-1}{in},
  \end{align*}
  and, for \(n = 0\),
  \[
    \inn{f, 1} = \frac{1}{2\pi} \int_0^{2\pi} t(1) \; dt = \pi.
  \]
  As a result of these computations, and using \cref{ex:6.2.16}, we obtain an upper bound for the sum of a special infinite series as follows:
  \begin{align*}
    \norm{f}^2 & \geq \sum_{n = -k}^{-1} \abs{\inn{f, f_n}}^2 + \abs{\inn{f, 1}}^2 + \sum_{i = 1}^k \abs{\inn{f, f_n}}^2 \\
               & = \sum_{n = -k}^1 \frac{1}{n^2} + \pi^2 + \sum_{n = 1}^k \frac{1}{n^2}                                  \\
               & = 2 \sum_{n = 1}^k \frac{1}{n^2} + \pi^2
  \end{align*}
  for every \(k \in \Z^+\).
  Now, using the fact that \(\norm{f}^2 = \frac{4}{3} \pi^2\), we obtain
  \[
    \frac{4}{3} \pi^2 \geq 2 \sum_{n = 1}^k \frac{1}{n^2} + \pi^2,
  \]
  or
  \[
    \frac{\pi^2}{6} \geq \sum_{n = 1}^k \frac{1}{n^2}.
  \]
  Because this inequality holds for all \(k \in \Z^+\), we may let \(k \to \infty\) to obtain
  \[
    \frac{\pi^2}{6} \geq \sum_{n = 1}^\infty \frac{1}{n^2}.
  \]
  Additional results may be produced by replacing \(f\) by other functions.
\end{eg}

\begin{defn}\label{6.2.9}
  Let \(S\) be a nonempty subset of an inner product space \(\V\) over \(\F\).
  We define \(S^{\perp}\) (read ``\(S\) perp'') to be the set of all vectors in \(\V\) that are orthogonal to every vector in \(S\);
  that is, \(S^{\perp} = \set{x \in \V : \inn{x, y} = 0 \text{ for all } y \in S}\).
  The set \(S^{\perp}\) is called the \textbf{orthogonal complement} of \(S\).
\end{defn}

\begin{prop}\label{6.2.10}
  Let \(S\) be a nonempty subset of an inner product space \(\V\) over \(\F\).
  Then \(S^{\perp}\) is a subspace of \(\V\) over \(\F\).
\end{prop}

\begin{proof}[\pf{6.2.10}]
  Let \(x, y \in S^{\perp}\) and let \(c \in \F\).
  Since
  \begin{align*}
             & \forall z \in S, \inn{x, z} = \inn{y, z} = 0 &  & \text{(by \cref{6.2.9})}       \\
    \implies & \forall z \in S, \inn{cx + y, z} = 0         &  & \text{(by \cref{6.1.1}(a)(b))} \\
    \implies & cx + y \in S^{\perp}                         &  & \text{(by \cref{6.2.9})}
  \end{align*}
  and
  \begin{align*}
             & \forall z \in S, \inn{\zv, z} = 0 &  & \text{(by \cref{6.1}(c))} \\
    \implies & \zv \in S^{\perp},                &  & \text{(by \cref{6.2.9})}
  \end{align*}
  by \cref{ex:1.3.18} we see that \(S^{\perp}\) is a subspace of \(\V\) over \(\F\).
\end{proof}

\begin{eg}\label{6.2.11}
  \(\set{\zv}^{\perp} = \V\) and \(\V^{\perp} = \set{\zv}\) for any inner product space \(\V\) over \(\F\).
\end{eg}

\begin{proof}[\pf{6.2.11}]
  We have
  \begin{align*}
             & \forall x \in \V, \inn{x, \zv} = 0        &  & \text{(by \cref{6.1}(c))} \\
    \implies & \forall x \in \V, x \in \set{\zv}^{\perp} &  & \text{(by \cref{6.2.9})}  \\
    \implies & \V = \set{\zv}^{\perp}                    &  & \text{(by \cref{6.2.10})}
  \end{align*}
  and
  \begin{align*}
             & \forall x \in \V^{\perp}, \inn{x, x} = 0 &  & \text{(by \cref{6.2.9})}  \\
    \implies & \forall x \in \V^{\perp}, x = \zv        &  & \text{(by \cref{6.1}(d))} \\
    \implies & \V^{\perp} = \set{\zv}.
  \end{align*}
\end{proof}

\begin{thm}\label{6.6}
  Let \(\W\) be a finite-dimensional subspace of an inner product space \(\V\) over \(\F\), and let \(y \in \V\).
  Then there exist unique vectors \(u \in \W\) and \(z \in \W^{\perp}\) such that \(y = u + z\).
  Furthermore, if \(\set{\seq{v}{1,,k}}\) is an orthonormal basis for \(\W\), then
  \[
    u = \sum_{i = 1}^k \inn{y, v_i} v_i.
  \]
\end{thm}

\begin{proof}[\pf{6.6}]
  Let \(\set{\seq{v}{1,,k}}\) be an orthonormal basis for \(\W\) over \(\F\), let \(u\) be as defined in the preceding equation, and let \(z = y - u\).
  Clearly \(u \in \W\) and \(y = u + z\).

  To show that \(z \in \W^{\perp}\), it suffices to show, by \cref{ex:6.2.7}, that \(z\) is orthogonal to each \(v_j\).
  For any \(j \in \set{1, \dots, k}\), we have
  \begin{align*}
    \inn{z, v_j} & = \inn{y - \sum_{i = 1}^k \inn{y, v_i} v_i, v_j}                                                \\
                 & = \inn{y, v_j} - \sum_{i = 1}^k \inn{y, v_i} \inn{v_i, v_j} &  & \text{(by \cref{6.1.1}(a)(b))} \\
                 & = \inn{y, v_j} - \inn{y, v_j}                               &  & \text{(by \cref{6.1.12})}      \\
                 & = 0.
  \end{align*}
  To show uniqueness of \(u\) and \(z\), suppose that \(y = u + z = u' + z'\), where \(u' \in \W\) and \(z' \in \W^{\perp}\).
  Then \(u - u' = z' - z \in \W \cap \W^{\perp} = \set{\zv}\) (by \cref{6.2.11}).
  Therefore, \(u = u'\) and \(z = z'\).
\end{proof}

\begin{cor}\label{6.2.12}
  In the notation of \cref{6.6}, the vector \(u\) is the unique vector in \(\W\) that is ``closest'' to \(y\);
  that is, for any \(x \in \W\), \(\norm{y - x} \geq \norm{y - u}\), and this inequality is an equality iff \(x = u\).
  The vector \(u\) is called the \textbf{orthogonal projection} of \(y\) on \(W\).
\end{cor}

\begin{proof}[\pf{6.2.12}]
  As in \cref{6.6}, we have that \(y = u + z\), where \(z \in \W^{\perp}\).
  Let \(x \in \W\).
  Then \(u - x\) is orthogonal to \(z\), so we have
  \begin{align*}
    \norm{y - x}^2 & = \norm{u + z - x}^2                                          \\
                   & = \norm{(u - x) + z}^2                                        \\
                   & = \norm{u - x} + \norm{z}^2 &  & \text{(by \cref{ex:6.1.10})} \\
                   & \geq \norm{z}^2             &  & \text{(by \cref{6.2}(b))}    \\
                   & = \norm{y - u}^2.
  \end{align*}
  Now suppose that \(\norm{y - x} = \norm{y - u}\).
  Then the inequality above becomes an equality, and therefore \(\norm{u - x}^2 + \norm{z}^2 = \norm{z}^2\).
  It follows that \(\norm{u - x} = 0\), and hence \(x = u\).
  The proof of the converse is obvious.
\end{proof}

\begin{thm}\label{6.7}
  Suppose that \(S = \set{\seq{v}{1,,k}}\) is an orthonormal set in an \(n\)-dimensional inner product space \(\V\) over \(\F\).
  Then
  \begin{enumerate}
    \item \(S\) can be extended to an orthonormal basis \(\set{\seq{v}{1,,k,k+1,,n}}\) for \(\V\) over \(\F\).
    \item If \(\W = \spn{S}\), then \(S_1 = \set{\seq{v}{k+1,,n}}\) is an orthonormal basis for \(\W^{\perp}\) over \(\F\) (using the preceding notation).
    \item If \(\W\) is any subspace of \(\V\), then \(\dim(\V) = \dim(\W) + \dim(\W^{\perp})\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{6.7}(a)]
  By \cref{1.6.15}(c), \(S\) can be extended to an ordered basis \(S' = \set{\seq{v}{1,,k}, \seq{w}{k+1,,n}}\) for \(\V\) over \(\F\).
  Now apply the Gram--Schmidt process (\cref{6.4}) to \(S'\).
  The first \(k\) vectors resulting from this process are the vectors in \(S\) by \cref{ex:6.2.8}, and this new set spans \(\V\).
  Normalizing the last \(n - k\) vectors of this set produces an orthonormal set that spans \(\V\).
  The result now follows.
\end{proof}

\begin{proof}[\pf{6.7}(b)]
  Because \(S_1\) is a subset of a basis, it is linearly independent.
  Since \(S_1\) is clearly a subset of \(\W^{\perp}\), we need only show that it spans \(\W^{\perp}\).
  Note that, for any \(x \in \V\), we have
  \[
    x = \sum_{i = 1}^n \inn{x, v_i} v_i.
  \]
  If \(x \in \W^{\perp}\), then \(\inn{x, v_i} = 0\) for \(i \in \set{1, \dots, k}\).
  Therefore,
  \[
    x = \sum_{i = k + 1}^n \inn{x, v_i} v_i \in \spn{S_1}.
  \]
\end{proof}

\begin{proof}[\pf{6.7}(c)]
  Let \(\W\) be a subspace of \(\V\) over \(\F\).
  It is a finite-dimensional inner product space because \(\V\) is, and so it has an orthonormal basis \(\set{\seq{v}{1,,k}}\).
  By \cref{6.7}(a)(b), we have
  \[
    \dim(\V) = n = k + (n - k) = \dim(\W) + \dim(\W^{\perp}).
  \]
\end{proof}

\exercisesection

\setcounter{ex}{5}
\begin{ex}\label{ex:6.2.6}
  Let \(\V\) be an inner product space over \(\F\), and let \(\W\) be a finite-dimensional subspace of \(\V\) over \(\F\).
  If \(x \notin \W\), prove that there exists \(y \in \V\) such that \(y \in \W^{\perp}\), but \(\inn{x, y} \neq 0\).
\end{ex}

\begin{proof}[\pf{ex:6.2.6}]
  Since \(x \in \V\), by \cref{6.6} there exist a \((u, v) \in \W \times \W^{\perp}\) such that \(x = u + v\).
  Then we have
  \begin{align*}
             & \begin{dcases}
                 x \notin \W \\
                 \zv \in \W
               \end{dcases} &  & \text{(by \cref{1.3}(c))} \\
    \implies & x \neq \zv                                  \\
    \implies & v \neq \zv     &  & (x \notin \W)
  \end{align*}
  and
  \begin{align*}
    \inn{x, v} & = \inn{u + v, v}                                           \\
               & = \inn{u, v} + \inn{v, v} &  & \text{(by \cref{6.1.1}(a))} \\
               & = \inn{v, v}              &  & \text{(by \cref{6.2.9})}    \\
               & > 0.                      &  & \text{(by \cref{6.1.1}(d))}
  \end{align*}
  By setting \(y = v\) we are done.
\end{proof}

\begin{ex}\label{ex:6.2.7}
  Let \(\beta\) be a basis for a subspace \(\W\) of an inner product space \(\V\) over \(\F\), and let \(z \in \V\).
  Prove that \(z \in \W^{\perp}\) iff \(\inn{z, v} = 0\) for every \(v \in \beta\).
\end{ex}

\begin{proof}[\pf{ex:6.2.7}]
  We have
  \begin{align*}
             & z \in \W^{\perp}                                                  \\
    \implies & \forall w \in \W, \inn{z, w} = 0    &  & \text{(by \cref{6.2.9})} \\
    \implies & \forall v \in \beta, \inn{z, v} = 0 &  & (\beta \subseteq \W)
  \end{align*}
  and
  \begin{align*}
             & \forall v \in \beta, \inn{z, v} = 0                                                                                         \\
    \implies & \forall w \in \W, \begin{dcases}
                                   \exists \seq{v}{1,,k} \in \beta \\
                                   \exists \seq{a}{1,,k} \in \F
                                 \end{dcases} :                                                                           \\
             & \inn{z, w} = \inn{z, \sum_{i = 1}^k a_i v_i} = \sum_{i = 1}^k \conj{a_i} \inn{z, v_i} = 0 &  & \text{(by \cref{6.1}(a)(b))} \\
    \implies & z \in \W^{\perp}.                                                                         &  & \text{(by \cref{6.2.9})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:6.2.8}
  Prove that if \(\set{\seq{w}{1,,n}}\) is an orthogonal set of nonzero vectors, then the vectors \(\seq{v}{1,,n}\) derived from the Gram--Schmidt process satisfy \(v_i = w_i\) for \(i \in \set{1, \dots, n}\).
\end{ex}

\begin{proof}[\pf{ex:6.2.8}]
  We use induction on \(n\).
  For \(n = 1\) we see by \cref{6.4} that \(v_1 = w_1\).
  Thus the base case holds.
  Suppose inductively that the statement is true for some \(n \geq 1\).
  We need to show that for \(n + 1\) the statement is still true.
  Let \(\set{\seq{w}{1,,n+1}}\) be an orthogonal set of nonzero vectors and let \(\set{\seq{v}{1,,n+1}}\) derived from the Gram--Schmidt process.
  By induction hypothesis we see that \(\set{\seq{v}{1,,n+1}} = \set{\seq{w}{1,,n}, v_{n + 1}}\).
  Thus we have
  \begin{align*}
    v_{n + 1} & = w_{n + 1} + \sum_{i = 1}^n \frac{\inn{w_{n + 1}, v_i}}{\norm{v_i}^2} v_i &  & \text{(by \cref{6.4})}           \\
              & = w_{n + 1} + \sum_{i = 1}^n \frac{\inn{w_{n + 1}, w_i}}{\norm{w_i}^2} w_i &  & \text{(by induction hypothesis)} \\
              & = w_{n + 1}                                                                &  & \text{(by \cref{6.1.12})}
  \end{align*}
  and this closes the induction.
\end{proof}

\begin{ex}\label{ex:6.2.16}

\end{ex}
