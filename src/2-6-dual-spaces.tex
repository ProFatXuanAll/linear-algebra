\section{Dual Spaces}\label{sec:2.6}

\begin{defn}\label{2.6.1}
  In this section, we are concerned exclusively with linear transformations from a vector space \(\V\) into its field of scalars \(\F\), which is itself a vector space of dimension \(1\) over \(\F\).
  Such a linear transformation is called a \textbf{linear functional} on \(\V\).
\end{defn}

\begin{eg}\label{2.6.2}
  Let \(\V\) be the vector space of continuous real-valued functions on the interval \([0, 2\pi]\).
  Fix a function \(g \in \V\).
  The function \(h : \V \to \R\) defined by
  \[
    h(x) = \frac{1}{2\pi} \int_{0}^{2\pi} x(t) g(t) \; dt
  \]
  is a linear functional on \(\V\).
  In the cases that \(g(t)\) equals \(\sin(nt)\) or \(\cos(nt)\), \(h(x)\) is often called the \textbf{\(n\)th Fourier coefficient of \(x\)}.
\end{eg}

\begin{proof}[\pf{2.6.2}]
  Let \(f_1, f_2 \in \V\) and let \(c \in \R\).
  Then we have
  \begin{align*}
    h(cf_1 + f_2) & = \frac{1}{2\pi} \int_{0}^{2\pi} (cf_1 + f_2)(t) g(t) \; dt                                           &  & \text{(by \cref{2.6.2})} \\
                  & = \frac{1}{2\pi} \int_{0}^{2\pi} cf_1(t) g(t) + f_2(t) g(t) \; dt                                                                   \\
                  & = \frac{c}{2\pi} \int_{0}^{2\pi} f_1(t) g(t) \; dt + \frac{1}{2\pi} \int_{0}^{2\pi} f_2(t) g(t) \; dt                               \\
                  & = c h(f_1) + h(f_2)                                                                                   &  & \text{(by \cref{2.6.2})}
  \end{align*}
  and thus by \cref{2.1.2}(b) \(h \in \ls(\V, \F)\).
\end{proof}

\begin{eg}\label{2.6.3}
  The trace function \(\tr : \ms{n}{n}{\F} \to \F\) is a linear functional.
\end{eg}

\begin{proof}[\pf{2.6.3}]
  By \cref{ex:1.3.6} we see that this is true.
\end{proof}

\begin{eg}\label{2.6.4}
  Let \(\V\) be a finite-dimensional vector space over \(\F\), and let \(\beta = \set{\seq{x}{1,,n}}\) be an ordered basis for \(\V\) over \(\F\).
  For each \(i \in \set{1, \dots, n}\), define \(f_i(x) = a_i\), where
  \[
    [x]_{\beta} = \begin{pmatrix}
      a_1    \\
      \vdots \\
      a_n
    \end{pmatrix}
  \]
  is the coordinate vector of \(x\) relative to \(\beta\).
  Then \(f_i\) is a linear functional on \(\V\) called the \textbf{\(i\)th coordinate function with respect to the basis \(\beta\)}.
  Note that \(f_i(x_j) = \delta_{i j}\), where \(\delta_{i j}\) is the Kronecker delta.
  These linear functionals play an important role in the theory of dual spaces (see \cref{2.24}).
\end{eg}

\begin{proof}[\pf{2.6.4}]
  Let \(a, b \in \V\) and let \(c \in \F\).
  By \cref{1.8} there exist some \(\seq{a}{1,,n}, \seq{b}{1,,n} \in \F\) such that
  \[
    a = \sum_{j = 1}^n a_j x_j \quad \text{and} \quad b = \sum_{j = 1}^n b_j x_j.
  \]
  Then we have
  \begin{align*}
    f_i(ca + b) & = f_i\pa{c \pa{\sum_{j = 1}^n a_j x_j} + \sum_{j = 1}^n b_j x_j}                               \\
                & = f_i\pa{\sum_{j = 1}^n (c a_j + b_j) x_j}                       &  & \text{(by \cref{1.2.1})} \\
                & = c a_i + b_i                                                    &  & \text{(by \cref{2.6.4})} \\
                & = c f_i(a) + f_i(b)                                              &  & \text{(by \cref{2.6.4})}
  \end{align*}
  and thus by \cref{2.1.2}(b) \(f_i \in \ls(\V, \F)\).
\end{proof}

\begin{defn}\label{2.6.5}
  For a vector space \(\V\) over \(\F\), we define the \textbf{dual space} of \(\V\) to be the vector space \(\ls(\V, \F)\), denoted by \(\V^*\).
  Thus \(\V^*\) is the vector space consisting of all linear functionals on \(\V\) with the operations of addition and scalar multiplication as defined in \cref{sec:2.2}.
  Note that if \(\V\) is finite-dimensional, then by the \cref{2.4.10}
  \[
    \dim(\V^*) = \dim(\ls(\V, \F)) = \dim(\V) \cdot \dim(\F) = \dim(\V).
  \]
  Hence by \cref{2.19} \(\V\) and \(\V^*\) are isomorphic.
  We also define the \textbf{double dual} \(\V^{**}\) of \(\V\) to be the dual of \(\V^*\).
  In \cref{2.26}, we show, in fact, that there is a natural identification of \(\V\) and \(\V^{**}\) in the case that \(\V\) is finite-dimensional.
\end{defn}

\begin{thm}\label{2.24}
  Suppose that \(\V\) is a finite-dimensional vector space over \(\F\) with the ordered basis \(\beta = \set{\seq{x}{1,,n}}\).
  Let \(f_i\) (\(1 \leq i \leq n\)) be the \(i\)th coordinate function with respect to \(\beta\) as defined in \cref{2.6.4}, and let \(\beta^* = \set{\seq{f}{1,,n}}\).
  Then \(\beta^*\) is an ordered basis for \(\V^*\), and, for any \(g \in \V^*\), we have
  \[
    g = \sum_{i = 1}^n g(x_i) f_i.
  \]
\end{thm}

\begin{proof}[\pf{2.24}]
  Let \(g \in \V^*\).
  Since \(\dim(\V^*) = n\), we need only show that
  \[
    g = \sum_{i = 1}^n g(x_i) f_i,
  \]
  from which it follows that \(\beta^*\) generates \(\V^*\), and hence is a basis by \cref{1.6.15}(a).
  Let
  \[
    h = \sum_{i = 1}^n g(x_i) f_i.
  \]
  For \(1 \leq j \leq n\), we have
  \begin{align*}
    h(x_j) & = \pa{\sum_{i = 1}^n g(x_i) f_i}(x_j)                               \\
           & = \sum_{i = 1}^n g(x_i) f_i(x_j)      &  & \text{(by \cref{2.2.5})} \\
           & = \sum_{i = 1}^n g(x_i) \delta_{i j}  &  & \text{(by \cref{2.6.4})} \\
           & = g(x_j).
  \end{align*}
  Therefore \(g = h\) by \cref{2.1.13}.
\end{proof}

\begin{defn}\label{2.6.6}
  Using the notation of \cref{2.24}, we call the ordered basis \(\beta^* = \set{\seq{f}{1,,n}}\) of \(\V^*\) over \(\F\) that satisfies \(f_i(x_j) = \delta_{i j}\) (\(1 \leq i, j \leq n\)) the \textbf{dual basis} of \(\beta\).
\end{defn}

\begin{note}
  We now assume that \(\V\) and \(\W\) are finite-dimensional vector spaces over \(\F\) with ordered bases \(\beta\) and \(\gamma\) over \(\F\), respectively.
  In \cref{sec:2.4}, we proved that there is a one-to-one correspondence between linear transformations \(\T : \V \to \W\) and \(m \times n\) matrices (over \(\F\)) via the correspondence \(\T \leftrightarrow [\T]_{\beta}^{\gamma}\).
  For a matrix of the form \(A = [\T]_{\beta}^{\gamma}\), the question arises as to whether or not there exists a linear transformation \(\U\) associated with \(\T\) in some natural way such that \(\U\) may be represented in some basis as \(\tp{A}\).
  Of course, if \(m \neq n\), it would be impossible for \(\U\) to be a linear transformation from \(\V\) into \(\W\).
  We now answer this question by applying what we have already learned about dual spaces.
\end{note}

\begin{thm}\label{2.25}
  Let \(\V\) and \(\W\) be finite-dimensional vector spaces over \(\F\) with ordered bases \(\beta\) and \(\gamma\), respectively.
  For any \(\T \in \ls(\V, \W)\), the mapping \(\tp{\T} : \W^* \to \V^*\) defined by \(\tp{\T}(g) = g \T\) for all \(g \in \W^*\) is a linear transformation with the property that \([\tp{\T}]_{\gamma^*}^{\beta^*} = \tp{\pa{[\T]_{\beta}^{\gamma}}}\).
\end{thm}

\begin{proof}[\pf{2.25}]
  For \(g \in \W^*\), it is clear that \(\tp{\T}(g) = g \T\) is a linear functional on \(\V\) and hence is in \(\V^*\) (see \cref{2.9}).
  Thus \(\tp{\T}\) maps \(\W^*\) into \(\V^*\).
  Now we show that \(\tp{\T} \in \ls(\W^*, \V^*)\).
  Let \(x, y \in \W^*\) and let \(c \in \F\).
  Since
  \begin{align*}
    \tp{\T}(cx + y) & = (cx + y) \T                &  & \text{(by \cref{2.25})}  \\
                    & = c x \T + y \T              &  & \text{(by \cref{2.2.5})} \\
                    & = c \tp{\T}(x) + \tp{\T}(y), &  & \text{(by \cref{2.25})}
  \end{align*}
  by \cref{2.1.2} we see that \(\tp{\T} \in \ls(\W^*, \V^*)\).

  To complete the proof, let \(\beta = \set{\seq{x}{1,,n}}\) and \(\gamma = \set{\seq{y}{1,,m}}\) with dual bases \(\beta^* = \set{\seq{f}{1,,n}}\) and \(\gamma^* = \set{\seq{g}{1,,m}}\), respectively.
  For convenience, let \(A = [\T]_{\beta}^{\gamma}\).
  To find the \(j\)th column of \([\tp{\T}]_{\gamma^*}^{\beta^*}\), we begin by expressing \(\tp{\T}(g_j)\) as a linear combination of the vectors of \(\beta^*\).
  By \cref{2.24} we have
  \[
    \tp{\T}(g_j) = g_j \T = \sum_{i = 1}^n (g_j \T)(x_i) f_i.
  \]
  So the row \(i\), column \(j\) entry of \([\tp{\T}]_{\gamma^*}^{\beta^*}\) is
  \begin{align*}
    (g_j \T)(x_i) & = g_j(\T(x_i))                                                         \\
                  & = g_j \pa{\sum_{k = 1}^m A_{k i} y_k} &  & \text{(by \cref{2.2.4})}    \\
                  & = \sum_{k = 1}^m A_{k i} g_j(y_k)     &  & \text{(by \cref{2.1.2}(d))} \\
                  & = \sum_{k = 1}^m A_{k i} \delta_{j k} &  & \text{(by \cref{2.6.4})}    \\
                  & = A_{j i}.
  \end{align*}
  Hence \([\tp{\T}]_{\gamma^*}^{\beta^*} = \tp{A}\).
\end{proof}

\begin{note}
  The linear transformation \(\tp{\T}\) defined in \cref{2.25} is called the \textbf{transpose} of \(\T\).
  It is clear that \(\tp{\T}\) is the unique linear transformation \(\U\) such that \([\U]_{\gamma^*}^{\beta^*} = \tp{\pa{[\T]_{\beta}^{\gamma}}}\)
  (see \cref{2.1.13}).
\end{note}

\begin{note}
  We now concern ourselves with demonstrating that any finite-dimensional vector space \(\V\) over \(\F\) \textbf{can be identified} in a natural way with its double dual \(\V^{**}\).
  There is, in fact, an isomorphism between \(\V\) and \(\V^{**}\) that does not depend on any choice of bases for the two vector spaces.
\end{note}

\begin{defn}\label{2.6.7}
  For a vector \(x \in \V\), we define \(\widehat{x} : \V^* \to \F\) by \(\widehat{x}(f) = f(x)\) for every \(f \in \V^*\).
  It is easy to verify that \(\widehat{x}\) is a linear functional on \(\V^*\), so \(\widehat{x} \in \V^{**}\).
  The correspondence \(x \leftrightarrow \widehat{x}\) allows us to define the desired isomorphism between \(\V\) and \(\V^{**}\).
\end{defn}

\begin{lem}\label{2.6.8}
  Let \(\V\) be a finite-dimensional vector space over \(\F\), and let \(x \in \V\).
  If \(\widehat{x}(f) = 0\) for all \(f \in \V^*\), then \(x = \zv\).
\end{lem}

\begin{proof}[\pf{2.6.8}]
  Let \(x \neq \zv\).
  We show that there exists \(f \in \V^*\) such that \(\widehat{x}(f) \neq 0\).
  Choose an ordered basis \(\beta = \set{\seq{x}{1,,n}}\) for \(\V\) over \(\F\) such that \(x_1 = x\).
  Let \(\set{\seq{f}{1,,n}}\) be the dual basis of \(\beta\).
  Then \(f_1(x_1) = 1 \neq 0\).
  Let \(f = f_1\).
\end{proof}

\begin{thm}\label{2.26}
  Let \(\V\) be a finite-dimensional vector space over \(\F\), and define \(\psi : \V \to \V^{**}\) by \(\psi(x) = \widehat{x}\).
  Then \(\psi\) is an isomorphism.
\end{thm}

\begin{proof}[\pf{2.26}]
  \begin{description}
    \item[\(\psi\) is linear:]
      Let \(x, y \in \V\) and \(c \in \F\).
      For \(f \in \V^*\), we have
      \begin{align*}
        \psi(cx + y)(f) & = f(cx + y)                         &  & \text{(by \cref{2.6.7})} \\
                        & = cf(x) + f(y)                      &  & \text{(by \cref{2.6.5})} \\
                        & = c \widehat{x}(f) + \widehat{y}(f) &  & \text{(by \cref{2.6.7})} \\
                        & = (c \widehat{x} + \widehat{y})(f). &  & \text{(by \cref{2.2.5})}
      \end{align*}
      Therefore
      \[
        \psi(cx + y) = c \widehat{x} + \widehat{y} = c \psi(x) + \psi(y).
      \]
    \item[\(\psi\) is one-to-one:]
      Suppose that \(\psi(x)\) is the zero functional on \(\V^*\) for some \(x \in \V\).
      Then \(\widehat{x}(f) = 0\) for every \(f \in \V^*\).
      By \cref{2.6.8} we conclude that \(x = \zv\).
      Since \(\ns{\psi} = \set{\zv}\) and \(\psi \in \ls(\V, \V^{**})\), by \cref{2.4} we know that \(\psi\) is one-to-one.
    \item[\(\psi\) is an isomorphism:]
      Since \(\psi \in \ls(\V, \V^{**})\), \(\psi\) is one-to-one and \(\dim(\V) = \dim(\V^{**})\), by \cref{2.5} and \cref{2.4.8} we know that \(\psi\) is isomorphism.
  \end{description}
\end{proof}

\begin{cor}\label{2.6.9}
  Let \(\V\) be a finite-dimensional vector space over \(\F\) with dual space \(\V^*\).
  Then every ordered basis for \(\V^*\) over \(\F\) is the dual basis for some basis for \(\V\) over \(\F\).
\end{cor}

\begin{proof}[\pf{2.6.9}]
  Let \(\set{\seq{f}{1,,n}}\) be an ordered basis for \(\V^*\) over \(\F\).
  We may combine \cref{2.24,2.26} to conclude that for this basis for \(\V^*\) over \(\F\), there exists a dual basis \(\set{\seq{\widehat{x}}{1,,n}}\) in \(\V^{**}\), that is,
  \begin{align*}
    \delta_{i j} & = \widehat{x}_i(f_j) &  & \text{(by \cref{2.24,2.26})} \\
                 & = f_j(x_i)           &  & \text{(by \cref{2.6.7})}
  \end{align*}
  for all \(i\) and \(j\).
  Thus by \cref{2.24} \(\set{\seq{f}{1,,n}}\) is the dual basis of \(\set{\seq{x}{1,,n}}\).
\end{proof}

\begin{note}
  Although many of the ideas of \cref{sec:2.6}, (e.g., the existence of a dual space), can be extended to the case where \(\V\) is not finite-dimensional, only a finite-dimensional vector space is isomorphic to its double dual via the map \(x \mapsto \widehat{x}\).
  In fact, for infinite-dimensional vector spaces, no two of \(\V\), \(\V^*\), and \(\V^{**}\) are isomorphic.
\end{note}

\exercisesection

\setcounter{ex}{7}
\begin{ex}\label{ex:2.6.8}
  Show that every plane through the origin in \(\R^3\) may be identified with the null space of a vector in \((\R^3)^*\).
  State an analogous result for \(\R^2\).
\end{ex}

\begin{proof}[\pf{ex:2.6.8}]
  First we prove the statement in the case of \(\R^3\).
  Let \((a, b, c) \in \R^3\) and let \(P\) be the following plane through the origin
  \[
    P = \set{(x, y, z) \in \R^3 : ax + by + cz = 0}.
  \]
  Let \(\T : \R^3 \to \R\) be the function \(\T(x, y, z) = ax + by + cz\).
  Clearly \(\T \in (\R^3)^*\).
  Then we have
  \begin{align*}
    \ns{\T} & = \set{(x, y, z) \in \R^3 : \T(x, y, z) = 0}       &  & \text{(by \cref{2.1.10})} \\
            & = \set{(x, y, z) \in \R^3 : ax + by + cz = 0} = P.
  \end{align*}

  Now we prove the statement in the case of \(\R^2\).
  Let \((a, b) \in \R^2\) and let \(P\) be the following plane through the origin
  \[
    P = \set{(x, y) \in \R^2 : ax + by = 0}.
  \]
  Let \(\T : \R^2 \to \R\) be the function \(\T(x, y) = ax + by\).
  Clearly \(\T \in (\R^2)^*\).
  Then we have
  \begin{align*}
    \ns{\T} & = \set{(x, y) \in \R^2 : \T(x, y) = 0}     &  & \text{(by \cref{2.1.10})} \\
            & = \set{(x, y) \in \R^2 : ax + by = 0} = P.
  \end{align*}
\end{proof}

\begin{ex}\label{ex:2.6.9}
  Prove that a function \(\T : \vs{F}^n \to \vs{F}^m\) is linear iff there exist \(\seq{f}{1,,m} \in (\vs{F}^n)^*\) such that \(\T(x) = (f_1(x), f_2(x), \dots, f_m(x))\) for all \(x \in \vs{F}^n\).
\end{ex}

\begin{proof}[\pf{ex:2.6.9}]
  First suppose that \(\T \in \ls(\vs{F}^n, \vs{F}^m)\).
  Let \(\gamma\) be the standard ordered basis for \(\vs{F}^m\) over \(\F\) and let \(\set{\seq{g}{1,,m}}\) be the dual basis of \(\gamma\).
  For each \(i \in \set{1, \dots, m}\), we define \(f_i : \vs{F}^n \to \F\) as follow:
  \[
    f_i = \tp{\T}(g_i) = g_i \T.
  \]
  By \cref{2.9,2.24} we see that \(f_i \in \ls(\vs{F}^n, \F)\).
  Thus by \cref{2.6.1} \(f_i \in (\vs{F}^n)^*\) and we have
  \begin{align*}
    \forall x \in \vs{F}^n, \T(x) & = [\T(x)]_{\gamma}                &  & \text{(by \cref{2.2.3})} \\
                                  & = \begin{pmatrix}
                                        g_1(\T(x)) & \cdots & g_m(\T(x))
                                      \end{pmatrix} &  & \text{(by \cref{2.6.4})}                   \\
                                  & = \begin{pmatrix}
                                        f_1(x) & \cdots & f_m(x)
                                      \end{pmatrix}.
  \end{align*}

  Now suppose that \(\T : \vs{F}^n \to \vs{F}^m\) is a function and there exist \(\seq{f}{1,,m} \in (\vs{F}^n)^*\) such that \(\T(x) = (f_1(x), \dots, f_m(x))\) for all \(x \in \vs{F}^n\).
  Let \(x, y \in \vs{F}^n\) and let \(c \in \F\).
  Then we have
  \begin{align*}
    \T(cx + y) & = \begin{pmatrix}
                     f_1(cx + y) & \cdots & f_m(cx + y)
                   \end{pmatrix}             \\
               & = \begin{pmatrix}
                     c f_1(x) + f_1(y) & \cdots & c f_m(x) + f_m(y)
                   \end{pmatrix} &  & \text{(by \cref{2.1.2}(b))} \\
               & = c \T(x) + \T(y)
  \end{align*}
  and thus by \cref{2.1.2}(b) \(\T \in \ls(\vs{F}^n, \vs{F}^m)\).
\end{proof}

\begin{ex}\label{ex:2.6.10}
  Let \(\seq{c}{0,,n}\) be distinct scalars in \(\F\).
  \begin{enumerate}
    \item For \(0 \leq i \leq n\), define \(f_i \in (\ps[n]{\F})^*\) by \(f_i(p) = p(c_i)\).
          Prove that \(\set{\seq{f}{0,,n}}\) is a basis for \((\ps[n]{\F})^*\).
    \item Use \cref{2.6.9} and (a) to show that there exist unique polynomials \(\seq{p}{0,,n}\) such that \(p_i(c_j) = \delta_{i j}\) for \(0 \leq i \leq n\).
          These polynomials are the Lagrange polynomials defined in \cref{1.6.20}.
    \item For any scalars \(\seq{a}{0,,n} \in \F\) (not necessarily distinct), deduce that there exists a unique polynomial \(q(x)\) of degree at most \(n\) such that \(q(c_i) = a_i\) for \(0 \leq i \leq n\).
          In fact,
          \[
            q(x) = \sum_{i = 0}^n a_i p_i(x).
          \]
    \item Deduce the Lagrange interpolation formula:
          \[
            p(x) = \sum_{i = 0}^n p(c_i) p_i(x)
          \]
          for any \(p \in \ps[n]{\F}\).
    \item Prove that
          \[
            \int_a^b p(t) \; dt = \sum_{i = 0}^n p(c_i) d_i,
          \]
          where
          \[
            d_i = \int_a^b p_i(t) \; dt.
          \]
          Suppose now that
          \[
            c_i = a + \frac{i (b - a)}{n} \quad \text{for } i \in \set{0, \dots, n}.
          \]
          For \(n = 1\), the preceding result yields the trapezoidal rule for evaluating the definite integral of a polynomial.
          For \(n = 2\), this result yields Simpson's rule for evaluating the definite integral of a polynomial.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:2.6.10}(a)]
  Let \(\seq{a}{0,,n} \in \F\) such that
  \[
    \sum_{i = 0}^n a_i f_i = \zv.
  \]
  For each \(j \in \set{0, \dots, n}\), we define \(p_j \in \ps[n]{\F}\) as follow:
  \[
    \forall x \in \F, p_j(x) = \prod_{\substack{i = 0 \\ i \neq j}}^n (x - c_i).
  \]
  Then we have
  \begin{align*}
             & p_j(c_i) \neq 0 \iff j = i                                                                                                    \\
    \implies & \forall i \in \set{0, \dots, n}, \pa{\sum_{i = 0}^n a_i f_i}(p_j) = \sum_{i = 0}^n a_i f_i(p_j) &  & \text{(by \cref{2.2.5})} \\
             & = \sum_{i = 0}^n a_i p_j(c_i) = a_j p_j(c_j) = 0                                                                              \\
    \implies & \forall i \in \set{0, \dots, n}, a_j = 0.                                                       &  & (p_j(c_j) \neq 0)
  \end{align*}
  Thus by \cref{1.5.3} \(\set{\seq{f}{0,,n}}\) is linearly independent.
  Since
  \begin{align*}
    \dim((\ps[n]{\F})^*) & = \dim(\ls(\ps[n]{\F}, \F))       &  & \text{(by \cref{2.6.5})}  \\
                         & = \dim(\ps[n]{\F}) \cdot \dim(\F) &  & \text{(by \cref{2.4.10})} \\
                         & = n + 1                           &  & \text{(by \cref{1.6.12})} \\
                         & = \#(\set{\seq{f}{0,,n}}),
  \end{align*}
  by \cref{1.6.15}(b) we know that \(\set{\seq{f}{0,,n}}\) is a basis for \((\ps[n]{\F})^*\).
\end{proof}

\begin{proof}[\pf{ex:2.6.10}(b)]
  By \cref{2.6.9} there exists a set \(\beta = \set{\seq{p}{0,,n}}\) such that \(\beta\) is an ordered basis for \(\ps[n]{\F}\) over \(\F\) and the set \(\beta^* = \set{\seq{f}{0,,n}}\) defined in \cref{ex:2.6.10}(a) is the dual basis of \(\beta\).
  Then we have
  \begin{align*}
             & \forall i, j \in \set{0, \dots, n}, f_j(p_i) = \delta_{j i} = \delta_{i j} &  & \text{(by \cref{2.6.6})}        \\
    \implies & \forall i, j \in \set{0, \dots, n}, p_i(c_j) = \delta_{i j}.               &  & \text{(by \cref{ex:2.6.10}(a))}
  \end{align*}
  Now we show that \(\beta\) is unique.
  Suppose for sake of contradiction that there exists a set \(\gamma = \set{\seq{q}{0,,n}}\) such that \(\beta \neq \gamma\), \(q_i(c_j) = \delta_{i j}\) for \(i \in \set{0, \dots, n}\), \(\gamma\) is an ordered basis for \(\ps[n]{\F}\) over \(\F\) and \(\beta^*\) is also the dual basis for \(\gamma\).
  Since \(q_0 \in \ps[n]{\F}\), by \cref{1.6.1} we know that there exist some \(\seq{a}{0,,n} \in \F\) such that
  \[
    q_0 = \sum_{i = 0}^n a_i p_i.
  \]
  But
  \begin{align*}
    \forall j \in \set{0, \dots, n}, q_0(c_j) & = \delta_{0 j}                                                        \\
                                              & = \pa{\sum_{i = 0}^n a_i p_i}(c_j) &  & \text{(by \cref{2.2.5})}      \\
                                              & = \sum_{i = 0}^n a_i p_i(c_j)                                         \\
                                              & = \sum_{i = 0}^n a_i \delta_{i j}  &  & \text{(from the proof above)} \\
                                              & = a_j
  \end{align*}
  implies \(a_0 = 1\) and \(a_j = 0\) for all \(j \in \set{0, \dots, n} \setminus \set{0}\).
  Thus we have \(q_0 = p_0\).
  Using similar argument we see that \(q_i = p_i\) for all \(i \in \set{0, \dots, n}\), a contradiction.
  Thus \(\beta\) is unique.
\end{proof}

\begin{proof}[\pf{ex:2.6.10}(c)]
  Defined \(\seq{p}{0,,n} \in \ps[n]{\F}\) as in \cref{ex:2.6.10}(b).
  For any \(\seq{a}{0,,n} \in \F\), we can define \(q \in \ps[n]{\F}\) as follow:
  \[
    q = \sum_{j = 0}^n a_j p_j.
  \]
  Note that by \cref{ex:2.6.10}(b) \(q\) is unique.
  Then we have
  \[
    \forall i \in \set{0, \dots, n}, q(c_i) = \pa{\sum_{j = 0}^n a_j p_j}(c_i) = \sum_{j = 0}^n a_j p_j(c_i) = \sum_{j = 0}^n a_j \delta_{j i} = a_i
  \]
  and thus
  \[
    q = \sum_{j = 0}^n q(c_j) p_j.
  \]
\end{proof}

\begin{proof}[\pf{ex:2.6.10}(d)]
  By \cref{ex:2.6.10}(c) we see that
  \[
    \forall p \in \ps[n]{\F}, p = \sum_{i = 0}^n p(c_i) p_i.
  \]
\end{proof}

\begin{proof}[\pf{ex:2.6.10}(e)]
  By \cref{ex:2.6.10}(d) we have
  \begin{align*}
    \forall p \in \ps[n]{\F}, \int_a^b p(t) \; dt & = \int_a^b \pa{\sum_{i = 0}^n p(c_i) p_i(t)} \; dt \\
                                                  & = \sum_{i = 0}^n \pa{p(c_i) \int_a^b p_i(t) \; dt} \\
                                                  & = \sum_{i = 0}^n p(c_i) d_i.
  \end{align*}
\end{proof}

\begin{ex}\label{ex:2.6.11}
  Let \(\V\) and \(\W\) be finite-dimensional vector spaces over \(\F\), and let \(\psi_1\) and \(\psi_2\) be the isomorphisms between \(\V\) and \(\V^{**}\) and \(\W\) and \(\W^{**}\), respectively, as defined in \cref{2.26}.
  Let \(\T \in \ls(\V, \W)\), and define \(\tp{\tp{\T}} = \tp{\pa{\tp{\T}}}\).
  Prove that \(\psi_2 \T = \tp{\tp{\T}} \psi_1\).
\end{ex}

\begin{proof}[\pf{ex:2.6.11}]
  By \cref{2.25} we see that \(\tp{\tp{\T}} \in \ls(\V^{**}, \W^{**})\).
  Let \(x \in \V\) and let \(y \in \W^*\).
  Then we have
  \begin{align*}
    \pa{\pa{\tp{\tp{\T}} \psi_1}(x)}(y) & = \pa{\tp{\tp{\T}}(\psi_1(x))}(y)                                 \\
                                        & = \pa{\tp{\tp{\T}}(\widehat{x})}(y) &  & \text{(by \cref{2.26})}  \\
                                        & = \pa{\widehat{x} \tp{\T}}(y)       &  & \text{(by \cref{2.25})}  \\
                                        & = \widehat{x}\pa{\tp{\T}(y)}                                      \\
                                        & = \widehat{x}(y \T)                 &  & \text{(by \cref{2.26})}  \\
                                        & = (y \T)(x)                         &  & \text{(by \cref{2.6.7})} \\
                                        & = y(\T(x))                                                        \\
                                        & = \widehat{\T(x)}(y)                &  & \text{(by \cref{2.6.7})} \\
                                        & = \psi_2(\T(x))(y)                  &  & \text{(by \cref{2.26})}  \\
                                        & = ((\psi_2 \T)(x))(y).
  \end{align*}
  Thus \(\tp{\tp{\T}} \psi_1 = \psi_2 \T\).
\end{proof}

\begin{ex}\label{ex:2.6.12}
  Let \(\V\) be a finite-dimensional vector space over \(\F\) with the ordered basis \(\beta\) over \(\F\).
  Prove that \(\psi(\beta) = \beta^{**}\), where \(\psi\) is defined in \cref{2.26}.
\end{ex}

\begin{proof}[\pf{ex:2.6.12}]
  Let \(\beta = \set{\seq{x}{1,,n}}\) and let \(\beta^* = \set{\seq{f}{1,,n}}\) be the dual basis of \(\beta\).
  By \cref{2.26} we have \(\psi(\beta) = \set{\seq{\widehat{x}}{1,,n}}\).
  Since
  \begin{align*}
    \forall i, j \in \set{1, \dots, n}, \widehat{x}_i(f_j) & = f_j(x_i)      &  & \text{(by \cref{2.6.7})} \\
                                                           & = \delta_{i j}, &  & \text{(by \cref{2.6.6})}
  \end{align*}
  by \cref{2.6.6} we see that \(\beta^{**} = \set{\seq{\widehat{x}}{1,,n}}\).
\end{proof}
