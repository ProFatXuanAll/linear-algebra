\section{Inner Products and Norms}\label{sec:6.1}

\begin{defn}\label{6.1.1}
  Let \(\V\) be a vector space over \(\F\).
  An \textbf{inner product} on \(\V\) over \(\F\) is a function that assigns, to every ordered pair of vectors \(x\) and \(y\) in \(\V\), a scalar in \(\F\), denoted \(\inn{x, y}\), such that for all \(x\), \(y\), and \(z\) in \(\V\) and all \(c\) in \(\F\), the following hold:
  \begin{enumerate}
    \item \(\inn{x + z, y} = \inn{x, y} + \inn{z, y}\).
    \item \(\inn{cx, y} = c \inn{x, y}\).
    \item \(\conj{\inn{x, y}} = \inn{y, x}\), where the bar denotes complex conjugation.
    \item \(\inn{x, x} > 0\) if \(x \neq \zv\).
  \end{enumerate}
\end{defn}

\begin{note}
  Note that \cref{6.1.1}(c) reduces to \(\inn{x, y} = \inn{y, x}\) if \(\F = \R\).
  \cref{6.1.1}(a)(b) simply require that the inner product be linear in the first component.
  It is easily shown that if \(\seq{a}{1,,n} \in \F\) and \(y, \seq{v}{1,,n} \in \V\), then
  \[
    \inn{\sum_{i = 1}^n a_i v_i, y} = \sum_{i = 1}^n a_i \inn{v_i, y}.
  \]
\end{note}

\begin{eg}\label{6.1.2}
  For \(x = \tuple{a}{1,,n}\) and \(y = \tuple{b}{1,,n}\) in \(\vs{F}^n\), define
  \[
    \inn{x, y} = \sum_{i = 1}^n a_i \conj{b_i}.
  \]
  The inner product defined above is called the \textbf{standard inner product} on \(\vs{F}^n\).
  When \(\F = \R\) the conjugations are not needed, and in early courses this standard inner product is usually called the \emph{dot product} and is denoted by \(x \cdot y\) instead of \(\inn{x, y}\).
\end{eg}

\begin{proof}[\pf{6.1.2}]
  Let \(x, y, z \in \vs{F}^n\) and let \(c \in \F\).
  Since
  \begin{align*}
    \inn{x + y, z}    & = \sum_{i = 1}^n (x + y)_i \conj{z_i}                           &  & \text{(by \cref{6.1.2})}        \\
                      & = \sum_{i = 1}^n x_i \conj{z_i} + \sum_{i = 1}^n y_i \conj{z_i} &  & \text{(by \cref{c.0.1})}        \\
                      & = \inn{x, z} + \inn{y, z}                                       &  & \text{(by \cref{6.1.2})}        \\
    \inn{cx, y}       & = \sum_{i = 1}^n (cx)_i \conj{y_i}                              &  & \text{(by \cref{6.1.2})}        \\
                      & = c \sum_{i = 1}^n x_i \conj{y_i}                               &  & \text{(by \cref{c.0.1})}        \\
                      & = c \inn{x, y}                                                  &  & \text{(by \cref{6.1.2})}        \\
    \conj{\inn{x, y}} & = \conj{\sum_{i = 1}^n x_i \conj{y_i}}                          &  & \text{(by \cref{6.1.2})}        \\
                      & = \sum_{i = 1}^n \conj{x_i} y_i                                 &  & \text{(by \cref{d.2}(a)(b)(c))} \\
                      & = \sum_{i = 1}^n y_i \conj{x_i}                                 &  & \text{(by \cref{c.0.1})}        \\
                      & = \inn{y, x}                                                    &  & \text{(by \cref{6.1.2})}
  \end{align*}
  and
  \begin{align*}
             & x \neq \zv                                                                                       \\
    \implies & \exists i \in \set{1, \dots, n} : x_i \neq 0                                                     \\
    \implies & \exists i \in \set{1, \dots, n} : \abs{x_i}^2 = x_i \conj{x_i} > 0 &  & \text{(by \cref{d.0.5})} \\
    \implies & \inn{x, x} = \sum_{i = 1}^n x_i \conj{x_i} > 0,                    &  & \text{(by \cref{6.1.2})}
  \end{align*}
  by \cref{6.1.1} we know that \(\inn{\cdot, \cdot}\) is an inner product on \(\vs{F}^n\) over \(\F\).
\end{proof}

\begin{eg}\label{6.1.3}
  If \(\inn{x, y}\) is any inner product on a vector space \(\V\) over \(\F\) and \(r > 0\), we may define another inner product by the rule \(\inn{x, y}' = r \inn{x, y}\).
  If \(r \leq 0\), then \cref{6.1.1}(d) would not hold.
\end{eg}

\begin{proof}[\pf{6.1.3}]
  Let \(x, y, z \in \V\), let \(c \in \F\) and let \(r \in \R^+\).
  Since
  \begin{align*}
    \inn{x + y, z}'    & = r\inn{x + y, z}             &  & \text{(by \cref{6.1.3})}    \\
                       & = r (\inn{x, z} + \inn{y, z}) &  & \text{(by \cref{6.1.1}(a))} \\
                       & = r \inn{x, z} + r \inn{y, z} &  & \text{(by \cref{c.0.1})}    \\
                       & = \inn{x, z}' + \inn{y, z}'   &  & \text{(by \cref{6.1.3})}    \\
    \inn{cx, y}'       & = r \inn{cx, y}               &  & \text{(by \cref{6.1.3})}    \\
                       & = rc \inn{x, y}               &  & \text{(by \cref{6.1.1}(b))} \\
                       & = cr \inn{x, y}               &  & \text{(by \cref{c.0.1})}    \\
                       & = c \inn{x, y}'               &  & \text{(by \cref{6.1.3})}    \\
    \conj{\inn{x, y}'} & = \conj{r \inn{x, y}}         &  & \text{(by \cref{6.1.3})}    \\
                       & = \conj{r} \conj{\inn{x, y}}  &  & \text{(by \cref{d.2}(c))}   \\
                       & = \conj{r} \inn{y, x}         &  & \text{(by \cref{6.1.1}(c))} \\
                       & = r \inn{y, x}                &  & (r \in \R^+)                \\
                       & = \inn{y, x}'                 &  & \text{(by \cref{6.1.3})}
  \end{align*}
  and
  \begin{align*}
             & \begin{dcases}
                 x \neq \zv \\
                 r > 0
               \end{dcases}                                                    \\
    \implies & \inn{x, x}' = r \inn{x, x} > 0, &  & \text{(by \cref{6.1.1}(d))}
  \end{align*}
  by \cref{6.1.1} we see that \(\inn{\cdot, \cdot}'\) is an inner product on \(\V\) over \(\F\).
\end{proof}

\begin{eg}\label{6.1.4}
  Let \(\V = \cfs([0, 1], \R)\), the vector space of real-valued continuous functions on \([0, 1]\).
  For \(f, g \in \V\), define \(\inn{f, g} = \int_0^1 f(t) g(t) \; dt\).
  Since the preceding integral is linear in \(f\), \cref{6.1.1}(a)(b) are immediate, and \cref{6.1.1}(c) is trivial.
  If \(f \neq \zv\), then \(f^2\) is bounded away from zero on some subinterval of \([0, 1]\) (continuity is used here), and hence \(\inn{f, f} = \int_0^1 f^2(t) \; dt > 0\).
\end{eg}

\begin{defn}\label{6.1.5}
  Let \(A \in \MS\).
  We define the \textbf{conjugate transpose} or \textbf{adjoint} of \(A\) to be the \(n \times m\) matrix \(A^*\) such that \((A^*)_{i j} = \conj{A_{j i}}\) for all \(i \in \set{1, \dots, m}\) and \(j \in \set{1, \dots, n}\).
\end{defn}

\begin{note}
  If \(x\) and \(y\) are viewed as column vectors in \(\vs{F}^n\), then \(\inn{x, y} = y^* x\) where \(\inn{\cdot, \cdot}\) is the standard inner product.
  The conjugate transpose of a matrix plays a very important role in the remainder of \cref{ch:6}.
  In the case that \(A\) has real entries, \(A^*\) is simply the transpose of \(A\).
\end{note}

\begin{eg}\label{6.1.6}
  Let \(\V = \ms{n}{n}{\F}\), and define \(\inn{A, B} = \tr(B^* A)\) for \(A, B \in \V\).
  Then \(\inn{\cdot, \cdot}\) is called the \textbf{Frobenius inner product} and is an inner product on \(\V\).
\end{eg}

\begin{proof}[\pf{6.1.6}]
  Let \(A, B, C \in \V\) and let \(k \in \F\).
  Then
  \begin{align*}
    \inn{A + B, C}    & = \tr(C^* (A + B))                                                      &  & \text{(by \cref{6.1.6})}     \\
                      & = \tr(C^* A + C^* B)                                                    &  & \text{(by \cref{2.3.5})}     \\
                      & = \tr(C^* A) + \tr(C^* B)                                               &  & \text{(by \cref{ex:1.3.6})}  \\
                      & = \inn{A, C} + \inn{B, C}                                               &  & \text{(by \cref{6.1.6})}     \\
    \inn{kA, B}       & = \tr(B^* (kA))                                                         &  & \text{(by \cref{6.1.6})}     \\
                      & = k \tr(B^* A)                                                          &  & \text{(by \cref{ex:1.3.6})}  \\
                      & = k \inn{A, B}                                                          &  & \text{(by \cref{6.1.6})}     \\
    \conj{\inn{A, B}} & = \conj{\tr(B^* A)}                                                     &  & \text{(by \cref{6.1.6})}     \\
                      & = \conj{\sum_{i = 1}^n (B^* A)_{i i}}                                   &  & \text{(by \cref{1.3.9})}     \\
                      & = \conj{\sum_{i = 1}^n \sum_{j = 1}^n (B^*)_{i j} \cdot A_{j i}}        &  & \text{(by \cref{2.3.1})}     \\
                      & = \sum_{i = 1}^n \sum_{j = 1}^n \conj{(B^*)_{i j}} \cdot \conj{A_{j i}} &  & \text{(by \cref{d.2}(b)(c))} \\
                      & = \sum_{i = 1}^n \sum_{j = 1}^n B_{j i} \cdot (A^*)_{i j}               &  & \text{(by \cref{6.1.5})}     \\
                      & = \sum_{i = 1}^n (A^* B)_{i i}                                          &  & \text{(by \cref{2.3.1})}     \\
                      & = \tr(A^* B)                                                            &  & \text{(by \cref{1.3.9})}     \\
                      & = \inn{B, A}.                                                           &  & \text{(by \cref{6.1.6})}
  \end{align*}
  Also
  \begin{align*}
    \inn{A, A} & = \tr(A^* A)                                           &  & \text{(by \cref{6.1.6})} \\
               & = \sum_{i = 1}^n (A^* A)_{i i}                         &  & \text{(by \cref{1.3.9})} \\
               & = \sum_{i = 1}^n \sum_{k = 1}^n (A^*)_{i k} A_{k i}    &  & \text{(by \cref{2.3.1})} \\
               & = \sum_{i = 1}^n \sum_{k = 1}^n \conj{A_{k i}} A_{k i} &  & \text{(by \cref{6.1.5})} \\
               & = \sum_{i = 1}^n \sum_{k = 1}^n \abs{A_{k i}}^2.       &  & \text{(by \cref{d.0.5})}
  \end{align*}
  Now if \(A \neq \zm\), then \(A_{k i} \neq 0\) for some \(i, k \in \set{1, \dots, n}\).
  So \(\inn{A, A} > 0\).
  Thus by \cref{6.1.1} we see that \(\inn{\cdot, \cdot}\) is an inner product on \(\V\).
\end{proof}

\begin{defn}\label{6.1.7}
  A vector space \(\V\) over \(\F\) endowed with a specific inner product is called an \textbf{inner product space}.
  If \(\F = \C\), we call \(\V\) a \textbf{complex inner product space}, whereas if \(\F = \R\), we call \(\V\) a \textbf{real inner product space}.

  It is clear that if \(\V\) has an inner product \(\inn{x, y}\) and \(\W\) is a subspace of \(\V\) over \(\F\), then \(\W\) is also an inner product space when the same function \(\inn{x, y}\) is restricted to the vectors \(x, y \in \W\).
\end{defn}

\begin{note}
  For the remainder of \cref{ch:6}, \(\vs{F}^n\) denotes the inner product space with the standard inner product as defined in \cref{6.1.2}.
  Likewise, \(\ms{n}{n}{\F}\) denotes the inner product space with the Frobenius inner product as defined in \cref{6.1.6}.
\end{note}

\begin{eg}\label{6.1.8}
  Let \(\vs{H} = \cfs([0, 2\pi], \C)\).
  For \(f, g \in \vs{H}\), define
  \[
    \inn{f, g} = \frac{1}{2\pi} \int_0^{2\pi} f(t) \conj{g(t)} \; dt.
  \]
  This inner product space, which arises often in the context of physical situations, is examined more closely in later sections.
\end{eg}

\begin{proof}
  Let \(f, g, h \in \vs{H}\) and let \(c \in \C\).
  Then
  \begin{align*}
    \inn{f + g, h}    & = \frac{1}{2\pi} \int_0^{2\pi} (f + g)(t) \conj{h(t)} \; dt                                                 &  & \text{(by \cref{6.1.8})}  \\
                      & = \frac{1}{2\pi} \int_0^{2\pi} f(t) \conj{h(t)} \; dt + \frac{1}{2\pi} \int_0^{2\pi} g(t) \conj{h(t)} \; dt                                \\
                      & = \inn{f, h} + \inn{g, h}                                                                                   &  & \text{(by \cref{6.1.8})}  \\
    \inn{cf, g}       & = \frac{1}{2\pi} \int_0^{2\pi} (cf)(t) \conj{g(t)} \; dt                                                    &  & \text{(by \cref{6.1.8})}  \\
                      & = \frac{c}{2\pi} \int_0^{2\pi} f(t) \conj{g(t)} \; dt                                                                                      \\
                      & = c \inn{f, g}                                                                                                                             \\
    \conj{\inn{f, g}} & = \conj{\frac{1}{2\pi} \int_0^{2\pi} f(t) \conj{g(t)} \; dt}                                                &  & \text{(by \cref{6.1.8})}  \\
                      & = \frac{1}{2\pi} \int_0^{2\pi} \conj{f(t)} g(t) \; dt                                                       &  & \text{(by \cref{d.2}(c))} \\
                      & = \inn{g, f}.                                                                                               &  & \text{(by \cref{6.1.8})}
  \end{align*}
  Also, if \(f \neq \zv\), then we have
  \begin{align*}
    \inn{f, f} & = \frac{1}{2\pi} \int_0^{2\pi} f(t) \conj{f(t)} \; dt &  & \text{(by \cref{6.1.8})} \\
               & = \frac{1}{2\pi} \int_0^{2\pi} \abs{f(t)}^2 \; dt     &  & \text{(by \cref{d.0.5})} \\
               & > 0.                                                  &  & (f \neq \zv)
  \end{align*}
  Thus by \cref{6.1.1} \(\inn{\cdot, \cdot}\) is an inner product on \(\vs{H}\).
\end{proof}

\begin{thm}\label{6.1}
  Let \(\V\) be an inner product space over \(\F\).
  Then for \(x, y, z \in \V\) and \(c \in \F\), the following statements are true.
  \begin{enumerate}
    \item \(\inn{x, y + z} = \inn{x, y} + \inn{x, z}\).
    \item \(\inn{x, cy} = \conj{c} \inn{x, y}\).
    \item \(\inn{x, \zv} = \inn{\zv, x} = 0\).
    \item \(\inn{x, x} = 0\) iff \(x = \zv\).
    \item If \(\inn{x, y} = \inn{x, z}\) for all \(x \in \V\), then \(y = z\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{6.1}(a)]
  We have
  \begin{align*}
    \inn{x, y + z} & = \conj{\inn{y + z, x}}                 &  & \text{(by \cref{6.1.1}(c))} \\
                   & = \conj{\inn{y, x} + \inn{z, x}}        &  & \text{(by \cref{6.1.1}(a))} \\
                   & = \conj{\inn{y, x}} + \conj{\inn{z, x}} &  & \text{(by \cref{d.2}(b))}   \\
                   & = \inn{x, y} + \inn{x, z}.              &  & \text{(by \cref{6.1.1}(c))}
  \end{align*}
\end{proof}

\begin{proof}[\pf{6.1}(b)]
  We have
  \begin{align*}
    \inn{x, cy} & = \conj{\inn{cy, x}}         &  & \text{(by \cref{6.1.1}(c))} \\
                & = \conj{c \inn{y, x}}        &  & \text{(by \cref{6.1.1}(b))} \\
                & = \conj{c} \conj{\inn{y, x}} &  & \text{(by \cref{d.2}(c))}   \\
                & = \conj{c} \inn{x, y}        &  & \text{(by \cref{6.1.1}(c))}
  \end{align*}
\end{proof}

\begin{proof}[\pf{6.1}(c)]
  We have
  \begin{align*}
    \inn{x, \zv} & = \inn{x, \zv + \zv}          &  & \text{(by \cref{1.2.1})}    \\
                 & = \inn{x, \zv} + \inn{x, \zv} &  & \text{(by \cref{6.1}(a))}   \\
    \inn{\zv, x} & = \inn{\zv + \zv, x}          &  & \text{(by \cref{1.2.1})}    \\
                 & = \inn{\zv, x} + \inn{\zv, x} &  & \text{(by \cref{6.1.1}(a))}
  \end{align*}
  and thus \(\inn{x, \zv} = \inn{\zv, x} = 0\).
\end{proof}

\begin{proof}[\pf{6.1}(d)]
  By \cref{6.1.1}(d) and \cref{6.1}(c) we have \(\inn{x, x} = 0 \iff x = \zv\).
\end{proof}

\begin{proof}[\pf{6.1}(e)]
  Since
  \begin{align*}
    \inn{y - z, y - z} & = \inn{y, y - z} - \inn{z, y - z}                   &  & \text{(by \cref{6.1.1}(a)(b))}              \\
                       & = \inn{y, y} - \inn{y, z} - \inn{z, y} + \inn{z, z} &  & \text{(by \cref{6.1}(a)(b))}                \\
                       & = \inn{z, y} - \inn{y, z} - \inn{z, y} + \inn{y, z} &  & (\forall x \in \V, \inn{x, y} = \inn{x, z}) \\
                       & = 0,
  \end{align*}
  by \cref{6.1}(d) we know that \(y - z = \zv\), thus \(y = z\).
\end{proof}

\begin{note}
  The reader should observe that \cref{6.1}(a)(b) show that the inner product is \textbf{conjugate linear} in the second component.
\end{note}

\begin{defn}\label{6.1.9}
  Let \(\V\) be an inner product space over \(\F\).
  For \(x \in \V\), we define the \textbf{norm} or \textbf{length} of \(x\) by \(\norm{x} = \sqrt{\inn{x, x}}\).
\end{defn}

\begin{eg}\label{6.1.10}
  Let \(\V = \vs{F}^n\).
  If \(x = \tuple{a}{1,,n}\), then
  \[
    \norm{x} = \norm{\tuple{a}{1,,n}} = \pa{\sum_{i = 1}^n \abs{a_i}^2}^{\frac{1}{2}}
  \]
  is the Euclidean definition of length.
  Note that if \(n = 1\), we have \(\norm{a} = \abs{a}\).
\end{eg}

\begin{thm}\label{6.2}
  Let \(\V\) be an inner product space over \(\F\).
  Then for all \(x, y \in \V\) and \(c \in \F\), the following statements are true.
  \begin{enumerate}
    \item \(\norm{cx} = \abs{c} \cdot \norm{x}\).
    \item \(\norm{x} = 0\) iff \(x = \zv\).
          In any case \(\norm{x} \geq 0\).
    \item (Cauchy--Schwarz Inequality)
          \(\abs{\inn{x, y}} \leq \norm{x} \cdot \norm{y}\).
    \item (Triangle Inequality)
          \(\norm{x + y} \leq \norm{x} + \norm{y}\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{6.2}(a)]
  We have
  \begin{align*}
    \norm{cx} & = \sqrt{\inn{cx, cx}}          &  & \text{(by \cref{6.1.9})}    \\
              & = \sqrt{c \inn{x, cx}}         &  & \text{(by \cref{6.1.1}(b))} \\
              & = \sqrt{c \conj{c} \inn{x, x}} &  & \text{(by \cref{6.1}(b))}   \\
              & = \sqrt{\abs{c}^2 \inn{x, x}}  &  & \text{(by \cref{d.0.5})}    \\
              & = \abs{c} \sqrt{\inn{x, x}}                                     \\
              & = \abs{c} \norm{x}.            &  & \text{(by \cref{6.1.9})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{6.2}(b)]
  We have
  \begin{align*}
         & x = \zv                                              \\
    \iff & \inn{x, x} = 0        &  & \text{(by \cref{6.1}(d))} \\
    \iff & \sqrt{\inn{x, x}} = 0                                \\
    \iff & \norm{x} = 0          &  & \text{(by \cref{6.1.9})}
  \end{align*}
  and
  \begin{align*}
             & \forall x \in \V, \begin{dcases}
                                   \inn{x, x} = 0 & \text{if } x = \zv    \\
                                   \inn{x, x} > 0 & \text{if } x \neq \zv
                                 \end{dcases}  &  & \text{(by \cref{6.1.1}(d) and \cref{6.1}(d))} \\
    \implies & \forall x \in \V, \inn{x, x} \geq 0                                                \\
    \implies & \forall x \in \V, \sqrt{\inn{x, x}} \geq 0                                         \\
    \implies & \forall x \in \V, \norm{x} \geq 0.         &  & \text{(by \cref{6.1.9})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{6.2}(c)]
  If \(y = \zv\), then the result is immediate.
  So assume that \(y \neq \zv\).
  For any \(c \in \F\), we have
  \begin{align*}
    0 & \leq \norm{x - cy}^2                                                       &  & \text{(by \cref{6.2}(b))}      \\
      & = \inn{x - cy, x - cy}                                                     &  & \text{(by \cref{6.1.9})}       \\
      & = \inn{x, x - cy} - c \inn{y, x - cy}                                      &  & \text{(by \cref{6.1.1}(a)(b))} \\
      & = \inn{x, x} - \conj{c} \inn{x, y} - c \inn{y, x} + c \conj{c} \inn{y, y}. &  & \text{(by \cref{6.1}(a)(b))}
  \end{align*}
  In particular, if we set
  \[
    c = \frac{\inn{x, y}}{\inn{y, y}},
  \]
  the inequality becomes
  \[
    0 \leq \inn{x, x} - \frac{\abs{\inn{x, y}}^2}{\inn{y, y}} = \norm{x}^2 - \frac{\abs{\inn{x, y}}^2}{\norm{y}^2},
  \]
  from which (c) follows.
\end{proof}

\begin{proof}[\pf{6.2}(d)]
  We have
  \begin{align*}
     & \norm{x + y}^2                                                                                         \\
     & = \inn{x + y, x + y}                                &  & \text{(by \cref{6.1.9})}                      \\
     & = \inn{x, x} + \inn{y, x} + \inn{x, y} + \inn{y, y} &  & \text{(by \cref{6.1.1}(a) and \cref{6.1}(a))} \\
     & = \norm{x}^2 + 2 \Re(\inn{x, y}) + \norm{y}^2       &  & \text{(by \cref{6.1.1}(d) and \cref{6.1.9})}  \\
     & \leq \norm{x}^2 + 2 \abs{\inn{x, y}} + \norm{y}^2   &  & \text{(by \cref{d.0.5})}                      \\
     & \leq \norm{x}^2 + 2 \norm{x} \norm{y} + \norm{y}^2  &  & \text{(by \cref{6.2}(c))}                     \\
     & = \pa{\norm{x} + \norm{y}}^2,
  \end{align*}
  where \(\Re(\inn{x, y})\) denotes the real part of the complex number \(\inn{x, y}\).
\end{proof}

\begin{eg}\label{6.1.11}
  For \(\vs{F}^n\), we may apply \cref{6.2}(c)(d) to the standard inner product to obtain the following well-known inequalities:
  \[
    \abs{\sum_{i = 1}^n a_i \conj{b_i}} \leq \pa{\sum_{i = 1}^n \abs{a_i}^2}^{\frac{1}{2}} \pa{\sum_{i = 1}^n \abs{b_i}^2}^{\frac{1}{2}}
  \]
  and
  \[
    \pa{\sum_{i = 1}^n \abs{a_i + b_i}^2}^{\frac{1}{2}} \leq \pa{\sum_{i = 1}^n \abs{a_i}^2}^{\frac{1}{2}} + \pa{\sum_{i = 1}^n \abs{b_i}^2}^{\frac{1}{2}}.
  \]
\end{eg}

\begin{note}
  The reader may recall from earlier courses that, for \(x\) and \(y\) in \(\R^3\) or \(\R^2\), we have that \(\inn{x, y} = \norm{x} \cdot \norm{y} \cos \theta\), where \(\theta \in [0, \pi]\) denotes the angle between \(x\) and \(y\).
  This equation implies \cref{6.2}(c) immediately since \(\abs{\cos \theta} \leq 1\).
  Notice also that nonzero vectors \(x\) and \(y\) are perpendicular iff \(\cos \theta = 0\), that is, iff \(\inn{x, y} = 0\).
\end{note}

\begin{defn}\label{6.1.12}
  Let \(\V\) be an inner product space over \(\F\).
  Vectors \(x\) and \(y\) in \(\V\) are \textbf{orthogonal} (\textbf{perpendicular}) if \(\inn{x, y} = 0\).
  A subset \(S\) of \(\V\) is \textbf{orthogonal} if any two distinct vectors in \(S\) are orthogonal.
  A vector \(x\) in \(\V\) is a \textbf{unit vector} if \(\norm{x} = 1\).
  Finally, a subset \(S\) of \(\V\) is \textbf{orthonormal} if \(S\) is orthogonal and consists entirely of unit vectors.

  Note that if \(S = \set{\seq{v}{1,2,}}\), then \(S\) is orthonormal iff \(\inn{v_i, v_j} = \delta_{i j}\), where \(\delta_{i j}\) denotes the Kronecker delta.
  Also, observe that multiplying vectors by nonzero scalars does not affect their orthogonality and that if \(x\) is any nonzero vector, then \((1 / \norm{x}) x\) is a unit vector.
  The process of multiplying a nonzero vector by the reciprocal of its length is called \textbf{normalizing}.
\end{defn}

\begin{eg}\label{6.1.13}
  Recall the inner product space \(\vs{H}\) (defined in \cref{6.1.8}).
  We introduce an important orthonormal subset \(S\) of \(\vs{H}\).
  For what follows, \(i\) is the imaginary number such that \(i^2 = -1\).
  For any integer \(n\), let \(f_n(t) = e^{int}\), where \(0 \leq t \leq 2\pi\).
  (Recall that \(e^{int} = \cos(nt) + i \sin(nt)\).)
  Now define \(S = \set{f_n : n \in \Z}\).
  Clearly \(S\) is a subset of \(\vs{H}\).
  Using the property that \(\conj{e^{it}} = e^{-it}\) for every real number \(t\), we have, for \(m \neq n\),
  \begin{align*}
    \inn{f_m, f_n} & = \frac{1}{2\pi} \int_0^{2\pi} e^{imt} \conj{e^{int}} \; dt &  & \text{(by \cref{6.1.8})} \\
                   & = \frac{1}{2\pi} \int_0^{2\pi} e^{i(m - n)t} \; dt                                        \\
                   & = \eval{\frac{1}{2\pi i(m - n)} e^{i(m - n)t}}_0^{2\pi}                                   \\
                   & = 0.
  \end{align*}
  Also,
  \begin{align*}
    \inn{f_n, f_n} & = \frac{1}{2\pi} \int_0^{2\pi} e^{i(n - n)t} \; dt &  & \text{(by \cref{6.1.8})} \\
                   & = \frac{1}{2\pi} \int_0^{2\pi} 1 \; dt                                           \\
                   & = 1.
  \end{align*}
  In other words, \(\inn{f_m, f_n} = \delta_{m n}\).
\end{eg}

\exercisesection

\setcounter{ex}{8}
\begin{ex}\label{ex:6.1.9}
  Let \(\beta\) be a basis for a \(n\)-dimensional inner product space \(\V\) over \(\F\).
  \begin{enumerate}
    \item Prove that if \(x \in \V\) and \(\inn{x, z} = 0\) for all \(z \in \beta\), then \(x = \zv\).
    \item Prove that if \(x \in \V\) and \(\inn{x, z} = \inn{y, z}\) for all \(z \in \beta\), then \(x = y\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.1.9}(a)]
  Let \(\beta = \set{\seq{v}{1,,n}}\).
  Then we have
  \begin{align*}
             & x \in \V = \spn{\beta}                                                                                                  \\
    \implies & \exists \seq{a}{1,,n} \in \F : x = \sum_{i = 1}^n a_i v_i                             &  & \text{(by \cref{1.6.1})}     \\
    \implies & \inn{x, x} = \inn{x, \sum_{i = 1}^n a_i v_i} = \sum_{i = 1}^n \conj{a_i} \inn{x, v_i} &  & \text{(by \cref{6.1}(a)(b))} \\
             & = \sum_{i = 1}^n \conj{a_i} 0 = 0                                                     &  & \text{(by hypothesis)}       \\
    \implies & x = \zv.                                                                              &  & \text{(by \cref{6.1}(d))}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.1.9}(b)]
  We have
  \begin{align*}
             & \forall z \in \beta, \inn{x, z} = \inn{y, z}                                              \\
    \implies & \forall v \in \V = \spn{\beta}, \inn{x, v} = \inn{y, v} &  & \text{(by \cref{6.1}(a)(b))} \\
    \implies & \forall v \in \V = \spn{\beta}, \inn{v, x} = \inn{v, y} &  & \text{(by \cref{6.1.1}(c))}  \\
    \implies & x = y.                                                  &  & \text{(by \cref{6.1}(e))}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:6.1.10}
  Let \(\V\) be an inner product space over \(\F\), and suppose that \(x\) and \(y\) are orthogonal vectors in \(\V\).
  Prove that \(\norm{x + y}^2 = \norm{x}^2 + \norm{y}^2\).
  Deduce the Pythagorean theorem in \(\R^2\).
\end{ex}

\begin{proof}[\pf{ex:6.1.10}]
  We have
  \begin{align*}
     & \norm{x + y}^2                                                                                         \\
     & = \inn{x + y, x + y}                                &  & \text{(by \cref{6.1.9})}                      \\
     & = \inn{x, x} + \inn{y, x} + \inn{x, y} + \inn{y, y} &  & \text{(by \cref{6.1.1}(a) and \cref{6.1}(a))} \\
     & = \inn{x, x} + \inn{y, y}                           &  & \text{(by \cref{6.1.12})}                     \\
     & = \norm{x}^2 + \norm{y}^2.                          &  & \text{(by \cref{6.1.9})}
  \end{align*}
  By setting \(\V = \R^2\) and define \(\inn{\cdot, \cdot}\) as in \cref{6.1.2} we see that the Pythagorean theorem is true.
\end{proof}

\begin{ex}\label{ex:6.1.11}
  Prove the \emph{parallelogram law} on an inner product space \(\V\) over \(\F\);
  that is, show that
  \[
    \norm{x + y}^2 + \norm{x - y}^2 = 2 \norm{x}^2 + 2 \norm{y}^2 \quad \text{for all } x, y \in \V.
  \]
  What does this equation state about parallelograms in \(\R^2\)?
\end{ex}

\begin{proof}[\pf{ex:6.1.11}]
  We have
  \begin{align*}
     & \norm{x + y}^2 + \norm{x - y}^2                                                                         \\
     & = \inn{x + y, x + y} + \inn{x - y, x - y}                           &  & \text{(by \cref{6.1.9})}       \\
     & = \inn{x, x + y} + \inn{y, x + y} + \inn{x, x - y} - \inn{y, x - y} &  & \text{(by \cref{6.1.1}(a)(b))} \\
     & = \inn{x, 2x} + \inn{y, 2y}                                         &  & \text{(by \cref{6.1}(a)(b))}   \\
     & = 2 \inn{x, x} + 2 \inn{y, y}                                       &  & \text{(by \cref{6.1}(b))}      \\
     & = 2 \norm{x}^2 + 2 \norm{y}^2.                                      &  & \text{(by \cref{6.1.9})}
  \end{align*}
  On \(\R^2\) we see that if \(x, y\) are the two vectors forming a parallelogram, then \(x + y\) and \(x - y\) are the two diagonal vectors of the parallelogram.
  Thus we see that the sum of the square of the length of the \(4\) sides of a parallelogram equal to the sum of the square of the length of diagonal lines of the same parallelogram.
\end{proof}

\begin{ex}\label{ex:6.1.15}

\end{ex}
