\section{Inner Products and Norms}\label{sec:6.1}

\begin{defn}\label{6.1.1}
  Let \(\V\) be a vector space over \(\F\).
  An \textbf{inner product} on \(\V\) over \(\F\) is a function that assigns, to every ordered pair of vectors \(x\) and \(y\) in \(\V\), a scalar in \(\F\), denoted \(\inn{x, y}\), such that for all \(x\), \(y\), and \(z\) in \(\V\) and all \(c\) in \(\F\), the following hold:
  \begin{enumerate}
    \item \(\inn{x + z, y} = \inn{x, y} + \inn{z, y}\).
    \item \(\inn{cx, y} = c \inn{x, y}\).
    \item \(\conj{\inn{x, y}} = \inn{y, x}\), where the bar denotes complex conjugation.
    \item \(\inn{x, x} > 0\) if \(x \neq \zv\).
  \end{enumerate}
\end{defn}

\begin{note}
  Note that \cref{6.1.1}(c) reduces to \(\inn{x, y} = \inn{y, x}\) if \(\F = \R\).
  \cref{6.1.1}(a)(b) simply require that the inner product be linear in the first component.
  It is easily shown that if \(\seq{a}{1,,n} \in \F\) and \(y, \seq{v}{1,,n} \in \V\), then
  \[
    \inn{\sum_{i = 1}^n a_i v_i, y} = \sum_{i = 1}^n a_i \inn{v_i, y}.
  \]
\end{note}

\begin{eg}\label{6.1.2}
  For \(x = \tuple{a}{1,,n}\) and \(y = \tuple{b}{1,,n}\) in \(\vs{F}^n\), define
  \[
    \inn{x, y} = \sum_{i = 1}^n a_i \conj{b_i}.
  \]
  The inner product defined above is called the \textbf{standard inner product} on \(\vs{F}^n\).
  When \(\F = \R\) the conjugations are not needed, and in early courses this standard inner product is usually called the \emph{dot product} and is denoted by \(x \cdot y\) instead of \(\inn{x, y}\).
\end{eg}

\begin{proof}[\pf{6.1.2}]
  Let \(x, y, z \in \vs{F}^n\) and let \(c \in \F\).
  Since
  \begin{align*}
    \inn{x + y, z}    & = \sum_{i = 1}^n (x + y)_i \conj{z_i}                           &  & \text{(by \cref{6.1.2})}        \\
                      & = \sum_{i = 1}^n x_i \conj{z_i} + \sum_{i = 1}^n y_i \conj{z_i} &  & \text{(by \cref{c.0.1})}        \\
                      & = \inn{x, z} + \inn{y, z}                                       &  & \text{(by \cref{6.1.2})}        \\
    \inn{cx, y}       & = \sum_{i = 1}^n (cx)_i \conj{y_i}                              &  & \text{(by \cref{6.1.2})}        \\
                      & = c \sum_{i = 1}^n x_i \conj{y_i}                               &  & \text{(by \cref{c.0.1})}        \\
                      & = c \inn{x, y}                                                  &  & \text{(by \cref{6.1.2})}        \\
    \conj{\inn{x, y}} & = \conj{\sum_{i = 1}^n x_i \conj{y_i}}                          &  & \text{(by \cref{6.1.2})}        \\
                      & = \sum_{i = 1}^n \conj{x_i} y_i                                 &  & \text{(by \cref{d.2}(a)(b)(c))} \\
                      & = \sum_{i = 1}^n y_i \conj{x_i}                                 &  & \text{(by \cref{c.0.1})}        \\
                      & = \inn{y, x}                                                    &  & \text{(by \cref{6.1.2})}
  \end{align*}
  and
  \begin{align*}
             & x \neq \zv                                                                                       \\
    \implies & \exists i \in \set{1, \dots, n} : x_i \neq 0                                                     \\
    \implies & \exists i \in \set{1, \dots, n} : \abs{x_i}^2 = x_i \conj{x_i} > 0 &  & \text{(by \cref{d.0.5})} \\
    \implies & \inn{x, x} = \sum_{i = 1}^n x_i \conj{x_i} > 0,                    &  & \text{(by \cref{6.1.2})}
  \end{align*}
  by \cref{6.1.1} we know that \(\inn{\cdot, \cdot}\) is an inner product on \(\vs{F}^n\) over \(\F\).
\end{proof}

\begin{eg}\label{6.1.3}
  If \(\inn{x, y}\) is any inner product on a vector space \(\V\) over \(\F\) and \(r > 0\), we may define another inner product by the rule \(\inn{x, y}' = r \inn{x, y}\).
  If \(r \leq 0\), then \cref{6.1.1}(d) would not hold.
\end{eg}

\begin{proof}[\pf{6.1.3}]
  Let \(x, y, z \in \V\), let \(c \in \F\) and let \(r \in \R^+\).
  Since
  \begin{align*}
    \inn{x + y, z}'    & = r\inn{x + y, z}             &  & \text{(by \cref{6.1.3})}    \\
                       & = r (\inn{x, z} + \inn{y, z}) &  & \text{(by \cref{6.1.1}(a))} \\
                       & = r \inn{x, z} + r \inn{y, z} &  & \text{(by \cref{c.0.1})}    \\
                       & = \inn{x, z}' + \inn{y, z}'   &  & \text{(by \cref{6.1.3})}    \\
    \inn{cx, y}'       & = r \inn{cx, y}               &  & \text{(by \cref{6.1.3})}    \\
                       & = rc \inn{x, y}               &  & \text{(by \cref{6.1.1}(b))} \\
                       & = cr \inn{x, y}               &  & \text{(by \cref{c.0.1})}    \\
                       & = c \inn{x, y}'               &  & \text{(by \cref{6.1.3})}    \\
    \conj{\inn{x, y}'} & = \conj{r \inn{x, y}}         &  & \text{(by \cref{6.1.3})}    \\
                       & = \conj{r} \conj{\inn{x, y}}  &  & \text{(by \cref{d.2}(c))}   \\
                       & = \conj{r} \inn{y, x}         &  & \text{(by \cref{6.1.1}(c))} \\
                       & = r \inn{y, x}                &  & (r \in \R^+)                \\
                       & = \inn{y, x}'                 &  & \text{(by \cref{6.1.3})}
  \end{align*}
  and
  \begin{align*}
             & \begin{dcases}
                 x \neq \zv \\
                 r > 0
               \end{dcases}                                                    \\
    \implies & \inn{x, x}' = r \inn{x, x} > 0, &  & \text{(by \cref{6.1.1}(d))}
  \end{align*}
  by \cref{6.1.1} we see that \(\inn{\cdot, \cdot}'\) is an inner product on \(\V\) over \(\F\).
\end{proof}

\begin{eg}\label{6.1.4}
  Let \(\V = \cfs([0, 1], \R)\), the vector space of real-valued continuous functions on \([0, 1]\).
  For \(f, g \in \V\), define \(\inn{f, g} = \int_0^1 f(t) g(t) \; dt\).
  Since the preceding integral is linear in \(f\), \cref{6.1.1}(a)(b) are immediate, and \cref{6.1.1}(c) is trivial.
  If \(f \neq \zv\), then \(f^2\) is bounded away from zero on some subinterval of \([0, 1]\) (continuity is used here), and hence \(\inn{f, f} = \int_0^1 f^2(t) \; dt > 0\).
\end{eg}

\begin{defn}\label{6.1.5}
  Let \(A \in \MS\).
  We define the \textbf{conjugate transpose} or \textbf{adjoint} of \(A\) to be the \(n \times m\) matrix \(A^*\) such that \((A^*)_{i j} = \conj{A_{j i}}\) for all \(i \in \set{1, \dots, m}\) and \(j \in \set{1, \dots, n}\).
\end{defn}

\begin{note}
  If \(x\) and \(y\) are viewed as column vectors in \(\vs{F}^n\), then \(\inn{x, y} = y^* x\) where \(\inn{\cdot, \cdot}\) is the standard inner product.
  The conjugate transpose of a matrix plays a very important role in the remainder of \cref{ch:6}.
  In the case that \(A\) has real entries, \(A^*\) is simply the transpose of \(A\).
\end{note}

\begin{eg}\label{6.1.6}
  Let \(\V = \ms{n}{n}{\F}\), and define \(\inn{A, B} = \tr(B^* A)\) for \(A, B \in \V\).
  Then \(\inn{\cdot, \cdot}\) is called the \textbf{Frobenius inner product} and is an inner product on \(\V\).
\end{eg}

\begin{proof}[\pf{6.1.6}]
  Let \(A, B, C \in \V\) and let \(k \in \F\).
  Then
  \begin{align*}
    \inn{A + B, C}    & = \tr(C^* (A + B))                                                      &  & \text{(by \cref{6.1.6})}     \\
                      & = \tr(C^* A + C^* B)                                                    &  & \text{(by \cref{2.3.5})}     \\
                      & = \tr(C^* A) + \tr(C^* B)                                               &  & \text{(by \cref{ex:1.3.6})}  \\
                      & = \inn{A, C} + \inn{B, C}                                               &  & \text{(by \cref{6.1.6})}     \\
    \inn{kA, B}       & = \tr(B^* (kA))                                                         &  & \text{(by \cref{6.1.6})}     \\
                      & = k \tr(B^* A)                                                          &  & \text{(by \cref{ex:1.3.6})}  \\
                      & = k \inn{A, B}                                                          &  & \text{(by \cref{6.1.6})}     \\
    \conj{\inn{A, B}} & = \conj{\tr(B^* A)}                                                     &  & \text{(by \cref{6.1.6})}     \\
                      & = \conj{\sum_{i = 1}^n (B^* A)_{i i}}                                   &  & \text{(by \cref{1.3.9})}     \\
                      & = \conj{\sum_{i = 1}^n \sum_{j = 1}^n (B^*)_{i j} \cdot A_{j i}}        &  & \text{(by \cref{2.3.1})}     \\
                      & = \sum_{i = 1}^n \sum_{j = 1}^n \conj{(B^*)_{i j}} \cdot \conj{A_{j i}} &  & \text{(by \cref{d.2}(b)(c))} \\
                      & = \sum_{i = 1}^n \sum_{j = 1}^n B_{j i} \cdot (A^*)_{i j}               &  & \text{(by \cref{6.1.5})}     \\
                      & = \sum_{i = 1}^n (A^* B)_{i i}                                          &  & \text{(by \cref{2.3.1})}     \\
                      & = \tr(A^* B)                                                            &  & \text{(by \cref{1.3.9})}     \\
                      & = \inn{B, A}.                                                           &  & \text{(by \cref{6.1.6})}
  \end{align*}
  Also
  \begin{align*}
    \inn{A, A} & = \tr(A^* A)                                           &  & \text{(by \cref{6.1.6})} \\
               & = \sum_{i = 1}^n (A^* A)_{i i}                         &  & \text{(by \cref{1.3.9})} \\
               & = \sum_{i = 1}^n \sum_{k = 1}^n (A^*)_{i k} A_{k i}    &  & \text{(by \cref{2.3.1})} \\
               & = \sum_{i = 1}^n \sum_{k = 1}^n \conj{A_{k i}} A_{k i} &  & \text{(by \cref{6.1.5})} \\
               & = \sum_{i = 1}^n \sum_{k = 1}^n \abs{A_{k i}}^2.       &  & \text{(by \cref{d.0.5})}
  \end{align*}
  Now if \(A \neq \zm\), then \(A_{k i} \neq 0\) for some \(i, k \in \set{1, \dots, n}\).
  So \(\inn{A, A} > 0\).
  Thus by \cref{6.1.1} we see that \(\inn{\cdot, \cdot}\) is an inner product on \(\V\).
\end{proof}

\begin{defn}\label{6.1.7}
  A vector space \(\V\) over \(\F\) endowed with a specific inner product is called an \textbf{inner product space}.
  If \(\F = \C\), we call \(\V\) a \textbf{complex inner product space}, whereas if \(\F = \R\), we call \(\V\) a \textbf{real inner product space}.

  It is clear that if \(\V\) has an inner product \(\inn{x, y}\) and \(\W\) is a subspace of \(\V\) over \(\F\), then \(\W\) is also an inner product space when the same function \(\inn{x, y}\) is restricted to the vectors \(x, y \in \W\).
\end{defn}

\begin{note}
  For the remainder of \cref{ch:6}, \(\vs{F}^n\) denotes the inner product space with the standard inner product as defined in \cref{6.1.2}.
  Likewise, \(\ms{n}{n}{\F}\) denotes the inner product space with the Frobenius inner product as defined in \cref{6.1.6}.
\end{note}

\begin{eg}\label{6.1.8}
  Let \(\vs{H} = \cfs([0, 2\pi], \C)\).
  For \(f, g \in \vs{H}\), define
  \[
    \inn{f, g} = \frac{1}{2\pi} \int_0^{2\pi} f(t) \conj{g(t)} \; dt.
  \]
  This inner product space, which arises often in the context of physical situations, is examined more closely in later sections.
\end{eg}

\begin{proof}
  Let \(f, g, h \in \vs{H}\) and let \(c \in \C\).
  Then
  \begin{align*}
    \inn{f + g, h}    & = \frac{1}{2\pi} \int_0^{2\pi} (f + g)(t) \conj{h(t)} \; dt                                                 &  & \text{(by \cref{6.1.8})}  \\
                      & = \frac{1}{2\pi} \int_0^{2\pi} f(t) \conj{h(t)} \; dt + \frac{1}{2\pi} \int_0^{2\pi} g(t) \conj{h(t)} \; dt                                \\
                      & = \inn{f, h} + \inn{g, h}                                                                                   &  & \text{(by \cref{6.1.8})}  \\
    \inn{cf, g}       & = \frac{1}{2\pi} \int_0^{2\pi} (cf)(t) \conj{g(t)} \; dt                                                    &  & \text{(by \cref{6.1.8})}  \\
                      & = \frac{c}{2\pi} \int_0^{2\pi} f(t) \conj{g(t)} \; dt                                                                                      \\
                      & = c \inn{f, g}                                                                                                                             \\
    \conj{\inn{f, g}} & = \conj{\frac{1}{2\pi} \int_0^{2\pi} f(t) \conj{g(t)} \; dt}                                                &  & \text{(by \cref{6.1.8})}  \\
                      & = \frac{1}{2\pi} \int_0^{2\pi} \conj{f(t)} g(t) \; dt                                                       &  & \text{(by \cref{d.2}(c))} \\
                      & = \inn{g, f}.                                                                                               &  & \text{(by \cref{6.1.8})}
  \end{align*}
  Also, if \(f \neq \zv\), then we have
  \begin{align*}
    \inn{f, f} & = \frac{1}{2\pi} \int_0^{2\pi} f(t) \conj{f(t)} \; dt &  & \text{(by \cref{6.1.8})} \\
               & = \frac{1}{2\pi} \int_0^{2\pi} \abs{f(t)}^2 \; dt     &  & \text{(by \cref{d.0.5})} \\
               & > 0.                                                  &  & (f \neq \zv)
  \end{align*}
  Thus by \cref{6.1.1} \(\inn{\cdot, \cdot}\) is an inner product on \(\vs{H}\).
\end{proof}

\begin{thm}\label{6.1}
  Let \(\V\) be an inner product space over \(\F\).
  Then for \(x, y, z \in \V\) and \(c \in \F\), the following statements are true.
  \begin{enumerate}
    \item \(\inn{x, y + z} = \inn{x, y} + \inn{x, z}\).
    \item \(\inn{x, cy} = \conj{c} \inn{x, y}\).
    \item \(\inn{x, \zv} = \inn{\zv, x} = 0\).
    \item \(\inn{x, x} = 0\) iff \(x = \zv\).
    \item If \(\inn{x, y} = \inn{x, z}\) for all \(x \in \V\), then \(y = z\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{6.1}(a)]
  We have
  \begin{align*}
    \inn{x, y + z} & = \conj{\inn{y + z, x}}                 &  & \text{(by \cref{6.1.1}(c))} \\
                   & = \conj{\inn{y, x} + \inn{z, x}}        &  & \text{(by \cref{6.1.1}(a))} \\
                   & = \conj{\inn{y, x}} + \conj{\inn{z, x}} &  & \text{(by \cref{d.2}(b))}   \\
                   & = \inn{x, y} + \inn{x, z}.              &  & \text{(by \cref{6.1.1}(c))}
  \end{align*}
\end{proof}

\begin{proof}[\pf{6.1}(b)]
  We have
  \begin{align*}
    \inn{x, cy} & = \conj{\inn{cy, x}}         &  & \text{(by \cref{6.1.1}(c))} \\
                & = \conj{c \inn{y, x}}        &  & \text{(by \cref{6.1.1}(b))} \\
                & = \conj{c} \conj{\inn{y, x}} &  & \text{(by \cref{d.2}(c))}   \\
                & = \conj{c} \inn{x, y}        &  & \text{(by \cref{6.1.1}(c))}
  \end{align*}
\end{proof}

\begin{proof}[\pf{6.1}(c)]
  We have
  \begin{align*}
    \inn{x, \zv} & = \inn{x, \zv + \zv}          &  & \text{(by \cref{1.2.1})}    \\
                 & = \inn{x, \zv} + \inn{x, \zv} &  & \text{(by \cref{6.1}(a))}   \\
    \inn{\zv, x} & = \inn{\zv + \zv, x}          &  & \text{(by \cref{1.2.1})}    \\
                 & = \inn{\zv, x} + \inn{\zv, x} &  & \text{(by \cref{6.1.1}(a))}
  \end{align*}
  and thus \(\inn{x, \zv} = \inn{\zv, x} = 0\).
\end{proof}

\begin{proof}[\pf{6.1}(d)]
  By \cref{6.1.1}(d) and \cref{6.1}(c) we have \(\inn{x, x} = 0 \iff x = \zv\).
\end{proof}

\begin{proof}[\pf{6.1}(e)]
  Since
  \begin{align*}
    \inn{y - z, y - z} & = \inn{y, y - z} - \inn{z, y - z}                   &  & \text{(by \cref{6.1.1}(a)(b))}              \\
                       & = \inn{y, y} - \inn{y, z} - \inn{z, y} + \inn{z, z} &  & \text{(by \cref{6.1}(a)(b))}                \\
                       & = \inn{z, y} - \inn{y, z} - \inn{z, y} + \inn{y, z} &  & (\forall x \in \V, \inn{x, y} = \inn{x, z}) \\
                       & = 0,
  \end{align*}
  by \cref{6.1}(d) we know that \(y - z = \zv\), thus \(y = z\).
\end{proof}

\begin{note}
  The reader should observe that \cref{6.1}(a)(b) show that the inner product is \textbf{conjugate linear} in the second component.
\end{note}

\begin{defn}\label{6.1.9}
  Let \(\V\) be an inner product space over \(\F\).
  For \(x \in \V\), we define the \textbf{norm} or \textbf{length} of \(x\) by \(\norm{x} = \sqrt{\inn{x, x}}\).
\end{defn}

\begin{eg}\label{6.1.10}
  Let \(\V = \vs{F}^n\).
  If \(x = \tuple{a}{1,,n}\), then
  \[
    \norm{x} = \norm{\tuple{a}{1,,n}} = \pa{\sum_{i = 1}^n \abs{a_i}^2}^{\frac{1}{2}}
  \]
  is the Euclidean definition of length.
  Note that if \(n = 1\), we have \(\norm{a} = \abs{a}\).
\end{eg}

\begin{thm}\label{6.2}
  Let \(\V\) be an inner product space over \(\F\).
  Then for all \(x, y \in \V\) and \(c \in \F\), the following statements are true.
  \begin{enumerate}
    \item \(\norm{cx} = \abs{c} \cdot \norm{x}\).
    \item \(\norm{x} = 0\) iff \(x = \zv\).
          In any case \(\norm{x} \geq 0\).
    \item (Cauchy--Schwarz Inequality)
          \(\abs{\inn{x, y}} \leq \norm{x} \cdot \norm{y}\).
    \item (Triangle Inequality)
          \(\norm{x + y} \leq \norm{x} + \norm{y}\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{6.2}(a)]
  We have
  \begin{align*}
    \norm{cx} & = \sqrt{\inn{cx, cx}}          &  & \text{(by \cref{6.1.9})}    \\
              & = \sqrt{c \inn{x, cx}}         &  & \text{(by \cref{6.1.1}(b))} \\
              & = \sqrt{c \conj{c} \inn{x, x}} &  & \text{(by \cref{6.1}(b))}   \\
              & = \sqrt{\abs{c}^2 \inn{x, x}}  &  & \text{(by \cref{d.0.5})}    \\
              & = \abs{c} \sqrt{\inn{x, x}}                                     \\
              & = \abs{c} \norm{x}.            &  & \text{(by \cref{6.1.9})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{6.2}(b)]
  We have
  \begin{align*}
         & x = \zv                                              \\
    \iff & \inn{x, x} = 0        &  & \text{(by \cref{6.1}(d))} \\
    \iff & \sqrt{\inn{x, x}} = 0                                \\
    \iff & \norm{x} = 0          &  & \text{(by \cref{6.1.9})}
  \end{align*}
  and
  \begin{align*}
             & \forall x \in \V, \begin{dcases}
                                   \inn{x, x} = 0 & \text{if } x = \zv    \\
                                   \inn{x, x} > 0 & \text{if } x \neq \zv
                                 \end{dcases}  &  & \text{(by \cref{6.1.1}(d) and \cref{6.1}(d))} \\
    \implies & \forall x \in \V, \inn{x, x} \geq 0                                                \\
    \implies & \forall x \in \V, \sqrt{\inn{x, x}} \geq 0                                         \\
    \implies & \forall x \in \V, \norm{x} \geq 0.         &  & \text{(by \cref{6.1.9})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{6.2}(c)]
  If \(y = \zv\), then the result is immediate.
  So assume that \(y \neq \zv\).
  For any \(c \in \F\), we have
  \begin{align*}
    0 & \leq \norm{x - cy}^2                                                       &  & \text{(by \cref{6.2}(b))}      \\
      & = \inn{x - cy, x - cy}                                                     &  & \text{(by \cref{6.1.9})}       \\
      & = \inn{x, x - cy} - c \inn{y, x - cy}                                      &  & \text{(by \cref{6.1.1}(a)(b))} \\
      & = \inn{x, x} - \conj{c} \inn{x, y} - c \inn{y, x} + c \conj{c} \inn{y, y}. &  & \text{(by \cref{6.1}(a)(b))}
  \end{align*}
  In particular, if we set
  \[
    c = \frac{\inn{x, y}}{\inn{y, y}},
  \]
  the inequality becomes
  \[
    0 \leq \inn{x, x} - \frac{\abs{\inn{x, y}}^2}{\inn{y, y}} = \norm{x}^2 - \frac{\abs{\inn{x, y}}^2}{\norm{y}^2},
  \]
  from which (c) follows.
\end{proof}

\begin{proof}[\pf{6.2}(d)]
  We have
  \begin{align*}
     & \norm{x + y}^2                                                                                         \\
     & = \inn{x + y, x + y}                                &  & \text{(by \cref{6.1.9})}                      \\
     & = \inn{x, x} + \inn{y, x} + \inn{x, y} + \inn{y, y} &  & \text{(by \cref{6.1.1}(a) and \cref{6.1}(a))} \\
     & = \norm{x}^2 + 2 \Re(\inn{x, y}) + \norm{y}^2       &  & \text{(by \cref{6.1.1}(d) and \cref{6.1.9})}  \\
     & \leq \norm{x}^2 + 2 \abs{\inn{x, y}} + \norm{y}^2   &  & \text{(by \cref{d.0.5})}                      \\
     & \leq \norm{x}^2 + 2 \norm{x} \norm{y} + \norm{y}^2  &  & \text{(by \cref{6.2}(c))}                     \\
     & = \pa{\norm{x} + \norm{y}}^2,
  \end{align*}
  where \(\Re(\inn{x, y})\) denotes the real part of the complex number \(\inn{x, y}\).
\end{proof}

\begin{eg}\label{6.1.11}
  For \(\vs{F}^n\), we may apply \cref{6.2}(c)(d) to the standard inner product to obtain the following well-known inequalities:
  \[
    \abs{\sum_{i = 1}^n a_i \conj{b_i}} \leq \pa{\sum_{i = 1}^n \abs{a_i}^2}^{\frac{1}{2}} \pa{\sum_{i = 1}^n \abs{b_i}^2}^{\frac{1}{2}}
  \]
  and
  \[
    \pa{\sum_{i = 1}^n \abs{a_i + b_i}^2}^{\frac{1}{2}} \leq \pa{\sum_{i = 1}^n \abs{a_i}^2}^{\frac{1}{2}} + \pa{\sum_{i = 1}^n \abs{b_i}^2}^{\frac{1}{2}}.
  \]
\end{eg}

\begin{note}
  The reader may recall from earlier courses that, for \(x\) and \(y\) in \(\R^3\) or \(\R^2\), we have that \(\inn{x, y} = \norm{x} \cdot \norm{y} \cos \theta\), where \(\theta \in [0, \pi]\) denotes the angle between \(x\) and \(y\).
  This equation implies \cref{6.2}(c) immediately since \(\abs{\cos \theta} \leq 1\).
  Notice also that nonzero vectors \(x\) and \(y\) are perpendicular iff \(\cos \theta = 0\), that is, iff \(\inn{x, y} = 0\).
\end{note}

\begin{defn}\label{6.1.12}
  Let \(\V\) be an inner product space over \(\F\).
  Vectors \(x\) and \(y\) in \(\V\) are \textbf{orthogonal} (\textbf{perpendicular}) if \(\inn{x, y} = 0\).
  A subset \(S\) of \(\V\) is \textbf{orthogonal} if any two distinct vectors in \(S\) are orthogonal.
  A vector \(x\) in \(\V\) is a \textbf{unit vector} if \(\norm{x} = 1\).
  Finally, a subset \(S\) of \(\V\) is \textbf{orthonormal} if \(S\) is orthogonal and consists entirely of unit vectors.

  Note that if \(S = \set{\seq{v}{1,2,}}\), then \(S\) is orthonormal iff \(\inn{v_i, v_j} = \delta_{i j}\), where \(\delta_{i j}\) denotes the Kronecker delta.
  Also, observe that multiplying vectors by nonzero scalars does not affect their orthogonality and that if \(x\) is any nonzero vector, then \((1 / \norm{x}) x\) is a unit vector.
  The process of multiplying a nonzero vector by the reciprocal of its length is called \textbf{normalizing}.
\end{defn}

\begin{eg}\label{6.1.13}
  Recall the inner product space \(\vs{H}\) (defined in \cref{6.1.8}).
  We introduce an important orthonormal subset \(S\) of \(\vs{H}\).
  For what follows, \(i\) is the imaginary number such that \(i^2 = -1\).
  For any integer \(n\), let \(f_n(t) = e^{int}\), where \(0 \leq t \leq 2\pi\).
  (Recall that \(e^{int} = \cos(nt) + i \sin(nt)\).)
  Now define \(S = \set{f_n : n \in \Z}\).
  Clearly \(S\) is a subset of \(\vs{H}\).
  Using the property that \(\conj{e^{it}} = e^{-it}\) for every real number \(t\), we have, for \(m \neq n\),
  \begin{align*}
    \inn{f_m, f_n} & = \frac{1}{2\pi} \int_0^{2\pi} e^{imt} \conj{e^{int}} \; dt &  & \text{(by \cref{6.1.8})} \\
                   & = \frac{1}{2\pi} \int_0^{2\pi} e^{i(m - n)t} \; dt                                        \\
                   & = \eval{\frac{1}{2\pi i(m - n)} e^{i(m - n)t}}_0^{2\pi}                                   \\
                   & = 0.
  \end{align*}
  Also,
  \begin{align*}
    \inn{f_n, f_n} & = \frac{1}{2\pi} \int_0^{2\pi} e^{i(n - n)t} \; dt &  & \text{(by \cref{6.1.8})} \\
                   & = \frac{1}{2\pi} \int_0^{2\pi} 1 \; dt                                           \\
                   & = 1.
  \end{align*}
  In other words, \(\inn{f_m, f_n} = \delta_{m n}\).
\end{eg}

\exercisesection

\setcounter{ex}{8}
\begin{ex}\label{ex:6.1.9}
  Let \(\beta\) be a basis for a \(n\)-dimensional inner product space \(\V\) over \(\F\).
  \begin{enumerate}
    \item Prove that if \(x \in \V\) and \(\inn{x, z} = 0\) for all \(z \in \beta\), then \(x = \zv\).
    \item Prove that if \(x \in \V\) and \(\inn{x, z} = \inn{y, z}\) for all \(z \in \beta\), then \(x = y\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.1.9}(a)]
  Let \(\beta = \set{\seq{v}{1,,n}}\).
  Then we have
  \begin{align*}
             & x \in \V = \spn{\beta}                                                                                                  \\
    \implies & \exists \seq{a}{1,,n} \in \F : x = \sum_{i = 1}^n a_i v_i                             &  & \text{(by \cref{1.6.1})}     \\
    \implies & \inn{x, x} = \inn{x, \sum_{i = 1}^n a_i v_i} = \sum_{i = 1}^n \conj{a_i} \inn{x, v_i} &  & \text{(by \cref{6.1}(a)(b))} \\
             & = \sum_{i = 1}^n \conj{a_i} 0 = 0                                                     &  & \text{(by hypothesis)}       \\
    \implies & x = \zv.                                                                              &  & \text{(by \cref{6.1}(d))}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.1.9}(b)]
  We have
  \begin{align*}
             & \forall z \in \beta, \inn{x, z} = \inn{y, z}                                              \\
    \implies & \forall v \in \V = \spn{\beta}, \inn{x, v} = \inn{y, v} &  & \text{(by \cref{6.1}(a)(b))} \\
    \implies & \forall v \in \V = \spn{\beta}, \inn{v, x} = \inn{v, y} &  & \text{(by \cref{6.1.1}(c))}  \\
    \implies & x = y.                                                  &  & \text{(by \cref{6.1}(e))}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:6.1.10}
  Let \(\V\) be an inner product space over \(\F\), and suppose that \(x\) and \(y\) are orthogonal vectors in \(\V\).
  Prove that \(\norm{x + y}^2 = \norm{x}^2 + \norm{y}^2\).
  Deduce the Pythagorean theorem in \(\R^2\).
\end{ex}

\begin{proof}[\pf{ex:6.1.10}]
  We have
  \begin{align*}
     & \norm{x + y}^2                                                                                         \\
     & = \inn{x + y, x + y}                                &  & \text{(by \cref{6.1.9})}                      \\
     & = \inn{x, x} + \inn{y, x} + \inn{x, y} + \inn{y, y} &  & \text{(by \cref{6.1.1}(a) and \cref{6.1}(a))} \\
     & = \inn{x, x} + \inn{y, y}                           &  & \text{(by \cref{6.1.12})}                     \\
     & = \norm{x}^2 + \norm{y}^2.                          &  & \text{(by \cref{6.1.9})}
  \end{align*}
  By setting \(\V = \R^2\) and define \(\inn{\cdot, \cdot}\) as in \cref{6.1.2} we see that the Pythagorean theorem is true.
\end{proof}

\begin{ex}\label{ex:6.1.11}
  Prove the \emph{parallelogram law} on an inner product space \(\V\) over \(\F\);
  that is, show that
  \[
    \norm{x + y}^2 + \norm{x - y}^2 = 2 \norm{x}^2 + 2 \norm{y}^2 \quad \text{for all } x, y \in \V.
  \]
  What does this equation state about parallelograms in \(\R^2\)?
\end{ex}

\begin{proof}[\pf{ex:6.1.11}]
  We have
  \begin{align*}
     & \norm{x + y}^2 + \norm{x - y}^2                                                                         \\
     & = \inn{x + y, x + y} + \inn{x - y, x - y}                           &  & \text{(by \cref{6.1.9})}       \\
     & = \inn{x, x + y} + \inn{y, x + y} + \inn{x, x - y} - \inn{y, x - y} &  & \text{(by \cref{6.1.1}(a)(b))} \\
     & = \inn{x, 2x} + \inn{y, 2y}                                         &  & \text{(by \cref{6.1}(a)(b))}   \\
     & = 2 \inn{x, x} + 2 \inn{y, y}                                       &  & \text{(by \cref{6.1}(b))}      \\
     & = 2 \norm{x}^2 + 2 \norm{y}^2.                                      &  & \text{(by \cref{6.1.9})}
  \end{align*}
  On \(\R^2\) we see that if \(x, y\) are the two vectors forming a parallelogram, then \(x + y\) and \(x - y\) are the two diagonal vectors of the parallelogram.
  Thus we see that the sum of the square of the length of the \(4\) sides of a parallelogram equal to the sum of the square of the length of diagonal lines of the same parallelogram.
\end{proof}

\begin{ex}\label{ex:6.1.12}
  Let \(\set{\seq{v}{1,,k}}\) be an orthogonal set in \(\V\) over \(\F\), and let \(\seq{a}{1,,k} \in \F\).
  Prove that
  \[
    \norm{\sum_{i = 1}^k a_i v_i}^2 = \sum_{i = 1}^k \abs{a_i}^2 \norm{v_i}^2.
  \]
\end{ex}

\begin{proof}[\pf{ex:6.1.12}]
  We have
  \begin{align*}
    \norm{\sum_{i = 1}^k a_i v_i}^2 & = \sum_{i = 1}^k \norm{a_i v_i}^2          &  & \text{(by \cref{ex:6.1.10})} \\
                                    & = \sum_{i = 1}^k \abs{a_i}^2 \norm{v_i}^2. &  & \text{(by \cref{6.2}(a))}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:6.1.13}
  Suppose that \(\inn{\cdot, \cdot}_1\) and \(\inn{\cdot, \cdot}_2\) are two inner products on a vector space \(\V\) over \(\F\).
  Prove that \(\inn{\cdot, \cdot} = \inn{\cdot, \cdot}_1 + \inn{\cdot, \cdot}_2\) is another inner product on \(\V\) over \(\F\).
\end{ex}

\begin{proof}[\pf{ex:6.1.13}]
  Let \(x, y, z \in \V\) and let \(c \in \F\).
  Then we have
  \begin{align*}
    \inn{x + y, z}    & = \inn{x + y, z}_1 + \inn{x + y, z}_2                       &  & \text{(by \cref{ex:6.1.13})} \\
                      & = \inn{x, z}_1 + \inn{y, z}_1 + \inn{x, z}_2 + \inn{y, z}_2 &  & \text{(by \cref{6.1.1}(a))}  \\
                      & = \inn{x, z} + \inn{y, z}                                   &  & \text{(by \cref{ex:6.1.13})} \\
    \inn{cx, y}       & = \inn{cx, y}_1 + \inn{cx, y}_2                             &  & \text{(by \cref{ex:6.1.13})} \\
                      & = c \inn{x, y}_1 + c \inn{x, y}_2                           &  & \text{(by \cref{6.1.1}(b))}  \\
                      & = c \inn{x, y}                                              &  & \text{(by \cref{ex:6.1.13})} \\
    \conj{\inn{x, y}} & = \conj{\inn{x, y}_1 + \inn{x, y}_2}                        &  & \text{(by \cref{ex:6.1.13})} \\
                      & = \conj{\inn{x, y}_1} + \conj{\inn{x, y}_2}                 &  & \text{(by \cref{d.2}(b))}    \\
                      & = \inn{y, x}_1 + \inn{y, x}_2                               &  & \text{(by \cref{6.1.1}(c))}  \\
                      & = \inn{y, x}.                                               &  & \text{(by \cref{ex:6.1.13})}
  \end{align*}
  If \(x \neq \zv\), then we have
  \begin{align*}
    \inn{x, x} & = \inn{x, x}_1 + \inn{x, x}_2 &  & \text{(by \cref{ex:6.1.13})} \\
               & > 0 + 0                       &  & \text{(by \cref{6.1.1}(d))}  \\
               & = 0.
  \end{align*}
  Thus by \cref{6.1.1} \(\inn{\cdot, \cdot}\) is an inner product on \(\V\) over \(\F\).
\end{proof}

\begin{ex}\label{ex:6.1.14}
  Let \(A, B \in \ms{n}{n}{\F}\) and let \(c \in \F\).
  Prove that \((A + cB)^* = A^* + \conj{c} B^*\).
\end{ex}

\begin{proof}[\pf{ex:6.1.14}]
  We have
  \begin{align*}
    \forall i, j \in \set{1, \dots, n}, ((A + cB)^*)_{i j} & = \conj{(A + cB)_{j i}}                    &  & \text{(by \cref{6.1.5})}     \\
                                                           & = \conj{A_{j i} + c B_{j i}}               &  & \text{(by \cref{1.2.9})}     \\
                                                           & = \conj{A_{j i}} + \conj{c} \conj{B_{j i}} &  & \text{(by \cref{d.2}(b)(c))} \\
                                                           & = (A^*)_{i j} + \conj{c} (B^*)_{i j}       &  & \text{(by \cref{6.1.5})}     \\
                                                           & = (A^* + \conj{c} B^*)_{i j}               &  & \text{(by \cref{1.2.9})}
  \end{align*}
  and thus by \cref{1.2.8} \((A + cB)^* = A^* + \conj{c} B^*\).
\end{proof}

\begin{ex}\label{ex:6.1.15}
  \begin{enumerate}
    \item Prove that if \(\V\) is an inner product space over \(\F\), then \(\abs{\inn{x, y}} = \norm{x} \cdot \norm{y}\) iff one of the vectors \(x\) or \(y\) is a multiple of the other.
    \item Derive a similar result for the equality \(\norm{x + y} = \norm{x} + \norm{y}\), and generalize it to the case of \(n\) vectors.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.1.15}(a)]
  If \(y = \zv\) then
  \begin{align*}
         & \abs{\inn{x, \zv}} = \abs{0} = 0 = \norm{x} \norm{\zv} &  & \text{(by \cref{6.1}(c))} \\
    \iff & y = 0x = \zv.                                          &  & \text{(by \cref{1.2}(a))}
  \end{align*}
  So suppose that \(y \neq \zv\).

  First suppose that \(\abs{\inn{x, y}} = \norm{x} \norm{y}\).
  Define
  \[
    a = \frac{\inn{x, y}}{\norm{y}^2} \quad \text{and} \quad z = x - ay.
  \]
  Then we have
  \begin{align*}
    \inn{z, y} & = \inn{x - \frac{\inn{x, y}}{\norm{y}^2} y, y}                                              \\
               & = \inn{x, y} - \frac{\inn{x, y}}{\norm{y}^2} \inn{y, y} &  & \text{(by \cref{6.1.1}(a)(b))} \\
               & = \inn{x, y} - \frac{\inn{x, y}}{\inn{y, y}} \inn{y, y} &  & \text{(by \cref{6.1.9})}       \\
               & = 0                                                     &  & (\inn{y, y} > 0)
  \end{align*}
  and thus by \cref{6.1.12} we see that \(y, z\) are orthogonal.
  Since
  \begin{align*}
             & \abs{\inn{x, y}} = \norm{x} \norm{y}                                                                     \\
    \implies & \abs{a} = \abs{\frac{\inn{x, y}}{\norm{y}^2}} = \frac{\norm{x}}{\norm{y}}                                \\
    \implies & \abs{a} \norm{y} = \norm{ay} = \norm{x}                                   &  & \text{(by \cref{6.2}(a))} \\
  \end{align*}
  we have
  \begin{align*}
             & \begin{dcases}
                 z \bot y \\
                 x = z + ay
               \end{dcases}                                          &  & \text{(from the proof above)}   \\
    \implies & \norm{x}^2 = \norm{z + ay}^2 = \norm{z}^2 + \norm{ay}^2 &  & \text{(by \cref{ex:6.1.10})}  \\
    \implies & \norm{x}^2 = \norm{z}^2 + \norm{x}^2                    &  & \text{(from the proof above)} \\
    \implies & \norm{z}^2 = 0                                                                             \\
    \implies & \inn{z, z} = 0                                          &  & \text{(by \cref{6.1.9})}      \\
    \implies & x - ay = z = \zv                                        &  & \text{(by \cref{6.1}(d))}     \\
    \implies & x = ay.
  \end{align*}

  Now suppose that there exists an \(a \in \F\) such that \(x = ay\).
  Then we have
  \begin{align*}
    \norm{x} \norm{y} & = \norm{ay} \norm{y}                                        \\
                      & = \abs{a} \norm{y}^2       &  & \text{(by \cref{6.2}(a))}   \\
                      & = \abs{a} \inn{y, y}       &  & \text{(by \cref{6.1.9})}    \\
                      & = \abs{a} \abs{\inn{y, y}} &  & \text{(by \cref{6.2}(b))}   \\
                      & = \abs{a \inn{y, y}}       &  & \text{(by \cref{d.3}(a))}   \\
                      & = \abs{\inn{ay, y}}        &  & \text{(by \cref{6.1.1}(b))} \\
                      & = \abs{\inn{x, y}}.
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.1.15}(b)]
  We claim that \(\norm{x + y} = \norm{x} + \norm{y}\) iff there exists a nonnegative scalar \(c \in \F\) such that \(x = cy\) or \(y = cx\).
  If \(y = \zv\), then we have
  \begin{align*}
         & \norm{x + \zv} = \norm{x} + \norm{\zv} = \norm{x} &  & \text{(by \cref{6.1}(c))} \\
    \iff & \zv = 0x.                                         &  & \text{(by \cref{1.2}(a))}
  \end{align*}
  So suppose that \(y \neq \zv\).
  As in the proof of \cref{6.2}(d), we see that
  \[
    \norm{x + y}^2 = \norm{x}^2 + 2 \Re(\inn{x, y}) + \norm{y}^2.
  \]
  Thus we have
  \begin{align*}
             & \norm{x + y} = \norm{x} + \norm{y}                                                                            \\
    \implies & \norm{x + y}^2 = \norm{x}^2 + 2 \norm{x} \norm{y} + \norm{y}^2                                                \\
    \implies & \norm{x} \norm{y} = \Re(\inn{x, y})                                                                           \\
    \implies & \abs{\inn{x, y}} \leq \norm{x} \norm{y} = \Re(\inn{x, y})                &  & \text{(by \cref{6.2}(d))}       \\
             & \leq \sqrt{(\Re(\inn{x, y}))^2 + (\Im(\inn{x, y}))^2} = \abs{\inn{x, y}} &  & \text{(by \cref{d.0.5})}        \\
    \implies & \abs{\inn{x, y}} = \norm{x} \norm{y}                                                                          \\
    \implies & \exists c \in \F : (x = cy) \lor (y = cx)                                &  & \text{(by \cref{ex:6.1.15}(a))}
  \end{align*}
  and
  \begin{align*}
             & \exists c \in \F : \begin{dcases}
                                    c > 0 \\
                                    x = cy
                                  \end{dcases}                                                             \\
    \implies & \norm{x} + \norm{y} = \norm{cy} + \norm{y} = (c + 1) \norm{y} &  & \text{(by \cref{6.2}(a))} \\
             & = \norm{(c + 1) y} = \norm{x + y}.                            &  & \text{(by \cref{6.2}(a))}
  \end{align*}
  In general we see that
  \[
    \norm{\sum_{i = 1}^n x_i} = \sum_{i = 1}^n \norm{x_i} \iff \begin{dcases}
      \exists \seq{c}{1,,n} \in \F \\
      \exists j \in \set{1, \dots, n}
    \end{dcases} : \forall i \in \set{1, \dots, n}, \begin{dcases}
      c_i > 0 \\
      x_i = c_i x_j
    \end{dcases}.
  \]
\end{proof}

\setcounter{ex}{16}
\begin{ex}\label{ex:6.1.17}
  Let \(\T\) be a linear operator on an inner product space \(\V\) over \(\F\), and suppose that \(\norm{\T(x)} = \norm{x}\) for all \(x \in \V\).
  Prove that \(\T\) is one-to-one.
\end{ex}

\begin{proof}[\pf{ex:6.1.17}]
  We have
  \begin{align*}
             & \norm{x} = \norm{\T(x)} = 0                                \\
    \implies & x = \T(x) = \zv             &  & \text{(by \cref{6.2}(b))} \\
    \implies & \T \text{ is one-to-one}.   &  & \text{(by \cref{2.4})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:6.1.18}
  Let \(\V\) be a vector space over \(\F\), where \(\F = \R\) or \(\F = \C\), and let \(\W\) be an inner product space over \(\F\) with inner product \(\inn{\cdot, \cdot}\).
  If \(\T \in \ls(\V, \W)\), prove that \(\inn{x, y}' = \inn{\T(x), \T(y)}\) defines an inner product on \(\V\) over \(\F\) iff \(\T\) is one-to-one.
\end{ex}

\begin{proof}[\pf{ex:6.1.18}]
  First suppose that \(\inn{\cdot, \cdot}'\) is an inner product on \(\V\) over \(\F\).
  Then we have
  \begin{align*}
             & \forall x \in \V, \inn{x, x}' = \inn{\T(x), \T(x)} = 0                             \\
    \implies & \begin{dcases}
                 x = \zv_{\V} \\
                 \T(x) = \zv_{\W}
               \end{dcases}                                       &  & \text{(by \cref{6.1}(d))}  \\
    \implies & \T \text{ is one-to-one}.                              &  & \text{(by \cref{2.4})}
  \end{align*}

  Now suppose that \(\T\) is one-to-one.
  Let \(x, y, z \in \V\) and let \(c \in \F\).
  Then we have
  \begin{align*}
    \inn{x + y, z}'    & = \inn{\T(x + y), \T(z)}                                                   \\
                       & = \inn{\T(x) + \T(y), \T(z)}              &  & \text{(by \cref{2.1.1}(a))} \\
                       & = \inn{\T(x), \T(z)} + \inn{\T(y), \T(z)} &  & \text{(by \cref{6.1.1}(a))} \\
                       & = \inn{x, z}' + \inn{y, z}'                                                \\
    \inn{cx, y}'       & = \inn{\T(cx), \T(y)}                                                      \\
                       & = \inn{c \T(x), \T(y)}                    &  & \text{(by \cref{2.1.1}(b))} \\
                       & = c \inn{\T(x), \T(y)}                    &  & \text{(by \cref{6.1.1}(b))} \\
                       & = c \inn{x, y}'                                                            \\
    \conj{\inn{x, y}'} & = \conj{\inn{\T(x), \T(y)}}                                                \\
                       & = \inn{\T(y), \T(x)}                      &  & \text{(by \cref{6.1.1}(c))} \\
                       & = \inn{y, x}'.
  \end{align*}
  If \(x \neq \zv_{\V}\), then we have
  \begin{align*}
             & \T(x) \neq \zv_{\W}       &  & \text{(by \cref{2.4})}      \\
    \implies & \inn{\T(x), \T(x)} \neq 0 &  & \text{(by \cref{6.1.1}(d))} \\
    \implies & \inn{x, x}' \neq 0.
  \end{align*}
  Thus by \cref{6.1.1} \(\inn{\cdot, \cdot}'\) is an inner product on \(\V\) over \(\F\).
\end{proof}

\begin{ex}\label{ex:6.1.19}
  Let \(\V\) be an inner product space over \(\F\).
  Prove that
  \begin{enumerate}
    \item \(\norm{x \pm y}^2 = \norm{x}^2 \pm 2 \Re(\inn{x, y}) + \norm{y}^2\) for all \(x, y \in \V\).
    \item \(\abs{\norm{x} - \norm{y}} \leq \norm{x - y}\) for all \(x, y \in \V\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.1.19}(a)]
  We have
  \begin{align*}
     & \norm{x \pm y}^2                                                                          \\
     & = \inn{x \pm y, x \pm y}                                &  & \text{(by \cref{6.1.9})}     \\
     & = \begin{dcases}
           \inn{x, x + y} + \inn{y, x + y} \\
           \inn{x, x - y} - \inn{y, x - y}
         \end{dcases}                      &  & \text{(by \cref{6.1.1}(a)(b))}                   \\
     & = \inn{x, x} \pm (\inn{x, y} + \inn{y, x}) + \inn{y, y} &  & \text{(by \cref{6.1}(a)(b))} \\
     & = \norm{x}^2 \pm 2 \Re(\inn{x, y}) + \norm{y}^2.        &  & \text{(by \cref{6.1.9})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.1.19}(b)]
  We have
  \begin{align*}
             & \norm{x} = \norm{x - y + y} \leq \norm{x - y} + \norm{y} &  & \text{(by \cref{6.2}(d))} \\
    \implies & \norm{x} - \norm{y} \leq \norm{x - y}
  \end{align*}
  and
  \begin{align*}
             & \norm{y} = \norm{y - x + x} \leq \norm{y - x} + \norm{x}                   &  & \text{(by \cref{6.2}(d))} \\
    \implies & \norm{y} - \norm{x} \leq \norm{y - x} = \norm{(-1)(x - y)} = \norm{x - y}. &  & \text{(by \cref{6.2}(a))}
  \end{align*}
  Thus \(\abs{\norm{x} - \norm{y}} \leq \norm{x - y}\).
\end{proof}

\begin{ex}\label{ex:6.1.20}
  Let \(\V\) be an inner product space over \(\F\).
  Prove the \emph{polar identities}:
  For all \(x, y \in \V\),
  \begin{enumerate}
    \item \(\inn{x, y} = \frac{1}{4} \norm{x + y}^2 - \frac{1}{4} \norm{x - y}^2\) if \(\F = \R\);
    \item \(\inn{x, y} = \frac{1}{4} \sum_{k = 1}^4 i^k \norm{x + i^k y}^2\) if \(\F = \C\), where \(i^2 = -1\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.1.20}(a)]
  We have
  \begin{align*}
    \frac{1}{4} \norm{x + y}^2 - \frac{1}{4} \norm{x - y}^2 & = \Re(\inn{x, y}) &  & \text{(by \cref{ex:6.1.19}(a))} \\
                                                            & = \inn{x, y}.     &  & (\F = \R)
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.1.20}(b)]
  We have
  \begin{align*}
     & \frac{1}{4} \sum_{k = 1}^4 i^k \norm{x + i^k y}^2                                                                                         \\
     & = \frac{1}{4} \sum_{k = 1}^4 i^k \pa{\norm{x}^2 + 2 \Re(\inn{x, i^k y}) + \norm{i^k y}^2}            &  & \text{(by \cref{ex:6.1.19}(a))} \\
     & = \frac{1}{4} \sum_{k = 1}^4 i^k \pa{\norm{x}^2 + 2 \Re((-i)^k \inn{x, y}) + \norm{i^k y}^2}         &  & \text{(by \cref{6.1}(b))}       \\
     & = \frac{1}{4} \sum_{k = 1}^4 i^k \pa{\norm{x}^2 + 2 \Re((-i)^k \inn{x, y}) + \abs{i^k}^2 \norm{y}^2} &  & \text{(by \cref{6.2}(a))}       \\
     & = \frac{1}{4} \sum_{k = 1}^4 i^k \pa{\norm{x}^2 + 2 \Re((-i)^k \inn{x, y}) + \norm{y}^2}             &  & (\abs{i} = 1)                   \\
     & = \frac{1}{4} \sum_{k = 1}^4 i^k 2 \Re((-i)^k \inn{x, y})                                            &  & (i + (-1) + (-i) + 1 = 0)       \\
     & = \frac{1}{2} \pa{i \Re(-i \inn{x, y}) - \Re(-\inn{x, y}) - i \Re(i \inn{x, y}) + \Re(\inn{x, y})}                                        \\
     & = \Re(\inn{x, y}) - i \Re(i \inn{x, y})                                                                                                   \\
     & = \Re(\inn{x, y}) - i \Im(-\inn{x, y})                                                                                                    \\
     & = \Re(\inn{x, y}) + i \Im(\inn{x, y})                                                                                                     \\
     & = \inn{x, y}.
  \end{align*}
\end{proof}

\begin{ex}\label{ex:6.1.21}
  Let \(A \in \ms{n}{n}{\F}\).
  Define
  \[
    A_1 = \frac{1}{2} (A + A^*) \quad \text{and} \quad A_2 = \frac{1}{2i} (A - A^*).
  \]
  \begin{enumerate}
    \item Prove that \(A_1^* = A_1\), \(A_2^* = A_2\), and \(A = A_1 + i A_2\).
          Would it be reasonable to define \(A_1\) and \(A_2\) to be the real and imaginary parts, respectively, of the matrix \(A\)?
    \item Prove that the representation in (a) is unique.
          That is, prove that if \(A = B_1 + i B_2\), where \(B_1^* = B_1\) and \(B_2^* = B_2\), then \(B_1 = A_1\) and \(B_2 = A_2\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.1.21}(a)]
  We have
  \begin{align*}
    \forall i, j \in \set{1, \dots, n}, (A_1^*)_{i j} & = \conj{(A_1)_{j i}}                                  &  & \text{(by \cref{6.1.5})}     \\
                                                      & = \conj{\pa{\frac{1}{2} (A + A^*)}_{j i}}             &  & \text{(by \cref{ex:6.1.21})} \\
                                                      & = \conj{\frac{1}{2} (A_{j i} + (A^*)_{j i})}          &  & \text{(by \cref{1.2.9})}     \\
                                                      & = \frac{1}{2} (\conj{A_{j i}} + \conj{(A^*)_{j i}})   &  & \text{(by \cref{d.2}(b)(c))} \\
                                                      & = \frac{1}{2} ((A^*)_{i j} + \conj{\conj{A_{i j}}})   &  & \text{(by \cref{6.1.5})}     \\
                                                      & = \frac{1}{2} ((A^*)_{i j} + A_{i j})                 &  & \text{(by \cref{d.2}(a))}    \\
                                                      & = \pa{\frac{1}{2} (A^* + A)}_{i j}                    &  & \text{(by \cref{1.2.9})}     \\
                                                      & = \pa{\frac{1}{2} (A + A^*)}_{i j}                    &  & \text{(by \cref{1.2.9})}     \\
                                                      & = (A_1)_{i j}                                         &  & \text{(by \cref{ex:6.1.21})} \\
    \forall i, j \in \set{1, \dots, n}, (A_2^*)_{i j} & = \conj{(A_2)_{j i}}                                  &  & \text{(by \cref{6.1.5})}     \\
                                                      & = \conj{\pa{\frac{1}{2i} (A - A^*)}_{j i}}            &  & \text{(by \cref{ex:6.1.21})} \\
                                                      & = \conj{\frac{1}{2i} (A_{j i} - (A^*)_{j i})}         &  & \text{(by \cref{1.2.9})}     \\
                                                      & = \frac{-1}{2i} (\conj{A_{j i}} - \conj{(A^*)_{j i}}) &  & \text{(by \cref{d.2}(b)(c))} \\
                                                      & = \frac{-1}{2i} ((A^*)_{i j} - \conj{\conj{A_{i j}}}) &  & \text{(by \cref{6.1.5})}     \\
                                                      & = \frac{-1}{2i} ((A^*)_{i j} - A_{i j})               &  & \text{(by \cref{d.2}(a))}    \\
                                                      & = \pa{\frac{-1}{2i} (A^* - A)}_{i j}                  &  & \text{(by \cref{1.2.9})}     \\
                                                      & = \pa{\frac{1}{2i} (A - A^*)}_{i j}                   &  & \text{(by \cref{1.2.9})}     \\
                                                      & = (A_2)_{i j}                                         &  & \text{(by \cref{ex:6.1.21})}
  \end{align*}
  and thus by \cref{1.2.8} \(A_1^* = A_1\) and \(A_2^* = A_2\).
  Observe that
  \begin{align*}
    A_1 + i A_2 & = \frac{1}{2} (A + A^*) + \frac{i}{2i} (A - A^*) &  & \text{(by \cref{ex:6.1.21})} \\
                & = A.                                             &  & \text{(by \cref{1.2.9})}
  \end{align*}
  If we let \(A \in \ms{2}{2}{\C}\) where \(A = \begin{pmatrix}
    1 & -i \\
    i & 1
  \end{pmatrix}\), then we have
  \begin{align*}
    A_1          & = \frac{1}{2} (A + A^*)           &  & \text{(by \cref{ex:6.1.21})}            \\
                 & = \frac{1}{2} \pa{\begin{pmatrix}
                                         1 & -i \\
                                         i & 1
                                       \end{pmatrix} + \begin{pmatrix}
                                                         1 & -i \\
                                                         i & 1
                                                       \end{pmatrix}} &  & \text{(by \cref{6.1.5})} \\
                 & = A                                                                            \\
    A_2          & = \frac{1}{2} (A - A^*)           &  & \text{(by \cref{ex:6.1.21})}            \\
                 & = \frac{1}{2} \pa{\begin{pmatrix}
                                         1 & -i \\
                                         i & 1
                                       \end{pmatrix} - \begin{pmatrix}
                                                         1 & -i \\
                                                         i & 1
                                                       \end{pmatrix}} &  & \text{(by \cref{6.1.5})} \\
                 & = \zm                                                                          \\
    A + \conj{A} & = \begin{pmatrix}
                       1 & -i \\
                       i & 1
                     \end{pmatrix} + \begin{pmatrix}
                                       1  & i \\
                                       -i & 1
                                     \end{pmatrix}   &  & \text{(by \cref{ex:4.3.13})}            \\
                 & = \begin{pmatrix}
                       2 & 0 \\
                       0 & 2
                     \end{pmatrix}                                                               \\
                 & \neq 2 A_1.
  \end{align*}
  Thus it does not make sense to define \(A_1\) as the real part of \(A\) and \(A_2\) as the imaginary part of \(A\).
\end{proof}

\begin{proof}[\pf{ex:6.1.21}(b)]
  We have
  \begin{align*}
             & A = B_1 + i B_2 = A_1 + i A_2                                                  \\
    \implies & (A_1 - B_1) + i (A_2 - B_2) = \zm         &  & \text{(by \cref{1.2.9})}        \\
    \implies & ((A_1 - B_1) + i (A_2 - B_2))^* = \zm     &  & \text{(by \cref{6.1.5})}        \\
    \implies & (A_1 - B_1)^* - i (A_2 - B_2)^* = \zm     &  & \text{(by \cref{ex:6.1.14})}    \\
    \implies & (A_1^* - B_1^*) - i (A_2^* - B_2^*) = \zm &  & \text{(by \cref{ex:6.1.14})}    \\
    \implies & (A_1 - B_1) - i (A_2 - B_2) = \zm         &  & \text{(by \cref{ex:6.1.21}(a))} \\
    \implies & \begin{dcases}
                 (A_1 - B_1) + i (A_2 - B_2) = \zm \\
                 (A_1 - B_1) - i (A_2 - B_2) = \zm
               \end{dcases}                                              \\
    \implies & \begin{dcases}
                 A_1 = B_1 \\
                 A_2 = B_2
               \end{dcases}.                            &  & \text{(by \cref{1.2.9})}
  \end{align*}
\end{proof}
