\section{Eigenvalues and Eigenvectors}\label{sec:5.1}

\begin{defn}\label{5.1.1}
  A linear operator \(\T\) on a finite-dimensional vector space \(\V\) over \(\F\) is called \textbf{diagonalizable} if there is an ordered basis \(\beta\) for \(\V\) over \(\F\) such that \([\T]_{\beta}\) is a diagonal matrix.
  A square matrix \(A\) is called \textbf{diagonalizable} if \(\L_A\) is diagonalizable.
\end{defn}

\begin{note}
  We want to determine when a linear operator \(\T\) on a finite-dimensional vector space \(\V\) over \(\F\) is diagonalizable and, if so, how to obtain an ordered basis \(\beta = \set{\seq{v}{1,,n}}\) for \(\V\) over \(\F\) such that \([\T]_{\beta}\) is a diagonal matrix.
  Note that, if \(D = [\T]_{\beta}\) is a diagonal matrix, then for each vector \(v_j \in \beta\), we have
  \[
    \T(v_j) = \sum_{i = 1}^n D_{i j} v_i = D_{j j} v_j = \lambda_j v_j,
  \]
  where \(\lambda_j = D_{j j}\).

  Conversely, if \(\beta = \set{\seq{v}{1,,n}}\) is an ordered basis for \(\V\) over \(\F\) such that \(\T(v_j) = \lambda_j v_j\) for some scalars \(\seq{\lambda}{1,,n}\), then clearly
  \[
    [\T]_{\beta} = \begin{pmatrix}
      \lambda_1 & 0         & \cdots & 0         \\
      0         & \lambda_2 & \cdots & 0         \\
      \vdots    & \vdots    &        & \vdots    \\
      0         & 0         & \cdots & \lambda_n
    \end{pmatrix}.
  \]
  In the preceding paragraph, each vector \(v\) in the basis \(\beta\) satisfies the condition that \(\T(v) = \lambda v\) for some scalar \(\lambda\).
  Moreover, because \(v\) lies in a basis, \(v\) is nonzero.
  These computations motivate \cref{5.1.2}.
\end{note}

\begin{defn}\label{5.1.2}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\).
  A nonzero vector \(v \in \V\) is called an \textbf{eigenvector} of \(\T\) if there exists a scalar \(\lambda\) such that \(\T(v) = \lambda v\).
  The scalar \(\lambda\) is called the \textbf{eigenvalue} corresponding to the eigenvector \(v\).

  Let \(A \in \ms{n}{n}{\F}\).
  A nonzero vector \(v \in \vs{F}^n\) is called an \textbf{eigenvector} of \(A\) if \(v\) is an eigenvector of \(\L_A\);
  that is, if \(Av = \lambda v\) for some scalar \(\lambda\).
  The scalar \(\lambda\) is called the \textbf{eigenvalue} of \(A\) corresponding to the eigenvector \(v\).
\end{defn}

\begin{note}
  The words \emph{characteristic vector} and \emph{proper vector} are also used in place of eigenvector.
  The corresponding terms for eigenvalue are \emph{characteristic value} and \emph{proper value}.
\end{note}

\begin{thm}\label{5.1}
  A linear operator \(\T\) on a finite-dimensional vector space \(\V\) over \(\F\) is diagonalizable iff there exists an ordered basis \(\beta\) for \(\V\) over \(\F\) consisting of eigenvectors of \(\T\).
  Furthermore, if \(\T\) is diagonalizable, \(\beta = \set{\seq{v}{1,,n}}\) is an ordered basis of eigenvectors of \(\T\), and \(D = [\T]_{\beta}\), then \(D\) is a diagonal matrix and \(D_{j j}\) is the eigenvalue corresponding to \(v_j\) for \(j \in \set{1, \dots, n}\).
\end{thm}

\begin{proof}[\pf{5.1}]
  This is simply a restatement of \cref{5.1.1,5.1.2}.
\end{proof}

\begin{note}
  To \emph{diagonalize} a matrix or a linear operator is to find a basis of eigenvectors and the corresponding eigenvalues.
\end{note}

\begin{eg}\label{5.1.3}
  Let \(\T\) be the linear operator on \(\R^2\) that rotates each vector in the plane through an angle of \(\pi / 2\).
  It is clear geometrically that for any nonzero vector \(v\), the vectors \(v\) and \(\T(v)\) are not collinear;
  hence \(\T(v)\) is not a multiple of \(v\).
  Therefore \(\T\) has no eigenvectors and, consequently, no eigenvalues.
  Thus there exist operators (and matrices) with no eigenvalues or eigenvectors.
  Of course, such operators and matrices are not diagonalizable.
\end{eg}

\begin{eg}\label{5.1.4}
  Let \(\cfs[\infty](\R)\) denote the set of all functions \(f : \R \to \R\) having derivatives of all orders.
  (Thus \(\cfs[\infty](\R)\) includes the polynomial functions, the sine and cosine functions, the exponential functions, etc.)
  Clearly, \(\cfs[\infty](\R)\) is a subspace of the vector space \(\fs(\R, \R)\) of all functions from \(\R\) to \(\R\) as defined in \cref{1.2.10}.
  Let \(\T : \cfs[\infty](\R) \to \cfs[\infty](\R)\) be the function defined by \(\T(f) = f'\), the derivative of \(f\).
  It is easily verified that \(\T\) is a linear operator on \(\cfs[\infty](\R)\) (\cref{ex:2.7.6}).
  We determine the eigenvalues and eigenvectors of \(\T\).

  Suppose that \(f\) is an eigenvector of \(\T\) with corresponding eigenvalue \(\lambda\).
  Then \(f' = T(f) = \lambda f\).
  This is a first-order differential equation whose solutions are of the form \(f(t) = ce^{\lambda t}\) for some constant \(c\) (\cref{2.30}).
  Consequently, every real number \(\lambda\) is an eigenvalue of \(\T\), and \(\lambda\) corresponds to eigenvectors of the form \(ce^{\lambda t}\) for \(c \neq 0\).
  Note that for \(\lambda = 0\), the eigenvectors are the nonzero constant functions.
\end{eg}

\begin{thm}\label{5.2}
  Let \(A \in \ms{n}{n}{\F}\).
  Then a scalar \(\lambda\) is an eigenvalue of \(A\) iff \(\det(A - \lambda I_n) = 0\).
\end{thm}

\begin{proof}[\pf{5.2}]
  A scalar \(\lambda \in \F\) is an eigenvalue of \(A\) iff there exists a nonzero vector \(v \in \vs{F}^n\) such that \(Av = \lambda v\), that is, \((A - \lambda I_n)(v) = 0\).
  By \cref{2.5}, this is true iff \(A - \lambda I_n\) is not invertible.
  However, by \cref{4.3.1} this result is equivalent to the statement that \(\det(A - \lambda I_n) = 0\).
\end{proof}

\begin{defn}\label{5.1.5}
  Let \(A \in \ms{n}{n}{\F}\).
  The polynomial \(f(t) = \det(A - t I_n)\) is called the \textbf{characteristic polynomial} of \(A\).
\end{defn}

\begin{note}
  The entries of the matrix \(A - t I_n\) are not scalars in the field \(\F\).
  They are, however, scalars in another field \(F(t)\), the field of quotients of polynomials in \(t\) with coefficients from \(\F\).
  Consequently, any results proved about determinants in \cref{ch:4} remain valid in this context.
\end{note}

\begin{note}
  \cref{5.2} states that the eigenvalues of a matrix are the zeros of its characteristic polynomial.
  When determining the eigenvalues of a matrix or a linear operator, we normally compute its characteristic polynomial.
\end{note}

\begin{note}
  It is easily shown that similar matrices have the same characteristic polynomial (see \cref{ex:5.1.12}).
  This fact enables us to define the characteristic polynomial of a linear operator as in \cref{5.1.6}.
\end{note}

\begin{defn}\label{5.1.6}
  Let \(\T\) be a linear operator on an \(n\)-dimensional vector space \(\V\) over \(\F\) with ordered basis \(\beta\).
  We define the \textbf{characteristic polynomial} \(f(t)\) of \(\T\) to be the characteristic polynomial of \(A = [\T]_{\beta}\).
  That is,
  \[
    f(t) = \det(A - t I_n).
  \]
\end{defn}

\begin{note}
  \cref{ex:5.1.12} shows that \cref{5.1.6} is independent of the choice of ordered basis \(\beta\).
  Thus if \(\T\) is a linear operator on a finite-dimensional vector space \(\V\) over \(\F\) and \(\beta\) is an ordered basis for \(\V\) over \(\F\), then \(\lambda\) is an eigenvalue of \(\T\) iff \(\lambda\) is an eigenvalue of \([\T]_{\beta}\).
  We often denote the characteristic polynomial of an operator \(\T\) by \(\det(\T - tI)\).
\end{note}

\begin{thm}\label{5.3}
  Let \(A \in \ms{n}{n}{\F}\).
  \begin{enumerate}
    \item The characteristic polynomial of \(A\) is a polynomial of degree \(n\) with leading coefficient \((-1)^n\).
    \item \(A\) has at most \(n\) distinct eigenvalues.
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{5.3}]

\end{proof}

\begin{thm}\label{5.4}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\), and let \(\lambda\) be an eigenvalue of \(\T\).
  A vector \(v \in \V\) is an eigenvector of \(\T\) corresponding to \(\lambda\) iff \(v \neq 0\) and \(v \in \ns{\T - \lambda \IT[\V]}\).
\end{thm}

\begin{proof}[\pf{5.4}]
  We have
  \begin{align*}
         & v \text{ is an eigenvector of } \T \text{ corresponds to eigenvalue } \lambda                             \\
    \iff & \begin{dcases}
             v \neq 0 \\
             \T(v) = \lambda v
           \end{dcases}                                                                &  & \text{(by \cref{5.1.2})} \\
    \iff & \begin{dcases}
             v \neq 0 \\
             (\T - \lambda \IT[\V])(v) = \T(v) - \lambda \IT[\V](v) = \T(v) - \lambda v = \zv
           \end{dcases} &  & \text{(by \cref{2.1.9})}                           \\
    \iff & \begin{dcases}
             v \neq 0 \\
             v \in \ns{\T - \lambda \IT[\V]}
           \end{dcases}.                                   &  & \text{(by \cref{2.1.10})}
  \end{align*}
\end{proof}

\exercisesection

\setcounter{ex}{5}
\begin{ex}\label{ex:5.1.6}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and let \(\beta\) be an ordered basis for \(\V\) over \(\F\).
  Prove that \(\lambda\) is an eigenvalue of \(\T\) iff \(\lambda\) is an eigenvalue of \([\T]_{\beta}\).
\end{ex}

\begin{proof}[\pf{ex:5.1.6}]
  We have
  \begin{align*}
         & \lambda \text{ is an eigenvalue of } \T                                                                             \\
    \iff & \exists v \in \V \setminus \set{\zv} : \T(v) = \lambda v                              &  & \text{(by \cref{5.1.2})} \\
    \iff & \exists v \in \V \setminus \set{\zv} : [\T(v)]_{\beta} = [\lambda v]_{\beta}          &  & \text{(by \cref{2.21})}  \\
    \iff & \exists v \in \V \setminus \set{\zv} : [\T]_{\beta} [v]_{\beta} = \lambda [v]_{\beta} &  & \text{(by \cref{2.14})}  \\
    \iff & \lambda \text{ is an eigenvalue of } [\T]_{\beta}.                                    &  & \text{(by \cref{5.1.2})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:5.1.7}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\).
  Let \(\dim(\V) = n\).
  We define the \textbf{determinant} of \(\T\), denoted \(\det(\T)\), as follows:
  Choose any ordered basis \(\beta\) for \(\V\) over \(\F\), and define \(\det(\T) = \det([\T]_{\beta})\).
  \begin{enumerate}
    \item Prove that the preceding definition is independent of the choice of an ordered basis for \(\V\) over \(\F\).
          That is, prove that if \(\beta\) and \(\gamma\) are two ordered bases for \(\V\) over \(\F\), then \(\det([\T]_{\beta}) = \det([\T]_{\gamma})\).
    \item Prove that \(\T\) is invertible iff \(\det(\T) \neq 0\).
    \item Prove that if \(\T\) is invertible, then \(\det(\T^{-1}) = [\det(\T)]^{-1}\).
    \item Prove that if \(\U\) is also a linear operator on \(\V\), then \(\det(\T \U) = \det(\T) \cdot \det(\U)\).
    \item Prove that \(\det(\T - \lambda \IT[\V]) = \det([\T]_{\beta} - \lambda I_n)\) for any scalar \(\lambda\) and any ordered basis \(\beta\) for \(\V\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:5.1.7}(a)]
  We have
  \begin{align*}
    \det([\T]_{\beta}) & = \det([\IT[\V]]_{\gamma}^{\beta} [\T]_{\gamma} [\IT[\V]]_{\beta}^{\gamma})             &  & \text{(by \cref{2.23})}         \\
                       & = \det([\IT[\V]]_{\gamma}^{\beta}) \det([\T]_{\gamma}) \det([\IT[\V]]_{\beta}^{\gamma}) &  & \text{(by \cref{4.7})}          \\
                       & = \det([\IT[\V]]_{\gamma}^{\beta}) \det([\IT[\V]]_{\beta}^{\gamma}) \det([\T]_{\gamma})                                      \\
                       & = \det([\IT[\V]]_{\gamma}^{\beta} [\IT[\V]]_{\beta}^{\gamma}) \det([\T]_{\gamma})       &  & \text{(by \cref{4.7})}          \\
                       & = \det(I_n) \det([\T]_{\gamma})                                                         &  & \text{(by \cref{ex:2.5.11}(b))} \\
                       & = \det([\T]_{\gamma}).                                                                  &  & \text{(by \cref{4.2.3})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:5.1.7}(b)]
  We have
  \begin{align*}
         & \T \text{ is invertible}                                            \\
    \iff & [\T]_{\beta} \text{ is invertible} &  & \text{(by \cref{2.4.6})}    \\
    \iff & \det([\T]_{\beta}) \neq 0          &  & \text{(by \cref{4.3.1})}    \\
    \iff & \det(\T) \neq 0.                   &  & \text{(by \cref{ex:5.1.7})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:5.1.7}(c)]
  We have
  \begin{align*}
             & \T \text{ is invertible}                                                                     \\
    \implies & \det(\T) \neq 0                                          &  & \text{(by \cref{ex:5.1.7}(b))} \\
    \implies & \det([\T]_{\beta}) \neq 0                                &  & \text{(by \cref{ex:5.1.7})}    \\
    \implies & \det(([\T]_{\beta})^{-1}) = \frac{1}{\det([\T]_{\beta})} &  & \text{(by \cref{4.3.1})}       \\
    \implies & \det([\T^{-1}]_{\beta}) = \frac{1}{\det([\T]_{\beta})}   &  & \text{(by \cref{2.4.6})}       \\
    \implies & \det(\T^{-1}) = \frac{1}{\det(\T)}.                      &  & \text{(by \cref{ex:5.1.7})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:5.1.7}(d)]
  We have
  \begin{align*}
    \det(\T \U) & = \det([\T \U]_{\beta})                 &  & \text{(by \cref{ex:5.1.7})} \\
                & = \det([\T]_{\beta} [\U]_{\beta})       &  & \text{(by \cref{2.3.3})}    \\
                & = \det([\T]_{\beta}) \det([\U]_{\beta}) &  & \text{(by \cref{4.7})}      \\
                & = \det(\T) \det(\U).                    &  & \text{(by \cref{ex:5.1.7})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:5.1.7}(e)]
  We have
  \begin{align*}
    \det(\T - \lambda \IT[\V]) & = \det([\T - \lambda \IT[\V]]_{\beta})           &  & \text{(by \cref{ex:5.1.7})} \\
                               & = \det([\T]_{\beta} - \lambda [\IT[\V]]_{\beta}) &  & \text{(by \cref{2.8})}      \\
                               & = \det([\T]_{\beta} - \lambda I_n).              &  & \text{(by \cref{2.12}(d))}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:5.1.8}
  \begin{enumerate}
    \item Prove that a linear operator \(\T\) on a finite-dimensional vector space \(\V\) over \(\F\) is invertible iff zero is not an eigenvalue of \(\T\).
    \item Let \(\T\) be an invertible linear operator.
          Prove that a scalar \(\lambda\) is an eigenvalue of \(\T\) iff \(\lambda^{-1}\) is an eigenvalue of \(\T^{-1}\).
    \item State and prove results analogous to (a) and (b) for matrices.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:5.1.8}(a)]
  We have
  \begin{align*}
         & \T \text{ is invertible}                                               \\
    \iff & \ns{\T} = \set{\zv}                    &  & \text{(by \cref{2.4,2.5})} \\
    \iff & \ns{\T - 0 \IT[\V]} = \set{\zv}        &  & \text{(by \cref{2.1.9})}   \\
    \iff & 0 \text{ is not an eigenvalue of } \T. &  & \text{(by \cref{5.4})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:5.1.8}(b)]
  We have
  \begin{align*}
         & \lambda \text{ is an eigenvalue of } \T                                                    \\
    \iff & \exists v \in \V : \T(v) = \lambda v                   &  & \text{(by \cref{5.1.2})}       \\
    \iff & \exists v \in \V : \T^{-1}(\T(v)) = \T^{-1}(\lambda v)                                     \\
    \iff & \exists v \in \V : v = \lambda \T^{-1}(v)              &  & \text{(by \cref{2.17})}        \\
    \iff & \exists v \in \V : \T^{-1}(v) = \lambda^{-1} v         &  & \text{(by \cref{ex:5.1.8}(a))} \\
    \iff & \lambda^{-1} \text{ is an eigenvalue of } \T^{-1}.     &  & \text{(by \cref{5.1.2})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:5.1.8}(c)]
  First we claim that if \(A \in \ms{n}{n}{\F}\), then \(A\) is invertible iff \(0\) is not an eigenvalue of \(A\).
  This is true since
  \begin{align*}
         & A \text{ is invertible}                                                     \\
    \iff & \L_A \text{ is invertible}              &  & \text{(by \cref{2.4.7})}       \\
    \iff & 0 \text{ is not an eigenvalue of } \L_A &  & \text{(by \cref{ex:5.1.8}(a))} \\
    \iff & 0 \text{ is not an eigenvalue of } A.   &  & \text{(by \cref{5.1.2})}
  \end{align*}

  Now we claim that if \(A \in \ms{n}{n}{\F}\), then \(\lambda\) is an eigenvalue of \(A\) iff \(\lambda^{-1}\) is an eigenvalue of \(A^{-1}\).
  This is true since
  \begin{align*}
         & \lambda \text{ is an eigenvalue of } A                                                    \\
    \iff & \lambda \text{ is an eigenvalue of } \L_A             &  & \text{(by \cref{5.1.2})}       \\
    \iff & \lambda^{-1} \text{ is an eigenvalue of } \L_A^{-1}   &  & \text{(by \cref{ex:5.1.8}(b))} \\
    \iff & \lambda^{-1} \text{ is an eigenvalue of } \L_{A^{-1}} &  & \text{(by \cref{2.4.7})}       \\
    \iff & \lambda^{-1} \text{ is an eigenvalue of } A^{-1}.     &  & \text{(by \cref{5.1.2})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:5.1.12}
  \begin{enumerate}
    \item Prove that similar matrices have the same characteristic polynomial.
    \item Show that the definition of the characteristic polynomial of a linear operator on a finite-dimensional vector space \(\V\) over \(\F\) is independent of the choice of basis for \(\V\) over \(\F\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:5.1.12}(a)]
  Let \(A, B \in \ms{n}{n}{\F}\) such that \(A, B\) are similar.
  By \cref{2.5.4} there exists some \(Q \in \ms{n}{n}{\F}\) such that \(B = Q^{-1} A Q\).
  Then we have
  \begin{align*}
    \det(A - t I_n) & = \det(I_n) \det(A - t I_n)            &  & \text{(by \cref{4.2.3})} \\
                    & = \det(Q^{-1} Q) \det(A - t I_n)       &  & \text{(by \cref{2.4.3})} \\
                    & = \det(Q^{-1}) \det(Q) \det(A - t I_n) &  & \text{(by \cref{4.7})}   \\
                    & = \det(Q^{-1}) \det(A - t I_n) \det(Q)                               \\
                    & = \det(Q^{-1} (A - t I_n) Q)           &  & \text{(by \cref{4.7})}   \\
                    & = \det(Q^{-1} A Q - t Q^{-1} I_n Q)    &  & \text{(by \cref{2.3.5})} \\
                    & = \det(B - t I_n).                     &  & \text{(by \cref{2.4.3})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:5.1.12}(b)]
  Let \(\beta, \beta'\) be ordered basis for \(\V\) over \(\F\).
  By \cref{2.23} we know that \([\T]_{\beta}\) and \([\T]_{\beta'}\) are similar.
  Thus by \cref{ex:5.1.12}(a) we have \(\det([\T]_{\beta} - t I_n) = \det([\T]_{\beta'} - t I_n)\).
  We conclude that the definition of the characteristic polynomial of a linear operator on a finite-dimensional vector space \(\V\) over \(\F\) is independent of the choice of basis for \(\V\) over \(\F\).
\end{proof}
