\section{The Rank of a Matrix and Matrix Inverses}\label{sec:3.2}

\begin{defn}\label{3.2.1}
  If \(A \in \MS\), we define the \textbf{rank} of \(A\), denoted \(\rk{A}\), to be the rank of the linear transformation \(\L_A : \vs{F}^n \to \vs{F}^m\).
\end{defn}

\begin{cor}\label{3.2.2}
  An \(n \times n\) matrix is invertible iff its rank is \(n\).
\end{cor}

\begin{proof}[\pf{3.2.2}]
  We have
  \begin{align*}
         & A \in \ms{n}{n}{\F} \text{ is invertible}                               \\
    \iff & \L_A \text{ is invertible}                &  & \text{(by \cref{2.4.7})} \\
    \iff & \rk{\L_A} = \dim(\vs{F}^n) = n            &  & \text{(by \cref{2.4.2})} \\
    \iff & \rk{A} = n.                               &  & \text{(by \cref{3.2.1})}
  \end{align*}
\end{proof}

\begin{note}
  Every matrix \(A\) is the matrix representation of the linear transformation \(\L_A\) with respect to the appropriate standard ordered bases.
  Thus the rank of the linear transformation \(\L_A\) is the same as the rank of one of its matrix representations, namely, \(A\).
  \cref{3.3} extends this fact to any matrix representation of any linear transformation defined on finite-dimensional vector spaces.
\end{note}

\begin{thm}\label{3.3}
  Let \(\T : \V \to \W\) be a linear transformation between finite-dimensional vector spaces, and let \(\beta\) and \(\gamma\) be ordered bases for \(\V\) and \(\W\), respectively.
  Then \(\rk{\T} = \rk{[\T]_{\beta}^{\gamma}}\).
\end{thm}

\begin{proof}[\pf{3.3}]
  This is a restatement of \cref{ex:2.4.20}.
\end{proof}

\begin{note}
  Now that the problem of finding the rank of a linear transformation has been reduced to the problem of finding the rank of a matrix, we need a result that allows us to perform rank-preserving operations on matrices.
  \cref{3.4} and \cref{3.2.3} tell us how to do this.
\end{note}

\begin{thm}\label{3.4}
  Let \(A \in \MS\).
  If \(P \in \ms{m}{m}{\F}\) and \(Q \in \ms{n}{n}{\F}\) are invertible, then
  \begin{enumerate}
    \item \(\rk{AQ} = \rk{A}\).
    \item \(\rk{PA} = \rk{A}\).
    \item \(\rk{PAQ} = \rk{A}\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{3.4}]
  First observe that
  \begin{align*}
    \rg{\L_{AQ}} & = \rg{\L_A \L_Q}       &  & \text{(by \cref{2.15}(e))} \\
                 & = \L_A \L_Q(\vs{F}^n)  &  & \text{(by \cref{2.1.10})}  \\
                 & = \L_A(\L_Q(\vs{F}^n))                                 \\
                 & = \L_A(\vs{F}^n)       &  & \text{(by \cref{2.4.7})}   \\
                 & = \rg{\L_A}            &  & \text{(by \cref{2.1.10})}
  \end{align*}
  since \(\L_Q\) is onto.
  Therefore
  \begin{align*}
    \rk{AQ} & = \dim(\rg{\L_{AQ}}) &  & \text{(by \cref{3.2.1})}      \\
            & = \dim(\rg{\L_A})    &  & \text{(from the proof above)} \\
            & = \rk{A}.            &  & \text{(by \cref{3.2.1})}
  \end{align*}
  This establishes (a).
  To establish (b), observe that
  \begin{align*}
    \rk{PA} & = \dim(\rg{\L_{PA}})         &  & \text{(by \cref{3.2.1})}        \\
            & = \dim(\rg{\L_P \L_A})       &  & \text{(by \cref{2.15}(e))}      \\
            & = \dim(\L_P(\L_A(\vs{F}^n))) &  & \text{(by \cref{2.1.10})}       \\
            & = \dim(\L_A(\vs{F}^n))       &  & \text{(by \cref{ex:2.4.17}(b))} \\
            & = \dim(\rg{\L_A})            &  & \text{(by \cref{2.1.10})}       \\
            & = \rk{A}.                    &  & \text{(by \cref{3.2.1})}
  \end{align*}
  Finally, applying (a) and (b), we have
  \[
    \rk{PAQ} = \rk{PA} = \rk{A}.
  \]
\end{proof}

\begin{cor}\label{3.2.3}
  Elementary row and column operations on a matrix are rank-preserving.
\end{cor}

\begin{proof}[\pf{3.2.3}]
  If \(B\) is obtained from a matrix \(A\) by an elementary row (column) operation, then by \cref{3.1} there exists an elementary matrix \(E\) such that \(B = EA\) (\(B = AE\)).
  By \cref{3.2} \(E\) is invertible, and hence \(\rk{B} = \rk{A}\) by \cref{3.4}.
\end{proof}

\begin{note}
  Now that we have a class of matrix operations that preserve rank, we need a way of examining a transformed matrix to ascertain its rank.
  \cref{3.5} is the first of several in this direction.
\end{note}

\begin{thm}\label{3.5}
  The rank of any matrix equals the maximum number of its linearly independent columns;
  that is, the rank of a matrix is the dimension of the subspace generated by its columns.
\end{thm}

\begin{proof}[\pf{3.5}]
  For any \(A \in \MS\),
  \[
    \rk{A} = \rk{\L_A} = \dim(\rg{\L_A}).
  \]
  Let \(\beta\) be the standard ordered basis for \(\vs{F}^n\) over \(\F\).
  Then \(\beta\) spans \(\vs{F}^n\) and hence, by \cref{2.2},
  \[
    \rg{\L_A} = \spn{\L_A(\beta)} = \spn{\set{\L_A(e_1), \dots, \L_A(e_n)}}.
  \]
  But, for any \(j\), we have seen in \cref{2.13}(b) that \(\L_A(e_j) = A e_j = a_j\), where \(a_j\) is the \(j\)th column of \(A\).
  Hence
  \[
    \rg{\L_A} = \spn{\set{\seq{a}{1,,n}}}.
  \]
  Thus
  \[
    \rk{A} = \dim(\rg{\L_A}) = \dim(\spn{\set{\seq{a}{1,,n}}}).
  \]
\end{proof}

\begin{note}
  To compute the rank of a matrix \(A\), it is frequently useful to postpone the use of \cref{3.5} until \(A\) has been suitably modified by means of appropriate elementary row and column operations so that the number of linearly independent columns is obvious.
  \cref{3.2.3} guarantees that the rank of the modified matrix is the same as the rank of \(A\).
  One such modification of \(A\) can be obtained by using elementary row and column operations to introduce zero entries.
\end{note}

\begin{thm}\label{3.6}
  Let \(A \in \MS\) be of rank \(r\).
  Then \(r \leq m\), \(r \leq n\), and, by means of a finite number of elementary row and column operations, \(A\) can be transformed into the matrix
  \[
    D = \begin{pmatrix}
      I_r   & \zm_1 \\
      \zm_2 & \zm_3
    \end{pmatrix},
  \]
  where \(\seq{\zm}{1,2,3}\) are zero matrices.
  Thus \(D_{i i} = 1\) for \(i \leq r\) and \(D_{i j} = 0\) otherwise.
\end{thm}

\begin{proof}[\pf{3.6}]
  If \(A\) is the zero matrix, \(r = 0\) by \cref{ex:3.2.3}.
  In this case, the conclusion follows with \(D = A\).

  Now suppose that \(A \neq \zm\) and \(r = \rk{A}\);
  then \(r > 0\).
  The proof is by mathematical induction on \(m\), the number of rows of \(A\).

  Suppose that \(m = 1\).
  By means of at most one type 1 column operation and at most one type 2 column operation, \(A\) can be transformed into a matrix with a \(1\) in the \(1,1\) position.
  By means of at most \(n - 1\) type 3 column operations, this matrix can in turn be transformed into the matrix
  \[
    D = \begin{pmatrix}
      1 & 0 & \cdots & 0
    \end{pmatrix}.
  \]
  Note that there is one linearly independent column in \(D\).
  So \(\rk{D} = \rk{A} = 1\) by \cref{3.2.3} and by \cref{3.5}.
  Thus the theorem is established for \(m = 1\).

  Next assume that the theorem holds for any matrix with at most \(m - 1\) rows (for some \(m > 1\)).
  We must prove that the theorem holds for any matrix with \(m\) rows.

  Suppose that \(A \in \MS\).
  If \(n = 1\), \cref{3.6} can be established in a manner analogous to that for \(m = 1\)
  (see \cref{ex:3.2.10}).

  We now suppose that \(n > 1\).
  Since \(A \neq \zm\), \(A_{i j} \neq 0\) for some \(i, j\).
  By means of at most one elementary row and at most one elementary column operation (each of type 1), we can move the nonzero entry to the \(1,1\) position.
  By means of at most one additional type 2 operation, we can assure a \(1\) in the \(1,1\) position.
  By means of at most \(m - 1\) type 3 row operations and at most \(n - 1\) type 3 column operations, we can eliminate all nonzero entries in the first row and the first column with the exception of the \(1\) in the \(1,1\) position.

  Thus, with a finite number of elementary operations, \(A\) can be transformed into a matrix
  \[
    B = \pa{\begin{array}{c|ccc}
        1      & 0 & \cdots & 0 \\
        \hline
        0      &   &        &   \\
        \vdots &   & B'     &   \\
        0      &   &        &
      \end{array}},
  \]
  where \(B' \in \ms{(m - 1)}{(n - 1)}{\F}\).

  By \cref{ex:3.2.11}, \(B'\) has rank one less than \(B\).
  Since \(\rk{A} = \rk{B} = r\), \(\rk{B'} = r - 1\).
  Therefore \(r - 1 \leq m - 1\) and \(r - 1 \leq n - 1\) by the induction hypothesis.
  Hence \(r \leq m\) and \(r \leq n\).

  Also by the induction hypothesis, \(B'\) can be transformed by a finite number of elementary row and column operations into the \((m-1) \times (n-1)\) matrix \(D'\) such that
  \[
    D' = \begin{pmatrix}
      I_{r - 1} & \zm_4 \\
      \zm_5     & \zm_6
    \end{pmatrix},
  \]
  where \(\zm_4, \zm_5, \zm_6\) are zero matrices.
  That is, \(D'\) consists of all zeros except for its first \(r - 1\) diagonal entries, which are ones.
  Let
  \[
    D = \pa{\begin{array}{c|ccc}
        1      & 0 & \cdots & 0 \\
        \hline
        0      &   &        &   \\
        \vdots &   & D'     &   \\
        0      &   &        &
      \end{array}}.
  \]
  We see that the theorem now follows once we show that \(D\) can be obtained from \(B\) by means of a finite number of elementary row and column operations.
  However this follows by repeated applications of \cref{ex:3.2.12}.

  Thus, since \(A\) can be transformed into \(B\) and \(B\) can be transformed into \(D\), each by a finite number of elementary operations, \(A\) can be transformed into \(D\) by a finite number of elementary operations.

  Finally, since \(D'\) contains ones as its first \(r - 1\) diagonal entries, \(D\) contains ones as its first \(r\) diagonal entries and zeros elsewhere.
  This establishes the theorem.
\end{proof}

\begin{cor}\label{3.2.4}
  Let \(A \in \MS\) be of rank \(r\).
  Then there exist invertible matrices \(B \in \ms{m}{m}{\F}\) and \(C \in \ms{n}{n}{\F}\) such that \(D = BAC\), where
  \[
    D = \begin{pmatrix}
      I_r   & \zm_1 \\
      \zm_2 & \zm_3
    \end{pmatrix}
  \]
  is the \(m \times n\) matrix in which \(\seq{\zm}{1,2,3}\) are zero matrices.
\end{cor}

\begin{proof}[\pf{3.2.4}]
  By \cref{3.6} \(A\) can be transformed by means of a finite number of elementary row and column operations into the matrix \(D\).
  We can appeal to \cref{3.1} each time we perform an elementary operation.
  Thus there exist elementary \(m \times m\) matrices \(\seq{E}{1,,p}\) and elementary \(n \times n\) matrices \(\seq{G}{1,,q}\) such that
  \[
    D = E_p E_{p - 1} \cdots E_2 E_1 A G_1 G_2 \cdots G_q.
  \]
  By \cref{3.2} each \(E_j\) and \(G_j\) is invertible.
  Let \(B = E_p E_{p - 1} \cdots E_1\) and \(C = G_1 G_2 \cdots G_q\).
  Then \(B\) and \(C\) are invertible by \cref{ex:2.4.4} and \(D = BAC\).
\end{proof}

\exercisesection

\setcounter{ex}{2}
\begin{ex}\label{ex:3.2.3}
  Prove that for any \(A \in \MS\), \(\rk{A} = 0\) iff \(A\) is the zero matrix.
\end{ex}

\begin{proof}[\pf{ex:3.2.3}]
  We have
  \begin{align*}
         & \rk{A} = 0                                                                         \\
    \iff & \rk{\L_A} = 0                                      &  & \text{(by \cref{3.2.1})}   \\
    \iff & \dim(\rg{\L_A}) = \dim(\L_A(\vs{F}^n)) = 0         &  & \text{(by \cref{2.1.12})}  \\
    \iff & \L_A(\vs{F}^n) = \set{\zv}                         &  & \text{(by \cref{1.11})}    \\
    \iff & \forall x \in \vs{F}^n, \L_A(x) = Ax = \zv = \zm x                                 \\
    \iff & A = \zm.                                           &  & \text{(by \cref{2.15}(b))}
  \end{align*}
\end{proof}

\setcounter{ex}{9}
\begin{ex}\label{ex:3.2.10}
  Prove \cref{3.6} for the case that \(A \in \ms{m}{1}{\F}\).
\end{ex}

\begin{proof}[\pf{ex:3.2.10}]
  By means of at most one type 1 row operation and at most one type 2 row operation, \(A\) can be transformed into a matrix with a \(1\) in the \(1,1\) position.
  By means of at most \(m - 1\) type 3 row operations, this matrix can in turn be transformed into the matrix
  \[
    D = \begin{pmatrix}
      1      \\
      0      \\
      \vdots \\
      0
    \end{pmatrix}.
  \]
  Note that there is one linearly independent row in \(D\).
  So \(\rk{D} = \rk{A} = 1\) by \cref{3.2.3} and by \cref{3.5}.
  Thus \cref{3.6} is established for \(A \in \ms{m}{1}{\F}\).
\end{proof}

\begin{ex}\label{ex:3.2.11}
  Let
  \[
    B = \pa{\begin{array}{c|ccc}
        1      & 0 & \cdots & 0 \\
        \hline
        0      &   &        &   \\
        \vdots &   & B'     &   \\
        0      &   &        &
      \end{array}},
  \]
  where \(B'\) is an \(m \times n\) submatrix of \(B\).
  Prove that if \(\rk{B} = r\), then \(\rk{B'} = r - 1\).
\end{ex}

\begin{proof}[\pf{ex:3.2.11}]
  For each \(i \in \set{1, \dots, n + 1}\), let \(b_i\) denote the \(i\)th column of \(B\).
  We claim that \(b_1 \notin \spn{\set{\seq{b}{2,,n+1}}}\).
  If not, then there exist \(\seq{a}{2,,n+1} \in \F\) such that
  \[
    \sum_{i = 2}^{n + 1} a_i b_i = b_1.
  \]
  But \(B_{1 i} = 0\) for all \(i \in \set{2, \dots, n + 1}\) implies
  \[
    \sum_{i = 2}^{n + 1} a_i B_{1 i} = 0 = B_{1 1} = 1,
  \]
  a contradiction.
  Thus \(b_1 \notin \spn{\set{\seq{b}{2,,n+1}}}\).
  Let \(\beta\) be a maximum linearly independent subset of \(\set{\seq{b}{2,,n+1}}\).
  Such maximality can be achieved since the set \(\set{\seq{b}{2,,n+1}}\) is finite.
  By \cref{1.7} we know that \(\beta \cup \set{b_1}\) is also linearly independent.
  By \cref{3.5} this means \(\rk{B} = \#(\beta \cup \set{b_1}) = 1 + \#(\beta)\).
  If we can show that \(\#(\beta) = \rk{B'}\), then we are done.
  For each \(i \in \set{1, \dots, n}\), let \(c_i\) be the \(i\)th column of \(B'\).
  Clearly we have
  \[
    \forall i \in \set{1, \dots, n}, b_{i + 1} = \begin{pmatrix}
      0           \\
      (c_i)_{1 1} \\
      \vdots      \\
      (c_i)_{1 m}
    \end{pmatrix}.
  \]
  Let \(\gamma\) be a maximum linearly independent subset of \(\set{\seq{c}{1,,n}}\).
  Again such maximality can be achieved since the set \(\set{\seq{c}{1,,n}}\) is finite.
  We claim that \(\#(\gamma) = \#(\beta)\).
  First let \(\beta = \set{\seq{v}{1,,k}}\) (note that \(0 \leq k \leq n\)).
  Since
  \begin{align*}
             & \sum_{i = 1}^k d_i v_i = \zv \iff \forall i \in \set{1, \dots, k}, d_i = 0           \\
    \implies & \sum_{i = 1}^k d_i \begin{pmatrix}
                                    0           \\
                                    (v_i)_{1 2} \\
                                    \vdots      \\
                                    (v_i)_{1 (m + 1)}
                                  \end{pmatrix} = \zv \iff \forall i \in \set{1, \dots, k}, d_i = 0 \\
    \implies & \sum_{i = 1}^k d_i \begin{pmatrix}
                                    (v_i)_{1 2} \\
                                    \vdots      \\
                                    (v_i)_{1 (m + 1)}
                                  \end{pmatrix} = \zv \iff \forall i \in \set{1, \dots, k}, d_i = 0
  \end{align*}
  and \(\gamma\) is a maximum linearly independent subset of \(\set{\seq{c}{1,,n}}\), we know that \(\#(\beta) \leq \#(\gamma)\).
  Now let \(\gamma = \set{\seq{w}{1,,p}}\).
  Since
  \begin{align*}
             & \sum_{i = 1}^p d_i w_i = \zv \iff \forall i \in \set{1, \dots, p}, d_i = 0           \\
    \implies & \sum_{i = 1}^p d_i \begin{pmatrix}
                                    (w_i)_{1 1} \\
                                    \vdots      \\
                                    (w_i)_{1 m}
                                  \end{pmatrix} = \zv \iff \forall i \in \set{1, \dots, p}, d_i = 0 \\
    \implies & \sum_{i = 1}^p d_i \begin{pmatrix}
                                    0           \\
                                    (w_i)_{1 1} \\
                                    \vdots      \\
                                    (w_i)_{1 m}
                                  \end{pmatrix} = \zv \iff \forall i \in \set{1, \dots, p}, d_i = 0 \\
  \end{align*}
  and \(\beta\) is a maximum linearly independent subset of \(\set{\seq{b}{2,,n+1}}\), we know that \(\#(\beta) \geq \#(\gamma)\).
  Thus we have \(\#(\beta) = \#(\gamma)\).
\end{proof}

\begin{ex}\label{ex:3.2.12}
  Let \(B', D' \in \MS\), and let \(B, D \in \ms{(m + 1)}{(n + 1)}{\F}\) defined by
  \[
    B = \pa{\begin{array}{c|ccc}
        1      & 0 & \cdots & 0 \\
        \hline
        0      &   &        &   \\
        \vdots &   & B'     &   \\
        0      &   &        &
      \end{array}} \quad \text{and} \quad D = \pa{\begin{array}{c|ccc}
        1      & 0 & \cdots & 0 \\
        \hline
        0      &   &        &   \\
        \vdots &   & D'     &   \\
        0      &   &        &
      \end{array}}.
  \]
  Prove that if \(B'\) can be transformed into \(D'\) by an elementary row (column) operation, then \(B\) can be transformed into \(D\) by an elementary row (column) operation.
\end{ex}

\begin{proof}[\pf{ex:3.2.12}]
  First suppose that \(D'\) can be obtained from \(B'\) by a elementary row operation.
  By \cref{3.1} there exists an elementary matrix \(E' \in \ms{m}{m}{\F}\) such that
  \[
    D' = E' B'.
  \]
  If we define \(E \in \ms{(m + 1)}{(m + 1)}{\F}\) such that
  \[
    E = \pa{\begin{array}{c|ccc}
        1      & 0 & \cdots & 0 \\
        \hline
        0      &   &        &   \\
        \vdots &   & E'     &   \\
        0      &   &        &
      \end{array}}
  \]
  then \(E\) is also an elementary matrix.
  Thus
  \begin{align*}
     & \forall (p, q) \in \set{1, \dots, m + 1} \times \set{1, \dots, n + 1},                                                           \\
     & (E B)_{p q} = \sum_{k = 1}^{m + 1} E_{p k} B_{k q}                                                 &  & \text{(by \cref{2.3.1})} \\
     & = \begin{dcases}
           \sum_{k = 1}^{m + 1} \delta_{1 k} \delta_{k 1}         & \text{if } (p = 1) \land (q = 1)       \\
           \sum_{k = 1}^{m + 1} \delta_{1 k} B_{k q}              & \text{if } (p = 1) \land (q \neq 1)    \\
           \sum_{k = 1}^{m + 1} E_{p k} \delta_{k 1}              & \text{if } (p \neq 1) \land (q = 1)    \\
           E_{p 1} B_{1 q} + \sum_{k = 2}^{m + 1} E_{p k} B_{k q} & \text{if } (p \neq 1) \land (q \neq 1)
         \end{dcases} &  & \text{(by \cref{2.3.4})}                                \\
     & = \begin{dcases}
           1                                    & \text{if } (p = 1) \land (q = 1)       \\
           0                                    & \text{if } (p = 1) \land (q \neq 1)    \\
           0                                    & \text{if } (p \neq 1) \land (q = 1)    \\
           \sum_{k = 2}^{m + 1} E_{p k} B_{k q} & \text{if } (p \neq 1) \land (q \neq 1)
         \end{dcases}                   &  & \text{(by \cref{2.3.4})}                                                  \\
     & = \begin{dcases}
           1                                                & \text{if } (p = 1) \land (q = 1)       \\
           0                                                & \text{if } (p = 1) \land (q \neq 1)    \\
           0                                                & \text{if } (p \neq 1) \land (q = 1)    \\
           \sum_{k = 1}^m (E')_{(p - 1) k} (B')_{k (q - 1)} & \text{if } (p \neq 1) \land (q \neq 1)
         \end{dcases}                                      \\
     & = \begin{dcases}
           1                         & \text{if } (p = 1) \land (q = 1)       \\
           0                         & \text{if } (p = 1) \land (q \neq 1)    \\
           0                         & \text{if } (p \neq 1) \land (q = 1)    \\
           (E' B')_{(p - 1) (q - 1)} & \text{if } (p \neq 1) \land (q \neq 1)
         \end{dcases}                              &  & \text{(by \cref{2.3.1})}                                                        \\
     & = \begin{dcases}
           1                      & \text{if } (p = 1) \land (q = 1)       \\
           0                      & \text{if } (p = 1) \land (q \neq 1)    \\
           0                      & \text{if } (p \neq 1) \land (q = 1)    \\
           (D')_{(p - 1) (q - 1)} & \text{if } (p \neq 1) \land (q \neq 1)
         \end{dcases}                                                                \\
     & = D_{p q}
  \end{align*}
  and we have \(D = EB\).
  By \cref{3.1} this means \(D\) can be obtained from \(B\) by a elementary row operation.
  Using similar argument we can prove the case for elementary column operation.
\end{proof}
