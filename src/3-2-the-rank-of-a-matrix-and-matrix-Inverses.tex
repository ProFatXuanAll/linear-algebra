\section{The Rank of a Matrix and Matrix Inverses}\label{sec:3.2}

\begin{defn}\label{3.2.1}
  If \(A \in \MS\), we define the \textbf{rank} of \(A\), denoted \(\rk{A}\), to be the rank of the linear transformation \(\L_A : \vs{F}^n \to \vs{F}^m\).
\end{defn}

\begin{cor}\label{3.2.2}
  An \(n \times n\) matrix is invertible iff its rank is \(n\).
\end{cor}

\begin{proof}[\pf{3.2.2}]
  We have
  \begin{align*}
         & A \in \ms{n}{n}{\F} \text{ is invertible}                               \\
    \iff & \L_A \text{ is invertible}                &  & \text{(by \cref{2.4.7})} \\
    \iff & \rk{\L_A} = \dim(\vs{F}^n) = n            &  & \text{(by \cref{2.4.2})} \\
    \iff & \rk{A} = n.                               &  & \text{(by \cref{3.2.1})}
  \end{align*}
\end{proof}

\begin{note}
  Every matrix \(A\) is the matrix representation of the linear transformation \(\L_A\) with respect to the appropriate standard ordered bases.
  Thus the rank of the linear transformation \(\L_A\) is the same as the rank of one of its matrix representations, namely, \(A\).
  \cref{3.3} extends this fact to any matrix representation of any linear transformation defined on finite-dimensional vector spaces.
\end{note}

\begin{thm}\label{3.3}
  Let \(\T : \V \to \W\) be a linear transformation between finite-dimensional vector spaces, and let \(\beta\) and \(\gamma\) be ordered bases for \(\V\) and \(\W\), respectively.
  Then \(\rk{\T} = \rk{[\T]_{\beta}^{\gamma}}\).
\end{thm}

\begin{proof}[\pf{3.3}]
  This is a restatement of \cref{ex:2.4.20}.
\end{proof}

\begin{note}
  Now that the problem of finding the rank of a linear transformation has been reduced to the problem of finding the rank of a matrix, we need a result that allows us to perform rank-preserving operations on matrices.
  \cref{3.4} and \cref{3.2.3} tell us how to do this.
\end{note}

\begin{thm}\label{3.4}
  Let \(A \in \MS\).
  If \(P \in \ms{m}{m}{\F}\) and \(Q \in \ms{n}{n}{\F}\) are invertible, then
  \begin{enumerate}
    \item \(\rk{AQ} = \rk{A}\).
    \item \(\rk{PA} = \rk{A}\).
    \item \(\rk{PAQ} = \rk{A}\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{3.4}]
  First observe that
  \begin{align*}
    \rg{\L_{AQ}} & = \rg{\L_A \L_Q}       &  & \text{(by \cref{2.15}(e))} \\
                 & = \L_A \L_Q(\vs{F}^n)  &  & \text{(by \cref{2.1.10})}  \\
                 & = \L_A(\L_Q(\vs{F}^n))                                 \\
                 & = \L_A(\vs{F}^n)       &  & \text{(by \cref{2.4.7})}   \\
                 & = \rg{\L_A}            &  & \text{(by \cref{2.1.10})}
  \end{align*}
  since \(\L_Q\) is onto.
  Therefore
  \begin{align*}
    \rk{AQ} & = \dim(\rg{\L_{AQ}}) &  & \text{(by \cref{3.2.1})}      \\
            & = \dim(\rg{\L_A})    &  & \text{(from the proof above)} \\
            & = \rk{A}.            &  & \text{(by \cref{3.2.1})}
  \end{align*}
  This establishes (a).
  To establish (b), observe that
  \begin{align*}
    \rk{PA} & = \dim(\rg{\L_{PA}})         &  & \text{(by \cref{3.2.1})}        \\
            & = \dim(\rg{\L_P \L_A})       &  & \text{(by \cref{2.15}(e))}      \\
            & = \dim(\L_P(\L_A(\vs{F}^n))) &  & \text{(by \cref{2.1.10})}       \\
            & = \dim(\L_A(\vs{F}^n))       &  & \text{(by \cref{ex:2.4.17}(b))} \\
            & = \dim(\rg{\L_A})            &  & \text{(by \cref{2.1.10})}       \\
            & = \rk{A}.                    &  & \text{(by \cref{3.2.1})}
  \end{align*}
  Finally, applying (a) and (b), we have
  \[
    \rk{PAQ} = \rk{PA} = \rk{A}.
  \]
\end{proof}

\begin{cor}\label{3.2.3}
  Elementary row and column operations on a matrix are rank-preserving.
\end{cor}

\begin{proof}[\pf{3.2.3}]
  If \(B\) is obtained from a matrix \(A\) by an elementary row (column) operation, then by \cref{3.1} there exists an elementary matrix \(E\) such that \(B = EA\) (\(B = AE\)).
  By \cref{3.2} \(E\) is invertible, and hence \(\rk{B} = \rk{A}\) by \cref{3.4}.
\end{proof}

\begin{note}
  Now that we have a class of matrix operations that preserve rank, we need a way of examining a transformed matrix to ascertain its rank.
  \cref{3.5} is the first of several in this direction.
\end{note}

\begin{thm}\label{3.5}
  The rank of any matrix equals the maximum number of its linearly independent columns;
  that is, the rank of a matrix is the dimension of the subspace generated by its columns.
\end{thm}

\begin{proof}[\pf{3.5}]
  For any \(A \in \MS\),
  \[
    \rk{A} = \rk{\L_A} = \dim(\rg{\L_A}).
  \]
  Let \(\beta\) be the standard ordered basis for \(\vs{F}^n\) over \(\F\).
  Then \(\beta\) spans \(\vs{F}^n\) and hence, by \cref{2.2},
  \[
    \rg{\L_A} = \spn{\L_A(\beta)} = \spn{\set{\L_A(e_1), \dots, \L_A(e_n)}}.
  \]
  But, for any \(j\), we have seen in \cref{2.13}(b) that \(\L_A(e_j) = A e_j = a_j\), where \(a_j\) is the \(j\)th column of \(A\).
  Hence
  \[
    \rg{\L_A} = \spn{\set{\seq{a}{1,,n}}}.
  \]
  Thus
  \[
    \rk{A} = \dim(\rg{\L_A}) = \dim(\spn{\set{\seq{a}{1,,n}}}).
  \]
\end{proof}

\begin{note}
  To compute the rank of a matrix \(A\), it is frequently useful to postpone the use of \cref{3.5} until \(A\) has been suitably modified by means of appropriate elementary row and column operations so that the number of linearly independent columns is obvious.
  \cref{3.2.3} guarantees that the rank of the modified matrix is the same as the rank of \(A\).
  One such modification of \(A\) can be obtained by using elementary row and column operations to introduce zero entries.
\end{note}

\begin{thm}\label{3.6}
  Let \(A \in \MS\) be of rank \(r\).
  Then \(r \leq m\), \(r \leq n\), and, by means of a finite number of elementary row and column operations, \(A\) can be transformed into the matrix
  \[
    D = \begin{pmatrix}
      I_r   & \zm_1 \\
      \zm_2 & \zm_3
    \end{pmatrix},
  \]
  where \(\seq{\zm}{1,2,3}\) are zero matrices.
  Thus \(D_{i i} = 1\) for \(i \leq r\) and \(D_{i j} = 0\) otherwise.
\end{thm}

\begin{proof}[\pf{3.6}]
  If \(A\) is the zero matrix, \(r = 0\) by \cref{ex:3.2.3}.
  In this case, the conclusion follows with \(D = A\).

  Now suppose that \(A \neq \zm\) and \(r = \rk{A}\);
  then \(r > 0\).
  The proof is by mathematical induction on \(m\), the number of rows of \(A\).

  Suppose that \(m = 1\).
  By means of at most one type 1 column operation and at most one type 2 column operation, \(A\) can be transformed into a matrix with a \(1\) in the \(1,1\) position.
  By means of at most \(n - 1\) type 3 column operations, this matrix can in turn be transformed into the matrix
  \[
    D = \begin{pmatrix}
      1 & 0 & \cdots & 0
    \end{pmatrix}.
  \]
  Note that there is one linearly independent column in \(D\).
  So \(\rk{D} = \rk{A} = 1\) by \cref{3.2.3} and by \cref{3.5}.
  Thus the theorem is established for \(m = 1\).

  Next assume that the theorem holds for any matrix with at most \(m - 1\) rows (for some \(m > 1\)).
  We must prove that the theorem holds for any matrix with \(m\) rows.

  Suppose that \(A \in \MS\).
  If \(n = 1\), \cref{3.6} can be established in a manner analogous to that for \(m = 1\)
  (see \cref{ex:3.2.10}).

  We now suppose that \(n > 1\).
  Since \(A \neq \zm\), \(A_{i j} \neq 0\) for some \(i, j\).
  By means of at most one elementary row and at most one elementary column operation (each of type 1), we can move the nonzero entry to the \(1,1\) position.
  By means of at most one additional type 2 operation, we can assure a \(1\) in the \(1,1\) position.
  By means of at most \(m - 1\) type 3 row operations and at most \(n - 1\) type 3 column operations, we can eliminate all nonzero entries in the first row and the first column with the exception of the \(1\) in the \(1,1\) position.

  Thus, with a finite number of elementary operations, \(A\) can be transformed into a matrix
  \[
    B = \pa{\begin{array}{c|ccc}
        1      & 0 & \cdots & 0 \\
        \hline
        0      &   &        &   \\
        \vdots &   & B'     &   \\
        0      &   &        &
      \end{array}},
  \]
  where \(B' \in \ms{(m - 1)}{(n - 1)}{\F}\).

  By \cref{ex:3.2.11}, \(B'\) has rank one less than \(B\).
  Since \(\rk{A} = \rk{B} = r\), \(\rk{B'} = r - 1\).
  Therefore \(r - 1 \leq m - 1\) and \(r - 1 \leq n - 1\) by the induction hypothesis.
  Hence \(r \leq m\) and \(r \leq n\).

  Also by the induction hypothesis, \(B'\) can be transformed by a finite number of elementary row and column operations into the \((m-1) \times (n-1)\) matrix \(D'\) such that
  \[
    D' = \begin{pmatrix}
      I_{r - 1} & \zm_4 \\
      \zm_5     & \zm_6
    \end{pmatrix},
  \]
  where \(\zm_4, \zm_5, \zm_6\) are zero matrices.
  That is, \(D'\) consists of all zeros except for its first \(r - 1\) diagonal entries, which are ones.
  Let
  \[
    D = \pa{\begin{array}{c|ccc}
        1      & 0 & \cdots & 0 \\
        \hline
        0      &   &        &   \\
        \vdots &   & D'     &   \\
        0      &   &        &
      \end{array}}.
  \]
  We see that the theorem now follows once we show that \(D\) can be obtained from \(B\) by means of a finite number of elementary row and column operations.
  However this follows by repeated applications of \cref{ex:3.2.12}.

  Thus, since \(A\) can be transformed into \(B\) and \(B\) can be transformed into \(D\), each by a finite number of elementary operations, \(A\) can be transformed into \(D\) by a finite number of elementary operations.

  Finally, since \(D'\) contains ones as its first \(r - 1\) diagonal entries, \(D\) contains ones as its first \(r\) diagonal entries and zeros elsewhere.
  This establishes the theorem.
\end{proof}

\begin{cor}\label{3.2.4}
  Let \(A \in \MS\) be of rank \(r\).
  Then there exist invertible matrices \(B \in \ms{m}{m}{\F}\) and \(C \in \ms{n}{n}{\F}\) such that \(D = BAC\), where
  \[
    D = \begin{pmatrix}
      I_r   & \zm_1 \\
      \zm_2 & \zm_3
    \end{pmatrix}
  \]
  is the \(m \times n\) matrix in which \(\seq{\zm}{1,2,3}\) are zero matrices.
\end{cor}

\begin{proof}[\pf{3.2.4}]
  By \cref{3.6} \(A\) can be transformed by means of a finite number of elementary row and column operations into the matrix \(D\).
  We can appeal to \cref{3.1} each time we perform an elementary operation.
  Thus there exist elementary \(m \times m\) matrices \(\seq{E}{1,,p}\) and elementary \(n \times n\) matrices \(\seq{G}{1,,q}\) such that
  \[
    D = E_p E_{p - 1} \cdots E_2 E_1 A G_1 G_2 \cdots G_q.
  \]
  By \cref{3.2} each \(E_j\) and \(G_j\) is invertible.
  Let \(B = E_p E_{p - 1} \cdots E_1\) and \(C = G_1 G_2 \cdots G_q\).
  Then \(B\) and \(C\) are invertible by \cref{ex:2.4.4} and \(D = BAC\).
\end{proof}

\begin{cor}\label{3.2.5}
  Let \(A \in \MS\).
  Then
  \begin{enumerate}
    \item \(\rk{\tp{A}} = \rk{A}\).
    \item The rank of any matrix equals the maximum number of its linearly independent rows;
          that is, the rank of a matrix is the dimension of the subspace generated by its rows.
    \item The rows and columns of any matrix generate subspaces of the same dimension, numerically equal to the rank of the matrix.
  \end{enumerate}
\end{cor}

\begin{proof}[\pf{3.2.5}(a)]
  By \cref{3.2.5} there exist invertible matrices \(B\) and \(C\) such that \(D = BAC\), where \(D\) satisfies the stated conditions of the corollary.
  Taking transposes, we have
  \[
    \tp{D} = \tp{\pa{BAC}} = \tp{C} \tp{A} \tp{B}.
  \]
  Since \(B\) and \(C\) are invertible, so are \(\tp{B}\) and \(\tp{C}\) by \cref{ex:2.4.5}.
  Hence by \cref{3.4},
  \[
    \rk{\tp{A}} = \rk{\tp{C} \tp{A} \tp{B}} = \rk{\tp{D}}.
  \]
  Suppose that \(r = \rk{A}\).
  Then \(\tp{D} \in \ms{n}{m}{\F}\) with the form of the matrix \(D\) in \cref{3.2.4}, and hence \(\rk{\tp{D}} = r\) by \cref{3.5}.
  Thus
  \[
    \rk{\tp{A}} = \rk{\tp{D}} = r = \rk{A}.
  \]
\end{proof}

\begin{proof}[\pf{3.2.5}(b)]
  We have
  \begin{align*}
     & \rk{A}                                                                                              \\
     & = \rk{\tp{A}}                                                      &  & \text{(by \cref{3.2.5}(a))} \\
     & = \text{maximum number of linearly independent columns of } \tp{A} &  & \text{(by \cref{3.5})}      \\
     & = \text{maximum number of linearly independent rows of } A.        &  & \text{(by \cref{1.3.3})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{3.2.5}(c)]
  This is the immediate consequence of \cref{3.2.5}(b) and \cref{1.6.8}.
\end{proof}

\begin{cor}\label{3.2.6}
  Every invertible matrix is a product of elementary matrices.
\end{cor}

\begin{proof}[\pf{3.2.6}]
  If \(A \in \ms{n}{n}{\F}\) is invertible, then \(\rk{A} = n\).
  Hence the matrix \(D\) in \cref{3.2.4} equals \(I_n\), and there exist invertible matrices \(B\) and \(C\) such that \(I_n = BAC\).

  As in the proof of \cref{3.2.4}, note that \(B = E_p E_{p - 1} \cdots E_1\) and \(C = G_1 G_2 \cdots G_q\), where the \(E_i\)'s and \(G_i\)'s are elementary matrices.
  Thus \(A = B^{-1} I_n C^{-1} = B^{-1} C^{-1}\), so that
  \[
    A = E_1^{-1} E_2^{-1} \cdots E_p^{-1} G_q^{-1} G_{q - 1}^{-1} \cdots G_1^{-1}.
  \]
  (this is done by \cref{ex:2.4.4}.)
  The inverses of elementary matrices are elementary matrices (see \cref{3.2}), however, and hence \(A\) is the product of elementary matrices.
\end{proof}

\begin{thm}\label{3.7}
  Let \(\V, \W, \vs{Z}\) be finite-dimensional vector spaces over \(\F\), let \(\T \in \ls(\V, \W)\) and \(\U \in \ls(\W, \vs{Z})\), and let \(A\) and \(B\) be matrices such that the product \(AB\) is defined.
  Then
  \begin{enumerate}
    \item \(\rk{\U \T} \leq \rk{\U}\).
    \item \(\rk{\U \T} \leq \rk{\T}\).
    \item \(\rk{AB} \leq \rk{A}\).
    \item \(\rk{AB} \leq \rk{B}\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{3.7}]
  We prove these items in the order:
  (a), (c), (d), and (b).
  \begin{description}
    \item[(a)]
      By \cref{2.1} we know that \(\rg{\T}\) is a subspace of \(\W\) over \(\F\).
      Hence
      \[
        \rg{\U \T} = \U \T(\V) = \U(\T(\V)) = \U(\rg{\T}) \subseteq \U(\W) = \rg{\U}.
      \]
      Thus
      \begin{align*}
        \rk{\U \T} & = \dim(\rg{\U \T}) &  & \text{(by \cref{2.1.12})} \\
                   & \leq \dim(\rg{\U}) &  & \text{(by \cref{1.11})}   \\
                   & = \rk{\U}.         &  & \text{(by \cref{2.1.12})}
      \end{align*}
    \item[(c)]
      \begin{align*}
        \rk{AB} & = \rk{\L_{AB}}   &  & \text{(by \cref{3.2.1})}   \\
                & = \rk{\L_A \L_B} &  & \text{(by \cref{2.15}(e))} \\
                & \leq \rk{\L_A}   &  & \text{(by \cref{3.7}(a))}  \\
                & = \rk{A}.        &  & \text{(by \cref{3.2.1})}
      \end{align*}
    \item[(d)]
      \begin{align*}
        \rk{AB} & = \rk{\tp{(AB)}}     &  & \text{(by \cref{3.2.5}(a))} \\
                & = \rk{\tp{B} \tp{A}} &  & \text{(by \cref{2.3.2})}    \\
                & \leq \rk{\tp{B}}     &  & \text{(by \cref{3.7}(c))}   \\
                & = \rk{B}.            &  & \text{(by \cref{3.2.5}(a))}
      \end{align*}
    \item[(b)]
      Let \(\alpha, \beta, \gamma\) be ordered bases for \(\V, \W, \vs{Z}\) over \(\F\), respectively, and let \(A' = [\U]_{\beta}^{\gamma}\) and \(B' = [\T]_{\alpha}^{\beta}\).
      Then \(A' B' = [\U \T]_{\alpha}^{\gamma}\) by \cref{2.11}.
      Hence
      \begin{align*}
        \rk{\U \T} & = \rk{A' B'} &  & \text{(by \cref{3.3})}    \\
                   & \leq \rk{B'} &  & \text{(by \cref{3.7}(d))} \\
                   & = \rk{\T}.   &  & \text{(by \cref{3.3})}
      \end{align*}
  \end{description}
\end{proof}

\begin{note}
  It is important to be able to compute the rank of any matrix.
  We can use \cref{3.2.3,3.5,3.6,3.2.5} to accomplish this goal.

  The object is to perform elementary row and column operations on a matrix to ``simplify'' it (so that the transformed matrix has many zero entries) to the point where a simple observation enables us to determine how many linearly independent rows or columns the matrix has, and thus to determine its rank.

  In summary, perform row and column operations until the matrix is simplified enough so that the maximum number of linearly independent rows or columns is obvious.
\end{note}

\begin{defn}\label{3.2.7}
  Let \(A \in \MS\) and \(B \in \ms{m}{p}{\F}\).
  By the \textbf{augmented matrix} \((A | B)\), we mean the \(m \times (n + p)\) matrix whose first \(n\) columns are the columns of \(A\), and whose last \(p\) columns are the columns of \(B\).
\end{defn}

\begin{cor}\label{3.2.8}
  If \(A \in \ms{n}{n}{\F}\) is invertible, then it is possible to transform the matrix \((A | I_n)\) into the matrix \((I_n | A^{-1})\) by means of a finite number of elementary row operations.
\end{cor}

\begin{proof}[\pf{3.2.8}]
  Let \(A \in \ms{n}{n}{\F}\) be invertible, and consider the \(n \times 2n\) augmented matrix \(C = (A | I_n)\).
  By \cref{ex:3.2.15} we have
  \[
    A^{-1} C = (A^{-1} A | A^{-1} I_n) = (I_n | A^{-1}).
  \]
  By \cref{3.2.6} \(A^{-1}\) is the product of elementary matrices, say \(A^{-1} = E_p E_{p - 1} \cdots E_1\).
  Thus the above equation becomes
  \[
    E_p E_{p - 1} \cdots E_1 (A | I_n) = A^{-1} C = (I_n | A^{-1}).
  \]
  Because multiplying a matrix on the left by an elementary matrix transforms the matrix by an elementary row operation (\cref{3.1}), we see that \cref{3.2.8} is true.
\end{proof}

\begin{cor}\label{3.2.9}
  If \(A \in \ms{n}{n}{\F}\) is invertible, and the matrix \((A | I_n)\) is transformed into a matrix of the form \((I_n | B)\) by means of a finite number of elementary row operations, then \(B = A^{-1}\).
\end{cor}

\begin{proof}[\pf{3.2.9}]
  Suppose that \(A\) is invertible and that, for some \(B \in \ms{n}{n}{\F}\), the matrix \((A | I_n)\) can be transformed into the matrix \((I_n | B)\) by a finite number of elementary row operations.
  Let \(\seq{E}{1,,p}\) be the elementary matrices associated with these elementary row operations as in \cref{3.1};
  then
  \[
    E_p E_{p - 1} \cdots E_1 (A | I_n) = (I_n | B).
  \]
  Letting \(M = E_p E_{p - 1} \cdots E_1\).
  From above equation we have
  \[
    (MA | M) = M (A | I_n) = (I_n | B).
  \]
  Hence \(MA = I_n\) and \(M = B\).
  It follows by \cref{ex:2.4.10}(b) that \(M = A^{-1}\).
  So \(B = A^{-1}\).
\end{proof}

\begin{cor}\label{3.2.10}
  If \(A \in \ms{n}{n}{\F}\) is not invertible, then any attempt to transform \((A | I_n)\) into a matrix of the form \((I_n | B)\) produces a row whose first \(n\) entries are zeros.
\end{cor}

\begin{proof}[\pf{3.2.10}]
  Let \(A \in \ms{n}{n}{\F}\) be non-invertible.
  Then \(\rk{A} < n\).
  Hence any attempt to transform \((A | I_n)\) into a matrix of the form \((I_n | B)\) by means of elementary row operations must fail because otherwise \(A\) can be transformed into \(I_n\) using the same row operations.
  This is impossible, however, because elementary row operations preserve rank
  (see \cref{3.2.3}).
  By \cref{3.2.5}(b) this means at least one row of \(A\) is a linear combination of other rows.
  Thus \(A\) can be transformed into a matrix with a row containing only zero entries with at most \(n - 1\) row operations of type 3.
\end{proof}

\begin{note}
  Being able to test for invertibility and compute the inverse of a matrix allows us, with the help of \cref{2.18,2.4.6,2.4.7}, to test for invertibility and compute the inverse of a linear transformation.
\end{note}

\exercisesection

\setcounter{ex}{2}
\begin{ex}\label{ex:3.2.3}
  Prove that for any \(A \in \MS\), \(\rk{A} = 0\) iff \(A\) is the zero matrix.
\end{ex}

\begin{proof}[\pf{ex:3.2.3}]
  We have
  \begin{align*}
         & \rk{A} = 0                                                                         \\
    \iff & \rk{\L_A} = 0                                      &  & \text{(by \cref{3.2.1})}   \\
    \iff & \dim(\rg{\L_A}) = \dim(\L_A(\vs{F}^n)) = 0         &  & \text{(by \cref{2.1.12})}  \\
    \iff & \L_A(\vs{F}^n) = \set{\zv}                         &  & \text{(by \cref{1.11})}    \\
    \iff & \forall x \in \vs{F}^n, \L_A(x) = Ax = \zv = \zm x                                 \\
    \iff & A = \zm.                                           &  & \text{(by \cref{2.15}(b))}
  \end{align*}
\end{proof}

\setcounter{ex}{7}
\begin{ex}\label{ex:3.2.8}
  Let \(A \in \MS\).
  Prove that if \(c \in \F \setminus \set{0}\), then \(\rk{cA} = \rk{A}\).
\end{ex}

\begin{proof}[\pf{ex:3.2.8}]
  For each \(i \in \set{1, \dots, m}\), we define \(E_i \in \ms{m}{m}{\F}\) to be the elementary row operation of type 2 such that the \(i\)th row of \(E_i\) is multiplied by \(c\).
  Then we have
  \begin{align*}
    cA & = c I_m A                            &  & \text{(by \cref{2.12}(c))} \\
       & = \begin{pmatrix}
             c      & 0      & \cdots & 0      \\
             0      & c      & \cdots & 0      \\
             \vdots & \vdots & \ddots & \vdots \\
             0      & 0      & \cdots & c
           \end{pmatrix} A                                  \\
       & = \seq[]{E}{1,,m} A.                 &  & \text{(by \cref{3.1})}
  \end{align*}
  Thus by \cref{3.2.3} we have \(\rk{cA} = \rk{\seq[]{E}{1,,m} A} = \rk{A}\).
\end{proof}

\setcounter{ex}{9}
\begin{ex}\label{ex:3.2.10}
  Prove \cref{3.6} for the case that \(A \in \ms{m}{1}{\F}\).
\end{ex}

\begin{proof}[\pf{ex:3.2.10}]
  By means of at most one type 1 row operation and at most one type 2 row operation, \(A\) can be transformed into a matrix with a \(1\) in the \(1,1\) position.
  By means of at most \(m - 1\) type 3 row operations, this matrix can in turn be transformed into the matrix
  \[
    D = \begin{pmatrix}
      1      \\
      0      \\
      \vdots \\
      0
    \end{pmatrix}.
  \]
  Note that there is one linearly independent row in \(D\).
  So \(\rk{D} = \rk{A} = 1\) by \cref{3.2.3} and by \cref{3.5}.
  Thus \cref{3.6} is established for \(A \in \ms{m}{1}{\F}\).
\end{proof}

\begin{ex}\label{ex:3.2.11}
  Let
  \[
    B = \pa{\begin{array}{c|ccc}
        1      & 0 & \cdots & 0 \\
        \hline
        0      &   &        &   \\
        \vdots &   & B'     &   \\
        0      &   &        &
      \end{array}},
  \]
  where \(B'\) is an \(m \times n\) submatrix of \(B\).
  Prove that if \(\rk{B} = r\), then \(\rk{B'} = r - 1\).
\end{ex}

\begin{proof}[\pf{ex:3.2.11}]
  For each \(i \in \set{1, \dots, n + 1}\), let \(b_i\) denote the \(i\)th column of \(B\).
  We claim that \(b_1 \notin \spn{\set{\seq{b}{2,,n+1}}}\).
  If not, then there exist \(\seq{a}{2,,n+1} \in \F\) such that
  \[
    \sum_{i = 2}^{n + 1} a_i b_i = b_1.
  \]
  But \(B_{1 i} = 0\) for all \(i \in \set{2, \dots, n + 1}\) implies
  \[
    \sum_{i = 2}^{n + 1} a_i B_{1 i} = 0 = B_{1 1} = 1,
  \]
  a contradiction.
  Thus \(b_1 \notin \spn{\set{\seq{b}{2,,n+1}}}\).
  Let \(\beta\) be a maximum linearly independent subset of \(\set{\seq{b}{2,,n+1}}\).
  Such maximality can be achieved since the set \(\set{\seq{b}{2,,n+1}}\) is finite.
  By \cref{1.7} we know that \(\beta \cup \set{b_1}\) is also linearly independent.
  By \cref{3.5} this means \(\rk{B} = \#(\beta \cup \set{b_1}) = 1 + \#(\beta)\).
  If we can show that \(\#(\beta) = \rk{B'}\), then we are done.
  For each \(i \in \set{1, \dots, n}\), let \(c_i\) be the \(i\)th column of \(B'\).
  Clearly we have
  \[
    \forall i \in \set{1, \dots, n}, b_{i + 1} = \begin{pmatrix}
      0           \\
      (c_i)_{1 1} \\
      \vdots      \\
      (c_i)_{1 m}
    \end{pmatrix}.
  \]
  Let \(\gamma\) be a maximum linearly independent subset of \(\set{\seq{c}{1,,n}}\).
  Again such maximality can be achieved since the set \(\set{\seq{c}{1,,n}}\) is finite.
  We claim that \(\#(\gamma) = \#(\beta)\).
  First let \(\beta = \set{\seq{v}{1,,k}}\) (note that \(0 \leq k \leq n\)).
  Since
  \begin{align*}
             & \sum_{i = 1}^k d_i v_i = \zv \iff \forall i \in \set{1, \dots, k}, d_i = 0           \\
    \implies & \sum_{i = 1}^k d_i \begin{pmatrix}
                                    0           \\
                                    (v_i)_{1 2} \\
                                    \vdots      \\
                                    (v_i)_{1 (m + 1)}
                                  \end{pmatrix} = \zv \iff \forall i \in \set{1, \dots, k}, d_i = 0 \\
    \implies & \sum_{i = 1}^k d_i \begin{pmatrix}
                                    (v_i)_{1 2} \\
                                    \vdots      \\
                                    (v_i)_{1 (m + 1)}
                                  \end{pmatrix} = \zv \iff \forall i \in \set{1, \dots, k}, d_i = 0
  \end{align*}
  and \(\gamma\) is a maximum linearly independent subset of \(\set{\seq{c}{1,,n}}\), we know that \(\#(\beta) \leq \#(\gamma)\).
  Now let \(\gamma = \set{\seq{w}{1,,p}}\).
  Since
  \begin{align*}
             & \sum_{i = 1}^p d_i w_i = \zv \iff \forall i \in \set{1, \dots, p}, d_i = 0           \\
    \implies & \sum_{i = 1}^p d_i \begin{pmatrix}
                                    (w_i)_{1 1} \\
                                    \vdots      \\
                                    (w_i)_{1 m}
                                  \end{pmatrix} = \zv \iff \forall i \in \set{1, \dots, p}, d_i = 0 \\
    \implies & \sum_{i = 1}^p d_i \begin{pmatrix}
                                    0           \\
                                    (w_i)_{1 1} \\
                                    \vdots      \\
                                    (w_i)_{1 m}
                                  \end{pmatrix} = \zv \iff \forall i \in \set{1, \dots, p}, d_i = 0 \\
  \end{align*}
  and \(\beta\) is a maximum linearly independent subset of \(\set{\seq{b}{2,,n+1}}\), we know that \(\#(\beta) \geq \#(\gamma)\).
  Thus we have \(\#(\beta) = \#(\gamma)\).
\end{proof}

\begin{ex}\label{ex:3.2.12}
  Let \(B', D' \in \MS\), and let \(B, D \in \ms{(m + 1)}{(n + 1)}{\F}\) defined by
  \[
    B = \pa{\begin{array}{c|ccc}
        1      & 0 & \cdots & 0 \\
        \hline
        0      &   &        &   \\
        \vdots &   & B'     &   \\
        0      &   &        &
      \end{array}} \quad \text{and} \quad D = \pa{\begin{array}{c|ccc}
        1      & 0 & \cdots & 0 \\
        \hline
        0      &   &        &   \\
        \vdots &   & D'     &   \\
        0      &   &        &
      \end{array}}.
  \]
  Prove that if \(B'\) can be transformed into \(D'\) by an elementary row (column) operation, then \(B\) can be transformed into \(D\) by an elementary row (column) operation.
\end{ex}

\begin{proof}[\pf{ex:3.2.12}]
  First suppose that \(D'\) can be obtained from \(B'\) by a elementary row operation.
  By \cref{3.1} there exists an elementary matrix \(E' \in \ms{m}{m}{\F}\) such that
  \[
    D' = E' B'.
  \]
  If we define \(E \in \ms{(m + 1)}{(m + 1)}{\F}\) such that
  \[
    E = \pa{\begin{array}{c|ccc}
        1      & 0 & \cdots & 0 \\
        \hline
        0      &   &        &   \\
        \vdots &   & E'     &   \\
        0      &   &        &
      \end{array}}
  \]
  then \(E\) is also an elementary matrix.
  Thus
  \begin{align*}
     & \forall (p, q) \in \set{1, \dots, m + 1} \times \set{1, \dots, n + 1},                                                           \\
     & (E B)_{p q} = \sum_{k = 1}^{m + 1} E_{p k} B_{k q}                                                 &  & \text{(by \cref{2.3.1})} \\
     & = \begin{dcases}
           \sum_{k = 1}^{m + 1} \delta_{1 k} \delta_{k 1}         & \text{if } (p = 1) \land (q = 1)       \\
           \sum_{k = 1}^{m + 1} \delta_{1 k} B_{k q}              & \text{if } (p = 1) \land (q \neq 1)    \\
           \sum_{k = 1}^{m + 1} E_{p k} \delta_{k 1}              & \text{if } (p \neq 1) \land (q = 1)    \\
           E_{p 1} B_{1 q} + \sum_{k = 2}^{m + 1} E_{p k} B_{k q} & \text{if } (p \neq 1) \land (q \neq 1)
         \end{dcases} &  & \text{(by \cref{2.3.4})}                                \\
     & = \begin{dcases}
           1                                    & \text{if } (p = 1) \land (q = 1)       \\
           0                                    & \text{if } (p = 1) \land (q \neq 1)    \\
           0                                    & \text{if } (p \neq 1) \land (q = 1)    \\
           \sum_{k = 2}^{m + 1} E_{p k} B_{k q} & \text{if } (p \neq 1) \land (q \neq 1)
         \end{dcases}                   &  & \text{(by \cref{2.3.4})}                                                  \\
     & = \begin{dcases}
           1                                                & \text{if } (p = 1) \land (q = 1)       \\
           0                                                & \text{if } (p = 1) \land (q \neq 1)    \\
           0                                                & \text{if } (p \neq 1) \land (q = 1)    \\
           \sum_{k = 1}^m (E')_{(p - 1) k} (B')_{k (q - 1)} & \text{if } (p \neq 1) \land (q \neq 1)
         \end{dcases}                                      \\
     & = \begin{dcases}
           1                         & \text{if } (p = 1) \land (q = 1)       \\
           0                         & \text{if } (p = 1) \land (q \neq 1)    \\
           0                         & \text{if } (p \neq 1) \land (q = 1)    \\
           (E' B')_{(p - 1) (q - 1)} & \text{if } (p \neq 1) \land (q \neq 1)
         \end{dcases}                              &  & \text{(by \cref{2.3.1})}                                                        \\
     & = \begin{dcases}
           1                      & \text{if } (p = 1) \land (q = 1)       \\
           0                      & \text{if } (p = 1) \land (q \neq 1)    \\
           0                      & \text{if } (p \neq 1) \land (q = 1)    \\
           (D')_{(p - 1) (q - 1)} & \text{if } (p \neq 1) \land (q \neq 1)
         \end{dcases}                                                                \\
     & = D_{p q}
  \end{align*}
  and we have \(D = EB\).
  By \cref{3.1} this means \(D\) can be obtained from \(B\) by a elementary row operation.
  Using similar argument we can prove the case for elementary column operation.
\end{proof}

\setcounter{ex}{13}
\begin{ex}\label{ex:3.2.14}
  Let \(\V, \W\) be vector spaces over \(\F\) and let \(\T, \U \in \ls(\V, \W)\).
  \begin{enumerate}
    \item Prove that \(\rg{\T + \U} \subseteq \rg{\T} + \rg{\U}\).
          (See \cref{1.3.10}.)
    \item Prove that if \(\W\) is finite-dimensional, then \(\rk{\T + \U} \leq \rk{\T} + \rk{\U}\).
    \item Deduce from (b) that \(\rk{A + B} \leq \rk{A} + \rk{B}\) for any \(A, B \in \MS\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:3.2.14}(a)]
  Let \(y \in \rg{\T + \U}\).
  Then there exists an \(x \in \V\) such that \((\T + \U)(x) = y\).
  Thus by \cref{1.3.10}
  \[
    y = (\T + \U)(x) = \T(x) + \U(x) \in \rg{\T} + \rg{\U}
  \]
  and therefore \(\rg{\T + \U} \subseteq \rg{\T} + \rg{\U}\).
\end{proof}

\begin{proof}[\pf{ex:3.2.14}(b)]
  We have
  \begin{align*}
    \rk{\T + \U} & = \dim(\rg{\T + \U})               &  & \text{(by \cref{2.1.12})}            \\
                 & \leq \dim(\rg{\T} + \rg{\U})       &  & \text{(by \cref{1.11,ex:1.3.23}(a))} \\
                 & \leq \dim(\rg{\T}) + \dim(\rg{\U}) &  & \text{(by \cref{ex:1.6.31}(b))}      \\
                 & = \rk{\T} + \rk{\U}.               &  & \text{(by \cref{2.1.12})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:3.2.14}(c)]
  We have
  \begin{align*}
    \rk{A + B} & = \rk{\L_{A + B}}          &  & \text{(by \cref{3.2.1})}        \\
               & = \rk{\L_A + \L_B}         &  & \text{(by \cref{2.15}(c))}      \\
               & \leq \rk{\L_A} + \rk{\L_B} &  & \text{(by \cref{ex:3.2.14}(b))} \\
               & = \rk{A} + \rk{B}.         &  & \text{(by \cref{3.2.1})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:3.2.15}
  Suppose that \(A \in \ms{n}{p}{\F}\) and \(B \in \ms{n}{q}{\F}\).
  Prove that \(M (A | B) = (MA | MB)\) for any \(M \in \MS\).
\end{ex}

\begin{proof}[\pf{ex:3.2.15}]
  Let \(i \in \set{1, \dots, m}\) and \(j \in \set{1, \dots, p + q}\).
  Then we have
  \begin{align*}
    (M (A | B))_{i j} & = \sum_{k = 1}^n M_{i k} (A | B)_{k j}                                        &  & \text{(by \cref{2.3.1})} \\
                      & = \begin{dcases}
                            \sum_{k = 1}^n M_{i k} A_{k j} & \text{if } j \in \set{1, \dots p}         \\
                            \sum_{k = 1}^n M_{i k} B_{k j} & \text{if } j \in \set{p + 1, \dots p + q}
                          \end{dcases} &  & \text{(by \cref{3.2.7})}                                \\
                      & = \begin{dcases}
                            (MA)_{i j} & \text{if } j \in \set{1, \dots p}         \\
                            (MB)_{i j} & \text{if } j \in \set{p + 1, \dots p + q}
                          \end{dcases}                     &  & \text{(by \cref{2.3.1})}                                            \\
                      & = (MA | MB)_{i j}                                                             &  & \text{(by \cref{3.2.7})}
  \end{align*}
  and thus by \cref{1.2.8} \(M (A | B) = (MA | MB)\).
\end{proof}

\setcounter{ex}{17}
\begin{ex}\label{ex:3.2.18}
  Let \(A \in \MS\) and \(B \in \ms{n}{p}{\F}\).
  Prove that \(AB\) can be written as a sum of \(n\) matrices of rank at most one.
\end{ex}

\begin{proof}[\pf{ex:3.2.18}]
  For each \(k \in \set{1, \dots, n}\), we define \(C_k \in \ms{m}{p}{\F}\) by setting \(C_{i j} = A_{i k} B_{k j}\).
  By \cref{2.3.1} we have
  \[
    \forall (i, j) \in \set{1, \dots, m} \times \set{1, \dots, p}, (AB)_{i j} = \sum_{k = 1}^n A_{i k} B_{k j} = \sum_{k = 1}^n (C_k)_{i j}.
  \]
  Thus we only need to show that \(\rk{C_k} \leq 1\) for all \(k \in \set{1, \dots, n}\).
  Let \(a_k\) be the \(k\)th column of \(A\).
  Since
  \[
    C_k = \begin{pmatrix}
      A_{1 k} B_{k 1} & A_{1 k} B_{k 2} & \cdots & A_{1 k} B_{k p} \\
      A_{2 k} B_{k 1} & A_{2 k} B_{k 2} & \cdots & A_{2 k} B_{k p} \\
      \vdots          & \vdots          & \ddots & \vdots          \\
      A_{m k} B_{k 1} & A_{m k} B_{k 2} & \cdots & A_{m k} B_{k p}
    \end{pmatrix} = \begin{pmatrix}
      B_{k 1} a_k & B_{k 2} a_k & \cdots & B_{k p} a_k
    \end{pmatrix},
  \]
  by \cref{3.5} we know that \(\rk{C_k} \leq 1\) for all \(k \in \set{1, \dots, n}\).
\end{proof}

\begin{ex}\label{ex:3.2.19}
  Let \(A \in \MS\) with rank \(m\) and \(B \in \ms{n}{p}{\F}\) with rank \(n\).
  Determine the rank of \(AB\).
  Justify your answer.
\end{ex}

\begin{proof}[\pf{ex:3.2.19}]
  We have
  \begin{align*}
             & \dim(\vs{F}^n) = n = \rk{B} = \rk{\L_B} = \dim(\L_B(\vs{F}^p)) &  & \text{(by \cref{3.2.1})}   \\
    \implies & \L_B(\vs{F}^p) = \vs{F}^n                                      &  & \text{(by \cref{1.11})}    \\
    \implies & \L_{AB}(\vs{F}^p) = \L_A(\L_B(\vs{F}^p)) = \L_A(\vs{F}^n)      &  & \text{(by \cref{2.15}(e))} \\
    \implies & \rk{AB} = \rk{\L_{AB}} = \rk{\L_A} = \rk{A} = m.               &  & \text{(by \cref{3.2.1})}
  \end{align*}
\end{proof}

\setcounter{ex}{20}
\begin{ex}\label{ex:3.2.21}
  Let \(A \in \MS\) with rank \(m\).
  Prove that there exists an \(B \in \ms{n}{m}{\F}\) such that \(AB = I_m\).
\end{ex}

\begin{proof}[\pf{ex:3.2.21}]
  By \cref{3.6} we know that \(m \leq n\).
  By \cref{3.5} we can use at most \(n - m\) elementary column operations of type 1 on \(A\) such that the first \(m\) columns of the resulting matrix, called it \(C\), is linearly independent.
  By \cref{3.1} there exist elementary column matrices \(\seq{E}{1,,p} \in \ms{n}{n}{\F}\) such that \(C = A \seq[]{E}{1,,p}\).
  Let \(D \in \ms{n}{m}{\F}\) be the matrix
  \[
    D = \begin{pmatrix}
      I_m   & \zm_1 \\
      \zm_2 & \zm_3
    \end{pmatrix}
  \]
  where \(\seq{\zm}{1,2,3}\) are zero matrices.
  We claim that for each \(i \in \set{1, \dots, m}\), the \(i\)th column of \(CD\) is also the \(i\)th column of \(C\).
  This is true since
  \begin{align*}
    \forall j \in \set{1, \dots, m}, (CD)_{i j} & = \sum_{k = 1}^n C_{i k} D_{k j}      &  & \text{(by \cref{2.3.1})}                           \\
                                                & = \sum_{k = 1}^m C_{i k} D_{k j}      &  & (\forall k \in \set{m + 1, \dots, n}, D_{k j} = 0) \\
                                                & = \sum_{k = 1}^m C_{i k} \delta_{k j} &  & \text{(by \cref{2.3.4})}                           \\
                                                & = C_{i j}.                            &  & \text{(by \cref{2.3.4})}
  \end{align*}
  Since the first \(m\) columns of \(C\) is linearly independent and \(CD \in \ms{m}{m}{\F}\), by \cref{3.5} we know that \(\rk{CD} = m\) and thus by \cref{3.2.2} \(CD\) is invertible.
  Then we have
  \[
    (A \seq[]{E}{1,,p} D) (CD)^{-1} = (CD) (CD)^{-1} = I_m.
  \]
  By setting \(B = \seq[]{E}{1,,p} D (CD)^{-1}\) we are done.
\end{proof}

\begin{ex}\label{ex:3.2.22}
  Let \(B \in \ms{n}{m}{\F}\) with rank \(m\).
  Prove that there exists an \(A \in \MS\) such that \(AB = I_m\).
\end{ex}

\begin{proof}[\pf{ex:3.2.22}]
  By \cref{3.6} we know that \(m \leq n\).
  By \cref{3.5} we can use at most \(n - m\) elementary row operations of type 1 on \(B\) such that the first \(m\) rows of the resulting matrix, called it \(C\), is linearly independent.
  By \cref{3.1} there exist elementary row matrices \(\seq{E}{1,,p} \in \ms{n}{n}{\F}\) such that \(C = \seq[]{E}{p,,1} B\).
  Let \(D \in \ms{m}{n}{\F}\) be the matrix
  \[
    D = \begin{pmatrix}
      I_m   & \zm_1 \\
      \zm_2 & \zm_3
    \end{pmatrix}
  \]
  where \(\seq{\zm}{1,2,3}\) are zero matrices.
  We claim that for each \(i \in \set{1, \dots, m}\), the \(i\)th row of \(DC\) is also the \(i\)th row of \(C\).
  This is true since
  \begin{align*}
    \forall j \in \set{1, \dots, m}, (DC)_{i j} & = \sum_{k = 1}^n D_{i k} C_{k j}      &  & \text{(by \cref{2.3.1})}                           \\
                                                & = \sum_{k = 1}^m D_{i k} C_{k j}      &  & (\forall k \in \set{m + 1, \dots, n}, D_{i k} = 0) \\
                                                & = \sum_{k = 1}^m \delta_{i k} C_{k j} &  & \text{(by \cref{2.3.4})}                           \\
                                                & = C_{i j}.                            &  & \text{(by \cref{2.3.4})}
  \end{align*}
  Since the first \(m\) rows of \(C\) is linearly independent and \(DC \in \ms{m}{m}{\F}\), by \cref{3.5} we know that \(\rk{DC} = m\) and thus by \cref{3.2.2} \(DC\) is invertible.
  Then we have
  \[
    (DC)^{-1} (D \seq[]{E}{p,,1} B) = (DC)^{-1} (DC) = I_m.
  \]
  By setting \(A = (DC)^{-1} D \seq[]{E}{p,,1}\) we are done.
\end{proof}
