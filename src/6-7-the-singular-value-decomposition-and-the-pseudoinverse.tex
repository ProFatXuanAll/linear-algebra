\section{The Singular Value Decomposition and the Pseudoinverse}\label{sec:6.7}

\begin{thm}[Singular Value Theorem for Linear Transformations]\label{6.26}
  Let \(\V\) and \(\W\) be finite-dimensional inner product spaces over \(\F\), and let \(\T \in \ls(\V, \W)\) be of rank \(r\).
  Then there exist orthonormal bases \(\set{\seq{v}{1,,n}}\) for \(\V\) over \(\F\) and \(\set{\seq{u}{1,,m}}\) for \(\W\) over \(\F\) and positive scalars \(\seq[\geq]{\sigma}{1,,r}\) such that
  \begin{equation}\label{eq:6.7.1}
    \T(v_i) = \begin{dcases}
      \sigma_i u_i & \text{if } i \in \set{1, \dots, r}     \\
      \zv          & \text{if } i \in \set{r + 1, \dots, n}
    \end{dcases}.
  \end{equation}
  Conversely, suppose that the preceding conditions are satisfied.
  Then for \(i \in \set{1, \dots, n}\), \(v_i\) is an eigenvector of \(\T^* \T\) with corresponding eigenvalue \(\sigma_i^2\) if \(i \in \set{1, \dots, r}\) and \(0\) if \(i \in \set{r + 1, \dots, n}\).
  Therefore the scalars \(\seq{\sigma}{1,,r}\) are uniquely determined by \(\T\).
\end{thm}

\begin{proof}[\pf{6.26}]
  Let \(\inn{\cdot, \cdot}_{\V}\) and \(\inn{\cdot, \cdot}_{\W}\) be inner products on \(\V\) and \(\W\), respectively.
  For each \(\T \in \ls(\V, \W)\), we denote the adjoint of \(\T\) with respect to \(\inn{\cdot, \cdot}_{\V}\) and \(\inn{\cdot, \cdot}_{\W}\) as \(\T^*\), i.e.,
  \[
    \forall (x, y) \in \V \times \W, \inn{\T(x), y}_{\W} = \inn{x, \T^*(y)}_{\V}.
  \]
  The existence and uniqueness of adjoint operator has been proved by \cref{ex:6.3.15}.

  We first establish the existence of the bases and scalars.
  By \cref{ex:6.4.18} and \cref{ex:6.3.15}(d), \(\T^* \T\) is a positive semidefinite linear operator of rank \(r\) on \(\V\);
  hence there is an orthonormal basis \(\set{\seq{v}{1,,n}}\) for \(\V\) over \(\F\) with respect to \(\inn{\cdot, \cdot}_{\V}\) consisting of eigenvectors of \(\T^* \T\) with corresponding eigenvalues \(\lambda_i\), where \(\seq[\geq]{\lambda}{1,,r} > 0\), and \(\lambda_i = 0\) for \(i \in \set{r + 1, \dots, n}\).
  For \(i \in \set{1, \dots, r}\), define \(\sigma_i = \sqrt{\lambda_i}\) and \(u_i = \frac{1}{\sigma_i} \T(v_i)\) (see \cref{ex:6.4.17}(a)).
  We show that \(\set{\seq{u}{1,,r}}\) is an orthonormal subset of \(\W\) with respect to \(\inn{\cdot, \cdot}_{\W}\).
  Suppose \(i, j \in \set{1, \dots, r}\).
  Then
  \begin{align*}
    \inn{u_i, u_j}_{\W} & = \inn{\frac{1}{\sigma_i} \T(v_i), \frac{1}{\sigma_j} \T(v_j)}_{\W}                        \\
                        & = \frac{1}{\sigma_i} \inn{\T(v_i), \frac{1}{\sigma_j} \T(v_j)}_{\W} &  & \by{6.1.1}[b]     \\
                        & = \frac{1}{\sigma_i \sigma_j} \inn{\T(v_i), \T(v_j)}_{\W}           &  & \by{6.1}[b]       \\
                        & = \frac{1}{\sigma_i \sigma_j} \inn{\T^*(\T(v_i)), v_j}_{\V}         &  & \by{ex:6.3.15}[a] \\
                        & = \frac{1}{\sigma_i \sigma_j} \inn{\lambda_i v_i, v_j}_{\V}         &  & \by{5.1.2}        \\
                        & = \frac{\sigma_i^2}{\sigma_i \sigma_j} \inn{v_i, v_j}_{\V}          &  & \by{6.1.1}[b]     \\
                        & = \frac{\sigma_i^2}{\sigma_i \sigma_j} \delta_{i j}                 &  & \by{6.1.12}       \\
                        & = \delta_{i j},                                                     &  & \by{2.3.4}
  \end{align*}
  and hence \(\set{\seq{u}{1,,r}}\) is orthonormal.
  By \cref{6.7}(a), this set extends to an orthonormal basis \(\set{\seq{u}{1,,r,,m}}\) for \(\W\) over \(\F\).
  Clearly \(\T(v_i) = \sigma_i u_i\) if \(i \in \set{1, \dots, r}\).
  If \(i \in \set{r + 1, \dots, n}\), then \(\T^* \T(v_i) = \zv\), and so \(\T(v_i) = \zv\) by \cref{ex:6.3.15}(d).

  To establish uniqueness, suppose that \(\set{\seq{v}{1,,n}}\), \(\set{\seq{u}{1,,m}}\), and \(\seq[\geq]{\sigma}{1,,r} > 0\) satisfy the properties stated in the first part of the theorem.
  Then for \(i \in \set{1, \dots, m}\) and \(j \in \set{1, \dots, n}\),
  \begin{align*}
    \inn{\T^*(u_i), v_j}_{\V} & = \inn{u_i, \T(v_j)}_{\W}                                           &  & \by{ex:6.3.15}[a] \\
                              & = \begin{dcases}
                                    \inn{u_i, \sigma_j u_j} & \text{if } j \in \set{1, \dots, r}     \\
                                    \inn{u_i, \zv}          & \text{if } j \in \set{r + 1, \dots, n}
                                  \end{dcases}                         \\
                              & = \begin{dcases}
                                    \sigma_i & \text{if } i = j \leq r \\
                                    0        & \text{otherwise}
                                  \end{dcases},                               &  & \by{6.1}[b,c]
  \end{align*}
  and hence by \cref{6.5}, for any \(i \in \set{1, \dots, m}\),
  \begin{equation}\label{eq:6.7.2}
    \T^*(u_i) = \sum_{j = 1}^n \inn{\T^*(u_i), v_j} v_j = \begin{dcases}
      \sigma_i v_i & \text{if } i = j \leq r \\
      \zv          & \text{otherwise}
    \end{dcases}.
  \end{equation}
  So for \(i \in \set{1, \dots, r}\),
  \[
    \T^* \T(v_i) = \T^*(\sigma_i u_i) = \sigma_i \T^*(u_i) = \sigma_i^2 v_i
  \]
  and \(\T^* \T(v_i) = \T^*(\zv) = \zv\) for \(i \in \set{r + 1, \dots, n}\).
  Therefore each \(v_i\) is an eigenvector of \(\T^* \T\) with corresponding eigenvalue \(\sigma_i^2\) if \(i \in \set{1, \dots, r}\) and \(0\) if \(i \in \set{r + 1, \dots, n}\).
\end{proof}

\begin{defn}\label{6.7.1}
  The unique scalars \(\seq{\sigma}{1,,r}\) in \cref{6.26} are called the \textbf{singular values} of \(\T\).
  If \(r\) is less than both \(m\) and \(n\), then the term singular value is extended to include \(\seq[=]{\sigma}{r+1,,k} = 0\), where \(k\) is the minimum of \(m\) and \(n\).
\end{defn}

\begin{note}
  Although the singular values of a linear transformation \(\T\) are uniquely determined by \(\T\), the orthonormal bases given in the statement of \cref{6.26} are not uniquely determined because there is more than one orthonormal basis of eigenvectors of \(\T^* \T\).

  In view of \cref{eq:6.7.2}, the singular values of a linear transformation \(\T \in \ls(\V, \W)\) and its adjoint \(\T^* \in \ls(\W, \V)\) are identical.
  Furthermore, the orthonormal bases for \(\V\) and \(\W\) given in \cref{6.26} are simply reversed for \(\T^*\).
\end{note}

\begin{defn}\label{6.7.2}
  Let \(A \in \ms\).
  We define the \textbf{singular values} of \(A\) to be the singular values of the linear transformation \(\L_A\).
\end{defn}

\begin{thm}[Singular Value Decomposition Theorem for Matrices]\label{6.27}
  Let \(A \in \ms\) be of rank \(r\) with the positive singular values \(\seq[\geq]{\sigma}{1,,r}\), and let \(\Sigma \in \ms\) defined by
  \[
    \Sigma_{i j} = \begin{dcases}
      \sigma_i & \text{if } i = j \leq r \\
      0        & \text{otherwise}
    \end{dcases}.
  \]
  Then there exists unitary matrices \(U \in \ms[m][m][\F]\) and \(\V \in \ms[n][n][\F]\) such that
  \[
    A = U \Sigma V^*.
  \]
\end{thm}

\begin{proof}[\pf{6.27}]
  Let \(\T = \L_A \in \ls(\vs{F}^n, \vs{F}^m)\).
  By \cref{6.26}, there exist orthonormal bases \(\beta = \set{\seq{v}{1,,n}}\) for \(\vs{F}^n\) over \(\F\) and \(\gamma = \set{\seq{u}{1,,m}}\) for \(\vs{F}^m\) over \(\F\) such that \(\T(v_i) = \sigma_i u_i\) for \(i \in \set{1, \dots, r}\) and \(\T(v_i) = \zv\) for \(i \in \set{r + 1, \dots, n}\).
  Let \(U \in \ms[m][m][\F]\) whose \(j\)th column is \(u_j\) for all \(j \in \set{1, \dots, m}\), and let \(V \in \ms[n][n][\F]\) whose \(j\)th column is \(v_j\) for all \(j \in \set{1, \dots, n}\).
  Note that both \(U\) and \(V\) are unitary matrices.

  By \cref{2.13}(a), the \(j\)th column of \(AV\) is \(A v_j = \sigma_j u_j\).
  Observe that the \(j\)th column of \(\Sigma\) is \(\sigma_j e_j\), where \(e_j\) is the \(j\)th standard vector of \(\vs{F}^m\).
  So by \cref{2.13}(a)(b), the \(j\)th column of \(U \Sigma\) is given by
  \[
    U(\sigma_j e_j) = \sigma_j (U e_j) = \sigma_j u_j.
  \]
  It follows that \(AV\) and \(U \Sigma\) are \(m \times n\) matrices whose corresponding columns are equal, and hence \(AV = U \Sigma\).
  Therefore \(A = A V V^* = U \Sigma V^*\).
\end{proof}

\begin{defn}\label{6.7.3}
  Let \(A \in \ms\) be of rank \(r\) with positive singular values \(\seq[\geq]{\sigma}{1,,r}\).
  A factorization \(A = U \Sigma V^*\) where \(U\) and \(V\) are unitary matrices and \(\Sigma \in \ms\) is defined as in \cref{6.27} is called a \textbf{singular value decomposition} of \(A\).
\end{defn}

\begin{note}
  In the proof of \cref{6.27}, the columns of \(V\) are the vectors in \(\beta\), and the columns of \(U\) are the vectors in \(\gamma\).
  Furthermore, the nonzero singular values of \(A\) are the same as those of \(\L_A\);
  hence they are the square roots of the nonzero eigenvalues of \(A^* A\) or of \(A A^*\).
  (See \cref{ex:6.7.9}.)
\end{note}

\begin{note}
  A singular value decomposition of a matrix can be used to factor a square matrix in a manner analogous to the factoring of a complex number as the product of a complex number of length \(1\) and a nonnegative number.
  In the case of matrices, the complex number of length \(1\) is replaced by a unitary matrix, and the nonnegative number is replaced by a positive semidefinite matrix.
\end{note}

\begin{thm}[Polar Decomposition]\label{6.28}
  For any square matrix \(A\), there exists a unitary matrix \(W\) and a positive semidefinite matrix \(P\) such that
  \[
    A = WP.
  \]
  Furthermore, if \(A\) is invertible, then the representation is unique.
\end{thm}

\begin{proof}[\pf{6.28}]
  By \cref{6.27}, there exist unitary matrices \(U\) and \(V\) and a diagonal matrix \(\Sigma\) with nonnegative diagonal entries such that \(A = U \Sigma V^*\).
  So
  \begin{align*}
    A & = U \Sigma \V^* \by{6.27}                 \\
      & = U I \Sigma \V^*         &  & \by{2.3.4} \\
      & = U V^* V \Sigma V^*      &  & \by{6.5.9} \\
      & = WP,
  \end{align*}
  where \(W = U V^*\) and \(P = V \Sigma V^*\).
  Since \(W\) is the product of unitary matrices, \(W\) is unitary (\cref{ex:6.5.3}), and since \(\Sigma\) is positive semidefinite (\cref{ex:6.4.17}(a)) and \(P\) is unitarily equivalent to \(\Sigma\), \(P\) is positive semidefinite by \cref{ex:6.5.14}.

  Now suppose that \(A\) is invertible and factors as the products
  \[
    A = WP = ZQ,
  \]
  where \(W\) and \(Z\) are unitary and \(P\) and \(Q\) are positive semidefinite.
  Since \(A\) is invertible, it follows that \(P\) and \(Q\) are positive definite and invertible \cref{ex:6.4.19}(c), and therefore \(Z^* W = Q P^{-1}\).
  Thus \(Q P^{-1}\) is unitary (\cref{ex:6.5.3}), and so
  \begin{align*}
    I & = (Q P^{-1})^* (Q P^{-1}) &  & \by{6.5.9}        \\
      & = (P^{-1})^* Q^* Q P^{-1} &  & \by{6.3.2}[c]     \\
      & = (P^{-1})^* Q^2 P^{-1}   &  & \by{6.4.11}       \\
      & = P^{-1} Q^2 P^{-1}.      &  & \by{ex:6.4.19}[c]
  \end{align*}
  Hence \(P^2 = Q^2\).
  Since both \(P\) and \(Q\) are positive definite, it follows that \(P = Q\) by \cref{ex:6.4.17}(d).
  Therefore \(W = Z\), and consequently the factorization is unique.
\end{proof}

\begin{defn}\label{6.7.4}
  The factorization of a square matrix \(A\) as \(WP\) where \(W\) is unitary and \(P\) is positive semidefinite, is called a \textbf{polar decomposition} of \(A\).
\end{defn}

\begin{prop}\label{6.7.5}
  Let \(\V\) and \(\W\) be finite-dimensional inner product spaces over the same field \(\F\), and let \(\T \in \ls(\V, \W)\).
  It is desirable to have a linear transformation from \(\W\) to \(\V\) that captures some of the essence of an inverse of \(\T\) even if \(\T\) is not invertible.
  A simple approach to this problem is to focus on the ``part'' of \(\T\) that is invertible, namely, the restriction of \(\T\) to \(\ns{\T}^{\perp}\).
  Let \(\lt{L} : \ns{\T}^{\perp} \to \rg{\T}\) be the linear transformation defined by \(\lt{L}(x) = \T(x)\) for all \(x \in \ns{\T}^{\perp}\).
  Then \(\lt{L}\) is invertible, and we can use the inverse of \(\lt{L}\) to construct a linear transformation from \(\W\) to \(\V\) that salvages some of the benefits of an inverse of \(\T\).
\end{prop}

\begin{proof}[\pf{6.7.5}]
  Let \(y \in \rg{\T}\).
  Then there exists an \(x \in \V\) such that \(\T(x) = y\).
  By \cref{ex:6.2.13}(d) we have \(\V = \ns{\T} \oplus \ns{\T}^{\perp}\), thus by \cref{6.6} there exists an unique tuple \(\tuple{x}{1,2} \in \ns{\T} \times \ns{\T}^{\perp}\) such that \(x = x_1 + x_2\).
  Then we have
  \begin{align*}
    y & = \T(x)                                 \\
      & = \T(x_1 + x_2)      &  & \by{6.6}      \\
      & = \T(x_1) + \T(x_2)  &  & \by{2.1.2}[a] \\
      & = \zv_{\W} + \T(x_2) &  & \by{2.1.10}   \\
      & = \T(x_2)                               \\
      & = \lt{L}(x_2).
  \end{align*}
  Thus \(\lt{L}\) is onto.
  Since
  \begin{align*}
    \dim(\ns{\T}^{\perp}) & = \dim(\V) - \dim(\ns{\T}) &  & \by{6.7}[c] \\
                          & = \dim(\V) - \nt{\T}       &  & \by{2.1.12} \\
                          & = \rk{\T},                 &  & \by{2.3}
  \end{align*}
  by \cref{2.5} we know that \(\lt{L}\) is one-to-one, and thus invertible.
\end{proof}

\begin{defn}\label{6.7.6}
  Let \(\V\) and \(\W\) be finite-dimensional inner product spaces over the same field \(\F\), and let \(\T \in \ls(\V, \W)\).
  Let \(\lt{L} \in \ls(\ns{\T}^{\perp}, \rg{\T})\) defined by \(\lt{L}(x) = \T(x)\) for all \(x \in \ns{\T}^{\perp}\).
  The \textbf{pseudoinverse} (or Moore-Penrose generalized inverse) of \(\T\), denoted by \(\T^{\dag}\), is defined as the unique linear transformation from \(\W\) to \(\V\) such that
  \[
    \T^{\dag}(y) = \begin{dcases}
      \lt{L}^{-1}(y) & \text{for } y \in \rg{\T}         \\
      \zv            & \text{for } y \in \rg{\T}^{\perp}
    \end{dcases}.
  \]
  The uniqueness of pseudoinverse is guaranteed by \cref{2.1.13}.
  The pseudoinverse of a linear transformation \(\T\) on a finite-dimensional inner product space exists even if \(\T\) is not invertible (see \cref{6.7.5}).
  Furthermore, if \(\T\) is invertible, then \(\T^{\dag} = \T^{-1}\) because \(\ns{\T}^{\perp} = \V\) (see \cref{2.4}), and \(\lt{L}\) (as just defined) coincides with \(\T\).
\end{defn}

\begin{eg}\label{6.7.7}
  Consider the zero transformation \(\zT \in \ls(\V, \W)\) between two finite-dimensional inner product spaces \(\V\) and \(\W\) over \(\F\).
  Then \(\rg{\zT} = \set{\zv}\), and therefore \(\zT^{\dag}\) is the zero transformation from \(\W\) to \(\V\).
\end{eg}

\begin{prop}\label{6.7.8}
  We can use the singular value theorem to describe the pseudoinverse of a linear transformation.
  Suppose that \(\V\) and \(\W\) are finite-dimensional vector spaces and \(\T \in \ls(\V, \W)\) is of rank \(r\).
  Let \(\set{\seq{v}{1,,n}}\) and \(\set{\seq{u}{1,,m}}\) be orthonormal bases for \(\V\) and \(\W\) over \(\F\), respectively, and let \(\seq[\geq]{\sigma}{1,,r}\) be the nonzero singular values of \(\T\) satisfying \cref{eq:6.7.1} in \cref{6.26}.
  Then \(\set{\seq{v}{1,,r}}\) is a basis for \(\ns{\T}^{\perp}\) over \(\F\), \(\set{\seq{v}{r+1,,n}}\) is a basis for \(\ns{\T}\) over \(\F\), \(\set{\seq{u}{1,,r}}\) is a basis for \(\rg{\T}\) over \(\F\), and \(\set{\seq{u}{r+1,,m}}\) is a basis for \(\rg{\T}^{\perp}\).
  Let \(\lt{L}\) be the restriction of \(\T\) to \(\ns{\T}^{\perp}\), as in the definition of pseudoinverse (\cref{6.7.6}).
  Then \(\lt{L}^{-1}(u_i) = \frac{1}{\sigma_i} v_i\) for \(i \in \set{1, \dots, r}\).
  Therefore
  \begin{equation}\label{eq:6.7.3}
    \T^{\dag}(u_i) = \begin{dcases}
      \frac{1}{\sigma_i} v_i & \text{if } i \in \set{1, \dots, r}     \\
      \zv                    & \text{if } i \in \set{r + 1, \dots, m}
    \end{dcases}.
  \end{equation}
\end{prop}

\begin{proof}[\pf{6.7.8}]
  By \cref{6.7}(c) and \cref{ex:6.2.13} we see that \(\set{\seq{v}{1,,r}}\) is a basis for \(\ns{\T}^{\perp}\) over \(\F\), \(\set{\seq{v}{r+1,,n}}\) is a basis for \(\ns{\T}\) over \(\F\), \(\set{\seq{u}{1,,r}}\) is a basis for \(\rg{\T}\) over \(\F\), and \(\set{\seq{u}{r+1,,m}}\) is a basis for \(\rg{\T}^{\perp}\).
  Then we have
  \begin{align*}
             & \forall x \in \ns{\T}^{\perp}, \lt{L}(x) = \T(x)                           &  & \by{6.7.6}                        \\
    \implies & \forall i \in \set{1, \dots, r}, \lt{L}(v_i) = \T(v_i) = \sigma_i u_i      &  & \by{6.26}                         \\
    \implies & \forall i \in \set{1, \dots, r}, \lt{L}^{-1}(u_i) = \frac{1}{\sigma_i} v_i &  & \by{2.17}                         \\
    \implies & \forall i \in \set{1, \dots, m}, \T^{\dag}(u_i) = \begin{dcases}
                                                                   \frac{1}{\sigma_i} v_i & \text{if } i \in \set{1, \dots, r}     \\
                                                                   \zv                    & \text{if } i \in \set{r + 1, \dots, m}
                                                                 \end{dcases}.         &  & \by{6.7.6}
  \end{align*}
\end{proof}

\begin{defn}\label{6.7.9}
  Let \(A \in \ms\).
  Then there exists a unique \(B \in \ms[n][m][\F]\) such that \((\L_A)^{\dag} : \vs{F}^m \to \vs{F}^n\) is equal to the left-multiplication transformation \(\L_B\) (see \cref{2.20}).
  We call \(B\) the \textbf{pseudoinverse} of \(A\) and denote it by \(B = A^{\dag}\).
  Thus
  \[
    (\L_A)^{\dag} = \L_{A^{\dag}}.
  \]
\end{defn}

\begin{thm}\label{6.29}
  Let \(A \in \ms\) be of rank \(r\) with a singular value decomposition \(A = U \Sigma V^*\) and nonzero singular values \(\seq[\geq]{\sigma}{1,,r}\).
  Let \(\Sigma^{\dag} \in \ms[n][m][\F]\) defined by
  \[
    \Sigma_{i j}^{\dag} = \begin{dcases}
      \frac{1}{\sigma_i} & \text{if } i = j \leq r \\
      0                  & \text{otherwise}
    \end{dcases}.
  \]
  Then \(A^{\dag} = V \Sigma^{\dag} U^*\), and this is a singular value decomposition of \(A^{\dag}\).
\end{thm}

\begin{proof}[\pf{6.29}]
  Let \(\beta\) and \(\gamma\) be the ordered bases whose vectors are the columns of \(V\) and \(U\), respectively.
  Then \(\beta\) and \(\gamma\) are orthonormal bases for \(\vs{F}^n\) and \(\vs{F}^m\) over \(\F\), respectively, and \cref{eq:6.7.1,eq:6.7.3} are satisfied for \(\T = \L_A\).
  Reversing the roles of \(\beta\) and \(\gamma\) in the proof of \cref{6.27}, we obtain the result.
  Note that \(\Sigma^{\dag}\) is actually the pseudoinverse of \(\Sigma\).
\end{proof}

\begin{note}
  Let \(A \in \ms\).
  Then for any \(b \in \vs{F}^m\), the matrix equation \(Ax = b\) is a system of linear equations, and so it either has no solutions, a unique solution, or infinitely many solutions.
  We know that the system has a unique solution for every \(b \in \vs{F}^m\) iff \(A\) is invertible, in which case the solution is given by \(A^{-1} b\).
  Furthermore, if \(A\) is invertible, then \(A^{-1} = A^{\dag}\), and so the solution can be written as \(x = A^{\dag} b\).
  If, on the other hand, \(A\) is not invertible or the system \(Ax = b\) is inconsistent, then \(A^{\dag} b\) still exists.
  We therefore pose the following question:
  In general, how is the vector \(A^{\dag} b\) related to the system of linear equations \(Ax = b\)?
  In order to answer this question, we need the following lemma.
\end{note}

\begin{lem}\label{6.7.10}
  Let \(\V\) and \(\W\) be finite-dimensional inner product spaces over \(\F\), and let \(\T \in \ls(\V, \W)\).
  Then
  \begin{enumerate}
    \item \(\T^{\dag} \T\) is the orthogonal projection of \(\V\) on \(\ns{\T}^{\perp}\).
    \item \(\T \T^{\dag}\) is the orthogonal projection of \(\W\) on \(\rg{\T}\).
  \end{enumerate}
\end{lem}

\begin{proof}[\pf{6.7.10}(a)]
  As in \cref{6.7.6}, we define \(\lt{L} \in \ls(\ns{\T}^{\perp}, \rg{\T})\) by \(\lt{L}(x) = \T(x)\) for all \(x \in \ns{\T}^{\perp}\).
  Observe that
  \begin{align*}
    \forall x \in \ns{\T}^{\perp}, (\T^{\dag} \T)(x) & = \T^{\dag}(\T(x))                       \\
                                                     & = \T^{\dag}(\lt{L}(x))   &  & \by{6.7.6} \\
                                                     & = \lt{L}^{-1}(\lt{L}(x)) &  & \by{6.7.6} \\
                                                     & = x,
  \end{align*}
  and
  \begin{align*}
    \forall x \in \ns{\T}, (\T^{\dag} \T)(x) & = \T^{\dag}(\T(x))                       \\
                                             & = \T^{\dag}(\zv_{\W}) &  & \by{2.1.10}   \\
                                             & = \zv_{\V}.           &  & \by{2.1.2}[a]
  \end{align*}
  Consequently \(\T^{\dag} \T\) is the orthogonal projection of \(\V\) on \(\ns{\T}^{\perp}\) (see \cref{6.6.1}).
\end{proof}

\begin{proof}[\pf{6.7.10}(b)]
  Continue the proof of \cref{6.7.10}(a), we have
  \begin{align*}
    \forall x \in \rg{\T}, (\T \T^{\dag})(x) & = \T(\T^{\dag}(x))                                                 \\
                                             & = \T(\lt{L}^{-1}(x))     &  & \by{6.7.6}                           \\
                                             & = \lt{L}(\lt{L}^{-1}(x)) &  & (\lt{L}^{-1}(x) \in \ns{\T}^{\perp}) \\
                                             & = x
  \end{align*}
  and
  \begin{align*}
    \forall x \in \rg{\T}^{\perp}, (\T \T^{\dag})(x) & = \T(\T^{\dag}(x))                    \\
                                                     & = \T(\zv_{\V})     &  & \by{6.7.6}    \\
                                                     & = \zv_{\W}.        &  & \by{2.1.2}[a]
  \end{align*}
  Thus by \cref{6.6.1} \(\T \T^{\dag}\) is the orthogonal projection of \(\W\) on \(\rg{\T}\).
\end{proof}

\begin{thm}\label{6.30}
  Consider the system of linear equations \(Ax = b\), where \(A \in \ms\) and \(b \in \vs{F}^m\).
  If \(z = A^{\dag} b\), then \(z\) has the following properties.
  \begin{enumerate}
    \item If \(Ax = b\) is consistent, then \(z\) is the unique solution to the system having minimum norm.
          That is, \(z\) is a solution to the system, and if \(y\) is any solution to the system, then \(\norm{z} \leq \norm{y}\) with equality iff \(z = y\).
    \item If \(Ax = b\) is inconsistent, then \(z\) is the unique best approximation to a solution having minimum norm.
          That is, \(\norm{Az - b} \leq \norm{Ay - b}\) for any \(y \in \vs{F}^n\), with equality iff \(Az = Ay\).
          Furthermore, if \(Az = Ay\), then \(\norm{z} \leq \norm{y}\) with equality iff \(z = y\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{6.30}(a)]
  Suppose that \(Ax = b\) is consistent, and let \(z = A^{\dag} b\).
  Observe that \(b \in \rg{\L_A}\), and therefore
  \begin{align*}
    Az & = A A^{\dag} b                             \\
       & = \L_A(\L_A^{\dag}(b)) &  & \by{2.3.8}     \\
       & = b.                   &  & \by{6.7.10}[b]
  \end{align*}
  Thus \(z\) is a solution to the system.
  Now suppose that \(y\) is any solution to the system.
  Then
  \begin{align*}
    \L_A^{\dag} \L_A(y) & = A^{\dag} A y &  & \by{2.3.8} \\
                        & = A^{\dag} b   &  & (Ay = b)   \\
                        & = z,
  \end{align*}
  and hence \(z\) is the orthogonal projection of \(y\) on \(\ns{\L_A}^{\perp}\) by \cref{6.7.10}(a).
  Therefore, by \cref{6.2.12}, we have that \(\norm{z} \leq \norm{y}\) with equality iff \(z = y\).
\end{proof}

\begin{proof}[\pf{6.30}(b)]
  Suppose that \(Ax = b\) is inconsistent.
  By \cref{6.7.10}, \(Az = A A^{\dag} b = \L_A \L_A^{\dag}(b) = b\) is the orthogonal projection of \(b\) on \(\rg{\L_A}\);
  therefore, by \cref{6.2.12}, \(Az\) is the vector in \(\rg{\L_A}\) nearest \(b\).
  That is, if \(Ay\) is any other vector in \(\rg{\L_A}\), then \(\norm{Az - b} \leq \norm{Ay - b}\) with equality iff \(Az = Ay\).

  Finally, suppose that \(y\) is any vector in \(\vs{F}^n\) such that \(Az = Ay = c\).
  Then
  \begin{align*}
    A^{\dag} c & = A^{\dag} A z                              \\
               & = A^{\dag} A A^{\dag} b                     \\
               & = A^{\dag} b            &  & \by{ex:6.7.23} \\
               & = z;
  \end{align*}
  hence we may apply \cref{6.30}(a) to the system \(Ax = c\) to conclude that \(\norm{z} \leq \norm{y}\) with equality iff \(z = y\).
\end{proof}

\begin{note}
  The vector \(z = A^{\dag} b\) in \cref{6.30} is the vector \(x_0\) described in \cref{6.12} that arises in the least squares application.
\end{note}

\exercisesection

\begin{ex}\label{ex:6.7.9}

\end{ex}

\begin{ex}\label{ex:6.7.23}

\end{ex}
