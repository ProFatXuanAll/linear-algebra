\section{The Singular Value Decomposition and the Pseudoinverse}\label{sec:6.7}

\begin{thm}[Singular Value Theorem for Linear Transformations]\label{6.26}
  Let \(\V\) and \(\W\) be finite-dimensional inner product spaces over \(\F\), and let \(\T \in \ls(\V, \W)\) be of rank \(r\).
  Then there exist orthonormal bases \(\set{\seq{v}{1,,n}}\) for \(\V\) over \(\F\) and \(\set{\seq{u}{1,,m}}\) for \(\W\) over \(\F\) and positive scalars \(\seq[\geq]{\sigma}{1,,r}\) such that
  \begin{equation}\label{eq:6.7.1}
    \T(v_i) = \begin{dcases}
      \sigma_i u_i & \text{if } i \in \set{1, \dots, r}     \\
      \zv_{\W}     & \text{if } i \in \set{r + 1, \dots, n}
    \end{dcases}.
  \end{equation}
  Conversely, suppose that the preceding conditions are satisfied.
  Then for \(i \in \set{1, \dots, n}\), \(v_i\) is an eigenvector of \(\T^* \T\) with corresponding eigenvalue \(\sigma_i^2\) if \(i \in \set{1, \dots, r}\) and \(0\) if \(i \in \set{r + 1, \dots, n}\).
  Therefore the scalars \(\seq{\sigma}{1,,r}\) are uniquely determined by \(\T\).
\end{thm}

\begin{proof}[\pf{6.26}]
  Let \(\inn{\cdot, \cdot}_{\V}\) and \(\inn{\cdot, \cdot}_{\W}\) be inner products on \(\V\) and \(\W\), respectively.
  For each \(\T \in \ls(\V, \W)\), we denote the adjoint of \(\T\) with respect to \(\inn{\cdot, \cdot}_{\V}\) and \(\inn{\cdot, \cdot}_{\W}\) as \(\T^*\), i.e.,
  \[
    \forall (x, y) \in \V \times \W, \inn{\T(x), y}_{\W} = \inn{x, \T^*(y)}_{\V}.
  \]
  The existence and uniqueness of adjoint operator has been proved by \cref{ex:6.3.15}.

  We first establish the existence of the bases and scalars.
  By \cref{ex:6.4.18} and \cref{ex:6.3.15}(d), \(\T^* \T\) is a positive semidefinite linear operator of rank \(r\) on \(\V\);
  hence there is an orthonormal basis \(\set{\seq{v}{1,,n}}\) for \(\V\) over \(\F\) with respect to \(\inn{\cdot, \cdot}_{\V}\) consisting of eigenvectors of \(\T^* \T\) with corresponding eigenvalues \(\lambda_i\), where \(\seq[\geq]{\lambda}{1,,r} > 0\), and \(\lambda_i = 0\) for \(i \in \set{r + 1, \dots, n}\) (see \cref{ex:6.4.17}(a)).
  For \(i \in \set{1, \dots, r}\), define \(\sigma_i = \sqrt{\lambda_i}\) and \(u_i = \frac{1}{\sigma_i} \T(v_i)\).
  We show that \(\set{\seq{u}{1,,r}}\) is an orthonormal subset of \(\W\) with respect to \(\inn{\cdot, \cdot}_{\W}\).
  Suppose \(i, j \in \set{1, \dots, r}\).
  Then
  \begin{align*}
    \inn{u_i, u_j}_{\W} & = \inn{\frac{1}{\sigma_i} \T(v_i), \frac{1}{\sigma_j} \T(v_j)}_{\W}                        \\
                        & = \frac{1}{\sigma_i} \inn{\T(v_i), \frac{1}{\sigma_j} \T(v_j)}_{\W} &  & \by{6.1.1}[b]     \\
                        & = \frac{1}{\sigma_i \sigma_j} \inn{\T(v_i), \T(v_j)}_{\W}           &  & \by{6.1}[b]       \\
                        & = \frac{1}{\sigma_i \sigma_j} \inn{\T^*(\T(v_i)), v_j}_{\V}         &  & \by{ex:6.3.15}[a] \\
                        & = \frac{1}{\sigma_i \sigma_j} \inn{\lambda_i v_i, v_j}_{\V}         &  & \by{5.1.2}        \\
                        & = \frac{\sigma_i^2}{\sigma_i \sigma_j} \inn{v_i, v_j}_{\V}          &  & \by{6.1.1}[b]     \\
                        & = \frac{\sigma_i^2}{\sigma_i \sigma_j} \delta_{i j}                 &  & \by{6.1.12}       \\
                        & = \delta_{i j},                                                     &  & \by{2.3.4}
  \end{align*}
  and hence \(\set{\seq{u}{1,,r}}\) is orthonormal.
  By \cref{6.7}(a), this set extends to an orthonormal basis \(\set{\seq{u}{1,,r,,m}}\) for \(\W\) over \(\F\).
  Clearly \(\T(v_i) = \sigma_i u_i\) if \(i \in \set{1, \dots, r}\).
  If \(i \in \set{r + 1, \dots, n}\), then \(\T^* \T(v_i) = \zv_{\V}\), and so \(\T(v_i) = \zv_{\W}\) by \cref{ex:6.3.15}(d).

  To establish uniqueness, suppose that \(\set{\seq{v}{1,,n}}\), \(\set{\seq{u}{1,,m}}\), and \(\seq[\geq]{\sigma}{1,,r} > 0\) satisfy the properties stated in the first part of the theorem.
  Then for \(i \in \set{1, \dots, m}\) and \(j \in \set{1, \dots, n}\),
  \begin{align*}
    \inn{\T^*(u_i), v_j}_{\V} & = \inn{u_i, \T(v_j)}_{\W}                                                            &  & \by{ex:6.3.15}[a] \\
                              & = \begin{dcases}
                                    \inn{u_i, \sigma_j u_j}_{\W} & \text{if } j \in \set{1, \dots, r}     \\
                                    \inn{u_i, \zv_{\W}}_{\W}     & \text{if } j \in \set{r + 1, \dots, n}
                                  \end{dcases}                       \\
                              & = \begin{dcases}
                                    \sigma_i & \text{if } i = j \leq r \\
                                    0        & \text{otherwise}
                                  \end{dcases},                                                &  & \by{6.1}[b,c]
  \end{align*}
  and hence by \cref{6.5}, for any \(i \in \set{1, \dots, m}\),
  \begin{equation}\label{eq:6.7.2}
    \T^*(u_i) = \sum_{j = 1}^n \inn{\T^*(u_i), v_j}_{\V} v_j = \begin{dcases}
      \sigma_i v_i & \text{if } i = j \leq r \\
      \zv_{\V}     & \text{otherwise}
    \end{dcases}.
  \end{equation}
  So for \(i \in \set{1, \dots, r}\),
  \[
    \T^* \T(v_i) = \T^*(\sigma_i u_i) = \sigma_i \T^*(u_i) = \sigma_i^2 v_i
  \]
  and \(\T^* \T(v_i) = \T^*(\zv_{\W}) = \zv_{\V}\) for \(i \in \set{r + 1, \dots, n}\).
  Therefore each \(v_i\) is an eigenvector of \(\T^* \T\) with corresponding eigenvalue \(\sigma_i^2\) if \(i \in \set{1, \dots, r}\) and \(0\) if \(i \in \set{r + 1, \dots, n}\).
\end{proof}

\begin{defn}\label{6.7.1}
  The unique scalars \(\seq{\sigma}{1,,r}\) in \cref{6.26} are called the \textbf{singular values} of \(\T\).
  If \(r\) is less than both \(m\) and \(n\), then the term singular value is extended to include \(\seq[=]{\sigma}{r+1,,k} = 0\), where \(k\) is the minimum of \(m\) and \(n\).
\end{defn}

\begin{note}
  Although the singular values of a linear transformation \(\T\) are uniquely determined by \(\T\), the orthonormal bases given in the statement of \cref{6.26} are not uniquely determined because there is more than one orthonormal basis of eigenvectors of \(\T^* \T\).

  In view of \cref{eq:6.7.2}, the singular values of a linear transformation \(\T \in \ls(\V, \W)\) and its adjoint \(\T^* \in \ls(\W, \V)\) are identical.
  Furthermore, the orthonormal bases for \(\V\) and \(\W\) given in \cref{6.26} are simply reversed for \(\T^*\).
\end{note}

\begin{defn}\label{6.7.2}
  Let \(A \in \ms\).
  We define the \textbf{singular values} of \(A\) to be the singular values of the linear transformation \(\L_A\).
\end{defn}

\begin{thm}[Singular Value Decomposition Theorem for Matrices]\label{6.27}
  Let \(A \in \ms\) be of rank \(r\) with the positive singular values \(\seq[\geq]{\sigma}{1,,r}\), and let \(\Sigma \in \ms\) defined by
  \[
    \Sigma_{i j} = \begin{dcases}
      \sigma_i & \text{if } i = j \leq r \\
      0        & \text{otherwise}
    \end{dcases}.
  \]
  Then there exists unitary matrices \(U \in \ms[m][m][\F]\) and \(\V \in \ms[n][n][\F]\) such that
  \[
    A = U \Sigma V^*.
  \]
\end{thm}

\begin{proof}[\pf{6.27}]
  Let \(\T = \L_A \in \ls(\vs{F}^n, \vs{F}^m)\).
  By \cref{6.26}, there exist orthonormal bases \(\beta = \set{\seq{v}{1,,n}}\) for \(\vs{F}^n\) over \(\F\) and \(\gamma = \set{\seq{u}{1,,m}}\) for \(\vs{F}^m\) over \(\F\) such that \(\T(v_i) = \sigma_i u_i\) for \(i \in \set{1, \dots, r}\) and \(\T(v_i) = \zv_{\W}\) for \(i \in \set{r + 1, \dots, n}\).
  Let \(U \in \ms[m][m][\F]\) whose \(j\)th column is \(u_j\) for all \(j \in \set{1, \dots, m}\), and let \(V \in \ms[n][n][\F]\) whose \(j\)th column is \(v_j\) for all \(j \in \set{1, \dots, n}\).
  Note that both \(U\) and \(V\) are unitary matrices.

  By \cref{2.13}(a), the \(j\)th column of \(AV\) is \(A v_j = \sigma_j u_j\).
  Observe that the \(j\)th column of \(\Sigma\) is \(\sigma_j e_j\), where \(e_j\) is the \(j\)th standard vector of \(\vs{F}^m\).
  So by \cref{2.13}(a)(b), the \(j\)th column of \(U \Sigma\) is given by
  \[
    U(\sigma_j e_j) = \sigma_j (U e_j) = \sigma_j u_j.
  \]
  It follows that \(AV\) and \(U \Sigma\) are \(m \times n\) matrices whose corresponding columns are equal, and hence \(AV = U \Sigma\).
  Therefore \(A = A V V^* = U \Sigma V^*\).
\end{proof}

\begin{defn}\label{6.7.3}
  Let \(A \in \ms\) be of rank \(r\) with positive singular values \(\seq[\geq]{\sigma}{1,,r}\).
  A factorization \(A = U \Sigma V^*\) where \(U\) and \(V\) are unitary matrices and \(\Sigma \in \ms\) is defined as in \cref{6.27} is called a \textbf{singular value decomposition} of \(A\).
\end{defn}

\begin{note}
  In the proof of \cref{6.27}, the columns of \(V\) are the vectors in \(\beta\), and the columns of \(U\) are the vectors in \(\gamma\).
  Furthermore, the nonzero singular values of \(A\) are the same as those of \(\L_A\);
  hence they are the square roots of the nonzero eigenvalues of \(A^* A\) or of \(A A^*\).
  (See \cref{ex:6.7.9}.)
\end{note}

\begin{note}
  A singular value decomposition of a matrix can be used to factor a square matrix in a manner analogous to the factoring of a complex number as the product of a complex number of length \(1\) and a nonnegative number.
  In the case of matrices, the complex number of length \(1\) is replaced by a unitary matrix, and the nonnegative number is replaced by a positive semidefinite matrix.
\end{note}

\begin{thm}[Polar Decomposition]\label{6.28}
  For any square matrix \(A\), there exists a unitary matrix \(W\) and a positive semidefinite matrix \(P\) such that
  \[
    A = WP.
  \]
  Furthermore, if \(A\) is invertible, then the representation is unique.
\end{thm}

\begin{proof}[\pf{6.28}]
  By \cref{6.27}, there exist unitary matrices \(U\) and \(V\) and a diagonal matrix \(\Sigma\) with nonnegative diagonal entries such that \(A = U \Sigma V^*\).
  So
  \begin{align*}
    A & = U \Sigma \V^* \by{6.27}                 \\
      & = U I \Sigma \V^*         &  & \by{2.3.4} \\
      & = U V^* V \Sigma V^*      &  & \by{6.5.9} \\
      & = WP,
  \end{align*}
  where \(W = U V^*\) and \(P = V \Sigma V^*\).
  Since \(W\) is the product of unitary matrices, \(W\) is unitary (\cref{ex:6.5.3}), and since \(\Sigma\) is positive semidefinite (\cref{ex:6.4.17}(a)) and \(P\) is unitarily equivalent to \(\Sigma\), \(P\) is positive semidefinite by \cref{ex:6.5.14}.

  Now suppose that \(A\) is invertible and factors as the products
  \[
    A = WP = ZQ,
  \]
  where \(W\) and \(Z\) are unitary and \(P\) and \(Q\) are positive semidefinite.
  Since \(A\) is invertible, it follows that \(P\) and \(Q\) are positive definite and invertible \cref{ex:6.4.19}(c), and therefore \(Z^* W = Q P^{-1}\).
  Thus \(Q P^{-1}\) is unitary (\cref{ex:6.5.3}), and so
  \begin{align*}
    I & = (Q P^{-1})^* (Q P^{-1}) &  & \by{6.5.9}        \\
      & = (P^{-1})^* Q^* Q P^{-1} &  & \by{6.3.2}[c]     \\
      & = (P^{-1})^* Q^2 P^{-1}   &  & \by{6.4.11}       \\
      & = P^{-1} Q^2 P^{-1}.      &  & \by{ex:6.4.19}[c]
  \end{align*}
  Hence \(P^2 = Q^2\).
  Since both \(P\) and \(Q\) are positive definite, it follows that \(P = Q\) by \cref{ex:6.4.17}(d).
  Therefore \(W = Z\), and consequently the factorization is unique.
\end{proof}

\begin{defn}\label{6.7.4}
  The factorization of a square matrix \(A\) as \(WP\) where \(W\) is unitary and \(P\) is positive semidefinite, is called a \textbf{polar decomposition} of \(A\).
\end{defn}

\begin{prop}\label{6.7.5}
  Let \(\V\) and \(\W\) be finite-dimensional inner product spaces over the same field \(\F\), and let \(\T \in \ls(\V, \W)\).
  It is desirable to have a linear transformation from \(\W\) to \(\V\) that captures some of the essence of an inverse of \(\T\) even if \(\T\) is not invertible.
  A simple approach to this problem is to focus on the ``part'' of \(\T\) that is invertible, namely, the restriction of \(\T\) to \(\ns{\T}^{\perp}\).
  Let \(\lt{L} : \ns{\T}^{\perp} \to \rg{\T}\) be the linear transformation defined by \(\lt{L}(x) = \T(x)\) for all \(x \in \ns{\T}^{\perp}\).
  Then \(\lt{L}\) is invertible, and we can use the inverse of \(\lt{L}\) to construct a linear transformation from \(\W\) to \(\V\) that salvages some of the benefits of an inverse of \(\T\).
\end{prop}

\begin{proof}[\pf{6.7.5}]
  Let \(y \in \rg{\T}\).
  Then there exists an \(x \in \V\) such that \(\T(x) = y\).
  By \cref{ex:6.2.13}(d) we have \(\V = \ns{\T} \oplus \ns{\T}^{\perp}\), thus by \cref{6.6} there exists an unique tuple \(\tuple{x}{1,2} \in \ns{\T} \times \ns{\T}^{\perp}\) such that \(x = x_1 + x_2\).
  Then we have
  \begin{align*}
    y & = \T(x)                                 \\
      & = \T(x_1 + x_2)      &  & \by{6.6}      \\
      & = \T(x_1) + \T(x_2)  &  & \by{2.1.2}[a] \\
      & = \zv_{\W} + \T(x_2) &  & \by{2.1.10}   \\
      & = \T(x_2)                               \\
      & = \lt{L}(x_2).
  \end{align*}
  Thus \(\lt{L}\) is onto.
  Since
  \begin{align*}
    \dim(\ns{\T}^{\perp}) & = \dim(\V) - \dim(\ns{\T}) &  & \by{6.7}[c] \\
                          & = \dim(\V) - \nt{\T}       &  & \by{2.1.12} \\
                          & = \rk{\T},                 &  & \by{2.3}
  \end{align*}
  by \cref{2.5} we know that \(\lt{L}\) is one-to-one, and thus invertible.
\end{proof}

\begin{defn}\label{6.7.6}
  Let \(\V\) and \(\W\) be finite-dimensional inner product spaces over the same field \(\F\), and let \(\T \in \ls(\V, \W)\).
  Let \(\lt{L} \in \ls(\ns{\T}^{\perp}, \rg{\T})\) defined by \(\lt{L}(x) = \T(x)\) for all \(x \in \ns{\T}^{\perp}\).
  The \textbf{pseudoinverse} (or Moore-Penrose generalized inverse) of \(\T\), denoted by \(\T^{\dag}\), is defined as the unique linear transformation from \(\W\) to \(\V\) such that
  \[
    \T^{\dag}(y) = \begin{dcases}
      \lt{L}^{-1}(y) & \text{for } y \in \rg{\T}         \\
      \zv_{\V}       & \text{for } y \in \rg{\T}^{\perp}
    \end{dcases}.
  \]
  The uniqueness of pseudoinverse is guaranteed by \cref{2.1.13}.
  The pseudoinverse of a linear transformation \(\T\) on a finite-dimensional inner product space exists even if \(\T\) is not invertible (see \cref{6.7.5}).
  Furthermore, if \(\T\) is invertible, then \(\T^{\dag} = \T^{-1}\) because \(\ns{\T}^{\perp} = \V\) (see \cref{2.4}), and \(\lt{L}\) (as just defined) coincides with \(\T\).
\end{defn}

\begin{eg}\label{6.7.7}
  Consider the zero transformation \(\zT \in \ls(\V, \W)\) between two finite-dimensional inner product spaces \(\V\) and \(\W\) over \(\F\).
  Then \(\rg{\zT} = \set{\zv_{\W}}\), and therefore \(\zT^{\dag}\) is the zero transformation from \(\W\) to \(\V\).
\end{eg}

\begin{prop}\label{6.7.8}
  We can use the singular value theorem to describe the pseudoinverse of a linear transformation.
  Suppose that \(\V\) and \(\W\) are finite-dimensional vector spaces and \(\T \in \ls(\V, \W)\) is of rank \(r\).
  Let \(\set{\seq{v}{1,,n}}\) and \(\set{\seq{u}{1,,m}}\) be orthonormal bases for \(\V\) and \(\W\) over \(\F\), respectively, and let \(\seq[\geq]{\sigma}{1,,r}\) be the nonzero singular values of \(\T\) satisfying \cref{eq:6.7.1} in \cref{6.26}.
  Then \(\set{\seq{v}{1,,r}}\) is a basis for \(\ns{\T}^{\perp}\) over \(\F\), \(\set{\seq{v}{r+1,,n}}\) is a basis for \(\ns{\T}\) over \(\F\), \(\set{\seq{u}{1,,r}}\) is a basis for \(\rg{\T}\) over \(\F\), and \(\set{\seq{u}{r+1,,m}}\) is a basis for \(\rg{\T}^{\perp}\).
  Let \(\lt{L}\) be the restriction of \(\T\) to \(\ns{\T}^{\perp}\), as in the definition of pseudoinverse (\cref{6.7.6}).
  Then \(\lt{L}^{-1}(u_i) = \frac{1}{\sigma_i} v_i\) for \(i \in \set{1, \dots, r}\).
  Therefore
  \begin{equation}\label{eq:6.7.3}
    \T^{\dag}(u_i) = \begin{dcases}
      \frac{1}{\sigma_i} v_i & \text{if } i \in \set{1, \dots, r}     \\
      \zv_{\V}               & \text{if } i \in \set{r + 1, \dots, m}
    \end{dcases}.
  \end{equation}
\end{prop}

\begin{proof}[\pf{6.7.8}]
  By \cref{6.7}(c) and \cref{ex:6.2.13} we see that \(\set{\seq{v}{1,,r}}\) is a basis for \(\ns{\T}^{\perp}\) over \(\F\), \(\set{\seq{v}{r+1,,n}}\) is a basis for \(\ns{\T}\) over \(\F\), \(\set{\seq{u}{1,,r}}\) is a basis for \(\rg{\T}\) over \(\F\), and \(\set{\seq{u}{r+1,,m}}\) is a basis for \(\rg{\T}^{\perp}\).
  Then we have
  \begin{align*}
             & \forall x \in \ns{\T}^{\perp}, \lt{L}(x) = \T(x)                           &  & \by{6.7.6}                        \\
    \implies & \forall i \in \set{1, \dots, r}, \lt{L}(v_i) = \T(v_i) = \sigma_i u_i      &  & \by{6.26}                         \\
    \implies & \forall i \in \set{1, \dots, r}, \lt{L}^{-1}(u_i) = \frac{1}{\sigma_i} v_i &  & \by{2.17}                         \\
    \implies & \forall i \in \set{1, \dots, m}, \T^{\dag}(u_i) = \begin{dcases}
                                                                   \frac{1}{\sigma_i} v_i & \text{if } i \in \set{1, \dots, r}     \\
                                                                   \zv_{\V}               & \text{if } i \in \set{r + 1, \dots, m}
                                                                 \end{dcases}.         &  & \by{6.7.6}
  \end{align*}
\end{proof}

\begin{defn}\label{6.7.9}
  Let \(A \in \ms\).
  Then there exists a unique \(B \in \ms[n][m][\F]\) such that \((\L_A)^{\dag} : \vs{F}^m \to \vs{F}^n\) is equal to the left-multiplication transformation \(\L_B\) (see \cref{2.20}).
  We call \(B\) the \textbf{pseudoinverse} of \(A\) and denote it by \(B = A^{\dag}\).
  Thus
  \[
    (\L_A)^{\dag} = \L_{A^{\dag}}.
  \]
\end{defn}

\begin{thm}\label{6.29}
  Let \(A \in \ms\) be of rank \(r\) with a singular value decomposition \(A = U \Sigma V^*\) and nonzero singular values \(\seq[\geq]{\sigma}{1,,r}\).
  Let \(\Sigma^{\dag} \in \ms[n][m][\F]\) defined by
  \[
    \Sigma_{i j}^{\dag} = \begin{dcases}
      \frac{1}{\sigma_i} & \text{if } i = j \leq r \\
      0                  & \text{otherwise}
    \end{dcases}.
  \]
  Then \(A^{\dag} = V \Sigma^{\dag} U^*\), and this is a singular value decomposition of \(A^{\dag}\).
\end{thm}

\begin{proof}[\pf{6.29}]
  Let \(\beta\) and \(\gamma\) be the ordered bases whose vectors are the columns of \(V\) and \(U\), respectively.
  Then \(\beta\) and \(\gamma\) are orthonormal bases for \(\vs{F}^n\) and \(\vs{F}^m\) over \(\F\), respectively, and \cref{eq:6.7.1,eq:6.7.3} are satisfied for \(\T = \L_A\).
  Reversing the roles of \(\beta\) and \(\gamma\) in the proof of \cref{6.27}, we obtain the result.
  Note that \(\Sigma^{\dag}\) is actually the pseudoinverse of \(\Sigma\).
\end{proof}

\begin{note}
  Let \(A \in \ms\).
  Then for any \(b \in \vs{F}^m\), the matrix equation \(Ax = b\) is a system of linear equations, and so it either has no solutions, a unique solution, or infinitely many solutions.
  We know that the system has a unique solution for every \(b \in \vs{F}^m\) iff \(A\) is invertible, in which case the solution is given by \(A^{-1} b\).
  Furthermore, if \(A\) is invertible, then \(A^{-1} = A^{\dag}\), and so the solution can be written as \(x = A^{\dag} b\).
  If, on the other hand, \(A\) is not invertible or the system \(Ax = b\) is inconsistent, then \(A^{\dag} b\) still exists.
  We therefore pose the following question:
  In general, how is the vector \(A^{\dag} b\) related to the system of linear equations \(Ax = b\)?
  In order to answer this question, we need the following lemma.
\end{note}

\begin{lem}\label{6.7.10}
  Let \(\V\) and \(\W\) be finite-dimensional inner product spaces over \(\F\), and let \(\T \in \ls(\V, \W)\).
  Then
  \begin{enumerate}
    \item \(\T^{\dag} \T\) is the orthogonal projection of \(\V\) on \(\ns{\T}^{\perp}\).
    \item \(\T \T^{\dag}\) is the orthogonal projection of \(\W\) on \(\rg{\T}\).
  \end{enumerate}
\end{lem}

\begin{proof}[\pf{6.7.10}(a)]
  As in \cref{6.7.6}, we define \(\lt{L} \in \ls(\ns{\T}^{\perp}, \rg{\T})\) by \(\lt{L}(x) = \T(x)\) for all \(x \in \ns{\T}^{\perp}\).
  Observe that
  \begin{align*}
    \forall x \in \ns{\T}^{\perp}, (\T^{\dag} \T)(x) & = \T^{\dag}(\T(x))                       \\
                                                     & = \T^{\dag}(\lt{L}(x))   &  & \by{6.7.6} \\
                                                     & = \lt{L}^{-1}(\lt{L}(x)) &  & \by{6.7.6} \\
                                                     & = x,
  \end{align*}
  and
  \begin{align*}
    \forall x \in \ns{\T}, (\T^{\dag} \T)(x) & = \T^{\dag}(\T(x))                       \\
                                             & = \T^{\dag}(\zv_{\W}) &  & \by{2.1.10}   \\
                                             & = \zv_{\V}.           &  & \by{2.1.2}[a]
  \end{align*}
  Consequently \(\T^{\dag} \T\) is the orthogonal projection of \(\V\) on \(\ns{\T}^{\perp}\) (see \cref{6.6.1}).
\end{proof}

\begin{proof}[\pf{6.7.10}(b)]
  Continue the proof of \cref{6.7.10}(a), we have
  \begin{align*}
    \forall x \in \rg{\T}, (\T \T^{\dag})(x) & = \T(\T^{\dag}(x))                                                 \\
                                             & = \T(\lt{L}^{-1}(x))     &  & \by{6.7.6}                           \\
                                             & = \lt{L}(\lt{L}^{-1}(x)) &  & (\lt{L}^{-1}(x) \in \ns{\T}^{\perp}) \\
                                             & = x
  \end{align*}
  and
  \begin{align*}
    \forall x \in \rg{\T}^{\perp}, (\T \T^{\dag})(x) & = \T(\T^{\dag}(x))                    \\
                                                     & = \T(\zv_{\V})     &  & \by{6.7.6}    \\
                                                     & = \zv_{\W}.        &  & \by{2.1.2}[a]
  \end{align*}
  Thus by \cref{6.6.1} \(\T \T^{\dag}\) is the orthogonal projection of \(\W\) on \(\rg{\T}\).
\end{proof}

\begin{thm}\label{6.30}
  Consider the system of linear equations \(Ax = b\), where \(A \in \ms\) and \(b \in \vs{F}^m\).
  If \(z = A^{\dag} b\), then \(z\) has the following properties.
  \begin{enumerate}
    \item If \(Ax = b\) is consistent, then \(z\) is the unique solution to the system having minimum norm.
          That is, \(z\) is a solution to the system, and if \(y\) is any solution to the system, then \(\norm{z} \leq \norm{y}\) with equality iff \(z = y\).
    \item If \(Ax = b\) is inconsistent, then \(z\) is the unique best approximation to a solution having minimum norm.
          That is, \(\norm{Az - b} \leq \norm{Ay - b}\) for any \(y \in \vs{F}^n\), with equality iff \(Az = Ay\).
          Furthermore, if \(Az = Ay\), then \(\norm{z} \leq \norm{y}\) with equality iff \(z = y\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{6.30}(a)]
  Suppose that \(Ax = b\) is consistent, and let \(z = A^{\dag} b\).
  Observe that \(b \in \rg{\L_A}\), and therefore
  \begin{align*}
    Az & = A A^{\dag} b                             \\
       & = \L_A(\L_A^{\dag}(b)) &  & \by{2.3.8}     \\
       & = b.                   &  & \by{6.7.10}[b]
  \end{align*}
  Thus \(z\) is a solution to the system.
  Now suppose that \(y\) is any solution to the system.
  Then
  \begin{align*}
    \L_A^{\dag} \L_A(y) & = A^{\dag} A y &  & \by{2.3.8} \\
                        & = A^{\dag} b   &  & (Ay = b)   \\
                        & = z,
  \end{align*}
  and hence \(z\) is the orthogonal projection of \(y\) on \(\ns{\L_A}^{\perp}\) by \cref{6.7.10}(a).
  Therefore, by \cref{6.2.12}, we have that \(\norm{z} \leq \norm{y}\) with equality iff \(z = y\).
\end{proof}

\begin{proof}[\pf{6.30}(b)]
  Suppose that \(Ax = b\) is inconsistent.
  By \cref{6.7.10}, \(Az = A A^{\dag} b = \L_A \L_A^{\dag}(b) = b\) is the orthogonal projection of \(b\) on \(\rg{\L_A}\);
  therefore, by \cref{6.2.12}, \(Az\) is the vector in \(\rg{\L_A}\) nearest \(b\).
  That is, if \(Ay\) is any other vector in \(\rg{\L_A}\), then \(\norm{Az - b} \leq \norm{Ay - b}\) with equality iff \(Az = Ay\).

  Finally, suppose that \(y\) is any vector in \(\vs{F}^n\) such that \(Az = Ay = c\).
  Then
  \begin{align*}
    A^{\dag} c & = A^{\dag} A z                              \\
               & = A^{\dag} A A^{\dag} b                     \\
               & = A^{\dag} b            &  & \by{ex:6.7.23} \\
               & = z;
  \end{align*}
  hence we may apply \cref{6.30}(a) to the system \(Ax = c\) to conclude that \(\norm{z} \leq \norm{y}\) with equality iff \(z = y\).
\end{proof}

\begin{note}
  The vector \(z = A^{\dag} b\) in \cref{6.30} is the vector \(x_0\) described in \cref{6.12} that arises in the least squares application.
\end{note}

\exercisesection

\setcounter{ex}{8}
\begin{ex}\label{ex:6.7.9}
  Let \(\V\) and \(\W\) be finite-dimensional inner product spaces over \(\F\), and suppose that \(\set{\seq{v}{1,,n}}\) and \(\set{\seq{u}{1,,m}}\) are orthonormal bases for \(\V\) and \(\W\) over \(\F\), respectively.
  Let \(\T \in \ls(\V, \W)\) be of rank \(r\), and suppose that \(\seq[\geq]{\sigma}{1,,r} > 0\) are such that
  \[
    \T(v_i) = \begin{dcases}
      \sigma_i u_i & \text{if } i \in \set{1, \dots, r}     \\
      \zv_{\W}     & \text{if } i \in \set{r + 1, \dots, n}
    \end{dcases}.
  \]
  \begin{enumerate}
    \item Prove that \(\set{\seq{u}{1,,m}}\) is a set of eigenvectors of \(\T \T^*\) with corresponding eigenvalues \(\seq{\lambda}{1,,m} \in \F\), where
          \[
            \lambda_i = \begin{dcases}
              \sigma_i^2 & \text{if } i \in \set{1, \dots, r}     \\
              0          & \text{if } i \in \set{r + 1, \dots, m}
            \end{dcases}.
          \]
    \item Let \(A \in \ms\) with real or complex entries.
          Prove that the nonzero singular values of \(A\) are the positive square roots of the nonzero eigenvalues of \(A A^*\), including repetitions.
    \item Prove that \(\T \T^*\) and \(\T^* \T\) have the same nonzero eigenvalues, including repetitions.
    \item State and prove a result for matrices analogous to (c).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.7.9}(a)]
  By \cref{6.26} we see that \(\set{\seq{v}{1,,n}}\) is consist of eigenvectors of \(\T^* \T\), and \(\seq{\sigma}{1,,r}\) are singular values of \(\T\).
  Since
  \begin{align*}
    \forall i \in \set{1, \dots, m}, (\T \T^*)(u_i) & = \T(\T^*(u_i))                                             \\
                                                    & = \begin{dcases}
                                                          \T(\sigma_i v_i) & \text{if } i \in \set{1, \dots, r}     \\
                                                          \T(\zv_{\V})     & \text{if } i \in \set{r + 1, \dots, m}
                                                        \end{dcases} &  & \by{eq:6.7.2} \\
                                                    & = \begin{dcases}
                                                          \sigma_i \T(v_i) & \text{if } i \in \set{1, \dots, r}     \\
                                                          \zv_{\W}         & \text{if } i \in \set{r + 1, \dots, m}
                                                        \end{dcases} &  & \by{2.1.2}[a,b] \\
                                                    & = \begin{dcases}
                                                          \sigma_i^2 u_i & \text{if } i \in \set{1, \dots, r}     \\
                                                          \zv_{\V}       & \text{if } i \in \set{r + 1, \dots, m}
                                                        \end{dcases}   &  & \by{eq:6.7.1}   \\
                                                    & = \begin{dcases}
                                                          \sigma_i^2 u_i & \text{if } i \in \set{1, \dots, r}     \\
                                                          0 u_i          & \text{if } i \in \set{r + 1, \dots, m}
                                                        \end{dcases}   &  & \by{1.2}[a]   \\
                                                    & = \begin{dcases}
                                                          \lambda_i u_i & \text{if } i \in \set{1, \dots, r}     \\
                                                          0 u_i         & \text{if } i \in \set{r + 1, \dots, m}
                                                        \end{dcases},
  \end{align*}
  by \cref{5.1.2} we know that \(\set{\seq{u}{1,,m}}\) is consist of eigenvectors of \(\T \T^*\).
\end{proof}

\begin{proof}[\pf{ex:6.7.9}(b)]
  Since
  \begin{align*}
    \L_{A A^*} & = \L_A \L_{A^*}  &  & \by{2.15}[e] \\
               & = \L_A (\L_A)^*, &  & \by{6.3.1}
  \end{align*}
  by \cref{5.1.2,6.7.2} and \cref{ex:6.7.9}(a) we see that the nonzero singular values of \(A\) are the positive square roots of the nonzero eigenvalues of \(A A^*\), including repetitions.
\end{proof}

\begin{proof}[\pf{ex:6.7.9}(c)]
  By \cref{6.26} and \cref{ex:6.7.9}(a) we see that this is true.
\end{proof}

\begin{proof}[\pf{ex:6.7.9}(d)]
  We claim that \(A A^*\) and \(A^* A\) have the same nonzero eigenvalues, including repetitions.
  By \cref{6.7.2} and \cref{ex:6.7.9}(c) we see that the claim is true.
\end{proof}

\begin{ex}\label{ex:6.7.10}
  Use \cref{ex:2.5.8} to obtain another proof of \cref{6.27}, the singular value decomposition theorem for matrices.
\end{ex}

\begin{proof}[\pf{ex:6.7.10}]
  Let \(A \in \ms\) be of rank \(r\).
  By \cref{6.26} there exist orthonormal basis (with respect to standard inner product, \cref{6.1.2}) \(\beta = \set{\seq{v}{1,,n}}\) and \(\gamma = \set{\seq{u}{1,,m}}\) for \(\vs{F}^n\) and \(\vs{F}^m\) over \(\F\), respectively, such that
  \[
    \forall i \in \set{1, \dots, n}, \L_A(v_i) = \begin{dcases}
      \sigma_i u_i   & \text{if } i \in \set{1, \dots, r}     \\
      \zv_{\vs{F}^m} & \text{if } i \in \set{r + 1, \dots, n}
    \end{dcases},
  \]
  where \(\seq[\geq]{\sigma}{1,,r}\) are the singular values of \(A\).
  Let \(\beta'\) and \(\gamma'\) be the standard ordered basis for \(\vs{F}^n\) and \(\vs{F}^m\) over \(\F\), respectively.
  Note that \(\beta'\) and \(\gamma'\) are orthonormal with respect to standard inner product (\cref{6.1.2}).
  Then we have
  \begin{align*}
    A & = [\L_A]_{\beta'}^{\gamma'}                                                                          &  & \by{2.15}[a]        \\
      & = [\IT[\vs{F}^m]]_{\gamma}^{\gamma'} [\L_A]_{\beta}^{\gamma} [\IT[\vs{F}^n]]_{\beta'}^{\beta}        &  & \by{ex:2.5.8}       \\
      & = [\IT[\vs{F}^m]]_{\gamma}^{\gamma'} [\L_A]_{\beta}^{\gamma} \pa{[\IT[\vs{F}^n]]_{\beta}^{\beta'}}^* &  & \by{ex:6.1.23}[c]   \\
      & = \begin{pmatrix}
            u_1 & \cdots & u_m
          \end{pmatrix} \begin{pmatrix}
                          \sigma_1 & \cdots & 0        &     \\
                          \vdots   & \ddots & \vdots   & \zm \\
                          0        & \cdots & \sigma_r &     \\
                                   & \zm    &          & \zm
                        \end{pmatrix} \begin{pmatrix}
                                        v_1 & \cdots & v_n
                                      \end{pmatrix}^*.                                                                &  & \by{2.2.4}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:6.7.11}
  This exercise relates the singular values of a well-behaved linear operator or matrix to its eigenvalues.
  \begin{enumerate}
    \item Let \(\T\) be a normal linear operator on an \(n\)-dimensional inner product space \(\V\) over \(\F\) with eigenvalues \(\seq{\lambda}{1,,n} \in \F\).
          Prove that the singular values of \(\T\) are \(\abs{\lambda_1}, \dots, \abs{\lambda_n}\).
    \item State and prove a result for matrices analogous to (a).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.7.11}(a)]
  By \cref{6.26} we see that if \(\lambda_i\) is an eigenvalue of \(\T^* \T\), then \(\sqrt{\lambda_i}\) is a singular value of \(\T\).
  Then we have
  \begin{align*}
             & \forall i \in \set{1, \dots, n}, \lambda_i \text{ is an eigenvalue of } \T                                                             \\
    \implies & \forall i \in \set{1, \dots, n}, \conj{\lambda_i} \text{ is an eigenvalue of } \T^*                                  &  & \by{6.15}[c] \\
    \implies & \forall i \in \set{1, \dots, n}, \abs{\lambda_i}^2 = \conj{\lambda_i} \lambda_i \text{ is an eigenvalue of } \T^* \T &  & \by{6.15}[c] \\
    \implies & \forall i \in \set{1, \dots, n}, \abs{\lambda_i} \text{ is a singular value of } \T.                                 &  & \by{6.26}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.7.11}(b)]
  Let \(A \in \ms[n][n][\F]\) be normal with eigenvalues \(\seq{\lambda}{1,,n} \in \F\).
  We claim that the singular values of \(A\) are \(\abs{\lambda_1}, \dots, \abs{\lambda_n}\).
  Since \(A\) is normal, by \cref{6.4.4} we see that \(\L_A\) is normal.
  Thus by \cref{5.1.2,6.7.2,ex:6.7.11}(a) we see that the claim is true.
\end{proof}

\begin{ex}\label{ex:6.7.12}
  Let \(A\) be a normal matrix with an orthonormal basis of eigenvectors \(\beta = \set{\seq{v}{1,,n}}\) and corresponding eigenvalues \(\seq{\lambda}{1,,n}\).
  Let \(V \in \ms[n][n][\F]\) whose columns are the vectors in \(\beta\).
  Prove that for each \(i \in \set{1, \dots, n}\) there is a scalar \(\theta_i\) of absolute value \(1\) such that if \(U\) is the \(n \times n\) matrix with \(\theta_i v_i\) as column \(i\) and \(\Sigma\) is the diagonal matrix such that \(\Sigma_{i i} = \abs{\lambda_i}\) for each \(i \in \set{1, \dots, n}\), then \(U \Sigma V^*\) is a singular value decomposition of \(A\).
\end{ex}

\begin{proof}[\pf{ex:6.7.12}]
  Since \(A\) is normal, by \cref{ex:6.7.11}(b) we know that the singular values of \(A\) are \(\abs{\lambda_1}, \dots, \abs{\lambda_n}\).
  Observe that
  \begin{align*}
    \forall i \in \set{1, \dots, n}, \L_A(v_i) & = A v_i                                                                                   &  & \by{2.3.8} \\
                                               & = \lambda_i v_i                                                                           &  & \by{5.1.2} \\
                                               & = \begin{dcases}
                                                     \abs{\lambda_i} \pa{\frac{\lambda_i}{\abs{\lambda_i}} v_i} & \text{if } \lambda_i \neq 0 \\
                                                     \zv                                                        & \text{if } \lambda_i = 0
                                                   \end{dcases} &  & \by{1.2}[a]                \\
                                               & = \begin{dcases}
                                                     \abs{\lambda_i} \pa{\frac{\lambda_i}{\abs{\lambda_i}} v_i} & \text{if } \lambda_i \neq 0 \\
                                                     1 \zv                                                      & \text{if } \lambda_i = 0
                                                   \end{dcases}. &  & \by{1.2}[c]
  \end{align*}
  Now we define \(\theta_i\) as follow:
  \[
    \forall i \in \set{1, \dots, n}, \theta_i = \begin{dcases}
      \frac{\lambda_i}{\abs{\lambda_i}} & \text{if } \lambda_i \neq 0 \\
      1                                 & \text{if } \lambda_i = 0
    \end{dcases}.
  \]
  Clearly \(\abs{\theta_i} = 1\) for all \(i \in \set{1, \dots, n}\).
  If we can show that \(\set{\seq{\theta,v}{1,,n}}\) is orthonormal, then by \cref{6.26} we know that \(U \Sigma V^*\) is a singular value decomposition of \(A\).
  This is true since
  \begin{align*}
    \forall i, j \in \set{1, \dots, n}, \inn{\theta_i v_i, \theta_j v_j} & = \theta_i \inn{v_i, \theta_j v_j}        &  & \by{6.1.1}[b]                                         \\
                                                                         & = \theta_i \conj{\theta_j} \inn{v_i, v_j} &  & \by{6.1}[b]                                           \\
                                                                         & = \theta_i \conj{\theta_j} \delta_{i j}   &  & \by{6.1.12}                                           \\
                                                                         & = \delta_{i j}.                           &  & (\forall i \in \set{1, \dots, n}, \abs{\theta_i} = i)
  \end{align*}
\end{proof}

\begin{ex}\label{ex:6.7.13}
  Prove that if \(A \in \ms[n][n][\F]\) is a positive semidefinite matrix, then the singular values of \(A\) are the same as the eigenvalues of \(A\).
\end{ex}

\begin{proof}[\pf{ex:6.7.13}]
  Since \(A\) is positive semidefinite, by \cref{6.4.11} we know that \(A\) is self-adjoint and by \cref{ex:6.4.17}(a) we know that the eigenvalues of \(A\) are nonnegative real numbers.
  If \(v \in \vs{F}^n\) is an eigenvector corresponding to eigenvalue \(\lambda \in \F\) of \(A\), then
  \begin{align*}
    A^* A v & = A A v       &  & \by{6.4.8} \\
            & = \lambda^2 v &  & \by{5.1.2}
  \end{align*}
  and thus by \cref{6.26} \(\sqrt{\lambda^2} = \lambda\) is a singular value of \(A\).
  We conclude that eigenvalues of \(A\) are the same as the singular values of \(A\).
\end{proof}

\begin{ex}\label{ex:6.7.14}
  Prove that if \(A \in \ms[n][n][\F]\) is a positive definite matrix and \(A = U \Sigma V^*\) is a singular value decomposition of \(A\), then \(U = V\).
\end{ex}

\begin{proof}[\pf{ex:6.7.14}]
  For each \(i \in \set{1, \dots, n}\), let \(u_i\) and \(v_i\) be the \(i\)th column of \(U\) and \(V\), respectively.
  Since \(A\) is positive definite, by \cref{6.4.11} we know that \(A\) is self-adjoint.
  If \(\seq{\lambda}{1,,n} \in \F\) are eigenvalues of \(A\) corresponding to \(\seq{v}{1,,n}\), then by \cref{ex:6.4.17}(a) we know that \(\lambda_i > 0\) for all \(i \in \set{1, \dots, n}\).
  Then we have
  \begin{align*}
    \forall i \in \set{1, \dots, n}, u_i & = \frac{1}{\lambda_i} \L_A(v_i)   &  & \by{ex:6.7.13} \\
                                         & = \frac{\lambda_i}{\lambda_i} v_i &  & \by{5.1.2}     \\
                                         & = v_i.
  \end{align*}
  Thus \(U = V\).
\end{proof}

\begin{ex}\label{ex:6.7.15}
  Let \(A\) be a square matrix with a polar decomposition \(A = WP\).
  \begin{enumerate}
    \item Prove that \(A\) is normal iff \(W P^2 = P^2 W\).
    \item Use (a) to prove that \(A\) is normal iff \(WP = PW\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.7.15}(a)]
  By \cref{6.7.4} we know that \(W\) is unitary and \(P\) is positive semidefinite.
  Then we have
  \begin{align*}
         & A \text{ is normal}                              \\
    \iff & A A^* = A^* A             &  & \by{6.4.3}        \\
    \iff & WP (WP)^* = (WP)^* WP     &  & \by{6.28}         \\
    \iff & W P P^* W^* = P^* W^* W P &  & \by{6.3.2}[c]     \\
    \iff & W P P^* W^* = P^* P       &  & \by{ex:6.1.23}[c] \\
    \iff & W P^2 W^* = P^2           &  & \by{6.4.11}       \\
    \iff & W P^2 = P^2 W.            &  & \by{ex:6.1.23}[c]
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.7.15}(b)]
  By \cref{6.7.4} we know that \(W\) is unitary and \(P\) is positive semidefinite.
  By \cref{ex:6.5.14} we know that \(W^* P W\) is positive semidefinite.
  Then we have
  \begin{align*}
         & A \text{ is normal}                                        \\
    \iff & W P^2 = P^2 W                       &  & \by{ex:6.7.15}[a] \\
    \iff & P^2 = W^* P^2 W                     &  & \by{ex:6.1.23}[c] \\
    \iff & P^2 = W^* P W W^* P W = (W^* P W)^2 &  & \by{ex:6.1.23}[c] \\
    \iff & P = W^* P W                         &  & \by{ex:6.4.17}[d] \\
    \iff & WP = PW.                            &  & \by{ex:6.1.23}[c]
  \end{align*}
\end{proof}

\begin{ex}\label{ex:6.7.16}
  Let \(A\) be a square matrix.
  Prove an alternate form of the polar decomposition for \(A\):
  There exists a unitary matrix \(W\) and a positive semidefinite matrix \(P\) such that \(A = PW\).
\end{ex}

\begin{proof}[\pf{ex:6.7.16}]
  Let \(U \Sigma V^*\) be a singular value decomposition of \(A\).
  Let \(P = U \Sigma U^*\) and let \(W = W V^*\).
  By \cref{6.27} we know that \(U, V\) are unitary, thus we have
  \begin{align*}
    A & = U \Sigma V^*       &  & \by{6.27}  \\
      & = U \Sigma U^* U V^* &  & \by{6.5.6} \\
      & = PW.
  \end{align*}
  Since \(W\) is the product of unitary matrices, \(W\) is unitary (\cref{ex:6.5.3}), and since \(\Sigma\) is positive semidefinite (\cref{ex:6.4.17}(a)) and \(P\) is unitarily equivalent to \(\Sigma\), \(P\) is positive semidefinite by \cref{ex:6.5.14}.
\end{proof}

\begin{ex}\label{ex:6.7.23}
\end{ex}
