\section{Bases and Dimension}\label{sec:1.6}

\begin{defn}\label{1.6.1}
  A \textbf{basis} \(\beta\) for a vector space \(\V\) over \(\F\) is a linearly independent subset of \(\V\) that generates \(\V\).
  If \(\beta\) is a basis for \(\V\), we also say that the vectors of \(\beta\) form a basis for \(\V\).
\end{defn}

\begin{eg}\label{1.6.2}
  Recalling that \(\spn{\varnothing} = \set{\zv}\) and \(\varnothing\) is linearly independent, we see that \(\varnothing\) is a basis for the zero vector space.
\end{eg}

\begin{eg}\label{1.6.3}
  In \(\vs{F}^n\), let \(e_1 = (1, 0, 0, \dots, 0)\), \(e_2 = (0, 1, 0, \dots, 0)\), \dots, \(e_n = (0, 0, \dots, 0, 1)\);
  \(\set{\seq{e}{1,2,,n}}\) is readily seen to be a basis for \(\vs{F}^n\) and is called the \textbf{standard basis} for \(\vs{F}^n\).
\end{eg}

\begin{proof}[\pf{1.6.3}]
  By \cref{ex:1.5.4} we know that \(\set{\seq{e}{1,2,,n}}\) is linearly independent.
  By \cref{ex:1.4.7} we know that \(\vs{F}^n = \spn{\set{\seq{e}{1,2,,n}}}\).
  Thus by \cref{1.6.1} \(\set{\seq{e}{1,2,,n}}\) is a basis for \(\vs{F}^n\) over \(\F\).
\end{proof}

\begin{eg}\label{1.6.4}
  In \(\MS\), let \(E^{i j}\) denote the matrix whose only nonzero entry is a \(1\) in the \(i\)th row and \(j\)th column.
  Then \(\set{E^{i j} : 1 \leq i \leq m, 1 \leq j \leq n}\) is a basis for \(\MS\).
\end{eg}

\begin{proof}[\pf{1.6.4}]
  By \cref{ex:1.5.6} we know that \(\set{E^{i j} : 1 \leq i \leq m, 1 \leq j \leq n}\) is linearly independent.
  Since
  \begin{align*}
             & \forall A \in \MS, A = \begin{pmatrix}
      A_{1 1} & A_{1 2} & \cdots & A_{1 n} \\
      A_{2 1} & A_{2 2} & \cdots & A_{2 n} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      A_{m 1} & A_{m 2} & \cdots & A_{m n}
    \end{pmatrix}                                                              \\
             & = \sum_{i = 1}^m \sum_{j = 1}^n A_{i j} E^{i j}                                 &  & \text{(by \cref{1.2.9})} \\
    \implies & \forall A \in \MS, A \in \spn{\set{E^{i j} : 1 \leq i \leq m, 1 \leq j \leq n}} &  & \text{(by \cref{1.4.3})} \\
    \implies & \MS = \spn{\set{E^{i j} : 1 \leq i \leq m, 1 \leq j \leq n}},                   &  & \text{(by \cref{1.5})}
  \end{align*}
  by \cref{1.6.1} we know that \(\set{E^{i j} : 1 \leq i \leq m, 1 \leq j \leq n}\) is a basis for \(\MS\) over \(\F\).
\end{proof}

\begin{eg}\label{1.6.5}
  In \(\ps[n]{\F}\) the set \(\set{1, x, x^2, \dots, x^n}\) is a basis.
  We call this basis the \textbf{standard basis} for \(\ps[n]{\F}\).
\end{eg}

\begin{proof}[\pf{1.6.5}]
  By \cref{ex:1.5.5} we know that \(\set{1, x, x^2, \dots, x^n}\) is linearly independent.
  By \cref{ex:1.4.8} we know that \(\ps[n]{\F} = \spn{\set{1, x, x^2, \dots, x^n}}\).
  Thus by \cref{1.6.1} \(\set{1, x, x^2, \dots, x^n}\) is a basis for \(\ps[n]{\F}\) over \(\F\).
\end{proof}

\begin{eg}\label{1.6.6}
  In \(\ps{\F}\) the set \(\set{1, x, x^2, \dots}\) is a basis.
\end{eg}

\begin{proof}[\pf{1.6.6}]
  Suppose for sake of contradiction that \(\set{1, x, x^2, \dots}\) is not a basis for \(\ps{\F}\).
  Then by \cref{1.6.1} we can split into two cases:
  \begin{itemize}
    \item If \(\set{1, x, x^2, \dots}\) is linearly dependent, then by \cref{ex:1.5.14} we have
          \[
            \begin{dcases}
              \exists x^n \in \set{1, x, x^2, \dots} \\
              \exists \set{\seq{a}{0,1,2,}} \subseteq \F
            \end{dcases} : x^n = \sum_{i \in \N : i \neq n} a_i x^i.
          \]
          By setting \(a_n = -1\) we have
          \begin{align*}
                     & \sum_{i \in \N} a_i x^i = \zv = \sum_{i \in \N} 0x^i                                \\
            \implies & \seq[=]{a}{0,1,2,} = 0.                              &  & \text{(by \cref{1.2.11})}
          \end{align*}
          But this means \(a_n = 0\), a contradiction.
    \item If \(\ps{\F} \neq \spn{\set{1, x, x^2, \dots}}\), then by \cref{1.4.3} we have
          \[
            \exists f \in \ps{\F} : \forall \set{\seq{a}{0,1,2,}} \subseteq \F, f(x) \neq \sum_{i \in \N} a_i x^i.
          \]
          Let \(m\) be the degree of \(f\).
          Then by \cref{1.2.11} we have
          \[
            \exists \seq{c}{0,1,,m} \in \F : f(x) = c_0 + c_1 x + \cdots + c_m x^m.
          \]
          But by setting
          \[
            \begin{dcases}
              a_i = c_i & \text{if } i \leq m \\
              a_i = 0   & \text{if } i > m
            \end{dcases}
          \]
          we have \(f(x) = \sum_{i \in \N} a_i x^i\), a contradiction.
  \end{itemize}
  From all cases above we derived contradictions.
  Thus \(\set{1, x, x^2, \dots}\) is a basis for \(\ps{\F}\).
\end{proof}

\begin{note}
  Observe that \cref{1.6.6} shows that a basis need not be finite.
  In fact, later in \cref{sec:1.6} it is shown that no basis for \(\ps{\F}\) can be finite.
  Hence not every vector space has a finite basis.
\end{note}

\begin{thm}\label{1.8}
  Let \(\V\) be a vector space over \(\F\) and \(\beta = \set{\seq{u}{1,2,,n}}\) be a subset of \(\V\).
  Then \(\beta\) is a basis for \(\V\) if and only if each \(v \in \V\) can be uniquely expressed as a linear combination of vectors of \(\beta\), that is, can be expressed in the form
  \[
    v = \seq[+]{a,u}{1,2,,n}
  \]
  for unique scalars \(\seq{a}{1,2,,n} \in \F\).
\end{thm}

\begin{proof}[\pf{1.8}]
  First suppose that \(\beta\) be a basis for \(\V\).
  If \(v \in V\), then \(v \in \spn{\beta}\) because \(\spn{\beta} = \V\).
  Thus \(v\) is a linear combination of the vectors of \(\beta\).
  Suppose that
  \[
    v = \seq[+]{a,u}{1,2,,n} \quad \text{and} \quad v = \seq[+]{b,u}{1,2,,n}
  \]
  are two such representations of \(v\).
  Subtracting the second equation from the first gives
  \[
    \zv = (a_1 - b_1) u_1 + (a_2 - b_2) u_2 + \cdots + (a_n - b_n) u_n.
  \]
  Since \(\beta\) is linearly independent, it follows that \(a_1 - b_1 = a_2 - b_2 = \cdots = a_n - b_n = 0\).
  Hence \(a_1 = b_1, a_2 = b_2, \dots, a_n = b_n\), and so \(v\) is uniquely expressible as a linear combination of the vectors of \(\beta\).

  Now suppose that each \(v \in \V\) can be uniquely expressed as a linear combination of vectors of \(\beta\).
  By \cref{1.4.3} and \cref{1.5} this means \(\V = \spn{\beta}\).
  Thus to show that \(\beta\) is a basis for \(\V\), by \cref{1.6.1} we need to show that \(\beta\) is linearly independent.
  This is true since
  \begin{align*}
             & \zv \in \V                                             &  & \text{(by \ref{vs3})}    \\
    \implies & \exists! \seq{a}{1,2,,n} \in \F :                                                    \\
             & \zv = \seq[+]{a,u}{1,2,,n} = \seq[+]{0u}{1,2,,n}       &  & \text{(by hypothesis)}   \\
    \implies & \seq[=]{a}{1,2,,n} = 0                                                               \\
    \implies & \set{\seq{u}{1,2,,n}} \text{ is linearly independent}. &  & \text{(by \cref{1.5.3})}
  \end{align*}
\end{proof}

\begin{note}
  \cref{1.8} shows that if the vectors \(\seq{u}{1,2,,n}\) form a basis for a vector space \(\V\), then every vector in \(\V\) can be uniquely expressed in the form
  \[
    v = \seq[+]{a,u}{1,2,,n}
  \]
  for appropriately chosen scalars \(\seq{a}{1,2,,n}\).
  Thus \(v\) determines a unique \(n\)-tuple of scalars \(\tuple{a}{1,2,,n}\) and, conversely, each \(n\)-tuple of scalars determines a unique vector \(v \in \V\) by using the entries of the \(n\)-tuple as the coefficients of a linear combination of \(\seq{u}{1,2,,n}\).
  This fact suggests that \(\V\) is like the vector space \(\vs{F}^n\), where \(n\) is the number of vectors in the basis for \(\V\).
  We see in \cref{sec:2.4} that this is indeed the case.
\end{note}

\begin{thm}\label{1.9}
  If a vector space \(\V\) over \(\F\) is generated by a finite set \(S\), then some subset of \(S\) is a basis for \(\V\).
  Hence \(\V\) has a finite basis.
\end{thm}

\begin{proof}[\pf{1.9}]
  If \(S = \varnothing\) or \(S = \set{\zv}\), then \(\V = \set{\zv}\) and \(\varnothing\) is a subset of \(S\) that is a basis for \(\V\).
  Otherwise \(S\) contains a nonzero vector \(u_1\).
  By \cref{1.5.4}(b), \(\set{u_1}\) is a linearly independent set.
  Continue, if possible, choosing vectors \(\seq{u}{2,,k}\) in \(S\) such that \(\set{\seq{u}{1,2,,k}}\) is linearly independent.
  Since \(S\) is a finite set, we must eventually reach a stage at which \(\beta = \set{\seq{u}{1,2,,k}}\) is a linearly independent subset of \(S\), but adjoining to \(\beta\) any vector in \(S \setminus \beta\) produces a linearly dependent set.
  We claim that \(\beta\) is a basis for \(\V\).
  Because \(\beta\) is linearly independent by construction, it suffices to show that \(\beta\) spans \(\V\).
  By \cref{1.5} we need to show that \(S \subseteq \spn{\beta}\).
  Let \(v \in S\).
  If \(v \in \beta\), then clearly \(v \in \spn{\beta}\).
  Otherwise, if \(v \notin \beta\), then the preceding construction shows that \(\beta \cup \set{v}\) is linearly dependent.
  So \(v \in \spn{\beta}\) by \cref{1.7}.
  Thus \(S \subseteq \spn{\beta}\).
\end{proof}

\begin{note}
  Because of the method by which the basis \(\beta\) was obtained in the proof of \cref{1.9}, this theorem is often remembered as saying that \emph{a finite spanning set for \(\V\) can be reduced to a basis for \(\V\).}
\end{note}

\begin{thm}[Replacement Theorem]\label{1.10}
  Let \(\V\) be a vector space over \(\F\) that is generated by a set \(G\) containing exactly \(n\) vectors, and let \(L\) be a linearly independent subset of \(\V\) containing exactly \(m\) vectors.
  Then \(m \leq n\) and there exists a subset \(H\) of \(G\) containing exactly \(n - m\) vectors such that \(L \cup H\) generates \(\V\).
\end{thm}

\begin{proof}[\pf{1.10}]
  The proof is by mathematical induction on \(m\).
  The induction begins with \(m = 0\);
  for in this case \(L = \varnothing\), and so taking \(H = G\) gives the desired result.

  Now suppose that the theorem is true for some integer \(m \geq 0\).
  We prove that the theorem is true for \(m + 1\).
  Let \(L = \set{\seq{v}{1,2,,m + 1}}\) be a linearly independent subset of \(\V\) consisting of \(m + 1\) vectors.
  By \cref{1.5.6}, \(\set{\seq{v}{1,2,,m}}\) is linearly independent, and so we may apply the induction hypothesis to conclude that \(m \leq n\) and that there is a subset \(\set{\seq{u}{1,2,n - m}}\) of \(G\) such that \(\set{\seq{v}{1,2,,m}} \cup \set{\seq{u}{1,2,n - m}}\) generates \(\V\).
  Thus there exist scalars \(\seq{a}{1,2,,m}, \seq{b}{1,2,,n - m} \in \F\) such that
  \begin{equation}\label{eq:1.6.1}
    \seq[+]{a,v}{1,2,,m} + \seq[+]{b,u}{1,2,,n - m} = v_{m + 1}
  \end{equation}
  Note that \(n - m > 0\), otherwise \(n - m = 0\) implies \(v_{m + 1}\) is a linear combination of \(\seq{v}{1,2,,m}\), which by \cref{1.7} contradicts the assumption that \(L\) is linearly independent.
  Hence \(n > m\);
  that is, \(n \geq m + 1\).
  Moreover, some \(b_i\), say \(b_1\), is nonzero, for otherwise we obtain the same contradiction.
  Solving \eqref{eq:1.6.1} for \(u_1\) gives
  \begin{multline*}
    u_1 = (-b_1^{-1} a_1) v_1 + (-b_1^{-1} a_2) v_2 + \cdots + (-b_1^{-1} a_m) v_m + (b_1^{-1}) v_{m + 1} \\
    + (-b_1^{-1} b_2) u_2 + \cdots + (-b_1^{-1} b_{n - m}) u_{n - m}.
  \end{multline*}
  Let \(H = \set{\seq{u}{2,,n - m}}\).
  Then \(u_1 \in \spn{L \cup H}\), and because \(\seq{v}{1,2,,m}\), \\
  \(\seq{u}{2,,n - m}\) are clearly in \(\spn{L \cup H}\), it follows that
  \[
    \set{\seq{v}{1,2,,m}, \seq{u}{1,2,,n - m}} \subseteq \spn{L \cup H}.
  \]
  Because \(\set{\seq{v}{1,2,,m}, \seq{u}{1,2,n - m}}\) generates \(\V\), \cref{1.5} implies that \\
  \(\spn{L \cup H} = \V\).
  Since \(H\) is a subset of \(G\) that contains \((n - m) - 1 = n - (m + 1)\) vectors, the theorem is true for \(m + 1\).
  This completes the induction.
\end{proof}

\begin{cor}\label{1.6.7}
  Let \(\V\) be a vector space over \(\F\) having a finite basis.
  Then every basis for \(\V\) contains the same number of vectors.
\end{cor}

\begin{proof}[\pf{1.6.7}]
  Suppose that \(\beta\) is a finite basis for \(\V\) that contains exactly \(n\) vectors, and let \(\gamma\) be any other basis for \(\V\).
  If \(\gamma\) contains more than \(n\) vectors, then we can select a subset \(S\) of \(\gamma\) containing exactly \(n + 1\) vectors.
  Since \(S\) is linearly independent and \(\beta\) generates \(\V\), the replacement theorem (\cref{1.10}) implies that \(n + 1 \leq n\), a contradiction.
  Therefore \(\gamma\) is finite, and the number \(m\) of vectors in \(\gamma\) satisfies \(m \leq n\).
  Reversing the roles of \(\beta\) and \(\gamma\) and arguing as above, we obtain \(n \leq m\).
  Hence \(m = n\).
\end{proof}

\begin{note}
  If a vector space has a finite basis, \cref{1.6.7} asserts that the number of vectors in \emph{any} basis for \(\V\) is an intrinsic property of \(\V\).
\end{note}

\begin{defn}\label{1.6.8}
  A vector space is called \textbf{finite-dimensional} if it has a basis consisting of a finite number of vectors.
  The unique number of vectors in each basis for \(\V\) is called the \textbf{dimension} of \(\V\) and is denoted by \(\dim(\V)\).
  A vector space that is not finite-dimensional is called \textbf{infinite-dimensional}.
\end{defn}
