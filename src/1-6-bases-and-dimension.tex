\section{Bases and Dimension}\label{sec:1.6}

\begin{defn}\label{1.6.1}
  A \textbf{basis} \(\beta\) for a vector space \(\V\) over \(\F\) is a linearly independent subset of \(\V\) that generates \(\V\).
  If \(\beta\) is a basis for \(\V\), we also say that the vectors of \(\beta\) form a basis for \(\V\).
\end{defn}

\begin{eg}\label{1.6.2}
  Recalling that \(\spn{\varnothing} = \set{\zv}\) and \(\varnothing\) is linearly independent, we see that \(\varnothing\) is a basis for the zero vector space.
\end{eg}

\begin{eg}\label{1.6.3}
  In \(\vs{F}^n\), let \(e_1 = (1, 0, 0, \dots, 0)\), \(e_2 = (0, 1, 0, \dots, 0)\), \dots, \(e_n = (0, 0, \dots, 0, 1)\);
  \(\set{\seq{e}{1,2,,n}}\) is readily seen to be a basis for \(\vs{F}^n\) and is called the \textbf{standard basis} for \(\vs{F}^n\).
\end{eg}

\begin{proof}[\pf{1.6.3}]
  By \cref{ex:1.5.4} we know that \(\set{\seq{e}{1,2,,n}}\) is linearly independent.
  By \cref{ex:1.4.7} we know that \(\vs{F}^n = \spn{\set{\seq{e}{1,2,,n}}}\).
  Thus by \cref{1.6.1} \(\set{\seq{e}{1,2,,n}}\) is a basis for \(\vs{F}^n\) over \(\F\).
\end{proof}

\begin{eg}\label{1.6.4}
  In \(\MS\), let \(E^{i j}\) denote the matrix whose only nonzero entry is a \(1\) in the \(i\)th row and \(j\)th column.
  Then \(\set{E^{i j} : 1 \leq i \leq m, 1 \leq j \leq n}\) is a basis for \(\MS\).
\end{eg}

\begin{proof}[\pf{1.6.4}]
  By \cref{ex:1.5.6} we know that \(\set{E^{i j} : 1 \leq i \leq m, 1 \leq j \leq n}\) is linearly independent.
  Since
  \begin{align*}
             & \forall A \in \MS, A = \begin{pmatrix}
      A_{1 1} & A_{1 2} & \cdots & A_{1 n} \\
      A_{2 1} & A_{2 2} & \cdots & A_{2 n} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      A_{m 1} & A_{m 2} & \cdots & A_{m n}
    \end{pmatrix}                                                              \\
             & = \sum_{i = 1}^m \sum_{j = 1}^n A_{i j} E^{i j}                                 &  & \text{(by \cref{1.2.9})} \\
    \implies & \forall A \in \MS, A \in \spn{\set{E^{i j} : 1 \leq i \leq m, 1 \leq j \leq n}} &  & \text{(by \cref{1.4.3})} \\
    \implies & \MS = \spn{\set{E^{i j} : 1 \leq i \leq m, 1 \leq j \leq n}},                   &  & \text{(by \cref{1.5})}
  \end{align*}
  by \cref{1.6.1} we know that \(\set{E^{i j} : 1 \leq i \leq m, 1 \leq j \leq n}\) is a basis for \(\MS\) over \(\F\).
\end{proof}

\begin{eg}\label{1.6.5}
  In \(\ps[n]{\F}\) the set \(\set{1, x, x^2, \dots, x^n}\) is a basis.
  We call this basis the \textbf{standard basis} for \(\ps[n]{\F}\).
\end{eg}

\begin{proof}[\pf{1.6.5}]
  By \cref{ex:1.5.5} we know that \(\set{1, x, x^2, \dots, x^n}\) is linearly independent.
  By \cref{ex:1.4.8} we know that \(\ps[n]{\F} = \spn{\set{1, x, x^2, \dots, x^n}}\).
  Thus by \cref{1.6.1} \(\set{1, x, x^2, \dots, x^n}\) is a basis for \(\ps[n]{\F}\) over \(\F\).
\end{proof}

\begin{eg}\label{1.6.6}
  In \(\ps{\F}\) the set \(\set{1, x, x^2, \dots}\) is a basis.
\end{eg}

\begin{proof}[\pf{1.6.6}]
  Suppose for sake of contradiction that \(\set{1, x, x^2, \dots}\) is not a basis for \(\ps{\F}\).
  Then by \cref{1.6.1} we can split into two cases:
  \begin{itemize}
    \item If \(\set{1, x, x^2, \dots}\) is linearly dependent, then by \cref{ex:1.5.14} we have
          \[
            \begin{dcases}
              \exists x^n \in \set{1, x, x^2, \dots} \\
              \exists \set{\seq{a}{0,1,2,}} \subseteq \F
            \end{dcases} : x^n = \sum_{i \in \N : i \neq n} a_i x^i.
          \]
          By setting \(a_n = -1\) we have
          \begin{align*}
                     & \sum_{i \in \N} a_i x^i = \zv = \sum_{i \in \N} 0x^i                                \\
            \implies & \seq[=]{a}{0,1,2,} = 0.                              &  & \text{(by \cref{1.2.11})}
          \end{align*}
          But this means \(a_n = 0\), a contradiction.
    \item If \(\ps{\F} \neq \spn{\set{1, x, x^2, \dots}}\), then by \cref{1.4.3} we have
          \[
            \exists f \in \ps{\F} : \forall \set{\seq{a}{0,1,2,}} \subseteq \F, f(x) \neq \sum_{i \in \N} a_i x^i.
          \]
          Let \(m\) be the degree of \(f\).
          Then by \cref{1.2.11} we have
          \[
            \exists \seq{c}{0,1,,m} \in \F : f(x) = c_0 + c_1 x + \cdots + c_m x^m.
          \]
          But by setting
          \[
            \begin{dcases}
              a_i = c_i & \text{if } i \leq m \\
              a_i = 0   & \text{if } i > m
            \end{dcases}
          \]
          we have \(f(x) = \sum_{i \in \N} a_i x^i\), a contradiction.
  \end{itemize}
  From all cases above we derived contradictions.
  Thus \(\set{1, x, x^2, \dots}\) is a basis for \(\ps{\F}\).
\end{proof}

\begin{note}
  Observe that \cref{1.6.6} shows that a basis need not be finite.
  In fact, later in \cref{sec:1.6} it is shown that no basis for \(\ps{\F}\) can be finite.
  Hence not every vector space has a finite basis.
\end{note}

\begin{thm}\label{1.8}
  Let \(\V\) be a vector space over \(\F\) and \(\beta = \set{\seq{u}{1,2,,n}}\) be a subset of \(\V\).
  Then \(\beta\) is a basis for \(\V\) if and only if each \(v \in \V\) can be uniquely expressed as a linear combination of vectors of \(\beta\), that is, can be expressed in the form
  \[
    v = \seq[+]{a,u}{1,2,,n}
  \]
  for unique scalars \(\seq{a}{1,2,,n} \in \F\).
\end{thm}

\begin{proof}[\pf{1.8}]
  First suppose that \(\beta\) be a basis for \(\V\).
  If \(v \in V\), then \(v \in \spn{\beta}\) because \(\spn{\beta} = \V\).
  Thus \(v\) is a linear combination of the vectors of \(\beta\).
  Suppose that
  \[
    v = \seq[+]{a,u}{1,2,,n} \quad \text{and} \quad v = \seq[+]{b,u}{1,2,,n}
  \]
  are two such representations of \(v\).
  Subtracting the second equation from the first gives
  \[
    \zv = (a_1 - b_1) u_1 + (a_2 - b_2) u_2 + \cdots + (a_n - b_n) u_n.
  \]
  Since \(\beta\) is linearly independent, it follows that \(a_1 - b_1 = a_2 - b_2 = \cdots = a_n - b_n = 0\).
  Hence \(a_1 = b_1, a_2 = b_2, \dots, a_n = b_n\), and so \(v\) is uniquely expressible as a linear combination of the vectors of \(\beta\).

  Now suppose that each \(v \in \V\) can be uniquely expressed as a linear combination of vectors of \(\beta\).
  By \cref{1.4.3} and \cref{1.5} this means \(\V = \spn{\beta}\).
  Thus to show that \(\beta\) is a basis for \(\V\), by \cref{1.6.1} we need to show that \(\beta\) is linearly independent.
  This is true since
  \begin{align*}
             & \zv \in \V                                             &  & \text{(by \ref{vs3})}    \\
    \implies & \exists! \seq{a}{1,2,,n} \in \F :                                                    \\
             & \zv = \seq[+]{a,u}{1,2,,n} = \seq[+]{0u}{1,2,,n}       &  & \text{(by hypothesis)}   \\
    \implies & \seq[=]{a}{1,2,,n} = 0                                                               \\
    \implies & \set{\seq{u}{1,2,,n}} \text{ is linearly independent}. &  & \text{(by \cref{1.5.3})}
  \end{align*}
\end{proof}

\begin{note}
  \cref{1.8} shows that if the vectors \(\seq{u}{1,2,,n}\) form a basis for a vector space \(\V\), then every vector in \(\V\) can be uniquely expressed in the form
  \[
    v = \seq[+]{a,u}{1,2,,n}
  \]
  for appropriately chosen scalars \(\seq{a}{1,2,,n}\).
  Thus \(v\) determines a unique \(n\)-tuple of scalars \(\tuple{a}{1,2,,n}\) and, conversely, each \(n\)-tuple of scalars determines a unique vector \(v \in \V\) by using the entries of the \(n\)-tuple as the coefficients of a linear combination of \(\seq{u}{1,2,,n}\).
  This fact suggests that \(\V\) is like the vector space \(\vs{F}^n\), where \(n\) is the number of vectors in the basis for \(\V\).
  We see in \cref{sec:2.4} that this is indeed the case.
\end{note}

\begin{thm}\label{1.9}
  If a vector space \(\V\) over \(\F\) is generated by a finite set \(S\), then some subset of \(S\) is a basis for \(\V\).
  Hence \(\V\) has a finite basis.
\end{thm}

\begin{proof}[\pf{1.9}]
  If \(S = \varnothing\) or \(S = \set{\zv}\), then \(\V = \set{\zv}\) and \(\varnothing\) is a subset of \(S\) that is a basis for \(\V\).
  Otherwise \(S\) contains a nonzero vector \(u_1\).
  By \cref{1.5.4}(b), \(\set{u_1}\) is a linearly independent set.
  Continue, if possible, choosing vectors \(\seq{u}{2,,k}\) in \(S\) such that \(\set{\seq{u}{1,2,,k}}\) is linearly independent.
  Since \(S\) is a finite set, we must eventually reach a stage at which \(\beta = \set{\seq{u}{1,2,,k}}\) is a linearly independent subset of \(S\), but adjoining to \(\beta\) any vector in \(S \setminus \beta\) produces a linearly dependent set.
  We claim that \(\beta\) is a basis for \(\V\).
  Because \(\beta\) is linearly independent by construction, it suffices to show that \(\beta\) spans \(\V\).
  By \cref{1.5} we need to show that \(S \subseteq \spn{\beta}\).
  Let \(v \in S\).
  If \(v \in \beta\), then clearly \(v \in \spn{\beta}\).
  Otherwise, if \(v \notin \beta\), then the preceding construction shows that \(\beta \cup \set{v}\) is linearly dependent.
  So \(v \in \spn{\beta}\) by \cref{1.7}.
  Thus \(S \subseteq \spn{\beta}\).
\end{proof}

\begin{note}
  Because of the method by which the basis \(\beta\) was obtained in the proof of \cref{1.9}, this theorem is often remembered as saying that \emph{a finite spanning set for \(\V\) can be reduced to a basis for \(\V\).}
\end{note}

\begin{thm}[Replacement Theorem]\label{1.10}
  Let \(\V\) be a vector space over \(\F\) that is generated by a set \(G\) containing exactly \(n\) vectors, and let \(L\) be a linearly independent subset of \(\V\) containing exactly \(m\) vectors.
  Then \(m \leq n\) and there exists a subset \(H\) of \(G\) containing exactly \(n - m\) vectors such that \(L \cup H\) generates \(\V\).
\end{thm}

\begin{proof}[\pf{1.10}]
  The proof is by mathematical induction on \(m\).
  The induction begins with \(m = 0\);
  for in this case \(L = \varnothing\), and so taking \(H = G\) gives the desired result.

  Now suppose that the theorem is true for some integer \(m \geq 0\).
  We prove that the theorem is true for \(m + 1\).
  Let \(L = \set{\seq{v}{1,2,,m + 1}}\) be a linearly independent subset of \(\V\) consisting of \(m + 1\) vectors.
  By \cref{1.5.6} \(\set{\seq{v}{1,2,,m}} \subseteq L\) is linearly independent, and so we may apply the induction hypothesis to conclude that \(m \leq n\) and that there is a subset \(\set{\seq{u}{1,2,,n - m}}\) of \(G\) such that \(\set{\seq{v}{1,2,,m}} \cup \set{\seq{u}{1,2,,n - m}}\) generates \(\V\).
  Thus there exist scalars \(\seq{a}{1,2,,m}, \seq{b}{1,2,,n - m} \in \F\) such that
  \begin{equation}\label{eq:1.6.1}
    \seq[+]{a,v}{1,2,,m} + \seq[+]{b,u}{1,2,,n - m} = v_{m + 1}
  \end{equation}
  Note that \(n - m > 0\), otherwise \(n - m = 0\) implies \(v_{m + 1}\) is a linear combination of \(\seq{v}{1,2,,m}\), which by \cref{1.7} contradicts the assumption that \(L\) is linearly independent.
  Hence \(n > m\);
  that is, \(n \geq m + 1\).
  Moreover, some \(b_i\), say \(b_1\), is nonzero, for otherwise we obtain the same contradiction.
  Solving \eqref{eq:1.6.1} for \(u_1\) gives
  \begin{multline*}
    u_1 = (-b_1^{-1} a_1) v_1 + (-b_1^{-1} a_2) v_2 + \cdots + (-b_1^{-1} a_m) v_m + (b_1^{-1}) v_{m + 1} \\
    + (-b_1^{-1} b_2) u_2 + \cdots + (-b_1^{-1} b_{n - m}) u_{n - m}.
  \end{multline*}
  Let \(H = \set{\seq{u}{2,,n - m}}\).
  Then \(u_1 \in \spn{L \cup H}\), and because \(\seq{v}{1,2,,m}\), \\
  \(\seq{u}{2,,n - m}\) are clearly in \(\spn{L \cup H}\), it follows that
  \[
    \set{\seq{v}{1,2,,m}, \seq{u}{1,2,,n - m}} \subseteq \spn{L \cup H}.
  \]
  Because \(\set{\seq{v}{1,2,,m}, \seq{u}{1,2,n - m}}\) generates \(\V\), \cref{1.5} implies that \\
  \(\spn{L \cup H} = \V\).
  Since \(H\) is a subset of \(G\) that contains \((n - m) - 1 = n - (m + 1)\) vectors, the theorem is true for \(m + 1\).
  This completes the induction.
\end{proof}

\begin{cor}\label{1.6.7}
  Let \(\V\) be a vector space over \(\F\) having a finite basis.
  Then every basis for \(\V\) contains the same number of vectors.
\end{cor}

\begin{proof}[\pf{1.6.7}]
  Suppose that \(\beta\) is a finite basis for \(\V\) that contains exactly \(n\) vectors, and let \(\gamma\) be any other basis for \(\V\).
  If \(\gamma\) contains more than \(n\) vectors, then we can select a subset \(S\) of \(\gamma\) containing exactly \(n + 1\) vectors.
  Since \(S\) is linearly independent and \(\beta\) generates \(\V\), the replacement theorem (\cref{1.10}) implies that \(n + 1 \leq n\), a contradiction.
  Therefore \(\gamma\) is finite, and the number \(m\) of vectors in \(\gamma\) satisfies \(m \leq n\).
  Reversing the roles of \(\beta\) and \(\gamma\) and arguing as above, we obtain \(n \leq m\).
  Hence \(m = n\).
\end{proof}

\begin{note}
  If a vector space has a finite basis, \cref{1.6.7} asserts that the number of vectors in \emph{any} basis for \(\V\) is an intrinsic property of \(\V\).
\end{note}

\begin{defn}\label{1.6.8}
  A vector space is called \textbf{finite-dimensional} if it has a basis consisting of a finite number of vectors.
  The unique number of vectors in each basis for \(\V\) is called the \textbf{dimension} of \(\V\) and is denoted by \(\dim(\V)\).
  A vector space that is not finite-dimensional is called \textbf{infinite-dimensional}.
\end{defn}

\begin{eg}\label{1.6.9}
  The vector space \(\set{\zv}\) has dimension zero.
\end{eg}

\begin{proof}[\pf{1.6.9}]
  By \cref{1.6.2} and \cref{1.6.7} we are done.
\end{proof}

\begin{eg}\label{1.6.10}
  The vector space \(\vs{F}^n\) has dimension \(n\).
\end{eg}

\begin{proof}[\pf{1.6.10}]
  By \cref{1.6.3} and \cref{1.6.7} we are done.
\end{proof}

\begin{eg}\label{1.6.11}
  The vector space \(\MS\) has dimension \(mn\).
\end{eg}

\begin{proof}[\pf{1.6.11}]
  By \cref{1.6.4} and \cref{1.6.7} we are done.
\end{proof}

\begin{eg}\label{1.6.12}
  The vector space \(\ps[n]{\F}\) has dimension \(n + 1\).
\end{eg}

\begin{proof}[\pf{1.6.12}]
  By \cref{1.6.5} and \cref{1.6.7} we are done.
\end{proof}

\begin{eg}\label{1.6.13}
  Over the field of complex numbers, the vector space of complex numbers has dimension \(1\).
  (A basis is \(\set{1}\).)
\end{eg}

\begin{proof}[\pf{1.6.13}]
  We have
  \begin{align*}
             & \forall c \in \C, c = c \cdot 1                               \\
    \implies & \spn{\set{1}} = \C              &  & \text{(by \cref{1.5})}   \\
    \implies & \#\pa{\set{1}} = 1 = \dim(\C).  &  & \text{(by \cref{1.6.8})}
  \end{align*}
\end{proof}

\begin{eg}\label{1.6.14}
  Over the field of real numbers, the vector space of complex numbers has dimension \(2\).
  (A basis is \(\set{1, i}\).)
\end{eg}

\begin{proof}[\pf{1.6.14}]
  We have
  \begin{align*}
             & \forall c \in \C, c = \Re(c) + i \Im(c) = \Re(c) \cdot 1 + \Im(c) \cdot i &  & (\Re(c), \Im(c) \in \R)  \\
    \implies & \spn{\set{1, i}} = \C                                                     &  & \text{(by \cref{1.5})}   \\
    \implies & \#\pa{\set{1, i}} = 2 = \dim(\C).                                         &  & \text{(by \cref{1.6.8})}
  \end{align*}
\end{proof}

\begin{note}
  From \cref{1.6.13} and \cref{1.6.14} we see that the dimension of a vector space depends on its field of scalars.
\end{note}

\begin{note}
  In the terminology of dimension, the first conclusion in the replacement theorem states that if \(\V\) is a finite-dimensional vector space over \(\F\), then no linearly independent subset of \(\V\) can contain more than \(\dim(\V)\) vectors.
  From this fact it follows that the vector space \(\ps{\F}\) over \(\F\) is infinite-dimensional because it has an infinite linearly independent set, namely \(\set{1, x, x^2, \dots}\).
  This set is, in fact, a basis for \(\ps{\F}\).
  Yet nothing that we have proved in this section guarantees an infinite-dimensional vector space must have a basis.
  In \cref{sec:1.7} it is shown, however, that \emph{every vector space has a basis}.
\end{note}

\begin{cor}\label{1.6.15}
  Let \(\V\) be a vector space over \(\F\) with dimension \(n\).
  \begin{enumerate}
    \item Any finite generating set for \(\V\) contains at least \(n\) vectors, and a generating set for \(\V\) that contains exactly \(n\) vectors is a basis for \(\V\).
    \item Any linearly independent subset of \(\V\) that contains exactly \(n\) vectors is a basis for \(\V\).
    \item Every linearly independent subset of \(\V\) can be extended to a basis for \(\V\).
  \end{enumerate}
\end{cor}

\begin{proof}[\pf{1.6.15}]
  Let \(\beta\) be a basis for \(\V\).
  \begin{enumerate}
    \item Let \(G\) be a finite generating set for \(\V\).
          By \cref{1.9} some subset \(H\) of \(G\) is a basis for \(\V\).
          \cref{1.6.7} implies that \(H\) contains exactly \(n\) vectors.
          Since a subset of \(G\) contains \(n\) vectors, \(G\) must contain at least \(n\) vectors.
          Moreover, if \(G\) contains exactly \(n\) vectors, then we must have \(H = G\), so that \(G\) is a basis for \(\V\).
    \item Let \(L\) be a linearly independent subset of \(\V\) containing exactly \(n\) vectors.
          It follows from the replacement theorem that there is a subset \(H\) of \(\beta\) containing \(n - n = 0\) vectors such that \(L \cup H\) generates \(\V\).
          Thus \(H = \varnothing\), and \(L\) generates \(\V\).
          Since \(L\) is also linearly independent, \(L\) is a basis for \(\V\).
    \item If \(L\) is a linearly independent subset of \(\V\) containing \(m\) vectors, then the replacement theorem asserts that there is a subset \(H\) of \(\beta\) containing exactly \(n - m\) vectors such that \(L \cup H\) generates \(\V\).
          Now \(L \cup H\) contains at most \(n\) vectors;
          therefore (a) implies that \(L \cup H\) contains exactly \(n\) vectors and that \(L \cup H\) is a basis for \(\V\).
  \end{enumerate}
\end{proof}

\begin{eg}\label{1.6.16}
  For \(k = 0, 1, \dots, n\), let \(p_k(x) = x^k + x^{k + 1} + \cdots + x^n\).
  It follows from \cref{1.5.5} and \cref{1.6.15}(b) that
  \[
    \set{p_0(x), p_1(x), \dots, p_n(x)}
  \]
  is a basis for \(\ps[n]{\F}\).
\end{eg}

\begin{thm}\label{1.11}
  Let \(\W\) be a subspace of a finite-dimensional vector space \(\V\) over \(\F\).
  Then \(\W\) is finite-dimensional and \(\dim(\W) \leq \dim(\V)\).
  Moreover, if \(\dim(\W) = \dim(\V)\), then \(\V = \W\).
\end{thm}

\begin{proof}[\pf{1.11}]
  Let \(\dim(\V) = n\).
  If \(\W = \set{\zv}\), then \(\W\) is finite-dimensional and \(\dim(\W) = 0 \leq n\).
  Otherwise, \(\W\) contains a nonzero vector \(x_1\);
  so \(\set{x_1}\) is a linearly independent set.
  Continue choosing vectors, \(\seq{x}{1,2,,k}\) in \(\W\) such that \(\set{\seq{x}{1,2,,k}}\) is linearly independent.
  Since no linearly independent subset of \(\V\) can contain more than \(n\) vectors, this process must stop at a stage where \(k \leq n\) and \(\set{\seq{x}{1,2,,k}}\) is linearly independent but adjoining any other vector from \(\W\) produces a linearly dependent set.
  \cref{1.7} implies that \(\set{\seq{x}{1,2,,k}}\) generates \(\W\), and hence it is a basis for \(\W\).
  Therefore \(\dim(\W) = k \leq n\).

  If \(\dim(\W) = n\), then a basis for \(\W\) is a linearly independent subset of \(\V\) containing \(n\) vectors.
  But \cref{1.6.15}(b) implies that this basis for \(\W\) is also a basis for \(\V\);
  so \(\W = \V\).
\end{proof}

\begin{eg}\label{1.6.17}
  The set of diagonal \(n \times n\) matrices is a subspace \(\W\) of \(\ms{n}{n}{\F}\)
  (see \cref{1.3.8}).
  A basis for \(\W\) is
  \[
    \set{E^{1 1}, E^{2 2}, \dots, E^{n n}},
  \]
  where \(E^{i j}\) is the matrix in which the only nonzero entry is a \(1\) in the \(i\)th row and \(j\)th column.
  Thus \(\dim(\W) = n\).
\end{eg}

\begin{proof}[\pf{1.6.17}]
  By \cref{ex:1.5.6} we know that \(\set{E^{1 1}, E^{2 2}, \dots, E^{n n}}\) is linearly independent.
  Since \(\W = \spn{\set{E^{1 1}, E^{2 2}, \dots, E^{n n}}}\), by \cref{1.6.15}(a) we know that \(\dim(\W) \leq n\).
  By \cref{1.6.15}(c) we also know that \(\dim(\W) \geq n\).
  Thus we have \(\dim(\W) = n\).
\end{proof}

\begin{eg}\label{1.6.18}
  The set of symmetric \(n \times n\) matrices is a subspace \(\W\) of \(\ms{n}{n}{\F}\) over \(\F\).
  A basis for \(\W\) is
  \[
    \set{A^{i j} : 1 \leq i \leq j \leq n}
  \]
  where \(A^{i j}\) is the \(n \times n\) matrix having \(1\) in the \(i\)th row and \(j\)th column, \(1\) in the \(j\)th row and \(i\)th column, and \(0\) elsewhere.
  It follows that
  \[
    \dim(\W) = n + (n - 1) + \cdots + 1 = \frac{1}{2} n(n + 1).
  \]
\end{eg}

\begin{proof}[\pf{1.6.18}]
  By \cref{ex:1.5.6} we see that each \(A^{i j}\) can only express as \(E^{i j} + E^{j i}\).
  Thus \(\set{A^{i j} : 1 \leq i \leq j \leq n}\) is linearly independent and by \cref{1.6.15}(c)we have  \(\dim(\W) \geq \#\pa{\set{A^{i j} : 1 \leq i \leq j \leq n}}\).
  Since
  \begin{align*}
             & \forall M \in \W, M_{i j} = M_{j i} = M_{i j} \cdot 1                                                 \\
    \implies & \forall M \in \W, M = \sum_{i = 1}^n \sum_{j = i}^n M_{i j} A^{i j} &  & \text{(by \cref{1.2.9})}     \\
    \implies & \W = \spn{\set{A^{i j} : 1 \leq i \leq j \leq n}}                   &  & \text{(by \cref{1.5})}       \\
    \implies & \dim(\W) \leq \#\pa{\set{A^{i j} : 1 \leq i \leq j \leq n}},        &  & \text{(by \cref{1.6.15}(a))}
  \end{align*}
  we have \(\dim(\W) = \#\pa{\set{A^{i j} : 1 \leq i \leq j \leq n}} = \frac{1}{2} n(n + 1)\).
\end{proof}

\begin{cor}\label{1.6.19}
  If \(\W\) is a subspace of a finite-dimensional vector space \(\V\) over \(\F\), then any basis for \(\W\) can be extended to a basis for \(\V\).
\end{cor}

\begin{proof}[\pf{1.6.19}]
  Let \(S\) be a basis for \(\W\).
  Because \(S\) is a linearly independent subset of \(\V\), \cref{1.6.15}(c) guarantees that \(S\) can be extended to a basis for \(\V\).
\end{proof}
