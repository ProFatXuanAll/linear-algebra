\section{Conditioning and the Rayleigh Quotient}\label{sec:6.10}

\begin{defn}\label{6.10.1}
  In \cref{sec:3.4}, we studied specific techniques that allow us to solve systems of linear equations in the form \(Ax = b\), where \(A \in \ms\) and \(b \in \ms[m][1][\F]\).
  Such systems often arise in applications to the real world.
  The coefficients in the system are frequently obtained from experimental data, and, in many cases, both \(m\) and \(n\) are so large that a computer must be used in the calculation of the solution.
  Thus two types of errors must be considered.
  First, experimental errors arise in the collection of data since no instruments can provide completely accurate measurements.
  Second, computers introduce roundoff errors.
  One might intuitively feel that small relative changes in the coefficients of the system cause small relative errors in the solution.
  A system that has this property is called \textbf{well-conditioned};
  otherwise, the system is called \textbf{ill-conditioned}.
\end{defn}

\begin{note}
  We now consider several examples of these types of errors, concentrating primarily on changes in \(b\) rather than on changes in the entries of \(A\).
  In addition, we assume that \(A\) is a square, complex (or real), invertible matrix since this is the case most frequently encountered in applications.

  Of course, we are really interested in \emph{relative changes} since a change in the solution of, say, \(10\), is considered large if the original solution is of the order \(10^{-2}\), but small if the original solution is of the order \(10^6\).
\end{note}

\begin{defn}\label{6.10.2}
  We use the notation \(\delta b\) to denote the vector \(b' - b\), where \(b\) is the vector in the original system and \(b'\) is the vector in the modified system.
  We now define the \textbf{relative change} in \(b\) to be the scalar \(\dfrac{\norm{\delta b}}{\norm{b}}\), where \(\norm{\cdot}\) denotes the standard norm on \(\C^n\) (or \(\R^n\));
  that is, \(\norm{b} = \sqrt{\inn{b, b}}\).
  Most of what follows, however, is true for any norm.
  Similar definitions hold for the \textbf{relative change} in \(x\).
\end{defn}

\begin{note}
  If the lines defined by the two equations are nearly coincident, then a small change in either line could greatly alter the point of intersection, that is, the solution to the system.
\end{note}

\begin{defn}\label{6.10.3}
  Let \(A\) be a complex (or real) \(n \times n\) matrix.
  Define the \textbf{(Euclidean) norm} of \(A\) by
  \[
    \norm{A} = \max_{x \neq \zv} \dfrac{\norm{Ax}}{\norm{x}},
  \]
  where \(x \in \C^n\) or \(x \in \R^n\).
\end{defn}

\begin{note}
  Intuitively, \(\norm{A}\) represents the maximum \emph{magnification} of a vector by the matrix \(A\).
  The question of whether or not this maximum exists, as well as the problem of how to compute it, can be answered by the use of the so-called \emph{Rayleigh quotient}.
\end{note}

\begin{defn}\label{6.10.4}
  Let \(B\) be an \(n \times n\) self-adjoint matrix.
  The \textbf{Rayleigh quotient} for \(x \neq \zv\) is defined to be the scalar \(R(x) = \dfrac{\inn{Bx, x}}{\norm{x}^2}\).
\end{defn}

\begin{thm}\label{6.43}
  For a self-adjoint matrix \(B \in \ms[n][n][\F]\), we have that \(\max_{x \neq \zv} R(x)\) is the largest eigenvalue of \(B\) and \(\min_{x \neq \zv} R(x)\) is the smallest eigenvalue of \(B\).
\end{thm}

\begin{proof}[\pf{6.10.4}]
  By \cref{6.19,6.20}, we may choose an orthonormal basis \(\set{\seq{v}{1,,n}}\) of eigenvectors of \(B\) over \(\F\) such that \(B v_i = \lambda_i v_i\) (\(i \in \set{1, \dots, n}\)), where \(\seq[\geq]{\lambda}{1,,n}\).
  (Recall that by \cref{6.4.10}(a), the eigenvalues of \(B\) are real.)
  Now, for \(x \in \vs{F}^n\), there exist scalars \(\seq{a}{1,,n} \in \F\) such that
  \[
    x = \sum_{i = 1}^n a_i v_i.
  \]
  Hence
  \begin{align*}
    R(x) & = \dfrac{\inn{Bx, x}}{\norm{x}^2}                                                           &  & \by{6.10.4}     \\
         & = \dfrac{\inn{\sum_{i = 1}^n a_i \lambda_i v_i, \sum_{j = 1}^n a_j v_j}}{\norm{x}^2}        &  & \by{5.1.2}      \\
         & = \dfrac{\sum_{i = 1}^n a_i \lambda_i \inn{v_i, \sum_{j = 1}^n a_j v_j}}{\norm{x}^2}        &  & \by{6.1.1}[a,b] \\
         & = \dfrac{\sum_{i = 1}^n \sum_{j = 1}^n a_i \conj{a_j} \lambda_i \inn{v_i, v_j}}{\norm{x}^2} &  & \by{6.1}[a,b]   \\
         & = \dfrac{\sum_{i = 1}^n a_i \conj{a_i} \lambda_i}{\norm{x}^2}                               &  & \by{6.1.12}     \\
         & = \dfrac{\sum_{i = 1}^n \lambda_i \abs{a_i}^2}{\norm{x}^2}                                  &  & \by{d.0.5}      \\
         & \leq \dfrac{\lambda_1 \sum_{i = 1}^n \abs{a_i}^2}{\norm{x}^2}                               &  & \by{6.2}[b]     \\
         & = \dfrac{\lambda_1 \norm{x}^2}{\norm{x}^2}                                                  &  & \by{ex:6.1.12}  \\
         & = \lambda_1.
  \end{align*}
  It is easy to see that \(R(v_1) = \lambda_1\), so we have demonstrated the first half of the theorem.
  The second half is proved similarly, i.e.,
  \begin{align*}
    R(x) & = \dfrac{\sum_{i = 1}^n \lambda_i \abs{a_i}^2}{\norm{x}^2}    &  & \text{(from the proof above)} \\
         & \geq \dfrac{\lambda_n \sum_{i = 1}^n \abs{a_i}^2}{\norm{x}^2} &  & \by{6.2}[b]                   \\
         & = \dfrac{\lambda_n \norm{x}^2}{\norm{x}^2}                    &  & \by{ex:6.1.12}                \\
         & = \lambda_n.
  \end{align*}
\end{proof}

\begin{cor}\label{6.10.5}
  For any square matrix \(A\), \(\norm{A}\) is finite and, in fact, equals \(\sqrt{\lambda}\), where \(\lambda\) is the largest eigenvalue of \(A^* A\).
\end{cor}

\begin{proof}[\pf{6.10.5}]
  Let \(B\) be the self-adjoint matrix \(A^* A\) (\cref{ex:6.4.18}(a)), and let \(\lambda\) be the largest eigenvalue of \(B\) (\cref{ex:6.4.17}(a)).
  Since, for \(x \neq \zv\),
  \begin{align*}
    0 & \leq \dfrac{\norm{Ax}^2}{\norm{x}^2}   &  & \by{6.2}[b] \\
      & = \dfrac{\inn{Ax, Ax}}{\norm{x}^2}     &  & \by{6.1.9}  \\
      & = \dfrac{\inn{A^* A x, x}}{\norm{x}^2} &  & \by{6.3.4}  \\
      & = \dfrac{\inn{Bx, x}}{\norm{x}^2}                       \\
      & = R(x),                                &  & \by{6.10.4}
  \end{align*}
  it follows from \cref{6.43} that \(\norm{A}^2 = \lambda\).
\end{proof}

\begin{note}
  Observe that the proof of \cref{6.10.5} shows that all the eigenvalues of \(A^* A\) are nonnegative
  (positive semidefinite by \cref{ex:6.4.17}(a)).
\end{note}

\begin{lem}\label{6.10.6}
  For any square matrix \(A\), \(\lambda\) is an eigenvalue of \(A^* A\) iff \(\lambda\) is an eigenvalue of \(A A^*\).
\end{lem}

\begin{proof}[\pf{6.10.6}]
  Let \(\lambda\) be an eigenvalue of \(A^* A\).
  If \(\lambda = 0\), then \(A^* A\) is not invertible.
  Hence \(A\) and \(A^*\) are not invertible (\cref{6.3.6}), so that \(\lambda\) is also an eigenvalue of \(A A^*\).
  The proof of the converse is similar.

  Now suppose that \(\lambda \neq 0\) and there exists \(x \neq \zv\) such that \(A^* A x = \lambda x\).
  Apply \(A\) to both sides to obtain
  \begin{align*}
    A A^* A x & = A (\lambda x) &  & \by{5.1.2}   \\
              & = \lambda (Ax). &  & \by{2.12}[b]
  \end{align*}
  Note that \(Ax \neq \zv\), otherwise we would have \(A^* A x = A^* \zv = \zv \neq \lambda x\).
  So we have that \(\lambda\) is an eigenvalue of \(A A^*\) and \(Ax\) is an eigenvector of \(A A^*\).

  Finally suppose that \(\lambda \neq 0\) and \(A A^* x = \lambda x\) for some \(x \neq \zv\).
  Apply \(A^*\) to both sides to obtain
  \begin{align*}
    A^* A A^* x & = A^* (\lambda x) &  & \by{5.1.2}   \\
                & = \lambda A^* x.  &  & \by{2.12}[b]
  \end{align*}
  Note that \(A^* x \neq \zv\), otherwise we would have \(A A^* x = A \zv = \zv \neq \lambda x\).
  So we have that \(\lambda\) is an eigenvalue of \(A^* A\) and \(A^* x\) is an eigenvector of \(A^* A\).
\end{proof}

\begin{cor}\label{6.10.7}
  Let \(A\) be an invertible matrix.
  Then \(\norm{A^{-1}} = 1 / \sqrt{\lambda}\), where \(\lambda\) is the smallest eigenvalue of \(A^* A\).
\end{cor}

\begin{proof}[\pf{6.10.7}]
  Recall that \(\lambda\) is an eigenvalue of an invertible matrix iff \(\lambda^{-1}\) is an eigenvalue of its inverse (\cref{ex:5.1.8}(b)).

  Now let \(\seq[\geq]{\lambda}{1,,n}\) be the eigenvalues of \(A^* A\), which by \cref{6.10.6} are the eigenvalues of \(A A^*\).
  Then \(\norm{A^{-1}}^2\) equals the largest eigenvalue of \((A A^*)^{-1}\) which equals \(1 / \lambda_n\).
  This is true since
  \begin{align*}
    \pa{A^{-1}}^* A^{-1} & = \pa{A^*}^{-1} A^{-1} &  & \by{ex:6.3.8} \\
                         & = (A A^*)^{-1}.        &  & \by{ex:2.4.4}
  \end{align*}
\end{proof}

\begin{note}
  For many applications, it is only the largest and smallest eigenvalues that are of interest.
  For example, in the case of vibration problems, the smallest eigenvalue represents the lowest frequency at which vibrations can occur.
\end{note}

\begin{cor}\label{6.10.8}
  Now that we know \(\norm{A}\) exists for every square matrix \(A\) (\cref{6.10.5}), we can make use of the inequality \(\norm{Ax} \leq \norm{A} \cdot \norm{x}\), which holds for every \(x\).
\end{cor}

\begin{proof}[\pf{6.10.8}]
  Suppose that \(x = \zv\).
  Then we have
  \begin{align*}
             & \zv = A \zv                                                    &  & \by{2.1.2}[a] \\
    \implies & 0 = \norm{\zv} = \norm{A \zv} = \norm{A} \cdot \norm{\zv} = 0. &  & \by{6.2}[b]
  \end{align*}

  Now suppose that \(x \neq \zv\).
  Then we have
  \begin{align*}
             & \norm{A} = \max_{y \neq \zv} \dfrac{\norm{Ay}}{\norm{y}} \geq \dfrac{\norm{Ax}}{\norm{x}} &  & \by{6.10.3} \\
    \implies & \norm{Ax} \leq \norm{A} \cdot \norm{x}.
  \end{align*}
\end{proof}

\begin{cor}\label{6.10.9}
  Assume in what follows that \(A\) is invertible, \(b \neq \zv\), and \(Ax = b\).
  For a given \(\delta b\), let \(\delta x\) be the vector that satisfies \(A(x + \delta x) = b + \delta b\).
  Prove that \(A(\delta x) = \delta b\), \(\delta x = A^{-1} (\delta b)\), and
  \[
    \dfrac{1}{\norm{A} \cdot \norm{A^{-1}}} \cdot \dfrac{\norm{\delta b}}{\norm{b}} \leq \dfrac{\norm{\delta x}}{\norm{x}} \leq \norm{A} \cdot \norm{A^{-1}} \cdot \dfrac{\norm{\delta b}}{\norm{b}}.
  \]
\end{cor}

\begin{proof}[\pf{6.10.9}]
  Since
  \begin{align*}
             & A (x + \delta x) = b + \delta b                 \\
    \implies & b + A (\delta x) = b + \delta b &  & (Ax = b)   \\
    \implies & A (\delta x) = \delta b         &  & \by{1.1}   \\
    \implies & (\delta x) = A^{-1} (\delta b), &  & \by{2.4.4}
  \end{align*}
  by \cref{6.10.8} we have
  \begin{align*}
    \norm{b}        & = \norm{Ax} \leq \norm{A} \cdot \norm{x};                            \\
    \norm{\delta b} & = \norm{A (\delta x)} \leq \norm{A} \cdot \norm{\delta x};           \\
    \norm{x}        & = \norm{A^{-1} b} \leq \norm{A^{-1}} \cdot \norm{b};                 \\
    \norm{\delta x} & = \norm{A^{-1} (\delta b)} \leq \norm{A^{-1}} \cdot \norm{\delta b}.
  \end{align*}
  Since \(b \neq \zv\), by \cref{2.4} we have \(x \neq \zv\).
  Thus
  \begin{align*}
    \dfrac{\norm{\delta x}}{\norm{x}} & \leq \norm{A} \cdot \dfrac{\norm{\delta x}}{\norm{b}}                     &  & (\norm{b} \leq \norm{A} \cdot \norm{x})                    \\
                                      & \leq \norm{A} \cdot \norm{A^{-1}} \cdot \dfrac{\norm{\delta b}}{\norm{b}} &  & (\norm{\delta x} \leq \norm{A^{-1}} \cdot \norm{\delta b})
  \end{align*}
  and
  \begin{align*}
    \dfrac{\norm{\delta x}}{\norm{x}} & \geq \dfrac{1}{\norm{A^{-1}}} \cdot \dfrac{\norm{\delta x}}{\norm{b}}                 &  & (\norm{x} \leq \norm{A^{-1}} \cdot \norm{b})          \\
                                      & \geq \dfrac{1}{\norm{A} \cdot \norm{A^{-1}}} \cdot \dfrac{\norm{\delta b}}{\norm{b}}. &  & (\norm{\delta b} \leq \norm{A} \cdot \norm{\delta x})
  \end{align*}
\end{proof}

\begin{defn}\label{6.10.10}
  The number \(\norm{A} \cdot \norm{A^{-1}}\) is called the \textbf{condition number} of \(A\) and is denoted \(\cond{A}\).
  It should be noted that the definition of \(\cond{A}\) depends on how the norm of \(A\) is defined.
  There are many reasonable ways of defining the norm of a matrix.
  In fact, the only property needed to establish the inequalities above is that \(\norm{Ax} \leq \norm{A} \cdot \norm{x}\) for all \(x\).
  We summarize these results in \cref{6.44}.
\end{defn}

\begin{thm}\label{6.44}
  For the system \(Ax = b\), where \(A\) is invertible and \(b \neq \zv\), the following statements are true.
  \begin{enumerate}
    \item For any norm \(\norm{\cdot}\), we have
          \[
            \dfrac{1}{\cond{A}} \dfrac{\norm{\delta b}}{\norm{b}} \leq \dfrac{\norm{\delta x}}{\norm{x}} \leq \cond{A} \dfrac{\norm{\delta b}}{\norm{b}}.
          \]
    \item If \(\norm{\cdot}\) is the Euclidean norm, then \(\cond{A} = \sqrt{\dfrac{\lambda_1}{\lambda_n}}\), where \(\lambda_1\) and \(\lambda_n\) are the largest and smallest eigenvalues, respectively, of \(A^* A\).
    \item \(\cond{A} \geq 1\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{6.44}(a)]
  See \cref{6.10.9,6.10.10}.
\end{proof}

\begin{proof}[\pf{6.44}(b)]
  See \cref{6.10.5,6.10.7,6.10.10}.
\end{proof}

\begin{proof}[\pf{6.44}(c)]
  This follows from \cref{6.44}(b).
\end{proof}

\begin{note}
  In \cref{ex:6.10.11} we show that \(\cond{A} = 1\) iff \(A\) is a scalar multiple of a unitary or orthogonal matrix.
  Moreover, it can be shown with some work that equality can be obtained in \cref{6.44}(a) by an appropriate choice of \(b\) and \(\delta b\).

  We can see immediately from \cref{6.44}(a) that if \(\cond{A}\) is close to \(1\), then a small relative error in \(b\) forces a small relative error in \(x\).
  If \(\cond{A}\) is large, however, then the relative error in \(x\) may be small even though the relative error in \(b\) is large, or the relative error in \(x\) may be large even though the relative error in \(b\) is small!
  In short, \(\cond{A}\) merely indicates the \emph{potential} for large relative errors.

  We have so far considered only errors in the vector \(b\).
  If there is an error \(\delta A\) in the coefficient matrix of the system \(Ax = b\), the situation is more complicated.
  For example, \(A + \delta A\) may fail to be invertible.
  But under the appropriate assumptions, it can be shown that a bound for the relative error in \(x\) can be given in terms of \(\cond{A}\).
  For example, Charles Cullen (Charles G. Cullen, An Introduction to Numerical Linear Algebra, PWS Publishing Co., Boston 1994, p. 60) shows that if \(A + \delta A\) is invertible, then
  \[
    \dfrac{\norm{\delta x}}{\norm{x + \delta x}} \leq \cond{A} \dfrac{\norm{\delta A}}{\norm{A}}.
  \]

  It should be mentioned that, in practice, one never computes \(\cond{A}\) from its definition, for it would be an unnecessary waste of time to compute \(A^{-1}\) merely to determine its norm.
  In fact, if a computer is used to find \(A^{-1}\), the computed inverse of \(A\) in all likelihood only approximates \(A^{-1}\), and the error in the computed inverse is affected by the size of \(\cond{A}\).
  So we are caught in a vicious circle!
  There are, however, some situations in which a usable approximation of \(\cond{A}\) can be found.
  Thus, in most cases, the estimate of the relative error in \(x\) is based on an estimate of \(\cond{A}\).
\end{note}

\exercisesection

\setcounter{ex}{2}
\begin{ex}\label{ex:6.10.3}
  Prove that if \(B\) is a symmetric real matrix, then \(\norm{B}\) is the eigenvalue of \(B\) with the largest absolute value.
\end{ex}

\begin{proof}[\pf{ex:6.10.3}]
  Since \(B\) is a symmetric real matrix, by \cref{6.4.8} we see that \(B^* = \tp{B} = B\).
  Thus by \cref{6.10.5} \(\norm{B}\) is the largest eigenvalue of \(B^* B = B^2\), which equals to the square of the eigenvalue of \(B\) with the largest absolute value.
\end{proof}

\setcounter{ex}{6}
\begin{ex}\label{ex:6.10.7}
  Let \(B\) be a symmetric matrix.
  Prove that \(\min_{x \neq \zv} R(x)\) equals the smallest eigenvalue of \(B\).
\end{ex}

\begin{proof}[\pf{ex:6.10.7}]
  See \cref{6.43}.
\end{proof}

\begin{ex}\label{ex:6.10.8}
  Prove that if \(\lambda\) is an eigenvalue of \(A A^*\), then \(\lambda\) is an eigenvalue of \(A^* A\).
  This completes the proof of \cref{6.10.6}.
\end{ex}

\begin{proof}[\pf{ex:6.10.8}]
  See \cref{6.10.6}.
\end{proof}

\begin{ex}\label{ex:6.10.9}
  Prove that if \(A\) is an invertible matrix and \(Ax = b\), then
  \[
    \dfrac{1}{\norm{A} \cdot \norm{A^{-1}}} \dfrac{\norm{\delta b}}{\norm{b}} \leq \dfrac{\norm{\delta x}}{\norm{x}}.
  \]
\end{ex}

\begin{proof}[\pf{ex:6.10.9}]
  See \cref{6.10.9}.
\end{proof}

\begin{ex}\label{ex:6.10.10}
  Prove the left inequality of \cref{6.44}(a).
\end{ex}

\begin{proof}[\pf{ex:6.10.10}]
  See \cref{6.10.9}.
\end{proof}

\begin{ex}\label{ex:6.10.11}
  Let \(A \in \ms[n][n][\F]\) be invertible.
  Prove that \(\cond{A} = 1\) iff \(A\) is a scalar multiple of an unitary or orthogonal matrix.
\end{ex}

\begin{proof}[\pf{ex:6.10.11}]
  Let \(\beta\) be the standard ordered basis for \(\vs{F}^n\) over \(\F\), and let \(\lambda_1, \lambda_n \in \F\) be the largest and the smallest eigenvalues, respectively, of \(A^* A\) (\cref{ex:6.4.18}(a)).

  First suppose that \(\cond{A} = 1\).
  Then we have
  \begin{align*}
             & 1 = \cond{A} = \sqrt{\dfrac{\lambda_1}{\lambda_n}}     &  & \by{6.44}[b]      \\
    \implies & \lambda_1 = \lambda_n                                                         \\
    \implies & \text{all eigenvalues of } A^* A \text{ are the same}. &  & \by{ex:6.4.17}[a]
  \end{align*}
  By \cref{6.19,6.20} there exists an unitary (orthogonal) matrix \(Q \in \ms[n][n][\F]\) such that \(Q^* A^* A Q = \lambda_1 I_n\).
  Then we have
  \begin{align*}
             & Q^* A^* A Q = \lambda_1 I_n                                                                                                       \\
    \implies & A^* A = Q (\lambda_1 I_n) Q^*                                                                              &  & \by{ex:6.1.23}[c] \\
             & = \lambda_1 Q I_n Q^*                                                                                      &  & \by{2.12}[b]      \\
             & = \lambda_1 I_n                                                                                            &  & \by{ex:6.1.23}[c] \\
    \implies & I_n = \dfrac{1}{\lambda_1} A^* A = \pa{\dfrac{1}{\sqrt{\lambda_1}} A}^* \pa{\dfrac{1}{\sqrt{\lambda_1}} A} &  & \by{6.3.2}[b]     \\
    \implies & \dfrac{1}{\sqrt{\lambda_1}} A \text{ is unitary (orthogonal)}                                              &  & \by{6.5.9}        \\
    \implies & A \text{ is a multiple of an unitary or orthogonal matrix}.
  \end{align*}

  Now suppose that \(A = \lambda Q\) where \(\lambda \in \F \setminus \set{0}\) and \(Q \in \ms[n][n][\F]\) is unitary (orthogonal).
  Then we have
  \begin{align*}
    A^* A & = \pa{\conj{\lambda} Q^*} (\lambda Q) &  & \by{6.3.2}[b]     \\
          & = \abs{\lambda}^2 I_n                 &  & \by{ex:6.1.23}[c]
  \end{align*}
  and thus \(\abs{\lambda}^2\) is the only eigenvalue of \(A^* A\).
  By \cref{6.10.5,6.10.7} this means \(\norm{A} = \sqrt{\abs{\lambda}^2}\) and \(\norm{A^{-1}} = 1 / \sqrt{\abs{\lambda}^2}\).
  By \cref{6.10.10} this means \(\cond{A} = 1\).
\end{proof}

\begin{ex}\label{ex:6.10.12}
  \begin{enumerate}
    \item Let \(A\) and \(B\) be square matrices that are unitarily equivalent.
          Prove that \(\norm{A} = \norm{B}\).
    \item Let \(\T\) be a linear operator on a finite-dimensional inner product space \(\V\) over \(\F\).
          Define
          \[
            \norm{\T} = \max_{x \neq \zv} \dfrac{\norm{\T(x)}}{\norm{x}}.
          \]
          Prove that \(\norm{\T} = \norm{[\T]_{\beta}}\), where \(\beta\) is any orthonormal basis for \(\V\) over \(\F\).
    \item Let \(\V\) be an infinite-dimensional inner product space with an orthonormal basis \(\set{\seq{v}{1,2,}}\).
          Let \(\T \in \ls(\V)\) such that \(\T(v_k) = k v_k\).
          Prove that \(\norm{\T}\) (defined in (b)) does not exist.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.10.12}(a)]
  By \cref{6.5.13} there exists an unitary matrix \(Q\) such that \(B = Q^* A Q\).
  Then we have
  \begin{align*}
    \norm{B} & = \norm{Q^* A Q}                                                        \\
             & = \max_{x \neq \zv} \dfrac{\norm{Q^* A Q x}}{\norm{x}} &  & \by{6.10.3} \\
             & = \max_{x \neq \zv} \dfrac{\norm{A Q x}}{\norm{Q x}}   &  & \by{6.5.1}  \\
             & = \max_{y \neq \zv} \dfrac{\norm{A y}}{\norm{y}}       &  & \by{2.5}    \\
             & = \norm{A}.                                            &  & \by{6.10.3}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.10.12}(b)]
  Let \(\dim(\V) = n\).
  Let \(\inn{\cdot, \cdot}\) be an inner product on \(\V\) over \(\F\) such that \(\norm{\cdot}^2 = \inn{\cdot, \cdot}\).
  Let \(\inn{\cdot, \cdot}'\) be the standard inner product on \(\vs{F}^n\) over \(\F\) and let \((\norm{\cdot}')^2 = \inn{\cdot, \cdot}'\).
  Let \(\beta = \set{\seq{v}{1,,n}}\) be an orthonormal basis for \(\V\) over \(\F\) with respect to \(\inn{\cdot, \cdot}\).
  Let \(\phi_{\beta} \in \ls(\V)\) be defined as in \cref{2.4.11}.
  Since
  \begin{align*}
    \forall x \in \V, \norm{x}^2 & = \norm{\sum_{i = 1}^n \inn{x, v_i} v_i}^2         &  & \by{6.5}       \\
                                 & = \sum_{i = 1}^k \abs{\inn{x, v_i}}^2 \norm{v_i}^2 &  & \by{ex:6.1.12} \\
                                 & = \sum_{i = 1}^k \abs{\inn{x, v_i}}^2              &  & \by{6.1.12}    \\
                                 & = \sum_{i = 1}^k \abs{(\phi_{\beta}(x))_i}^2       &  & \by{2.4.11}    \\
                                 & = \inn{\phi_{\beta}(x), \phi_{\beta}(x)}'          &  & \by{6.1.2}     \\
                                 & = (\norm{\phi_{\beta}(x)}')^2,                     &  & \by{6.1.9}
  \end{align*}
  we have
  \begin{align*}
    \norm{\T} & = \max_{x \in \V, x \neq \zv} \dfrac{\norm{\T(x)}}{\norm{x}}                                        &  & \by{6.10.3}                   \\
              & = \max_{x \in \V, x \neq \zv} \dfrac{\norm{\phi_{\beta}(\T(x))}'}{\norm{\phi_{\beta}(x)}'}          &  & \text{(from the proof above)} \\
              & = \max_{x \in \V, x \neq \zv} \dfrac{\norm{[\T]_{\beta} \phi_{\beta}(x)}'}{\norm{\phi_{\beta}(x)}'} &  & \by{2.4.12}                   \\
              & = \max_{x \in \vs{F}^n, x \neq \zv} \dfrac{\norm{[\T]_{\beta} x}'}{\norm{x}'}                                                          \\
              & = \norm{[\T]_{\beta}}.                                                                              &  & \by{6.10.3}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.10.12}(c)]
  Since
  \begin{align*}
             & \forall k \in \Z^+, \T(v_k) = k v_k                                                                                                                                \\
    \implies & \forall k \in \Z^+, \dfrac{\norm{\T(v_k)}}{\norm{v_k}} = \dfrac{\norm{k v_k}}{\norm{v_k}} = \dfrac{\abs{k} \norm{v_k}}{\norm{v_k}} = \abs{k} = k, &  & \by{6.2}[a]
  \end{align*}
  we know that \(\max_{x \neq \zv} \dfrac{\norm{\T(x)}}{\norm{x}} = \infty\), thus \(\norm{\T}\) is not well-defined.
\end{proof}

\begin{ex}\label{ex:6.10.13}
  Let \(A \in \ms[n][n][\F]\) be of rank \(r\) with the nonzero singular values \(\seq[\geq]{\sigma}{1,,r}\).
  Prove each of the following results.
  \begin{enumerate}
    \item \(\norm{A} = \sigma_1\).
    \item \(\norm{A^{\dag}} = \dfrac{1}{\sigma_r}\).
    \item If \(A\) is invertible (and hence \(r = n\)), then \(\cond{A} = \dfrac{\sigma_1}{\sigma_n}\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.10.13}(a)]
  By \cref{6.26} we know that \(\sigma_1^2\) is the largest eigenvalue of \(A^* A\).
  Thus by \cref{6.10.5} we have \(\norm{A} = \sigma_1\).
\end{proof}

\begin{proof}[\pf{ex:6.10.13}(b)]
  By \cref{6.29} we know that the nonzero singular values of \(A^{\dag}\) are \(\dfrac{1}{\sigma_r} \geq \dots \geq \dfrac{1}{\sigma_1}\).
  Thus by \cref{ex:6.10.13}(a) we know that \(\norm{A^{\dag}} = \dfrac{1}{\sigma_r}\).
\end{proof}

\begin{proof}[\pf{ex:6.10.13}(c)]
  By \cref{6.26} we know that \(\sigma_n^2\) is the smallest eigenvalue of \(A^* A\).
  Thus by \cref{6.10.7} we have \(\norm{A^{-1}} = \dfrac{1}{\sigma_n}\) and by \cref{6.10.10} \(\cond{A} = \dfrac{\sigma_1}{\sigma_n}\).
\end{proof}
