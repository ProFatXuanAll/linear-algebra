\section{Orthogonal Projections and the Spectral Theorem}\label{sec:6.6}

\begin{note}
  In this section, we rely heavily on \cref{6.16,6.17} to develop an elegant representation of a normal (if \(\F = \C\)) or a self-adjoint (if \(\F = \R\)) operator \(\T\) on a finite-dimensional inner product space.
  We prove that \(\T\) can be written in the form \(\seq[+]{\lambda,\T}{1,,k}\), where \(\seq{\lambda}{1,,k} \in \F\) are the distinct eigenvalues of \(\T\) and \(\seq{\T}{1,,k}\) are \emph{orthogonal projections}.

  Recall from \cref{2.1.14} that if \(\V = \W_1 \oplus \W_2\), then a linear operator \(\T\) on \(\V\) is the \textbf{projection on \(\W_1\) along \(\W_2\)} if, whenever \(x = x_1 + x_2\), with \(x_1 \in \W_1\) and \(x_2 \in \W_2\), we have \(\T(x) = x_1\).
  By \cref{ex:2.1.26}(a)(b), we have
  \[
    \rg{\T} = \W_1 = \set{x \in \V : \T(x) = x} \quad \text{and} \quad \ns{\T} = \W_2.
  \]
  So \(\V = \rg{\T} \oplus \ns{\T}\).
  Thus there is no ambiguity if we refer to \(\T\) as a ``projection on \(\W_1\)'' or simply as a ``projection.''
  In fact, it can be shown (see \cref{ex:2.3.17}) that \(\T\) is a projection iff \(\T = \T^2\).
  Because \(\V = \W_1 \oplus \W_2 = \W_1 \oplus \W_3\) does \emph{not} imply that \(\W_2 = \W_3\), we see that \(\W_1\) does not uniquely determine \(\T\).
  For an \emph{orthogonal} projection \(\T\), however, \(\T\) is uniquely determined by its range (see \cref{6.6.2}).
\end{note}

\begin{defn}\label{6.6.1}
  Let \(\V\) be an inner product space over \(\F\), and let \(\T \in \ls(\V)\) be a projection.
  We say that \(\T\) is an \textbf{orthogonal projection} if \(\rg{\T}^{\perp} = \ns{\T}\) and \(\ns{\T}^{\perp} = \rg{\T}\).
\end{defn}

\begin{note}
  By \cref{ex:6.2.13}(c), if \(\V\) is finite-dimensional, we need only assume that one of the preceding conditions holds.
  For example, if \(\rg{\T}^{\perp} = \ns{\T}\), then \(\rg{\T} = \rg{\T}^{\perp \perp} = \ns{\T}^{\perp}\).
\end{note}

\begin{prop}\label{6.6.2}
  Assume that \(\W\) is a finite-dimensional subspace of an inner product space \(\V\) over \(\F\).
  In the notation of \cref{6.6}, we can define a function \(\T : \V \to \V\) by \(\T(y) = u\).
  Then \(\T\) is the unique orthogonal projection on \(\W\).
  We call \(\T\) the \textbf{orthogonal projection} of \(\V\) on \(\W\).
\end{prop}

\begin{proof}[\pf{6.6.2}]
  For convenience, for each \(v \in \V\), we define the unique tuple \((v_1, v_2) \in \W \times \W^{\perp}\) such that \(v = v_1 + v_2\) (such definition is well-defined thanks to \cref{6.6}).

  First we show that \(\T\) is an orthogonal projection of \(\V\) on \(\W\).
  By \cref{2.1.14}, \cref{6.6} and \cref{ex:6.2.13}(d) we see that \(\T\) is a projection on \(\W\) along \(\W^{\perp}\).
  We claim that \(\inn{\T(x), y} = \inn{x, \T(y)}\) for all \(x, y \in \V\).
  This is true since
  \begin{align*}
    \forall x, y \in \V, \inn{x, \T(y)} & = \inn{x, y_1}                    &  & \by{2.1.14}   \\
                                        & = \inn{x_1 + x_2, y_1}            &  & \by{6.6}      \\
                                        & = \inn{x_1, y_1} + \inn{x_2, y_1} &  & \by{6.1.1}[a] \\
                                        & = \inn{x_1, y_1}                  &  & \by{6.2.9}    \\
                                        & = \inn{x_1, y_1} + \inn{x_1, y_2} &  & \by{6.2.9}    \\
                                        & = \inn{x_1, y_1 + y_2}            &  & \by{6.1}[a]   \\
                                        & = \inn{x_1, y}                    &  & \by{6.6}      \\
                                        & = \inn{\T(x), y}.                 &  & \by{2.1.14}
  \end{align*}
  Now we use the claim to show that \(\T\) is an orthogonal projection of \(\V\) on \(\W\).
  By \cref{ex:2.1.26}(b) we have \(\W = \rg{\T}\) and \(\W^{\perp} = \ns{\T}\).
  Since
  \begin{align*}
         & v \in \rg{\T}^{\perp}                                                    \\
    \iff & \forall y \in \rg{\T}, \inn{v, y} = 0 &  & \by{6.2.9}                    \\
    \iff & \forall x \in \V, \inn{v, \T(x)} = 0  &  & \by{2.1.10}                   \\
    \iff & \forall x \in \V, \inn{\T(v), x} = 0  &  & \text{(from the claim above)} \\
    \iff & \inn{\T(v), \T(v)} = 0                &  & (\T(v) \in \V)                \\
    \iff & \T(v) = \zv                           &  & \by{6.1}[d]                   \\
    \iff & v \in \ns{\T},                        &  & \by{2.1.10}
  \end{align*}
  we have \(\rg{\T}^{\perp} = \ns{\T}\).
  Since
  \begin{align*}
             & v \in \ns{\T}^{\perp}                                                                       \\
    \implies & \forall x \in \ns{\T}, \inn{v, x} = 0                    &  & \by{6.2.9}                    \\
    \implies & \forall x \in \ns{\T}, \inn{v, \T(x)} = \inn{v, \zv} = 0 &  & \by{2.1.10}                   \\
    \implies & \forall x \in \ns{\T}, \inn{\T(v), x} = 0                &  & \text{(from the claim above)} \\
    \implies & \forall x \in \ns{\T}, \inn{v - \T(v), x} = 0            &  & \by{6.1}[a,b]                 \\
    \implies & \inn{v - \T(v), v - \T(v)} = 0                           &  & (v - \T(v) \in \ns{\T})       \\
    \implies & v - \T(v) = \zv                                          &  & \by{6.1}[d]                   \\
    \implies & \T(v) = v                                                                                   \\
    \implies & v \in \rg{\T}                                            &  & \by{ex:2.1.26}[b]
  \end{align*}
  and
  \begin{align*}
             & v \in \rg{\T}                                                                         \\
    \implies & \forall x \in \ns{\T}, \inn{v, x} = \inn{\T(v), x} &  & \by{ex:2.1.26}[b]             \\
             & = \inn{v, \T(x)}                                   &  & \text{(from the claim above)} \\
             & = \inn{v, \zv}                                     &  & \by{2.1.10}                   \\
             & = 0                                                &  & \by{6.1}[c]                   \\
    \implies & v \in \ns{\T}^{\perp},                             &  & \by{6.2.9}
  \end{align*}
  we have \(\ns{\T}^{\perp} \subseteq \rg{\T}\) and \(\rg{\T} \subseteq \ns{\T}^{\perp}\).
  Thus \(\ns{\T}^{\perp} = \rg{\T}\).
  By \cref{6.6.1} \(\T\) is an orthogonal projection of \(\V\) on \(\W\).

  Now we show that \(\T\) is unique.
  For if \(\T\) and \(\U\) are orthogonal projections on \(\W\), then \(\rg{\T} = \W = \rg{\U}\).
  Hence \(\ns{\T} = \rg{\T}^{\perp} = \rg{\U}^{\perp} = \ns{\U}\), and since every projection is uniquely determined by its range and null space, we have \(\T = \U\).
\end{proof}

\begin{note}
  If \(\T\) is the orthogonal projection of \(\V\) on \(\W\), then \(\T(v)\) is the ``best approximation in \(\W\) to \(v\)'';
  that is, if \(w \in \W\), then \(\norm{w - v} \geq \norm{\T(v) - v}\).
  In fact, this approximation property characterizes \(\T\).
  These results follow immediately from \cref{6.2.12}.
\end{note}

\begin{defn}\label{6.6.3}
  As an application to Fourier analysis, recall the inner product space \(\vs{H}\) and the orthonormal set \(S\) in \cref{6.1.13}.
  Define a \textbf{trigonometric polynomial of degree \(n\)} to be a function \(g \in \vs{H}\) of the form
  \[
    g(t) = \sum_{j = -n}^n a_j f_j(t) = \sum_{j = -n}^n a_j e^{i j t},
  \]
  where \(a_n\) or \(a_{-n}\) is nonzero.
\end{defn}

\begin{prop}\label{6.6.4}
  Let \(f \in \vs{H}\).
  The best approximation to \(f\) by a trigonometric polynomial of degree less than or equal to \(n\) is the trigonometric polynomial whose coefficients are the Fourier coefficients of \(f\) relative to the orthonormal set \(S\) (see \cref{6.1.13}).
\end{prop}

\begin{proof}[\pf{6.6.4}]
  Let \(\W = \spn{\set{f_j : \abs{j} \leq n}}\), and let \(\T\) be the orthogonal projection of \(\vs{H}\) on \(\W\).
  The \cref{6.2.12} tells us that the best approximation to \(f\) by a function in \(\W\) is
  \[
    \T(f) = \sum_{j = -n}^n \inn{f, f_i} f_j.
  \]
\end{proof}

\begin{thm}\label{6.24}
  Let \(\V\) be an inner product space over \(\F\), and let \(\T\) be a linear operator on \(\V\).
  Then \(\T\) is an orthogonal projection iff \(\T\) has an adjoint \(\T^*\) and \(\T^2 = \T = \T^*\).
\end{thm}

\begin{proof}[\pf{6.24}]
  Suppose that \(\T\) is an orthogonal projection.
  Since \(\T^2 = \T\) because \(\T\) is a projection (see \cref{ex:2.3.17}), we only need to show that \(\T^*\) exists and \(\T = \T^*\).
  Now \(\V = \rg{\T} \oplus \ns{\T}\) and \(\rg{\T}^{\perp} = \ns{\T}\) (see \cref{6.6.2}).
  Let \(x, y \in \V\).
  Then we can write \(x = x_1 + x_2\) and \(y = y_1 + y_2\), where \(x_1, y_1 \in \rg{\T}\) and \(x_2, y_2 \in \ns{\T}\).
  Hence
  \begin{align*}
    \inn{x, \T(y)} & = \inn{x, y_1}                    &  & \by{2.1.14}   \\
                   & = \inn{x_1 + x_2, y_1}            &  & \by{6.6}      \\
                   & = \inn{x_1, y_1} + \inn{x_2, y_1} &  & \by{6.1.1}[a] \\
                   & = \inn{x_1, y_1}                  &  & \by{6.2.9}
  \end{align*}
  and
  \begin{align*}
    \inn{\T(x), y} & = \inn{x_1, y}                    &  & \by{2.1.14} \\
                   & = \inn{x_1, y_1 + y_2}            &  & \by{6.6}    \\
                   & = \inn{x_1, y_1} + \inn{x_1, y_2} &  & \by{6.1}[a] \\
                   & = \inn{x_1, y_1}.                 &  & \by{6.2.9}
  \end{align*}
  So \(\inn{x, \T(y)} = \inn{\T(x), y}\) for all \(x, y \in \V\);
  thus \(\T^*\) exists and \(\T = \T^*\).

  Now suppose that \(\T^2 = \T = \T^*\).
  Then \(\T\) is a projection by \cref{ex:2.3.17}, and hence we must show that \(\rg{\T} = \ns{\T}^{\perp}\) and \(\rg{\T}^{\perp} = \ns{\T}\).
  Let \(x \in \rg{\T}\) and \(y \in \ns{\T}\).
  Then \(x = \T(x) = \T^*(x)\), and so
  \begin{align*}
    \inn{x, y} & = \inn{\T(x), y}   &  & \by{2.1.14} \\
               & = \inn{\T^*(x), y}                  \\
               & = \inn{x, \T(y)}   &  & \by{6.9}    \\
               & = \inn{x, \zv}     &  & \by{2.1.14} \\
               & = 0.
  \end{align*}
  Therefore \(x \in \ns{\T}^{\perp}\), from which it follows that \(\rg{\T} \subseteq \ns{\T}^{\perp}\).

  Let \(y \in \ns{\T}^{\perp}\).
  We must show that \(y \in \rg{\T}\), that is, \(\T(y) = y\).
  Now
  \begin{align*}
    \norm{y - \T(y)}^2 & = \inn{y - \T(y), y - \T(y)}                   &  & \by{6.1.9}      \\
                       & = \inn{y, y - \T(y)} - \inn{\T(y), y - \T(y)}. &  & \by{6.1.1}[a,b]
  \end{align*}
  Since \(y - \T(y) \in \ns{\T}\) (see \cref{ex:2.3.17}), the first term must equal zero.
  But also
  \begin{align*}
    \inn{\T(y), y - \T(y)} & = \inn{y, \T^*(y - \T(y))} &  & \by{6.9}      \\
                           & = \inn{y, \T(y - \T(y))}   &  & (\T = \T^*)   \\
                           & = \inn{y, \T(y) - \T^2(y)} &  & \by{2.1.2}[c] \\
                           & = \inn{y, \zv}             &  & (\T^2 = \T)   \\
                           & = 0.                       &  & \by{6.1}[c]
  \end{align*}
  Thus by \cref{6.2}(b) \(y - \T(y) = \zv\);
  that is, \(y = \T(y) \in \rg{\T}\).
  Hence \(\rg{\T} = \ns{\T}^{\perp}\).

  Using the preceding results, we have \(\rg{\T}^{\perp} = \ns{\T}^{\perp \perp} \supseteq \ns{\T}\) by \cref{ex:6.2.13}(b).
  Now suppose that \(x \in \rg{\T}^{\perp}\).
  For any \(y \in \V\), we have \(\inn{\T(x), y} = \inn{x, \T^*(y)} = \inn{x, \T(y)} = 0\).
  So \(\T(x) = \zv\), and thus \(x \in \ns{\T}\).
  Hence \(\rg{\T}^{\perp} = \ns{\T}\).
\end{proof}

\begin{prop}\label{6.6.5}
  Let \(\V\) be a finite-dimensional inner product space over \(\F\), \(\W\) be a subspace of \(\V\) over \(\F\), and \(\T\) be the orthogonal projection of \(\V\) on \(\W\).
  We may choose an orthonormal basis \(\beta = \set{\seq{v}{1,,n}}\) for \(\V\) over \(\F\) such that \(\set{\seq{v}{1,,k}}\) is a basis for \(\W\) over \(\F\) (see \cref{6.5}).
  Then \([\T]_{\beta}\) is a diagonal matrix with ones as the first \(k\) diagonal entries and zeros elsewhere.
  In fact, \([\T]_{\beta}\) has the form
  \[
    \begin{pmatrix}
      I_k   & \zm_1 \\
      \zm_2 & \zm_3
    \end{pmatrix}.
  \]
  If \(\U\) is any projection on \(\W\), we may choose a basis \(\gamma\) for \(\V\) such that \([\U]_{\gamma}\) has the form above;
  however \(\gamma\) is not necessarily orthonormal.
\end{prop}

\begin{proof}[\pf{6.6.5}]
  Since
  \[
    \forall i \in \set{1, \dots, n}, \T(v_i) = \begin{dcases}
      v_i & \text{if } i \in \set{1, \dots, k}     \\
      \zv & \text{if } i \in \set{k + 1, \dots, n}
    \end{dcases},
  \]
  by \cref{2.2.4} we know that
  \[
    [\T]_{\beta} = \begin{pmatrix}
      I_k   & \zm_1 \\
      \zm_2 & \zm_3
    \end{pmatrix}.
  \]
  Thus by \cref{1.3.8} \([\T]_{\beta}\) is a diagonal matrix.
\end{proof}

\begin{thm}[The Spectral Theorem]\label{6.25}
  Suppose that \(\T\) is a linear operator on a finite-dimensional inner product space \(\V\) over \(\F\) with the distinct eigenvalues \(\seq{\lambda}{1,,k} \in \F\).
  Assume that \(\T\) is normal if \(\F = \C\) and that \(\T\) is self-adjoint if \(\F = \R\).
  For each \(i \in \set{1, \dots, k}\), let \(\W_i\) be the eigenspace of \(\T\) corresponding to the eigenvalue \(\lambda_i\), and let \(\T_i\) be the orthogonal projection of \(\V\) on \(\W_i\).
  Then the following statements are true.
  \begin{enumerate}
    \item \(\V = \seq[\oplus]{\W}{1,,k}\).
    \item If \(\W_i'\) denotes the direct sum of the subspaces \(\W_j\) for \(j \neq i\), then \(\W_i^{\perp} = \W_i'\).
    \item \(\T_i \T_j = \delta_{i j} \T_i\) for \(i, j \in \set{1, \dots, k}\).
    \item \(\IT[\V] = \seq[+]{\T}{1,,k}\).
    \item \(\T = \seq[+]{\lambda,\T}{1,,k}\).
  \end{enumerate}
  The set \(\set{\seq{\lambda}{1,,k}}\) of eigenvalues of \(\T\) is called the \textbf{spectrum} of \(\T\), the sum \(\IT[\V] = \seq[+]{\T}{1,,k}\) in (d) is called the \textbf{resolution of the identity operator} induced by \(\T\), and the sum \(\T = \seq[+]{\lambda,\T}{1,,k}\) in (e) is called the \textbf{spectral decomposition} of \(\T\).
  The spectral decomposition of \(\T\) is unique up to the order of its eigenvalues.
\end{thm}

\begin{proof}[\pf{6.25}(a)]
  By \cref{6.16,6.17}, \(\T\) is diagonalizable;
  so
  \[
    \V = \seq[\oplus]{\W}{1,,k}
  \]
  by \cref{5.11}.
\end{proof}

\begin{proof}[\pf{6.25}(b)]
  If \(x \in \W_i\) and \(y \in \W_j\) for some \(i \neq j\), then \(\inn{x, y} = 0\) by \cref{6.15}(d).
  It follows easily from this result that \(\W_i' \subseteq \W_i^{\perp}\).
  From \cref{6.25}(a), we have
  \[
    \dim(\W_i') = \sum_{\substack{j = 1 \\ j \neq i}}^k \dim(\W_j) = \dim(\V) - \dim(\W_i).
  \]
  On the other hand, we have \(\dim(\W_i^{\perp}) = \dim(\V) - \dim(\W_i)\) by \cref{6.7}(c).
  Hence \(\W_i' = \W_i^{\perp}\) (see \cref{1.11}).
\end{proof}

\begin{proof}[\pf{6.25}(c)]
  For each \(x \in \V\), we can define unique \(k\)-tuple \(\tuple{x}{1,,k} \in \seq[\times]{\W}{1,,k}\) such that \(x = \sum_{i = 1}^k x_i\) thanks to \cref{6.25}(a).
  Then we have
  \begin{align*}
    \forall x \in \V, \T_i(\T_j(x)) & = \T_i(x_j)                  &  & \by{6.6.1} \\
                                    & = \begin{dcases}
                                          x_i & \text{if } i = j    \\
                                          \zv & \text{if } i \neq j
                                        \end{dcases} &  & \by{6.6.1}               \\
                                    & = \delta_{i j} x_i           &  & \by{2.3.4} \\
                                    & = \delta_{i j} \T_i(x)       &  & \by{6.6.1}
  \end{align*}
  and thus \(\T_i \T_j = \delta_{i j} \T_i\).
\end{proof}

\begin{proof}[\pf{6.25}(d)]
  Since \(\T_i\) is the orthogonal projection of \(\V\) on \(\W_i\), it follows from \cref{6.25}(b) that \(\ns{\T_i} = \rg{\T_i}^{\perp} = \W_i^{\perp} = \W_i'\).
  Hence, for \(x \in \V\), we have \(x = \seq[+]{x}{1,,k}\), where \(\T_i(x) = x_i \in \W_i\) for all \(i \in \set{1, \dots, k}\).
\end{proof}

\begin{proof}[\pf{6.25}(e)]
  For \(x \in \V\), write \(x = \seq[+]{x}{1,,k}\), where \(x_i \in \W_i\) for all \(i \in \set{1, \dots, k}\).
  Then
  \begin{align*}
    \T(x) & = \T(\seq[+]{x}{1,,k})                                              \\
          & = \T(x_1) + \cdots + \T(x_k)                     &  & \by{2.1.1}[a] \\
          & = \seq[+]{\lambda,x}{1,,k}                       &  & \by{5.1.2}    \\
          & = \lambda_1 \T_1(x) + \cdots + \lambda_k \T_k(x) &  & \by{6.6.1}    \\
          & = (\lambda_1 \T_1 + \cdots + \lambda_k \T_k)(x). &  & \by{2.2.5}
  \end{align*}
\end{proof}

\begin{prop}\label{6.6.6}
  With the notation of \cref{6.25}, let \(\beta\) be the union of orthonormal bases of the \(\W_i\)'s and let \(m_i = \dim(\W_i)\).
  (Thus \(m_i\) is the multiplicity of \(\lambda_i\), see \cref{5.2.3,5.7}.)
  Then \([\T]_{\beta}\) has the form
  \[
    \begin{pmatrix}
      \lambda_1 I_{m_1} & \zm               & \cdots & \zm               \\
      \zm               & \lambda_2 I_{m_2} & \cdots & \zm               \\
      \vdots            & \vdots            &        & \vdots            \\
      \zm               & \zm               & \cdots & \lambda_k I_{m_k}
    \end{pmatrix};
  \]
  that is, \([\T]_{\beta}\) is a diagonal matrix in which the diagonal entries are the eigenvalues \(\lambda_i\) of \(\T\), and each \(\lambda_i\) is repeated \(m_i\) times.
  If \(\seq[+]{\lambda,\T}{1,,k}\) is the spectral decomposition of \(\T\), then it follows (from \cref{ex:6.6.7}(a)) that \(g(\T) = g(\lambda_1) \T_1 + \cdots + g(\lambda_k) \T_k\) for any polynomial \(g\).
\end{prop}

\begin{proof}[\pf{6.6.6}]
  By \cref{6.25}(a) and \cref{5.7} we see that this is true.
\end{proof}

\begin{cor}\label{6.6.7}
  Let \(\V\) be a finite-dimensional complex inner product space and let \(\T \in \ls(\V)\).
  Then \(\T\) is normal iff \(\T^* = g(\T)\) for some polynomial \(g\).
\end{cor}

\begin{proof}[\pf{6.6.7}]
  Suppose first that \(\T\) is normal.
  Let \(\T = \seq[+]{\lambda,\T}{1,,k}\) be the spectral decomposition of \(\T\).
  Taking the adjoint of both sides of the preceding equation, we have \(\T^* = \seq[+]{\conj{\lambda},\T}{1,,k}\) (\cref{6.11}(a)(b)) since each \(\T_i\) is self-adjoint (\cref{6.24}).
  Using the Lagrange interpolation formula (see \cref{1.6.20}), we may choose a polynomial \(g\) such that \(g(\lambda_i) = \conj{\lambda_i}\) for \(i \in \set{1, \dots, k}\).
  Then
  \begin{align*}
    g(\T) & = g(\lambda_1) \T_1 + \cdots + g(\lambda_k) \T_k             &  & \by{ex:6.6.7}[a] \\
          & = \seq[+]{\conj{\lambda},\T}{1,,k}                                                 \\
          & = \conj{\lambda}_1 \T_1^* + \cdots + \conj{\lambda}_k \T_k^* &  & \by{6.24}        \\
          & = (\seq[+]{\lambda,\T}{1,,k})^*                              &  & \by{6.11}[a,b]   \\
          & = \T^*.                                                      &  & \by{6.25}[e]
  \end{align*}

  Conversely, if \(\T^* = g(\T)\) for some polynomial \(g\), then \(\T\) commutes with \(\T^*\) since \(\T\) commutes with every polynomial in \(\T\).
  So \(\T\) is normal (\cref{6.4.3}).
\end{proof}

\begin{cor}\label{6.6.8}
  Let \(\V\) be a finite-dimensional complex inner product space and let \(\T \in \ls(\V)\).
  Then \(\T\) is unitary iff \(\T\) is normal and \(\abs{\lambda} = 1\) for every eigenvalue \(\lambda\) of \(\T\).
\end{cor}

\begin{proof}[\pf{6.6.8}]
  If \(\T\) is unitary, then \(\T\) is normal and every eigenvalue of \(\T\) has absolute value \(1\) by \cref{6.5.5}.

  Let \(\T = \seq[+]{\lambda,\T}{1,,k}\) be the spectral decomposition of \(\T\).
  If \(\abs{\lambda} = 1\) for every eigenvalue \(\lambda\) of \(\T\), then
  \begin{align*}
    \T \T^* & = (\seq[+]{\lambda,\T}{1,,k}) (\lambda_1 \T_1 + \cdots + \lambda_k \T_k)^*                 &  & \by{6.25}[e]                                           \\
            & = (\seq[+]{\lambda,\T}{1,,k}) (\conj{\lambda}_1 \T_1^* + \cdots + \conj{\lambda}_k \T_k^*) &  & \by{6.11}[a,b]                                         \\
            & = (\seq[+]{\lambda,\T}{1,,k}) (\seq[+]{\conj{\lambda},\T}{1,,k})                           &  & \by{6.24}                                              \\
            & = \abs{\lambda_1}^2 \T_1 + \cdots + \abs{\lambda_k}^2 \T_k                                 &  & \by{6.25}[c]                                           \\
            & = \seq[+]{\T}{1,,k}                                                                        &  & (\forall i \in \set{1, \dots, k}, \abs{\lambda_i} = 1) \\
            & = \IT[\V].                                                                                 &  & \by{6.25}[d]
  \end{align*}
  Hence \(\T\) is unitary by \cref{6.18}(a)(e).
\end{proof}

\begin{cor}\label{6.6.9}
  Let \(\V\) be a finite-dimensional complex inner product space and let \(\T \in \ls(\V)\).
  If \(\T\) is normal, then \(\T\) is self-adjoint iff every eigenvalue of \(\T\) is real.
\end{cor}

\begin{proof}[\pf{6.6.9}]
  Let \(\T = \seq[+]{\lambda,\T}{1,,k}\) be the spectral decomposition of \(\T\).
  Suppose that every eigenvalue of \(\T\) is real.
  Then
  \begin{align*}
    \T^* & = (\seq[+]{\lambda,\T}{1,,k})^*                                &  & \by{6.25}[e]                                        \\
         & = (\conj{\lambda}_1 \T_1^* + \cdots + \conj{\lambda}_k \T_k^*) &  & \by{6.11}[a,b]                                      \\
         & = \seq[+]{\conj{\lambda},\T}{1,,k}                             &  & \by{6.24}                                           \\
         & = \seq[+]{\lambda,\T}{1,,k}                                    &  & (\forall i \in \set{1, \dots, k}, \lambda_i \in \R) \\
         & = \T.                                                          &  & \by{6.25}[e]
  \end{align*}
  The converse has been proved by \cref{6.4.10}(a).
\end{proof}

\begin{cor}\label{6.6.10}
  Let \(\T\) be as in the spectral theorem with spectral decomposition \(\T = \seq[+]{\lambda,\T}{1,,k}\).
  Then each \(\T_j\) is a polynomial in \(\T\).
\end{cor}

\begin{proof}[\pf{6.6.10}]
  Using the Lagrange interpolation formula (see \cref{1.6.20}), we may choose a polynomial \(g_j\) (\(j \in \set{1, \dots, k}\)) such that \(g_j(\lambda_i) = \delta_{i j}\).
  Then
  \begin{align*}
    g_j(\T) & = g_j(\lambda_1) \T_1 + \cdots + g_j(\lambda_k) \T_k &  & \by{ex:6.6.7}[a] \\
            & = \delta_{1 j} \T_1 + \cdots + \delta_{k j} \T_k                           \\
            & = \T_j.                                              &  & \by{2.3.4}
  \end{align*}
\end{proof}

\exercisesection

\setcounter{ex}{3}
\begin{ex}\label{ex:6.6.4}
  Let \(\W\) be a finite-dimensional subspace of an inner product space \(\V\) over \(\F\).
  Show that if \(\T\) is the orthogonal projection of \(\V\) on \(\W\), then \(\IT[\V] - \T\) is the orthogonal projection of \(\V\) on \(\W^{\perp}\).
\end{ex}

\begin{proof}[\pf{ex:6.6.4}]
  For each \(x \in \V\), we can define a unique tuple \(\tuple{x}{1,2} \in \W \times \W^{\perp}\) such that \(x = x_1 + x_2\) thanks to \cref{6.6}.
  First we show that \(\IT[\V] - \T\) is a projection of \(\V\) on \(\W^{\perp}\).
  Since
  \begin{align*}
    \forall x \in \V, (\IT[\V] - \T)(x) & = \IT[\V](x) - \T(x)  &  & \by{2.2.5} \\
                                        & = x - \T(x)           &  & \by{2.1.9} \\
                                        & = x_1 + x_2 - x_1     &  & \by{6.6.1} \\
                                        & = x_2 \in \W^{\perp},
  \end{align*}
  by \cref{2.1.14} we see that \(\IT[\V] - \T\) is a projection of \(\V\) on \(\W^{\perp}\).

  Next we show that \(\inn{x, (\IT[\V] - \T)(y)} = \inn{(\IT[\V] - \T)(x), y}\) for all \(x, y \in \V\).
  This is true since
  \begin{align*}
    \forall x, y \in \V, \inn{x, (\IT[\V] - \T)(y)} & = \inn{x, y_2}                    &  & \text{(from the proof above)} \\
                                                    & = \inn{x_1 + x_2, y_2}            &  & \by{6.6}                      \\
                                                    & = \inn{x_1, y_2} + \inn{x_2, y_2} &  & \by{6.1.1}[a]                 \\
                                                    & = \inn{x_2, y_2}                  &  & \by{6.2.9}                    \\
                                                    & = \inn{x_2, y_1} + \inn{x_2, y_2} &  & \by{6.2.9}                    \\
                                                    & = \inn{x_2, y_1 + y_2}            &  & \by{6.1}[a]                   \\
                                                    & = \inn{x_2, y}                    &  & \by{6.6}                      \\
                                                    & = \inn{(\IT[\V] - \T)(x), y}.     &  & \text{(from the proof above)}
  \end{align*}
  By \cref{6.9} this means \((\IT[\V] - \T)^* = (\IT[\V] - \T)\).

  Now we show that \(\IT[\V] - \T\) is the orthogonal projection of \(\V\) on \(\W^{\perp}\).
  Since
  \begin{align*}
    (\IT[\V] - \T)^2 & = (\IT[\V] - \T)(\IT[\V] - \T)                                \\
                     & = \IT[\V]^2 - \IT[\V] \T - \T \IT[\V] + \T^2 &  & \by{2.2.5}  \\
                     & = \IT[\V] - 2 \T + \T^2                      &  & \by{2.1.10} \\
                     & = \IT[\V] - 2 \T + \T                        &  & \by{6.24}   \\
                     & = \IT[\V] - \T,
  \end{align*}
  by \cref{6.24} we know that \(\IT[\V] - \T\) is the orthogonal projection of \(\V\) on \(\W^{\perp}\).
\end{proof}

\begin{ex}\label{ex:6.6.5}
  Let \(\T\) be a linear operator on a finite-dimensional inner product space \(\V\) over \(\F\).
  \begin{enumerate}
    \item If \(\T\) is an orthogonal projection, prove that \(\norm{\T(x)} \leq \norm{x}\) for all \(x \in \V\).
          Give an example of a projection for which this inequality does not hold.
          What can be concluded about a projection for which the inequality is actually an equality for all \(x \in \V\)?
    \item Suppose that \(\T\) is a projection such that \(\norm{\T(x)} \leq \norm{x}\) for \(x \in \V\).
          Prove that \(\T\) is an orthogonal projection.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.6.5}(a)]
  Since \(\T\) is an orthogonal projection, by \cref{6.6.1} there exists a finite-dimensional subspace \(\W\) of \(\V\) over \(\F\) such that \(\V = \W \oplus \W^{\perp}\) and \(\W = \rg{\T}\).
  For each \(x \in \V\), we can define a unique tuple \(\tuple{x}{1,2} \in \W \times \W^{\perp}\) such that \(x = x_1 + x_2\) thanks to \cref{6.6}.
  Then we have
  \begin{align*}
    \forall x \in \V, \norm{\T(x)}^2 & = \norm{x_1}^2                   &  & \by{6.6}       \\
                                     & \leq \norm{x_1}^2 + \norm{x_2}^2 &  & \by{6.2}[b]    \\
                                     & = \norm{x_1 + x_2}^2             &  & \by{ex:6.1.10} \\
                                     & = \norm{x}^2.                    &  & \by{6.6}
  \end{align*}
  Thus by \cref{6.2}(b) we have \(\norm{\T(x)} \leq \norm{x}\) for all \(x \in \V\).

  Next we show that there exist some projection \(\U\) such that \(\norm{\U} > \norm{x}\) for some \(x \in \V\).
  Let \(\U \in \ls(\R^2)\) such that \(\U(x, y) = (x, x)\) for all \((x, y) \in \R^2\).
  Then we have
  \begin{align*}
             & \forall (x, y) \in \R^2, \U^2(x, y) = \U(\U(x, y)) = \U(x, x) = (x, x) = \U(x, y)                     \\
    \implies & \U \text{ is a projection of } \V \text{ on } \spn{\set{(1, 1)}}                  &  & \by{ex:2.3.17}
  \end{align*}
  and
  \begin{align*}
    \norm{\U(1, 0)} & = \norm{(1, 1)}                  \\
                    & = \sqrt{2}       &  & \by{6.1.2} \\
                    & > 1                              \\
                    & = \norm{(1, 0)}. &  & \by{6.1.2}
  \end{align*}

  Now suppose that \(\T\) is a projection such that \(\norm{\T(x)} = \norm{x}\) for all \(x \in \V\).
  Then by \cref{ex:6.1.17,2.4} we have \(\ns{\T} = \set{\zv}\).
  By \cref{ex:2.1.26}(b) we see that \(\rg{\T} = \V\), therefore \(\T = \IT[\V]\).
\end{proof}

\begin{proof}[\pf{ex:6.6.5}(b)]
  Since \(\T\) is a projection, by \cref{2.1.14} there exist two subspaces \(\W_1\) and \(\W_2\) of \(\V\) over \(\F\) such that \(\V = \W_1 \oplus \W_2\), \(\T_{\W_1} = \IT[\W_1]\) and \(\ns{\T} = \W_2\).
  For each \(x \in \V\), we define \((x_1, x_2) \in \W_1 \times \W_2\) such that \(x = x_1 + x_2\).
  We claim that \(\W_2 = \W_1^{\perp}\).
  Suppose for sake of contradiction that \(\W_2 \neq \W_1^{\perp}\).
  Then there exists a tuple \((y_1, y_2) \in \W_1 \times \W_2\) such that \(\inn{y_1, y_2} \neq 0\).
  Now fix one such tuple.
  For any \(c \in \F\), we have
  \begin{align*}
    \norm{c y_1 + y_2}^2     & = \norm{c y_1}^2 + 2 \Re(\inn{c y_1, y_2}) + \norm{y_2}^2 &  & \by{ex:6.1.19}[a] \\
                             & = \norm{c y_1}^2 + 2 \Re(c \inn{y_1, y_2}) + \norm{y_2}^2 &  & \by{6.1.1}[b]     \\
    \norm{\T(c y_1 + y_2)}^2 & = \norm{c \T(y_1) + \T(y_2)}                              &  & \by{2.1.2}[b]     \\
                             & = \norm{c y_1}^2.                                         &  & \by{2.1.14}
  \end{align*}
  By hypothesis we have
  \begin{align*}
             & \forall x \in \V, \norm{\T(x)} \leq \norm{x}                                \\
    \implies & \norm{\T(c y_1 + y_2)}^2 \leq \norm{c y_1 + y_2}^2                          \\
    \implies & \norm{c y_1}^2 \leq \norm{c y_1}^2 + 2 \Re(c \inn{y_1, y_2}) + \norm{y_2}^2 \\
    \implies & 0 \leq 2 \Re(c \inn{y_1, y_2}) + \norm{y_2}^2.
  \end{align*}
  Since \(\inn{y_1, y_2} \neq 0\), we can define \(c\) to be
  \[
    c = \frac{-\norm{y_2}^2}{\Re(\inn{y_1, y_2})}.
  \]
  Note that \(c \in \R\).
  Then we have
  \begin{align*}
    0 & \leq 2 \Re\pa{\frac{-\norm{y_2}^2}{\Re(\inn{y_1, y_2})} \inn{y_1, y_2}} + \norm{y_2}^2 \\
      & = 2 \frac{-\norm{y_2}^2}{\Re(\inn{y_1, y_2})} \Re\pa{\inn{y_1, y_2}} + \norm{y_2}^2    \\
      & = - 2 \norm{y_2}^2 + \norm{y_2}^2                                                      \\
      & = - \norm{y_2}^2
  \end{align*}
  and thus \(\norm{y_2}^2 \leq 0\).
  But by \cref{6.2}(b) this implies \(y_2 = \zv\), which means \(\inn{y_1, y_2} = 0\), a contradiction.
  Thus we must have \(\W_2 = \W_1^{\perp}\), and by \cref{6.6.2} \(\T\) is the orthogonal projection of \(\V\) on \(\W_1\).
\end{proof}

\begin{ex}\label{ex:6.6.6}
  Let \(\T\) be a normal operator on a finite-dimensional inner product space \(\V\) over \(\F\).
  Prove that if \(\T\) is a projection, then \(\T\) is also an orthogonal projection.
\end{ex}

\begin{proof}[\pf{ex:6.6.6}]
  By \cref{ex:2.1.26}(b) we have \(\V = \rg{\T} \oplus \ns{\T}\).
  We claim that \(\rg{\T}^{\perp} = \ns{\T}\).
  Since
  \begin{align*}
             & v \in \rg{\T}^{\perp}                                      \\
    \implies & \inn{\T(v), \T(v)} = \inn{v, \T^*(\T(v))} &  & \by{6.9}    \\
             & = \inn{v, \T(\T^*(v))}                    &  & \by{6.4.3}  \\
             & = 0                                       &  & \by{6.2.9}  \\
    \implies & \T(v) = \zv                               &  & \by{6.1}[d] \\
    \implies & v \in \ns{\T}                             &  & \by{2.1.10}
  \end{align*}
  and
  \begin{align*}
             & v \in \ns{\T}                                                            \\
    \implies & 0 = \inn{v, \zv}                                      &  & \by{6.1}[c]   \\
             & = \inn{v, \T^*(\zv)}                                  &  & \by{2.1.2}[a] \\
             & = \inn{v, \T^*(\T(v))}                                &  & \by{2.1.10}   \\
             & = \inn{v, \T(\T^*(v))}                                &  & \by{6.4.3}    \\
             & = \inn{\T^*(v), \T^*(v)}                              &  & \by{6.9}      \\
    \implies & \T^*(v) = \zv                                         &  & \by{6.1}[d]   \\
    \implies & \forall x \in \V, 0 = \inn{\zv, x} = \inn{\T^*(v), x} &  & \by{6.1}[d]   \\
             & = \inn{v, \T(x)}                                      &  & \by{6.9}      \\
    \implies & \forall y \in \rg{\T}, \inn{v, y} = 0                 &  & \by{2.1.10}   \\
    \implies & v \in \rg{\T}^{\perp},                                &  & \by{6.2.9}
  \end{align*}
  we have \(\rg{\T}^{\perp} \subseteq \ns{\T}\) and \(\ns{\T} \subseteq \rg{\T}^{\perp}\).
  Thus \(\rg{\T}^{\perp} = \ns{\T}\).
  Since \(\V\) is finite-dimensional, by \cref{ex:6.2.13}(c) we have \(\rg{\T} = (\rg{\T}^{\perp})^{\perp} = \ns{\T}^{\perp}\).
  Thus by \cref{6.6.1} \(\T\) is an orthogonal projection.
\end{proof}

\begin{ex}\label{ex:6.6.7}
  Let \(\T\) be a normal operator on a finite-dimensional complex inner product space \(\V\).
  Use the spectral decomposition \(\seq[+]{\lambda,\T}{1,,k}\) of \(\T\) to prove the following results.
  \begin{enumerate}
    \item If \(g\) is a polynomial, then
          \[
            g(\T) = \sum_{i = 1}^k g(\lambda_i) \T_i.
          \]
    \item If \(\T^n = \zT\) for some \(n \in \Z^+\), then \(\T = \zT\).
    \item Let \(\U\) be a linear operator on \(\V\).
          Then \(\U\) commutes with \(\T\) iff \(\U\) commutes with each \(\T_i\).
    \item There exists a normal operator \(\U\) on \(\V\) such that \(\U^2 = \T\).
    \item \(\T\) is invertible iff \(\lambda_i \neq 0\) for \(i \in \set{1, \dots, k}\).
    \item \(\T\) is a projection iff every eigenvalue of \(\T\) is \(1\) or \(0\).
    \item \(\T = -\T^*\) iff every \(\lambda_i\) is an imaginary number.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.6.7}(a)]
  Let \(\seq{a}{0,,n} \in \C\) such that \(g(x) = a_0 + a_1 x + \cdots + a_n x^n\) for all \(x \in \C\).
  Then we have
  \begin{align*}
    g(\T) & = a_0 \IT[\V] + a_1 \T + \cdots + a_n \T^n                    &  & \by{e.0.7}   \\
          & = a_0 (\seq[+]{\T}{1,,k})                                     &  & \by{6.25}[d] \\
          & \quad + a_1 (\seq[+]{\lambda,\T}{1,,k})                       &  & \by{6.25}[e] \\
          & \quad + \cdots                                                                  \\
          & \quad + a_n (\seq[+]{\lambda,\T}{1,,k})^n                     &  & \by{6.25}[e] \\
          & = a_0 (\seq[+]{\T}{1,,k})                                                       \\
          & \quad + a_1 (\seq[+]{\lambda,\T}{1,,k})                                         \\
          & \quad + \cdots                                                                  \\
          & \quad + a_n (\lambda_1^n \T_1 + \cdots + \lambda_k^n \T_k)    &  & \by{6.25}[c] \\
          & = (a_0 + a_1 \lambda_1 + \cdots + a_n \lambda_1^n) \T_1       &  & \by{2.2.5}   \\
          & \quad + \cdots                                                                  \\
          & \quad + (a_0 + a_1 \lambda_1 + \cdots + a_n \lambda_k^n) \T_k                   \\
          & = g(\lambda_1) \T_1 + \cdots + g(\lambda_k) \T_k                                \\
          & = \sum_{i = 1}^k g(\lambda_i) \T_i.
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.6.7}(b)]
  For each \(i \in \set{1, \dots, k}\), let \(\W_i\) be the eigenspace of \(\T\) corresponding to \(\lambda_i\).
  Then we have
  \begin{align*}
    \zT & = \T^n                                                           \\
        & = (\seq[+]{\lambda,\T}{1,,k})^n                &  & \by{6.25}[e] \\
        & = \lambda_1^n \T_1 + \cdots + \lambda_k^n \T_k &  & \by{6.25}[c] \\
        & = \sum_{i = 1}^k \lambda_i^n \T_i
  \end{align*}
  and
  \begin{align*}
             & \forall j \in \set{1, \dots, k}, \forall x \in \W_j, \zv = \zT(x) = \pa{\sum_{i = 1}^k \lambda_i^n \T_i}(x) &  & \by{2.1.9}   \\
    \implies & \forall j \in \set{1, \dots, k}, \forall x \in \W_j, \zv = \lambda_j^n x                                    &  & \by{6.6.1}   \\
    \implies & \forall j \in \set{1, \dots, k}, \lambda_j^n = 0                                                            &  & \by{5.7}     \\
    \implies & \forall j \in \set{1, \dots, k}, \lambda_j = 0                                                                                \\
    \implies & \T = \sum_{i = 1}^k 0 \T_i = \zT.                                                                           &  & \by{6.25}[e]
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.6.7}(c)]
  First suppose that \(\U \T = \T \U\).
  By \cref{6.6.10} we can define polynomial \(g_i\) such that \(g_i(\T) = \T_i\) for all \(i \in \set{1, \dots, k}\).
  Then we have
  \begin{align*}
    \forall i \in \set{1, \dots, k}, \U \T_i & = \U g_i(\T) &  & \by{6.6.10}     \\
                                             & = g_i(\T) \U &  & (\U \T = \T \U) \\
                                             & = \T_i \U.   &  & \by{6.6.10}
  \end{align*}

  Now suppose that \(\U \T_i = \T_i \U\) for all \(i \in \set{1, \dots, k}\).
  Then we have
  \begin{align*}
    \U \T & = \U (\seq[+]{\lambda,\T}{1,,k})                 &  & \by{6.25}[e] \\
          & = \lambda_1 \U \T_1 + \cdots + \lambda_k \U \T_k &  & \by{2.2.5}   \\
          & = \lambda_1 \T_1 \U + \cdots + \lambda_k \T_k \U                   \\
          & = (\seq[+]{\lambda,\T}{1,,k}) \U                 &  & \by{2.2.5}   \\
          & = \T \U.                                         &  & \by{6.25}[e]
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.6.7}(d)]
  For each \(i \in \set{1, \dots, k}\), define \(\mu_i \in \C\) such that \(\mu_i^2 = \lambda_i\).
  Let \(\U = \sum_{i = 1}^k \mu_i \T_i\).
  Then we have
  \begin{align*}
    \U^2 & = \pa{\sum_{i = 1}^k \mu_i \T_i}^2                   \\
         & = \sum_{i = 1}^k \mu_i^2 \T_i      &  & \by{6.25}[c] \\
         & = \sum_{i = 1}^k \lambda_i \T_i                      \\
         & = \T.                              &  & \by{6.25}[e]
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.6.7}(e)]
  We have
  \begin{align*}
         & \T \text{ is invertible}                                                                         \\
    \iff & \ns{\T} = \set{\zv}                                                            &  & \by{2.4,2.5} \\
    \iff & \forall x \in \V \setminus \set{\zv}, \T(x) \neq \zv                                             \\
    \iff & \forall x \in \V \setminus \set{\zv}, (\seq[+]{\lambda,\T}{1,,k})(x) \neq \zv  &  & \by{6.25}[e] \\
    \iff & \forall i \in \set{1, \dots, k}, \exists x \in \V : \lambda_i \T_i(x) \neq \zv &  & \by{6.25}[a] \\
    \iff & \forall i \in \set{1, \dots, k}, \lambda_i \neq 0.                             &  & \by{5.1.2}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.6.7}(f)]
  First suppose that \(\T\) is a projection.
  By \cref{ex:6.6.6} we see that \(\T\) is an orthogonal projection.
  By \cref{6.6.1,2.1.14} we see that \(\V = \rg{\T} \oplus \ns{\T}\) and \(\T_{\rg{\T}} = \IT[\rg{\T}]\).
  Then by \cref{2.1.10,5.1.2} we see that eigenvalues of \(\T\) are \(0\) or \(1\).

  Now suppose that eigenvalues of \(\T\) are \(0\) or \(1\).
  Let \(\lambda_1 = 1\) and \(\lambda_2 = 0\).
  Then by \cref{6.25}(e) we have \(\T = \seq[+]{\lambda,\T}{1,2} = \T_1\), where \(\T_1\) is a projection.
\end{proof}

\begin{proof}[\pf{ex:6.6.7}(g)]
  First suppose that \(\T = -\T^*\).
  Then we have
  \begin{align*}
             & \T = -\T^*                                                                                                           \\
    \implies & \seq[+]{\lambda,\T}{1,,k} = -(\seq[+]{\lambda,\T}{1,,k})^*                                      &  & \by{6.25}[e]    \\
             & = - (\conj{\lambda}_1 \T_1^* + \cdots + \conj{\lambda}_k \T_k^*)                                &  & \by{6.3.2}[a,b] \\
             & = - (\seq[+]{\conj{\lambda},\T}{1,,k})                                                          &  & \by{6.24}       \\
    \implies & (\lambda_1 + \conj{\lambda}_1) \T_1 + \cdots + (\lambda_k + \conj{\lambda}_k) \T_k = \zT                             \\
    \implies & \forall i \in \set{1, \dots, k}, \forall x \in \V, (\lambda_i + \conj{\lambda}_i) \T_i(x) = \zv &  & \by{6.25}[a]    \\
    \implies & \forall i \in \set{1, \dots, k}, \lambda_i + \conj{\lambda}_i = 0                                                    \\
    \implies & \forall i \in \set{1, \dots, k}, \Re(\lambda_i) = 0.                                            &  & \by{d.0.4}
  \end{align*}

  Now suppose that \(\Re(\lambda_i) = 0\) for all \(i \in \set{1, \dots, k}\).
  Then we have
  \begin{align*}
    -\T^* & = -(\seq[+]{\lambda,\T}{1,,k})^*                                 &  & \by{6.25}[e]    \\
          & = - (\conj{\lambda}_1 \T_1^* + \cdots + \conj{\lambda}_k \T_k^*) &  & \by{6.3.2}[a,b] \\
          & = - (\seq[+]{\conj{\lambda},\T}{1,,k})                           &  & \by{6.24}       \\
          & = - ((-\lambda_1) \T_1 + \cdots + (-\lambda_k) \T_k)                                  \\
          & = \seq[+]{\lambda,\T}{1,,k}                                      &  & \by{2.2.5}      \\
          & = \T.                                                            &  & \by{6.25}[e]
  \end{align*}
\end{proof}

\begin{ex}\label{ex:6.6.8}
  Use \cref{6.6.7} to show that if \(\T\) is a normal operator on a complex finite-dimensional inner product space and \(\U\) is a linear operator that commutes with \(\T\), then \(\U\) commutes with \(\T^*\).
\end{ex}

\begin{proof}[\pf{ex:6.6.8}]
  By \cref{6.6.7} there exists a polynomial \(g\) such that \(\T^* = g(\T)\).
  Then we have
  \begin{align*}
    \U \T^* & = \U g(\T) &  & \by{6.6.7}      \\
            & = g(\T) \U &  & (\U \T = \T \U) \\
            & = \T^* \U. &  & \by{6.6.7}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:6.6.9}
  Referring to \cref{ex:6.5.20}, prove the following facts about a partial isometry \(\U\).
  \begin{enumerate}
    \item \(\U^* \U\) is an orthogonal projection on \(\W\).
    \item \(\U \U^* \U = \U\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.6.9}(a)]
  Let \(\W\) be a subspace of \(\V\) over \(\F\) such that \(\norm{\U(x)} = \norm{x}\) for all \(x \in \W\) and \(\U(x) = \zv\) for all \(x \in \W^{\perp}\).
  By \cref{2.1.2}(a) this means \((\U^* \U)(\W^{\perp}) = \set{\zv}\).
  By \cref{ex:6.5.20}(e) we see that \((\U^* \U)_{\W} = \IT[\W]\).
  Thus by \cref{6.6.1,6.6.2} \(\U^* \U\) is the orthogonal projection of \(\V\) on \(\W\).
\end{proof}

\begin{proof}[\pf{ex:6.6.9}(b)]
  Continue from the proof of \cref{ex:6.6.9}(a), we see that
  \begin{align*}
    (\U \U^* \U)_{\W} & = \U((\U^* \U)_{\W})                       \\
                      & = \U(\IT[\W])        &  & \by{ex:6.6.9}[a] \\
                      & = \U_{\W}
  \end{align*}
  and
  \begin{align*}
    (\U \U^* \U)_{\W^{\perp}} & = \U((\U^* \U)_{\W^{\perp}})                       \\
                              & = \U(\zT)                    &  & \by{ex:6.6.9}[a] \\
                              & = \zT                        &  & \by{2.1.2}[a]    \\
                              & = \U_{\W^{\perp}}.           &  & \by{ex:6.5.20}
  \end{align*}
  Thus \(\U \U^* \U = \U\).
\end{proof}

\begin{ex}[Simultaneous diagonalization]\label{ex:6.6.10}
  Let \(\U\) and \(\T\) be normal operators on a finite-dimensional complex inner product space \(\V\) such that \(\T \U = \U \T\).
  Prove that there exists an orthonormal basis for \(\V\) over \(\C\) consisting of vectors that are eigenvectors of both \(\T\) and \(\U\).
\end{ex}

\begin{proof}[\pf{ex:6.6.10}]
  Let \(n = \dim(\V)\).
  We use induction on \(n\).
  For \(n = 1\), any orthonormal basis for \(\V\) over \(\C\) makes \([\T]_{\beta}\) and \([\U]_{\beta}\) diagonal matrices.
  Thus by \cref{5.2.8} \(\T, \U\) are simultaneously diagonalizable by some orthonormal bases and the base case holds.
  Suppose inductively that for some \(n \geq 1\), normal linear operators on \(\V\) which commute are simultaneously diagonalizable by some orthonormal bases.
  We need to show that this is true for \(n + 1\).
  Let \(\dim(\V) = n + 1\), let \(\T, \U \in \ls(\V)\) such that \(\T \T^* = \T^* \T\), \(\U \U^* = \U^* \U\) and \(\U \T = \T \U\).
  Since \(\T\) is normal, by \cref{6.16} there exists an orthonormal basis \(\beta\) for \(\V\) over \(\C\) consisting of eigenvectors of \(\T\).
  Let \(\lambda \in \C\) be an eigenvalue of \(\T\) and let \(E_{\lambda}\) be the eigenspace of \(\lambda\).
  Now we split into two cases:
  \begin{itemize}
    \item If \(E_{\lambda} = \V\), then any basis for \(\V\) over \(\C\) is consist of eigenvectors of \(\T\).
          Since \(\U\) is normal, by \cref{6.16} we know that there exists an orthonormal basis for \(\V\) over \(\C\) consist of eigenvectors of \(\U\).
          Thus by \cref{5.2.8} \(\T, \U\) are simultaneously diagonalizable by some orthonormal bases for \(\V\) over \(\C\).
    \item If \(E_{\lambda} \neq \V\), then by \cref{5.7} we have \(1 \leq \dim(E_{\lambda}) < n + 1\).
          By \cref{5.4.2}(e) we know that \(E_{\lambda}\) is \(\T\)-invariant.
          We claim that \(E_{\lambda}\) is \(\U\)-invariant.
          Since
          \begin{align*}
                     & \forall v \in E_{\lambda}, \T(v) = \lambda v             &  & \by{5.2.4}    \\
            \implies & \forall v \in E_{\lambda}, \lambda \U(v) = \U(\lambda v) &  & \by{2.1.1}[b] \\
                     & = \U(\T(v)) = \T(\U(v))                                  &  & \by{5.1.2}    \\
            \implies & \forall v \in E_{\lambda}, \U(v) \in E_{\lambda}         &  & \by{5.1.2}    \\
            \implies & \U(E_{\lambda}) \subseteq E_{\lambda},
          \end{align*}
          by \cref{5.4.1} we know that \(E_{\lambda}\) is \(\U\)-invariant.
          Since \(\T, \U\) are normal, by \cref{ex:6.4.7}(b) and \cref{ex:6.4.8} we know that \(E_{\lambda}^{\perp}\) is both \(\T\)- and \(\U\)-invariant.
          By \cref{6.7}(c) we know that \(\dim(E_{\lambda}^{\perp}) < n + 1\).
          Thus by induction hypothesis we can find some orthonormal bases \(\beta_1\) and \(\beta_2\) for \(E_{\lambda}\) and \(E_{\lambda}^{\perp}\) over \(\C\), respectively, consist of eigenvectors of both \(\T\) and \(\U\).
          By \cref{5.10}(d) and \cref{6.7}(c) we know that \(\beta = \beta_1 \cup \beta_2\) is an orthonormal basis for \(\V\) over \(\C\) consist of eigenvectors of both \(\T\) and \(\U\).
          Thus by \cref{5.2.8} \(\T, \U\) are simultaneously diagonalizable by some orthonormal bases for \(\V\) over \(\C\).
  \end{itemize}
  From all cases above the induction is closed.
\end{proof}
