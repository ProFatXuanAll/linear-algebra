\section{The Adjoint of a Linear Operator}\label{sec:6.3}

\begin{thm}\label{6.8}
  Let \(\V\) be a finite-dimensional inner product space over \(\F\), and let \(g \in \ls(\V, \F)\) be a linear transformation.
  Then there exists a unique vector \(y \in \V\) such that \(g(x) = \inn{x, y}\) for all \(x \in \V\).
\end{thm}

\begin{proof}[\pf{6.8}]
  Let \(\beta = \set{\seq{v}{1,,n}}\) be an orthonormal basis for \(\V\) over \(\F\), and let
  \[
    y = \sum_{i = 1}^n \conj{g(v_i)} v_i.
  \]
  Define \(h : \V \to \F\) by \(h(x) = \inn{x, y}\), which is clearly linear (by \cref{6.1.1}(a)(b)).
  Furthermore, for \(j \in \set{1, \dots, n}\) we have
  \begin{align*}
    h(v_j) & = \inn{v_j, y}                                                                  \\
           & = \inn{v_j, \sum_{i = 1}^n \conj{g(v_i)} v_i}                                   \\
           & = \sum_{i = 1}^n g(v_i) \inn{v_j, v_i}        &  & \text{(by \cref{6.1}(a)(b))} \\
           & = \sum_{i = 1}^n g(v_i) \delta_{j i}          &  & \text{(by \cref{6.1.12})}    \\
           & = g(v_j).
  \end{align*}
  Since \(g\) and \(h\) both agree on \(\beta\), we have that \(g = h\) by \cref{2.1.13}.
  To show that \(y\) is unique, suppose that \(g(x) = \inn{x, y'}\) for all \(x \in \V\).
  Then \(\inn{x, y} = \inn{x, y'}\) for all \(x \in \V\);
  so by \cref{6.1}(e), we have \(y = y'\).
\end{proof}

\begin{thm}\label{6.9}
  Let \(\V\) be a finite-dimensional inner product space over \(\F\), and let \(\T \in \ls(\V)\).
  Then there exists a unique function \(\T^* : \V \to \V\) such that \(\inn{\T(x), y} = \inn{x, \T^*(y)}\) for all \(x, y \in \V\).
  Furthermore, \(\T^*\) is linear.
  \(\T^*\) is called the \textbf{adjoint} of the operator \(\T\).
  The symbol \(\T^*\) is read ``\(\T\) star.''
\end{thm}

\begin{proof}[\pf{6.9}]
  Let \(y \in \V\).
  Define \(g : \V \to \F\) by \(g(x) = \inn{\T(x), y}\) for all \(x \in \V\).
  We first show that \(g\) is linear.
  Let \(x_1, x_2 \in \V\) and \(c \in \F\).
  Then
  \begin{align*}
    g(c x_1 + x_2) & = \inn{\T(c x_1 + x_2), y}                                                  \\
                   & = \inn{c \T(x_1) + \T(x_2), y}          &  & \text{(by \cref{2.1.2}(b))}    \\
                   & = c \inn{\T(x_1), y} + \inn{\T(x_2), y} &  & \text{(by \cref{6.1.1}(a)(b))} \\
                   & = c g(x_1) + g(x_2).
  \end{align*}
  Hence by \cref{2.1.2}(b) \(g \in \ls(\V, \F)\).

  We now apply \cref{6.8} to obtain a unique vector \(y' \in \V\) such that \(g(x) = \inn{x, y'}\);
  that is, \(\inn{\T(x), y} = \inn{x, y'}\) for all \(x \in \V\).
  Defining \(\T^* : \V \to \V\) by \(\T^*(y) = y'\), we have \(\inn{\T(x), y} = \inn{x, \T^*(y)}\).

  To show that \(\T^*\) is linear, let \(y_1, y_2 \in \V\) and \(c \in \F\).
  Then for any \(x \in \V\),
  we have
  \begin{align*}
    \inn{x, \T^*(c y_1 + y_2)} & = \inn{\T(x), c y_1 + y_2}                                                           \\
                               & = \conj{c} \inn{\T(x), y_1} + \inn{\T(x), y_2}     &  & \text{(by \cref{6.1}(a)(b))} \\
                               & = \conj{c} \inn{x, \T^*(y_1)} + \inn{x, \T^*(y_2)}                                   \\
                               & = \inn{x, c \T^*(y_1) + \T^*(y_2)}.                &  & \text{(by \cref{6.1}(a)(b))}
  \end{align*}
  Since \(x\) is arbitrary, \(\T^*(c y_1 + y_2) = c \T^*(y_1) + \T^*(y_2)\) by \cref{6.1}(e).

  Finally, we need to show that \(\T^*\) is unique. Suppose that \(\U \in \ls(\V)\) and that it satisfies \(\inn{\T(x), y} = \inn{x, \U(y)}\) for all \(x, y \in \V\).
  Then \(\inn{x, \T^*(y)} = \inn{x, \U(y)}\) for all \(x, y \in \V\), so \(\T^* = \U\).
\end{proof}

\begin{note}
  Thus by \cref{6.9} \(\T^*\) is the unique operator on \(\V\) satisfying \(\inn{\T(x), y} = \inn{x, \T^*(y)}\) for all \(x, y \in \V\).
  Note that we also have
  \begin{align*}
    \inn{x, \T(y)} & = \conj{\inn{\T(y), x}}   &  & \text{(by \cref{6.1.1}(c))} \\
                   & = \conj{\inn{y, \T^*(x)}} &  & \text{(by \cref{6.9})}      \\
                   & = \inn{\T^*(x), y};       &  & \text{(by \cref{6.1.1}(c))}
  \end{align*}
  so \(\inn{x, \T(y)} = \inn{\T^*(x), y}\) for all \(x, y \in \V\).
  We may view these equations symbolically as adding a \(*\) to \(\T\) when shifting its position inside the inner product symbol.
\end{note}

\begin{note}
  For an infinite-dimensional inner product space, the adjoint of a linear operator \(\T\) may be defined to be the function \(\T^*\) such that \(\inn{\T(x), y} = \inn{x, \T^*(y)}\) for all \(x, y \in \V\), provided it exists.
  Although the uniqueness and linearity of \(\T^*\) follow as before, the existence of the adjoint is not guaranteed (see \cref{ex:6.3.24}).
  The reader should observe the necessity of the hypothesis of finite-dimensionality in the proof of \cref{6.8}.
  Many of the theorems we prove about adjoints, nevertheless, do not depend on \(\V\) being finite-dimensional.
  \emph{Thus, unless stated otherwise, for the remainder of this chapter we adopt the convention that a reference to the adjoint of a linear operator on an infinite-dimensional inner product space assumes its existence}.
\end{note}

\begin{thm}\label{6.10}
  Let \(\V\) be a finite-dimensional inner product space over \(\F\), and let \(\beta\) be an orthonormal basis for \(\V\) over \(\F\).
  If \(\T \in \ls(\V)\), then
  \[
    [\T^*]_{\beta} = [\T]_{\beta}^*.
  \]
\end{thm}

\begin{proof}[\pf{6.10}]
  Let \(A = [\T]_{\beta}\), \(B = [\T^*]_{\beta}\), and \(\beta = \set{\seq{v}{1,,n}}\).
  Then we have
  \begin{align*}
    \forall i, j \in \set{1, \dots, n}, B_{i j} & = \inn{\T^*(v_j), v_i}        &  & \text{(by \cref{6.2.6})}    \\
                                                & = \conj{\inn{v_i, \T^*(v_j)}} &  & \text{(by \cref{6.1.1}(c))} \\
                                                & = \conj{\inn{\T(v_i), v_j}}   &  & \text{(by \cref{6.9})}      \\
                                                & = \conj{A_{j i}}              &  & \text{(by \cref{6.2.6})}    \\
                                                & = (A^*)_{i j}.                &  & \text{(by \cref{6.1.5})}
  \end{align*}
  Hence \(B = A^*\).
\end{proof}

\begin{cor}\label{6.3.1}
  Let \(A \in \ms{n}{n}{\F}\).
  Then \(\L_{A^*} = (\L_A)^*\).
\end{cor}

\begin{proof}[\pf{6.3.1}]
  If \(\beta\) is the standard ordered basis for \(\vs{F}^n\), then, by \cref{2.15}(a), we have \([\L_A]_{\beta} = A\).
  Hence \([(\L_A)^*]_{\beta} = [\L_A]_{\beta}^* = A^* = [\L_{A^*}]_{\beta}\), and so \((\L_A)^* = \L_{A^*}\).
\end{proof}

\begin{thm}\label{6.11}
  Let \(\V\) be an inner product space over \(\F\) and let \(\T, \U \in \ls(\V)\).
  Then
  \begin{enumerate}
    \item \((\T + \U)^* = \T^* + \U^*\);
    \item \((c \T)^* = \conj{c} \T^*\) for any \(c \in \F\);
    \item \((\T \U)^* = \U^* \T^*\);
    \item \(\T^{**} = \T\);
    \item \(\IT[\V]^* = \IT[\V]\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{6.11}(a)]
  Since
  \begin{align*}
    \forall x, y \in \V, \inn{x, (\T + \U)^*(y)} & = \inn{(\T + \U)(x), y}               &  & \text{(by \cref{6.9})}      \\
                                                 & = \inn{\T(x) + \U(x), y}              &  & \text{(by \cref{2.2.5})}    \\
                                                 & = \inn{\T(x), y} + \inn{\U(x), y}     &  & \text{(by \cref{6.1.1}(a))} \\
                                                 & = \inn{x, \T^*(y)} + \inn{x, \U^*(y)} &  & \text{(by \cref{6.9})}      \\
                                                 & = \inn{x, \T^*(y) + \U^*(y)}          &  & \text{(by \cref{6.1}(a))}   \\
                                                 & = \inn{x, (\T^* + \U^*)(y)},          &  & \text{(by \cref{2.2.5})}
  \end{align*}
  by \cref{6.1}(e) we know that \(\T^* + \U^* = (\T + \U)^*\).
\end{proof}

\begin{proof}[\pf{6.11}(b)]
  Since
  \begin{align*}
    \forall x, y \in \V, \inn{x, (c \T)^*(y)} & = \inn{(c \T)(x), y}           &  & \text{(by \cref{6.9})}      \\
                                              & = \inn{c \T(x), y}             &  & \text{(by \cref{2.2.5})}    \\
                                              & = c \inn{\T(x), y}             &  & \text{(by \cref{6.1.1}(b))} \\
                                              & = c \inn{x, \T^*(y)}           &  & \text{(by \cref{6.9})}      \\
                                              & = \inn{x, \conj{c} \T^*(y)}    &  & \text{(by \cref{6.1}(b))}   \\
                                              & = \inn{x, (\conj{c} \T^*)(y)}, &  & \text{(by \cref{2.2.5})}
  \end{align*}
  by \cref{6.1}(e) we know that \((c \T)^* = \conj{c} \T^*\).
\end{proof}

\begin{proof}[\pf{6.11}(c)]
  Since
  \begin{align*}
    \forall x, y \in \V, \inn{x, (\T \U)^*(y)} & = \inn{(\T \U)(x), y}      &  & \text{(by \cref{6.9})} \\
                                               & = \inn{\T(\U(x)), y}                                   \\
                                               & = \inn{\U(x), \T^*(y)}     &  & \text{(by \cref{6.9})} \\
                                               & = \inn{x, \U^*(\T^*(y))}   &  & \text{(by \cref{6.9})} \\
                                               & = \inn{x, (\U^* \T^*)(y)},
  \end{align*}
  by \cref{6.1}(e) we know that \((\T \U)^* = \U^* \T^*\).
\end{proof}

\begin{proof}[\pf{6.11}(d)]
  Since
  \begin{align*}
    \forall x, y \in \V, \inn{x, \T(y)} & = \inn{\T^*(x), y}     &  & \text{(by \cref{6.9})} \\
                                        & = \inn{x, \T^{**}(y)}, &  & \text{(by \cref{6.9})}
  \end{align*}
  by \cref{6.1}(e) we know that \(\T^{**} = \T\).
\end{proof}

\begin{proof}[\pf{6.11}(e)]
  Since
  \begin{align*}
    \forall x, y \in \V, \inn{x, \IT[\V](y)} & = \inn{x, y}             &  & \text{(by \cref{2.1.9})} \\
                                             & = \inn{\IT[\V](x), y}    &  & \text{(by \cref{2.1.9})} \\
                                             & = \inn{x, \IT[\V]^*(y)}, &  & \text{(by \cref{6.9})}
  \end{align*}
  by \cref{6.1}(e) we know that \(\IT[\V]^* = \IT[\V]\).
\end{proof}

\begin{note}
  The same proof works for \cref{6.11} in the infinite-dimensional case, provided that the existence of \(\T^*\) and \(\U^*\) is assumed.
\end{note}

\begin{cor}\label{6.3.2}
  Let \(A, B \in \ms{n}{n}{\F}\).
  Then
  \begin{enumerate}
    \item \((A + B)^* = A^* + B^*\);
    \item \((cA)^* = \conj{c} A^*\) for all \(c \in \F\).
    \item \((AB)^* = B^* A^*\);
    \item \(A^{**} = A\);
    \item \(I_n^* = I_n\).
  \end{enumerate}
\end{cor}

\begin{proof}[\pf{6.3.2}(a)]
  Since
  \begin{align*}
    \L_{(A + B)^*} & = (\L_{A + B})^*      &  & \text{(by \cref{6.3.1})}   \\
                   & = (\L_A + \L_B)^*     &  & \text{(by \cref{2.15}(c))} \\
                   & = (\L_A)^* + (\L_B)^* &  & \text{(by \cref{6.11}(a))} \\
                   & = \L_{A^*} + \L_{B^*} &  & \text{(by \cref{6.3.1})}   \\
                   & = \L_{A^* + B^*},     &  & \text{(by \cref{2.15}(c))}
  \end{align*}
  by \cref{2.15}(b) we have \((A + B)^* = A^* + B^*\).
\end{proof}

\begin{proof}[\pf{6.3.2}(b)]
  Since
  \begin{align*}
    \L_{(cA)^*} & = (\L_{cA})^*        &  & \text{(by \cref{6.3.1})}   \\
                & = (c \L_A)^*         &  & \text{(by \cref{2.15}(c))} \\
                & = \conj{c} (\L_A)^*  &  & \text{(by \cref{6.11}(b))} \\
                & = \conj{c} \L_{A^*}  &  & \text{(by \cref{6.3.1})}   \\
                & = \L_{\conj{c} A^*}, &  & \text{(by \cref{2.15}(c))}
  \end{align*}
  by \cref{2.15}(b) we have \((cA)^* = \conj{c} A^*\).
\end{proof}

\begin{proof}[\pf{6.3.2}(c)]
  Since
  \begin{align*}
    \L_{(AB)^*} & = (\L_{AB})^*       &  & \text{(by \cref{6.3.1})}   \\
                & = (\L_A \L_B)^*     &  & \text{(by \cref{2.15}(e))} \\
                & = (\L_B)^* (\L_A)^* &  & \text{(by \cref{6.11}(c))} \\
                & = \L_{B^*} \L_{A^*} &  & \text{(by \cref{6.3.1})}   \\
                & = \L_{B^* A^*},     &  & \text{(by \cref{2.15}(e))}
  \end{align*}
  by \cref{2.15}(b) we have \((AB)^* = B^* A^*\).
\end{proof}

\begin{proof}[\pf{6.3.2}(d)]
  Since
  \begin{align*}
    \L_A & = (\L_A)^{**}  &  & \text{(by \cref{6.11}(d))} \\
         & = (\L_{A^*})^* &  & \text{(by \cref{6.3.1})}   \\
         & = \L_{A^{**}}, &  & \text{(by \cref{6.3.1})}
  \end{align*}
  by \cref{2.15}(b) we have \(A = A^{**}\).
\end{proof}

\begin{proof}[\pf{6.3.2}(e)]
  Since
  \begin{align*}
    \L_{I_n} & = \IT[\vs{F}^n]   &  & \text{(by \cref{2.15}(f))} \\
             & = \IT[\vs{F}^n]^* &  & \text{(by \cref{6.11}(e))} \\
             & = (\L_{I_n})^*    &  & \text{(by \cref{2.15}(f))} \\
             & = \L_{I_n^*},     &  & \text{(by \cref{6.3.1})}
  \end{align*}
  by \cref{2.15}(b) we have \(I_n = I_n^*\).
\end{proof}

\begin{defn}\label{6.3.3}
  For \(x, y \in \vs{F}^n\), let \(\inn{x, y}_n\) denote the standard inner product of \(x\) and \(y\) in \(\vs{F}^n\).
  Recall that if \(x\) and \(y\) are regarded as column vectors, then \(\inn{x, y}_n = y^* x\).
\end{defn}

\begin{lem}\label{6.3.4}
  Let \(A \in \MS\), \(x \in \vs{F}^n\), and \(y \in \vs{F}^m\).
  Then
  \[
    \inn{Ax, y}_m = \inn{x, A^* y}_n.
  \]
\end{lem}

\begin{proof}[\pf{6.3.4}]
  We have
  \begin{align*}
    \inn{Ax, y}_m & = y^* (Ax)          &  & \text{(by \cref{6.3.3})}          \\
                  & = (y^* A) x         &  & \text{(by \cref{2.16})}           \\
                  & = (A^* y)^* x       &  & \text{(by \cref{ex:6.3.5}(c)(d))} \\
                  & = \inn{x, A^* y}_n. &  & \text{(by \cref{6.3.3})}
  \end{align*}
\end{proof}

\begin{lem}\label{6.3.5}
  Let \(A \in \MS\).
  Then \(\rk{A^* A} = \rk{A}\).
\end{lem}

\begin{proof}[\pf{6.3.5}]
  By the dimension theorem (\cref{2.3}), we need only show that, for \(x \in \vs{F}^n\), we have \(A^* Ax = \zv\) iff \(Ax = \zv\).
  Clearly, \(Ax = \zv\) implies that \(A^* Ax = \zv\).
  So assume that \(A^* Ax = \zv\).
  Then
  \begin{align*}
    0 & = \inn{A^* Ax, x}_n    &  & \text{(by \cref{6.1}(c))}      \\
      & = \inn{Ax, A^{**} x}_m &  & \text{(by \cref{6.3.4})}       \\
      & = \inn{Ax, Ax}_m,      &  & \text{(by \cref{ex:6.3.5}(d))}
  \end{align*}
  so that \(Ax = \zv\) by \cref{6.1}(d).
\end{proof}

\begin{cor}\label{6.3.6}
  If \(A \in \MS\) such that \(\rk{A} = n\), then \(A^* A\) is invertible.
\end{cor}

\begin{proof}[\pf{6.3.6}]
  By \cref{6.3.5} we have \(n = \rk{A} = \rk{A^* A}\).
  Since \(A^* A \in \ms{n}{n}{\F}\), by \cref{3.2.2} we know that \(A^* A\) is invertible.
\end{proof}

\begin{thm}\label{6.12}
  Let \(A \in \MS\) and \(y \in \vs{F}^m\).
  Then there exists \(x_0 \in \vs{F}^n\) such that \((A^* A) x_0 = A^* y\) and \(\norm{A x_0 - y} \leq \norm{Ax - y}\) for all \(x \in \vs{F}^n\).
  Furthermore, if \(\rk{A} = n\), then \(x_0 = (A^* A)^{-1} A^* y\).
\end{thm}

\begin{proof}[\pf{6.12}]
  Define \(\W = \set{Ax : x \in \vs{F}^n}\);
  that is, \(\W = \rg{\L_A}\).
  By \cref{6.2.12}, there exists a unique vector in \(\W\) that is closest to \(y\).
  Call this vector \(A x_0\), where \(x_0 \in \vs{F}^n\).
  Then \(\norm{A x_0 - y} \leq \norm{Ax - y}\) for all \(x \in \vs{F}^n\);
  so \(x_0\) has the property that \(E = \norm{A x_0 - y}\) is minimal, as desired.

  To develop a practical method for finding such an \(x_0\), we note from \cref{6.6,6.2.12} that \(A x_0 - y \in \W^{\perp}\);
  so \(\inn{Ax, A x_0 - y}_m = 0\) for all \(x \in \vs{F}^n\).
  Thus, by \cref{6.3.4}, we have that \(\inn{x, A^* (A x_0 - y)}_n = 0\) for all \(x \in \vs{F}^n\).
  By \cref{6.1}(e) we have \(A^* (A x_0 - y) = 0\).
  So we need only find a solution \(x_0\) to \(A^* Ax = A^* y\).
  If, in addition, we assume that \(\rk{A} = n\), then by \cref{6.3.5} we have \(x_0 = (A^* A)^{-1} A^* y\).
\end{proof}

\begin{note}
  Consider the following problem:
  An experimenter collects data by taking measurements \(\seq{y}{1,,m}\) at times \(\seq{t}{1,,m}\), respectively.
  Suppose that the data \((t_1, y_1), \dots, (t_m, y_m)\) are plotted as points in the plane.
  From this plot, the experimenter feels that there exists an essentially linear relationship between \(y\) and \(t\), say \(y = ct + d\), and would like to find the constants \(c\) and \(d\) so that the line \(y = ct + d\) represents the best possible fit to the data collected.
  One such estimate of fit is to calculate the error \(E\) that represents the sum of the squares of the vertical distances from the points to the line;
  that is,
  \[
    E = \sum_{i = 1}^m (y_i - (c t_i + d))^2.
  \]
  Thus the problem is reduced to finding the constants \(c\) and \(d\) that minimize \(E\).
  (For this reason the line \(y = ct + d\) is called the \textbf{least squares line}.)
  If we let
  \[
    A = \begin{pmatrix}
      t_1    & 1      \\
      \vdots & \vdots \\
      t_m    & 1
    \end{pmatrix}, x = \begin{pmatrix}
      c \\
      d
    \end{pmatrix}, y = \begin{pmatrix}
      y_1    \\
      \vdots \\
      y_m
    \end{pmatrix},
  \]
  then it follows that \(E = \norm{y - Ax}^2\).

  \cref{6.12} develop a general method for finding an explicit vector \(x_0 \in \vs{F}^n\) that minimizes \(E\);
  that is, given \(A \in \MS\), we find \(x_0 \in \vs{F}^n\) such that \(\norm{y - A x_0} \leq \norm{y - Ax}\) for all vectors \(x \in \vs{F}^n\).
  This method not only allows us to find the linear function that best fits the data, but also, for any positive integer \(n\), the best fit using a polynomial of degree at most \(n\).

  Suppose that the experimenter chose the times \(t_i\) for \(i \in \set{1, \dots, m}\) to satisfy
  \[
    \sum_{i = 1}^m t_i = 0.
  \]
  Then the two columns of \(A\) would be orthogonal, so \(A^* A\) would be a diagonal matrix (see \cref{ex:6.3.19}).
  In this case, the computations are greatly simplified.

  In practice, the \(m \times 2\) matrix \(A\) in our least squares application has rank equal to two, and hence \(A^* A\) is invertible by \cref{6.3.6}.
  For, otherwise, the first column of \(A\) is a multiple of the second column, which consists only of ones.
  But this would occur only if the experimenter collects all the data at exactly one time.

  Finally, the method in \cref{6.12} may also be applied if, for some \(k\), the experimenter wants to fit a polynomial of degree at most \(k\) to the data.
  For instance, if a polynomial \(y = a_0 + a_1 t + \cdots + a_k t^k\) of degree at most \(k\) is desired, the appropriate model is
  \[
    A = \begin{pmatrix}
      t_1^k  & \cdots & t_1    & 1      \\
      \vdots &        & \vdots & \vdots \\
      t_m^k  & \cdots & t_m    & 1
    \end{pmatrix}, x = \begin{pmatrix}
      a_k    \\
      \vdots \\
      a_0
    \end{pmatrix}, y = \begin{pmatrix}
      y_1    \\
      \vdots \\
      y_m
    \end{pmatrix}.
  \]
\end{note}

\begin{defn}\label{6.3.7}
  A solution \(s\) to \(Ax = b\) is called a \textbf{minimal solution} if \(\norm{s} \leq \norm{u}\) for all other solutions \(u\).
\end{defn}

\begin{thm}\label{6.13}
  Let \(A \in \MS\) and \(b \in \vs{F}^m\).
  Suppose that \(Ax = b\) is consistent.
  Then the following statements are true.
  \begin{enumerate}
    \item There exists exactly one minimal solution \(s\) of \(Ax = b\), and \(s \in \rg{\L_{A^*}}\).
    \item The vector \(s\) is the only solution to \(Ax = b\) that lies in \(\rg{\L_{A^*}}\);
          that is, if \(u\) satisfies \((A A^*) u = b\), then \(s = A^* u\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{6.13}(a)]
  For simplicity of notation, we let \(\W = \rg{\L_{A^*}}\) and \(\W' = \ns{\L_A}\).
  Let \(x\) be any solution to \(Ax = b\).
  By \cref{6.6}, \(x = s + y\) for some \(s \in \W\) and \(y \in \W^{\perp}\).
  But \(\W^{\perp} = \W'\) by \cref{ex:6.3.12}, and therefore \(b = Ax = As + Ay = As\).
  So \(s\) is a solution to \(Ax = b\) that lies in \(\W\).
  To prove (a), we need only show that \(s\) is the unique minimal solution.
  Let \(v\) be any solution to \(Ax = b\).
  By \cref{3.9}, we have that \(v = s + u\), where \(u \in \W'\).
  Since \(s \in \W\), which equals \(\W'\) by \cref{ex:6.3.12}, we have
  \[
    \norm{v}^2 = \norm{s + u}^2 = \norm{s}^2 + \norm{u}^2 \geq \norm{s}^2
  \]
  by \cref{ex:6.1.10}.
  Thus \(s\) is a minimal solution.
  We can also see from the preceding calculation that if \(\norm{v} = \norm{s}\), then \(u = \zv\);
  hence \(v = s\).
  Therefore \(s\) is the unique minimal solution to \(Ax = b\), proving (a).
\end{proof}

\begin{proof}[\pf{6.13}(b)]
  Assume that \(v\) is also a solution to \(Ax = b\) that lies in \(\W\).
  Then
  \[
    v - s \in \W \cap \W' = \W \cap \W^{\perp} = \set{\zv};
  \]
  so \(v = s\).

  Finally, suppose that \((A A^*) u = b\), and let \(v = A^* u\).
  Then \(v \in \W\) and \(Av = b\).
  Therefore \(s = v = A^* u\) by the discussion above.
\end{proof}

\begin{note}
  \cref{6.13} assures that every consistent system of linear equations has a unique minimal solution and provides a method for computing it.
\end{note}

\exercisesection

\setcounter{ex}{4}
\begin{ex}\label{ex:6.3.5}
  Let \(A, B \in \MS\) and let \(C \in \ms{n}{p}{\F}\).
  Then
  \begin{enumerate}
    \item \((A + B)^* = A^* + B^*\);
    \item \((cA)^* = \conj{c} A^*\) for all \(c \in \F\).
    \item \((AC)^* = C^* A^*\);
    \item \(A^{**} = A\);
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.3.5}(a)]
  We have
  \begin{align*}
    ((A + B)^*)_{i j} & = \conj{(A + B)_{j i}}            &  & \text{(by \cref{6.1.5})}  \\
                      & = \conj{A_{j i} + B_{j i}}        &  & \text{(by \cref{1.2.9})}  \\
                      & = \conj{A_{j i}} + \conj{B_{j i}} &  & \text{(by \cref{d.2}(b))} \\
                      & = (A^*)_{i j} + (B^*)_{i j}       &  & \text{(by \cref{6.1.5})}  \\
                      & = (A^* + B^*)_{i j}               &  & \text{(by \cref{1.2.9})}
  \end{align*}
  where \(i \in \set{1, \dots, n}\) and \(j \in \set{1, \dots, m}\).
  Thus by \cref{1.2.8} \((A + B)^* = A^* + B^*\).
\end{proof}

\begin{proof}[\pf{ex:6.3.5}(b)]
  We have
  \begin{align*}
    ((cA)^*)_{i j} & = \conj{(cA)_{j i}}       &  & \text{(by \cref{6.1.5})}  \\
                   & = \conj{c A_{j i}}        &  & \text{(by \cref{1.2.9})}  \\
                   & = \conj{c} \conj{A_{j i}} &  & \text{(by \cref{d.2}(c))} \\
                   & = \conj{c} (A^*)_{i j}    &  & \text{(by \cref{6.1.5})}  \\
                   & = (\conj{c} A^*)_{i j}    &  & \text{(by \cref{1.2.9})}
  \end{align*}
  where \(i \in \set{1, \dots, n}\) and \(j \in \set{1, \dots, m}\).
  Thus by \cref{1.2.8} \((cA)^* = \conj{c} A^*\).
\end{proof}

\begin{proof}[\pf{ex:6.3.5}(c)]
  We have
  \begin{align*}
    ((AC)^*)_{i j} & = \conj{(AC)_{j i}}                            &  & \text{(by \cref{6.1.5})}     \\
                   & = \conj{\sum_{k = 1}^n A_{j k} C_{k i}}        &  & \text{(by \cref{2.3.1})}     \\
                   & = \sum_{k = 1}^n \conj{A_{j k}} \conj{C_{k i}} &  & \text{(by \cref{d.2}(b)(c))} \\
                   & = \sum_{k = 1}^n (A^*)_{k j} (C^*)_{i k}       &  & \text{(by \cref{6.1.5})}     \\
                   & = (C^* A^*)_{i j}                              &  & \text{(by \cref{2.3.1})}
  \end{align*}
  where \(i \in \set{1, \dots, n}\) and \(j \in \set{1, \dots, m}\).
  Thus by \cref{1.2.8} \((AC)^* = C^* A^*\).
\end{proof}

\begin{proof}[\pf{ex:6.3.5}(d)]
  We have
  \begin{align*}
    (A^{**})_{i j} & = \conj{(A^*)_{j i}}    &  & \text{(by \cref{6.1.5})}  \\
                   & = \conj{\conj{A_{i j}}} &  & \text{(by \cref{6.1.5})}  \\
                   & = A_{i j}               &  & \text{(by \cref{d.2}(a))}
  \end{align*}
  where \(i \in \set{1, \dots, m}\) and \(j \in \set{1, \dots, n}\).
  Thus by \cref{1.2.8} \(A = A^{**}\).
\end{proof}

\begin{ex}\label{ex:6.3.12}

\end{ex}

\begin{ex}\label{ex:6.3.19}

\end{ex}

\begin{ex}\label{ex:6.3.24}

\end{ex}
