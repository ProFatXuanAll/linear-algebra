\section{Matrix Limits and Markov Chains}\label{sec:5.3}

\begin{defn}\label{5.3.1}
  Let \(L, \seq{A}{1,2,}\) be \(n \times p\) matrices having complex entries.
  The sequence \(\seq{A}{1,2,}\) is said to \textbf{converge} to the \(n \times p\) matrix \(L\), called the \textbf{limit} of the sequence, if
  \[
    \lim_{m \to \infty} (A_m)_{i j} = L_{i j}
  \]
  for all \(i \in \set{1, \dots, n}\) and \(j \in \set{1, \dots, p}\).
  To designate that \(L\) is the limit of the sequence, we write
  \[
    \lim_{m \to \infty} A_m = L.
  \]
\end{defn}

\begin{thm}\label{5.12}
  Let \(\seq{A}{1,2,}\) be a sequence of \(n \times p\) matrices with complex entries that converges to the matrix \(L\).
  Then for any \(P \in \ms{r}{n}{\C}\) and \(Q \in \ms{p}{s}{\C}\),
  \[
    \lim_{m \to \infty} P A_m = PL \quad \text{and} \quad \lim_{m \to \infty} A_m Q = LQ.
  \]
\end{thm}

\begin{proof}[\pf{5.12}]
  For any \(i \in \set{1, \dots, r}\) and \(j \in \set{1, \dots, p}\),
  \begin{align*}
    \lim_{m \to \infty} (P A_m)_{i j} & = \lim_{m \to \infty} \sum_{k = 1}^n P_{i k} (A_m)_{k j}      &  & \text{(by \cref{2.3.1})} \\
                                      & = \sum_{k = 1}^n P_{i k} \pa{\lim_{m \to \infty} (A_m)_{k j}}                               \\
                                      & = \sum_{k = 1}^n P_{i k} L_{k j}                              &  & \text{(by \cref{5.3.1})} \\
                                      & = (PL)_{i j}.                                                 &  & \text{(by \cref{2.3.1})}
  \end{align*}
  Hence \(\lim_{m \to \infty} P A_m = PL\).
  The proof that \(\lim_{m \to \infty} A_m Q = LQ\) is similar.
\end{proof}

\begin{cor}\label{5.3.2}
  Let \(A \in \ms{n}{n}{\C}\) be such that \(\lim_{m \to \infty} A^m = L\).
  Then for any invertible matrix \(Q \in \ms{n}{n}{\C}\),
  \[
    \lim_{m \to \infty} (Q A Q^{-1})^m = Q L Q^{-1}.
  \]
\end{cor}

\begin{proof}[\pf{5.3.2}]
  Since
  \[
    (Q A Q^{-1})^m = (Q A Q^{-1}) (Q A Q^{-1}) \cdots (Q A Q^{-1}) = Q A^m Q^{-1},
  \]
  we have
  \[
    \lim_{m \to \infty} (Q A Q^{-1})^m = \lim_{m \to \infty} Q A^m Q^{-1} = Q \pa{\lim_{m \to \infty} A^m} Q^{-1} = Q L Q^{-1}
  \]
  by applying \cref{5.12} twice.
\end{proof}

\begin{defn}\label{5.3.3}
  In the discussion that follows, we frequently encounter the set
  \[
    S = \set{\lambda \in \C : \abs{\lambda} < 1 \text{ or } \lambda = 1}.
  \]
  Geometrically, this set consists of the complex number \(1\) and the interior of the unit disk (the disk of radius \(1\) centered at the origin).
  This set is of interest because if \(\lambda\) is a complex number, then \(\lim_{m \to \infty} \lambda^m\) exists iff \(\lambda \in S\).
  This fact, which is obviously true if \(\lambda\) is real, can be shown to be true for complex numbers also.
\end{defn}

\begin{thm}\label{5.13}
  Let \(A\) be a square matrix with complex entries.
  Then \(\lim_{m \to \infty} A^m\) exists iff both of the following conditions hold.
  \begin{enumerate}
    \item Every eigenvalue of \(A\) is contained in \(S\).
    \item If \(1\) is an eigenvalue of \(A\), then the dimension of the eigenspace corresponding to \(1\) equals the multiplicity of \(1\) as an eigenvalue of \(A\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{5.13}]
  The necessity of condition (a) is easily justified.
  For suppose that \(\lambda\) is an eigenvalue of \(A\) such that \(\lambda \notin S\).
  Let \(v\) be an eigenvector of \(A\) corresponding to \(\lambda\).
  Regarding \(v\) as an \(n \times 1\) matrix, we see that
  \[
    \lim_{m \to \infty} (A^m v) = \pa{\lim_{m \to \infty} A^m} v = Lv
  \]
  by \cref{5.12}, where \(L = \lim_{m \to \infty} A^m\).
  But \(\lim_{m \to \infty} (A^m v) = \lim_{m \to \infty} (\lambda^m v)\) diverges because \(\lim_{m \to \infty} \lambda^m\) does not exist.
  Hence if \(\lim_{m \to \infty} A^m\) exists, then condition (a) of \cref{5.13} must hold.

  We see in \cref{ch:7} that if \(A\) is a matrix for which condition (b) fails, then \(A\) is similar to a matrix whose upper left \(2 \times 2\) submatrix is precisely this matrix \(B = \begin{pmatrix}
    1 & 1 \\
    0 & 1
  \end{pmatrix}\).
\end{proof}

\begin{thm}\label{5.14}
  Let \(A \in \ms{n}{n}{\C}\) satisfy the following two conditions.
  \begin{enumerate}
    \item Every eigenvalue of \(A\) is contained in \(S\).
    \item \(A\) is diagonalizable.
  \end{enumerate}
  Then \(\lim_{m \to \infty} A^m\) exists.
\end{thm}

\begin{proof}[\pf{5.14}]
  Since \(A\) is diagonalizable, there exists an invertible matrix \(Q\) such that \(Q^{-1} A Q = D\) is a diagonal matrix.
  Suppose that
  \[
    D = \begin{pmatrix}
      \lambda_1 & 0         & \cdots & 0         \\
      0         & \lambda_2 & \cdots & 0         \\
      \vdots    & \vdots    &        & \vdots    \\
      0         & 0         & \cdots & \lambda_n
    \end{pmatrix}.
  \]
  Because \(\seq{\lambda}{1,,n}\) are the eigenvalues of \(A\), condition (i) requires that for each \(i \in \set{1, \dots, n}\), either \(\lambda_i = 1\) or \(\abs{\lambda_i} < 1\).
  Thus
  \[
    \lim_{m \to \infty} \lambda_i^m = \begin{dcases}
      1 & \text{if } \lambda_i = 1 \\
      0 & \text{otherwise}
    \end{dcases}.
  \]
  But since
  \[
    D^m = \begin{pmatrix}
      \lambda_1^m & 0           & \cdots & 0           \\
      0           & \lambda_2^m & \cdots & 0           \\
      \vdots      & \vdots      &        & \vdots      \\
      0           & 0           & \cdots & \lambda_n^m
    \end{pmatrix},
  \]
  the sequence \(D, D^2, \cdots\) converges to a limit \(L\).
  Hence
  \[
    \lim_{m \to \infty} A^m = \lim_{m \to \infty} (Q D Q^{-1})^m = Q L Q^{-1}
  \]
  by \cref{5.3.2}.
\end{proof}

\begin{defn}\label{5.3.4}
  Any square matrix having these two properties (nonnegative entries and columns that sum to \(1\)) is called a \textbf{transition matrix} or a \textbf{stochastic matrix}.
  For an arbitrary \(n \times n\) transition matrix \(M\), the rows and columns correspond to \(n\) \textbf{states}, and the entry \(M_{i j}\) represents the probability of moving from state \(j\) to state \(i\) in one \textbf{stage}.
  In general, for any transition matrix \(M\), the entry \((M^m)_{i j}\) represents the probability of moving from state \(j\) to state \(i\) in \(m\) stages.

  A column vector \(P\) contains nonnegative entries that sum to \(1\) is called a \textbf{probability vector}.
  In this terminology, each column of a transition matrix is a probability vector.
  It is often convenient to regard the entries of a transition matrix or a probability vector as proportions or percentages instead of probabilities.
\end{defn}

\begin{thm}\label{5.15}
  Let \(M \in \ms{n}{n}{\R}\) having real nonnegative entries, let \(v \in \R^n\) having nonnegative coordinates, and let \(u \in \R^n\) be the column vector in which each coordinate equals \(1\).
  Then
  \begin{enumerate}
    \item \(M\) is a transition matrix iff \(\tp{M} u = u\);
    \item \(v\) is a probability vector iff \(\tp{u} v = (1)\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{5.15}]
  We have
  \begin{align*}
         & M \text{ is a transition matrix}                                                                                                 \\
    \iff & \forall j \in \set{1, \dots, n}, \sum_{i = 1}^n M_{i j} = 1              &  & \text{(by \cref{5.3.4})}                           \\
    \iff & \forall j \in \set{1, \dots, n}, \sum_{i = 1}^n (\tp{M})_{j i} = 1       &  & \text{(by \cref{1.3.3})}                           \\
    \iff & \forall j \in \set{1, \dots, n}, \sum_{i = 1}^n (\tp{M})_{j i} u_i = u_i &  & (u_i = 1 \text{ for all } i \in \set{1, \dots, n}) \\
    \iff & \tp{M} u = u                                                             &  & \text{(by \cref{2.3.1})}
  \end{align*}
  and
  \begin{align*}
         & v \text{ is a probability vector}                                                         \\
    \iff & \sum_{i = 1}^n v_i = 1            &  & \text{(by \cref{5.3.4})}                           \\
    \iff & \sum_{i = 1}^n (\tp{v})_i = 1     &  & \text{(by \cref{1.3.3})}                           \\
    \iff & \sum_{i = 1}^n (\tp{v})_i u_i = 1 &  & (u_i = 1 \text{ for all } i \in \set{1, \dots, n}) \\
    \iff & \tp{v} u = (1).                   &  & \text{(by \cref{2.3.1})}
  \end{align*}
\end{proof}

\begin{cor}\label{5.3.5}
  \begin{enumerate}
    \item The product of two \(n \times n\) transition matrices is an \(n \times n\) transition matrix.
          In particular, any power of a transition matrix is a transition matrix.
    \item The product of a transition matrix and a probability vector is a probability vector.
  \end{enumerate}
\end{cor}

\begin{proof}[\pf{5.3.5}]
  Let \(A, B \in \ms{n}{n}{\R}\) be transition matrices, let \(v \in \R^n\) be a probability vector and let \(u \in \R^n\) such that \(u_i = 1\) for all \(i \in \set{1, \dots, n}\).
  Since
  \begin{align*}
    \tp{(AB)} u & = (\tp{B} \tp{A}) u &  & \text{(by \cref{2.3.2})}   \\
                & = \tp{B} (\tp{A} u) &  & \text{(by \cref{2.16})}    \\
                & = \tp{B} u          &  & \text{(by \cref{5.15}(a))} \\
                & = u,                &  & \text{(by \cref{5.15}(a))}
  \end{align*}
  by \cref{5.15} we see that \(AB\) is a transition matrix.
  Since
  \begin{align*}
    \tp{(Av)} u & = (\tp{v} \tp{A}) u &  & \text{(by \cref{2.3.2})}   \\
                & = \tp{v} (\tp{A} u) &  & \text{(by \cref{2.16})}    \\
                & = \tp{v} u          &  & \text{(by \cref{5.15}(a))} \\
                & = 1,                &  & \text{(by \cref{5.15}(b))}
  \end{align*}
  by \cref{5.15} we see that \(Av\) is a probability vector.
\end{proof}

\begin{defn}\label{5.3.6}
  The city--suburb problem is an example of a process in which elements of a set are each classified as being in one of several fixed states that can switch over time.
  In general, such a process is called a \textbf{stochastic process}.
  The switching to a particular state is described by a probability, and in general this probability depends on such factors as the state in question, the time in question, some or all of the previous states in which the object has been (including the current state), and the states that other objects are in or have been in.

  If, however, the probability that an object in one state changes to a different state in a fixed interval of time depends only on the two states (and not on the time, earlier states, or other factors), then the stochastic process is called a \textbf{Markov process}.
  If, in addition, the number of possible states is finite, then the Markov process is called a \textbf{Markov chain}.
  Of course, a Markov process is usually only an idealization of reality because the probabilities involved are almost never constant over time.

  The vector that describes the initial probability of being in each state is called the \textbf{initial probability vector} for the Markov chain.
  We saw that \(\lim_{m \to \infty} A^m P\), where \(A\) is the transition matrix and \(P\) is the initial probability vector of the Markov chain, gives the eventual proportions in each state.
  In general, however, the limit of powers of a transition matrix need not exist.
  In fact, it can be shown (\cref{ex:7.2.20}) that the only transition matrices \(A\) such that \(\lim_{m \to \infty} A^m\) does not exist are precisely those matrices for which condition (a) of \cref{5.13} fails to hold.
\end{defn}

\begin{note}
  Even if the limit of powers of the transition matrix exists, the computation of the limit may be quite difficult.
  Fortunately, there is a large and important class of transition matrices for which this limit exists and is easily computed
  --- this is the class of regular transition matrices.
\end{note}

\begin{defn}\label{5.3.7}
  A transition matrix is called \textbf{regular} if some power of the matrix contains only positive entries.
\end{defn}

\begin{defn}\label{5.3.8}
  Let \(A \in \ms{n}{n}{\C}\).
  For \(i, j \in \set{1, \dots, n}\), define \(\rho_i(A)\) to be the sum of the absolute values of the entries of row \(i\) of \(A\), and define \(\nu_j(A)\) to be equal to the sum of the absolute values of the entries of column \(j\) of \(A\).
  Thus
  \[
    \rho_i(A) = \sum_{j = 1}^n \abs{A_{i j}} \quad \text{for } i \in \set{1, \dots, n}
  \]
  and
  \[
    \nu_j(A) = \sum_{i = 1}^n \abs{A_{i j}} \quad \text{for } j \in \set{1, \dots, n}.
  \]
  The \textbf{row sum} of \(A\), denoted \(\rho(A)\), and the \textbf{column sum} of \(A\), denoted \(\nu(A)\), are defined as
  \[
    \rho(A) = \max\set{\rho_i(A) : i \in \set{1, \dots, n}} \quad \text{and} \quad \nu(A) = \max\set{\nu_j(A) : j \in \set{1, \dots, n}}.
  \]
\end{defn}

\begin{defn}\label{5.3.9}
  For \(A \in \ms{n}{n}{\C}\), we define the \(i\)th \textbf{Gerschgorin disk} \(C_i\) to be the disk in the complex plane with center \(A_{i i}\) and radius \(r_i = \rho_i(A) - \abs{A_{i i}}\);
  that is,
  \[
    C_i = \set{z \in \C : \abs{z - A_{i i}} \leq r_i}
  \]
\end{defn}

\begin{thm}[Gerschgorin's Disk Theorem]\label{5.16}
  Let \(A \in \ms{n}{n}{\C}\).
  Then every eigenvalue of \(A\) is contained in a Gerschgorin disk.
\end{thm}

\begin{proof}[\pf{5.16}]
  Let \(\lambda\) be an eigenvalue of \(A\) with the corresponding eigenvector
  \[
    v = \begin{pmatrix}
      v_1    \\
      \vdots \\
      v_n
    \end{pmatrix}.
  \]
  Then \(v\) satisfies the matrix equation \(Av = \lambda v\), which can be written
  \[
    \sum_{j = 1}^n A_{i j} v_j = \lambda v_i \quad \text{for } i \in \set{1, \dots, n}.
  \]
  Suppose that \(v_k\) is the coordinate of \(v\) having the largest absolute value;
  note that \(v_k \neq 0\) because \(v\) is an eigenvector of \(A\).

  We show that \(\lambda\) lies in \(C_k\), that is, \(\abs{\lambda - A_{k k}} \leq r_k\).
  For \(i = k\), it follows from above equation that
  \begin{align*}
    \abs{\lambda v_k - A_{k k} v_k} & = \abs{\sum_{j = 1}^n A_{k j} v_j - A_{k k} v_k}                               \\
                                    & = \abs{\sum_{\substack{j = 1                                                   \\ j \neq k}}^n A_{k j} v_j}              \\
                                    & \leq \sum_{\substack{j = 1                                                     \\ j \neq k}}^n \abs{A_{k j}} \abs{v_j}    && \text{(by \cref{d.3}(a)(c))} \\
                                    & \leq \sum_{\substack{j = 1                                                     \\ j \neq k}}^n \abs{A_{k j}} \abs{v_k}     \\
                                    & = \abs{v_k} \sum_{\substack{j = 1                                              \\ j \neq k}}^n \abs{A_{k j}}        \\
                                    & = \abs{v_k} r_k.                                 &  & \text{(by \cref{5.3.9})}
  \end{align*}
  Thus
  \[
    \abs{v_k} \abs{\lambda - A_{k k}} \leq \abs{v_k} r_k;
  \]
  so
  \[
    \abs{\lambda - A_{k k}} \leq r_k
  \]
  because \(\abs{v_k} > 0\).
\end{proof}

\begin{cor}\label{5.3.10}
  Let \(\lambda\) be any eigenvalue of \(A \in \ms{n}{n}{\C}\).
  Then \(\abs{\lambda} \leq \rho(A)\).
\end{cor}

\begin{proof}[\pf{5.3.10}]
  By Gerschgorin's disk theorem (\cref{5.16}), \(\abs{\lambda - A_{k k}} \leq r_k\) for some \(k\).
  Hence
  \begin{align*}
    \abs{\lambda} & = \abs{(\lambda - A_{k k}) + A_{k k}}                                       \\
                  & \leq \abs{\lambda - A_{k k}} + \abs{A_{k k}} &  & \text{(by \cref{d.3}(c))} \\
                  & \leq r_k + \abs{A_{k k}}                     &  & \text{(by \cref{5.16})}   \\
                  & = \rho_k(A)                                  &  & \text{(by \cref{5.3.9})}  \\
                  & \leq \rho(A).                                &  & \text{(by \cref{5.3.8})}
  \end{align*}
\end{proof}

\begin{cor}\label{5.3.11}
  Let \(\lambda\) be any eigenvalue of \(A \in \ms{n}{n}{\C}\).
  Then
  \[
    \abs{\lambda} \leq \min\set{\rho(A), \nu(A)}.
  \]
\end{cor}

\begin{proof}[\pf{5.3.11}]
  Since \(\abs{\lambda} \leq \rho(A)\) by \cref{5.3.10}, it suffices to show that \(\abs{\lambda} \leq \nu(A)\).
  By \cref{ex:5.1.14}, \(\lambda\) is an eigenvalue of \(\tp{A}\), and so \(\abs{\lambda} \leq \rho(\tp{A})\) by \cref{5.3.10}.
  But the rows of \(\tp{A}\) are the columns of \(A\);
  consequently \(\rho(\tp{A}) = \nu(A)\).
  Therefore \(\abs{\lambda} \leq \nu(A)\).
\end{proof}

\begin{cor}\label{5.3.12}
  If \(\lambda\) is an eigenvalue of a transition matrix, then \(\abs{\lambda} \leq 1\).
\end{cor}

\begin{proof}[\pf{5.3.12}]
  This is the immediate consequence of \cref{5.3.4} and \cref{5.3.11}.
\end{proof}

\begin{thm}\label{5.17}
  Every transition matrix has \(1\) as an eigenvalue.
\end{thm}

\begin{proof}[\pf{5.17}]
  Let \(A \in \ms{n}{n}{\R}\) be a transition matrix, and let \(u \in \R^n\) be the column vector in which each coordinate is \(1\).
  Then \(\tp{A} u = u\) by \cref{5.15}, and hence \(u\) is an eigenvector of \(\tp{A}\) corresponding to the eigenvalue \(1\).
  But since \(A\) and \(\tp{A}\) have the same eigenvalues (\cref{ex:5.1.14}), it follows that \(1\) is also an eigenvalue of \(A\).
\end{proof}

\begin{note}
  Suppose that \(A\) is a transition matrix for which some eigenvector corresponding to the eigenvalue \(1\) has only nonnegative coordinates.
  Then some multiple of this vector is a probability vector \(P\) as well as an eigenvector of \(A\) corresponding to eigenvalue \(1\).
  It is interesting to observe that if \(P\) is the initial probability vector of a Markov chain having \(A\) as its transition matrix, then the Markov chain is completely static.
  For in this situation, \(A^m P = P\) for every positive integer \(m\);
  hence the probability of being in each state never changes.
\end{note}

\begin{thm}\label{5.18}
  Let \(A \in \ms{n}{n}{\C}\) be a matrix in which each entry is positive, and let \(\lambda\) be an eigenvalue of \(A\) such that \(\abs{\lambda} = \rho(A)\).
  Then \(\lambda = \rho(A)\) and \(\set{u}\) is a basis for \(E_{\lambda}\) over \(\C\), where \(u \in \C^n\) is the column vector in which each coordinate equals \(1\).
\end{thm}

\begin{proof}[\pf{5.18}]
  Let \(v\) be an eigenvector of \(A\) corresponding to \(\lambda\), with coordinates \(\seq{v}{1,,n}\).
  Suppose that \(v_k\) is the coordinate of \(v\) having the largest absolute value, and let \(b = \abs{v_k}\).
  Then
  \begin{align*}
    \abs{\lambda} b & = \abs{\lambda} \abs{v_k}                                               \\
                    & = \abs{\lambda v_k}                      &  & \text{(by \cref{d.3}(a))} \\
                    & = \abs{\sum_{j = 1}^n A_{k j} v_j}       &  & \text{(by \cref{5.1.2})}  \\
                    & \leq \sum_{j = 1}^n \abs{A_{k j} v_j}    &  & \text{(by \cref{d.3}(c))} \\
                    & = \sum_{j = 1}^n \abs{A_{k j}} \abs{v_j} &  & \text{(by \cref{d.3}(a))} \\
                    & \leq \sum_{j = 1}^n \abs{A_{k j}} b                                     \\
                    & = \rho_k(A) b                            &  & \text{(by \cref{5.3.8})}  \\
                    & \leq \rho(A) b.                          &  & \text{(by \cref{5.3.8})}
  \end{align*}
  Since \(\abs{\lambda} = \rho(A)\), the three inequalities above are actually equalities;
  that is,
  \begin{enumerate}
    \item \(\abs{\sum_{j = 1}^n A_{k j} v_j} = \sum_{j = 1}^n \abs{A_{k j} v_j}\),
    \item \(\sum_{j = 1}^n \abs{A_{k j}} \abs{v_j} = \sum_{j = 1}^n \abs{A_{k j}} b\), and
    \item \(\rho_k(A) = \rho(A)\).
  \end{enumerate}

  We see in \cref{ex:6.1.15}(b) that (a) holds iff all the terms \(A_{k j} v_j\) (\(j \in \set{1, \dots, n}\)) are nonnegative multiples of some nonzero complex number \(z\).
  Without loss of generality, we assume that \(\abs{z} = 1\).
  Thus there exist nonnegative real numbers \(\seq{c}{1,,n}\) such that
  \[
    A_{k j} v_j = c_j z
  \]

  By (b) and the assumption that \(A_{k j} \neq 0\) for all \(k, j \in \set{1, \dots, n}\), we have
  \[
    \abs{v_j} = b \quad \text{for } j \in \set{1, \dots, n}.
  \]
  Combining equations above, we obtain
  \[
    b = \abs{v_j} = \abs{\frac{c_j}{A_{k j}} z} = \frac{c_j}{A_{k j}} \quad \text{for } j \in \set{1, \dots, n},
  \]
  and therefore we have \(v_j = bz\) for all \(j\).
  So
  \[
    v = \begin{pmatrix}
      v_1    \\
      \vdots \\
      v_n
    \end{pmatrix} = \begin{pmatrix}
      bz     \\
      \vdots \\
      bz
    \end{pmatrix} = bzu,
  \]
  and hence \(\set{u}\) is a basis for \(E_{\lambda}\) over \(\C\).

  Finally, observe that all of the entries of \(Au\) are positive because the same is true for the entries of both \(A\) and \(u\).
  But \(Au = \lambda u\), and hence \(\lambda > 0\).
  Therefore, \(\lambda = \abs{\lambda} = \rho(A)\).
\end{proof}
