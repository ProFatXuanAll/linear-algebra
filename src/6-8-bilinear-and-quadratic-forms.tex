\section{Bilinear and Quadratic Forms}\label{sec:6.8}

\begin{defn}\label{6.8.1}
  Let \(\V\) be a vector space over a field \(\F\).
  A function \(H\) from the set \(\V \times \V\) of ordered pairs of vectors to \(\F\) is called a \textbf{bilinear form} on \(\V\) if \(H\) is linear in each variable when the other variable is held fixed;
  that is, \(H\) is a bilinear form on \(\V\) if
  \begin{enumerate}
    \item \(H(a x_1 + x_2, y) = a H(x_1, y) + H(x_2, y)\) for all \(x_1, x_2, y \in \V\) and \(a \in \F\).
    \item \(H(x, a y_1 + y_2) = a H(x, y_1) + H(x, y_2)\) for all \(x, y_1, y_2 \in \V\) and \(a \in \F\).
  \end{enumerate}
  We denote the set of all bilinear forms on \(\V\) by \(\bi(\V)\).
  Observe that an inner product on a vector space is a bilinear form if the underlying field is real, but not if the underlying field is complex (\cref{6.1}(b)).
\end{defn}

\begin{eg}\label{6.8.2}
  Let \(\V = \vs{F}^n\), where the vectors are considered as column vectors.
  For any \(A \in \ms[n][n][\F]\), define \(H : \V \times \V \to \F\) by
  \[
    H(x, y) = \tp{x} A y \quad \text{for } x, y \in \V.
  \]
  Notice that since \(x\) and \(y\) are \(n \times 1\) matrices and \(A \in \ms[n][n][\F]\), \(H(x, y)\) is a \(1 \times 1\) matrix.
  Then \(H \in \bi(\V)\).
\end{eg}

\begin{proof}[\pf{6.8.2}]
  Let \(x, y, z \in \V\) and let \(c \in \F\).
  Since
  \begin{align*}
    H(ax + y, z) & = \tp{(ax + y)} A z         &  & \by{6.8.2}     \\
                 & = (a \tp{x} + \tp{y}) A z   &  & \by{ex:1.3.3}  \\
                 & = a \tp{x} A z + \tp{y} A z &  & \by{2.12}[a,b] \\
                 & = a H(x, z) + H(y, z);      &  & \by{6.8.2}     \\
    H(z, ax + y) & = \tp{z} A (ax + y)         &  & \by{6.8.2}     \\
                 & = a \tp{z} A x + \tp{z} A y &  & \by{2.12}[a,b] \\
                 & = a H(z, x) + H(z, y),      &  & \by{6.8.2}
  \end{align*}
  by \cref{6.8.1} we know that \(H \in \bi(\V)\).
\end{proof}

\begin{prop}\label{6.8.3}
  For any bilinear form \(H\) on a vector space \(\V\) over a field \(\F\), the following properties hold.
  \begin{enumerate}
    \item If, for any \(x \in \V\), the functions \(\lt{L}_x, \lt{R}_x : \V \to \F\) are defined by
          \[
            \lt{L}_x(y) = H(x, y) \quad \text{and} \quad \lt{R}_x(y) = H(y, x) \quad \text{for all } y \in \V,
          \]
          then \(\lt{L}_x, \lt{R}_x \in \ls(\V, \F)\).
    \item \(H(\zv, x) = H(x, \zv) = 0\) for all \(x \in \V\).
    \item For all \(x, y, z, w \in \V\),
          \[
            H(x + y, z + w) = H(x, z) + H(x, w) + H(y, z) + H(y, w).
          \]
    \item If \(J : \V \times \V \to \F\) is defined by \(J(x, y) = H(y, x)\), then \(J\) is a bilinear form.
  \end{enumerate}
\end{prop}

\begin{proof}[\pf{6.8.3}(a)]
  Let \(y, z \in \V\) and let \(c \in \F\).
  Since
  \begin{align*}
    \lt{L}_x(cy + z) & = H(x, cy + z)                 &  & \by{6.8.3}[a] \\
                     & = c H(x, y) + H(x, z)          &  & \by{6.8.1}[b] \\
                     & = c \lt{L}_x(y) + \lt{L}_x(z); &  & \by{6.8.3}[a] \\
    \lt{R}_x(cy + z) & = H(cy + z, x)                 &  & \by{6.8.3}[a] \\
                     & = c H(y, x) + H(z, x)          &  & \by{6.8.1}[a] \\
                     & = c \lt{R}_x(y) + \lt{R}_x(z), &  & \by{6.8.3}[a]
  \end{align*}
  by \cref{2.1.2}(b) we know that \(\lt{L}_x, \lt{R}_x \in \ls(\V, \F)\).
\end{proof}

\begin{proof}[\pf{6.8.3}(b)]
  Since
  \begin{align*}
    H(\zv, x) & = H(\zv + \zv, x)        &  & \by{1.2.1}    \\
              & = H(\zv, x) + H(\zv, x); &  & \by{6.8.1}[a] \\
    H(x, \zv) & = H(x, \zv + \zv)        &  & \by{1.2.1}    \\
              & = H(x, \zv) + H(x, \zv), &  & \by{6.8.1}[b]
  \end{align*}
  by \cref{c.1} we know that
  \[
    H(\zv, x) = 0 = H(x, \zv).
  \]
\end{proof}

\begin{proof}[\pf{6.8.3}(c)]
  We have
  \begin{align*}
    H(x + y, z + w) & = H(x, z + w) + H(y, z + w)              &  & \by{6.8.1}[a] \\
                    & = H(x, z) + H(x, w) + H(y, z) + H(y, w). &  & \by{6.8.1}[b]
  \end{align*}
\end{proof}

\begin{proof}[\pf{6.8.3}(d)]
  Let \(x, y, z \in \V\) and \(c \in \F\).
  Since
  \begin{align*}
    J(cx + y, z) & = H(z, cx + y)         &  & \by{6.8.3}[d] \\
                 & = c H(z, x) + H(z, y)  &  & \by{6.8.1}[b] \\
                 & = c J(x, z) + J(y, z); &  & \by{6.8.3}[d] \\
    J(z, cx + y) & = H(cx + y, z)         &  & \by{6.8.3}[d] \\
                 & = c H(x, z) + H(y, z)  &  & \by{6.8.1}[a] \\
                 & = c J(z, x) + J(z, y), &  & \by{6.8.3}[d]
  \end{align*}
  by \cref{6.8.1} we know that \(J \in \bi(\V)\).
\end{proof}

\begin{defn}\label{6.8.4}
  Let \(\V\) be a vector space over \(\F\), let \(H_1, H_2 \in \bi(\V)\), and let \(a \in \F\).
  We define the \textbf{sum} \(H_1 + H_2\) and the \textbf{scalar product} \(a H_1\) by the equations
  \[
    (H_1 + H_2)(x, y) = H_1(x, y) + H_2(x, y)
  \]
  and
  \[
    (a H_1)(x, y) = a (H_1(x, y)) \quad \text{for all } x, y \in \V.
  \]
\end{defn}

\begin{thm}\label{6.31}
  For any vector space \(\V\) over \(\F\), the sum of two bilinear forms and the product of a scalar and a bilinear form on \(\V\) are again bilinear forms on \(\V\).
  Furthermore, \(\bi(\V)\) is a vector space with respect to these operations.
\end{thm}

\begin{proof}[\pf{6.31}]
  By \cref{1.2.10} we know that the set \(\fs(\V \times \V, \F)\) is a vector space and \(\bi(\V) \subseteq \fs(\V \times \V, \F)\).
  Let \(\zT\) be the zero function of \(\fs(\V \times \V, \F)\).
  Let \(f, g \in \bi(\V)\), let \(x, y, z \in \V\) and let \(a, c \in \F\).
  Since
  \begin{align*}
    (cf + g)(ax + y, z) & = c f(ax + y, z) + g(ax + y, z)                 &  & \by{6.8.4}    \\
                        & = c (a f(x, z) + f(y, z)) + a g(x, z) + g(y, z) &  & \by{6.8.1}[a] \\
                        & = a (c f(x, z) + g(x, z)) + c f(y, z) + g(y, z)                    \\
                        & = a (cf + g)(x, z) + (cf + g)(y, z);            &  & \by{6.8.4}    \\
    (cf + g)(z, ax + y) & = c f(z, ax + y) + g(z, ax + y)                 &  & \by{6.8.4}    \\
                        & = c (a f(z, x) + f(z, y)) + a g(z, x) + g(z, y) &  & \by{6.8.1}[b] \\
                        & = a (c f(z, x) + g(z, x)) + c f(z, y) + g(z, y)                    \\
                        & = a (cf + g)(z, x) + (cf + g)(z, y),            &  & \by{6.8.4}
  \end{align*}
  by \cref{6.8.1} we have \(cf + g \in \bi(\V)\).
  Since
  \begin{align*}
    \zT(ax + y, z) & = 0                        \\
                   & = a 0 + 0                  \\
                   & = a \zT(x, z) + \zT(y, z); \\
    \zT(z, ax + y) & = 0                        \\
                   & = a 0 + 0                  \\
                   & = a \zT(z, x) + \zT(z, y),
  \end{align*}
  by \cref{6.8.1} we have \(\zT \in \bi(\V)\).
  Thus by \cref{ex:1.3.17} we know that \(\bi(\V)\) is a vector space over \(\F\).
\end{proof}

\begin{defn}\label{6.8.5}
  Let \(\beta = \set{\seq{v}{1,,n}}\) be an ordered basis for an \(n\)-dimensional vector space \(\V\) over \(\F\), and let \(H \in \bi(\V)\).
  We can associate \(H\) with an \(n \times n\) matrix \(A\) whose entry in row \(i\) and column \(j\) is defined by
  \[
    A_{i j} = H(v_i, v_j) \quad \text{for } i, j \in \set{1, \dots, n}.
  \]
  The matrix \(A\) above is called the \textbf{matrix representation} of \(H\) with respect to the ordered basis \(\beta\) and is denoted by \(\psi_{\beta}(H)\).
  We can therefore regard \(\psi_{\beta}\) as a mapping from \(\bi(\V)\) to \(\ms[n][n][\F]\), where \(\F\) is the field of scalars for \(\V\), that takes a bilinear form \(H\) into its matrix representation \(\psi_{\beta}(H)\).
\end{defn}

\begin{thm}\label{6.32}
  For any \(n\)-dimensional vector space \(\V\) over \(\F\) and any ordered basis \(\beta\) for \(\V\) over \(\F\), \(\psi_{\beta} : \bi(\V) \to \ms[n][n][\F]\) is an isomorphism.
\end{thm}

\begin{proof}[\pf{6.32}]
  First we show that \(\psi_{\beta} \in \ls(\bi(\V), \ms[n][n][\F])\).
  Let \(f, g \in \bi(\V)\) and let \(c \in \F\).
  Since
  \begin{align*}
    \forall i, j \in \set{1, \dots, n}, \pa{\psi_{\beta}(cf + g)}_{i j} & = (cf + g)(v_i, v_j)                                        &  & \by{6.8.5} \\
                                                                        & = c f(v_i, v_j) + g(v_i, v_j)                               &  & \by{6.8.4} \\
                                                                        & = c \pa{\psi_{\beta}(f)}_{i j} + \pa{\psi_{\beta}(g)}_{i j} &  & \by{6.8.5} \\
                                                                        & = \pa{c \psi_{\beta}(f) + \psi_{\beta}(g)}_{i j},           &  & \by{1.2.9}
  \end{align*}
  by \cref{1.2.8} we know that \(\psi_{\beta}(cf + g) = c \psi_{\beta}(f) + \psi_{\beta}(g)\) and thus by \cref{2.1.2}(b) \(\psi_{\beta} \in \ls(\bi(\V), \ms[n][n][\F])\).

  To show that \(\psi_{\beta}\) is one-to-one, suppose that \(\psi_{\beta}(H) = \zm\) for some \(H \in \bi(\V)\).
  Fix \(v_i \in \beta\), and recall the mapping \(\lt{L}_{v_i} : \V \to \F\), which is linear by \cref{6.8.3}(a).
  By hypothesis, \(\lt{L}_{v_i}(v_j) = H(v_i, v_j) = 0\) for all \(v_j \in \beta\).
  Hence \(\lt{L}_{v_i}\) is the zero transformation from \(\V\) to \(\F\) by \cref{2.1.13}.
  So
  \[
    H(v_i, x) = \lt{L}_{v_i}(x) = 0 \quad \text{for all } x \in \V \text{ and } v_i \in \beta.
  \]
  Next fix an arbitrary \(y \in \V\), and recall the linear mapping \(\lt{R}_y : \V \to \F\) defined in \cref{6.8.3}(a).
  From the equation above we have \(\lt{R}_y(v_i) = H(v_i, y) = 0\) for all \(v_i \in \beta\), and hence \(\lt{R}_y\) is the zero transformation by \cref{2.1.13}.
  So \(H(x, y) = \lt{R}_y(x) = 0\) for all \(x, y \in \V\).
  Thus \(H\) is the zero bilinear form, and therefore \(\psi_{\beta}\) is one-to-one by \cref{2.4}.

  To show that \(\psi_{\beta}\) is onto, consider any \(A \in \ms[n][n][\F]\).
  Recall the isomorphism \(\phi_{\beta} \in \ls(\V, \vs{\F}^n)\) defined in \cref{2.4.11}.
  For \(x \in \V\), we view \(\phi_{\beta}(x) \in \vs{F}^n\) as a column vector.
  Let \(H : \V \times \V \to \F\) be the mapping defined by
  \[
    H(x, y) = \tp{(\phi_{\beta}(x))} A (\phi_{\beta}(y)) \quad \text{for all } x, y \in \V.
  \]
  By \cref{6.8.2} we know that \(H \in \bi(\V)\).
  We show that \(\psi_{\beta}(H) = A\).
  Let \(v_i, v_j \in \beta\).
  Then \(\phi_{\beta}(v_i) = e_i\) and \(\phi_{\beta}(v_j) = e_j\);
  hence, for any \(i, j \in \set{1, \dots, n}\),
  \[
    H(v_i, v_j) = \tp{(\phi_{\beta}(v_i))} A (\phi_{\beta}(v_j)) = \tp{e_i} A e_j = A_{i j}.
  \]
  We conclude that \(\psi_{\beta}(H) = A\) and \(\psi_{\beta}\) is onto.
\end{proof}

\begin{cor}\label{6.8.6}
  For any \(n\)-dimensional vector space \(\V\), \(\bi(\V)\) has dimension \(n^2\).
\end{cor}

\begin{proof}[\pf{6.8.6}]
  By \cref{1.6.11,2.19,6.32} we see that this is true.
\end{proof}

\begin{cor}\label{6.8.7}
  Let \(\V\) be an \(n\)-dimensional vector space over \(\F\) with ordered basis \(\beta\).
  If \(H \in \bi(\V)\) and \(A \in \ms[n][n][\F]\), then \(\psi_{\beta}(H) = A\) iff \(H(x, y) = \tp{(\phi_{\beta}(x))} A (\phi_{\beta}(y))\) for all \(x, y \in \V\).
\end{cor}

\begin{proof}[\pf{6.8.7}]
  Let \(\beta = \set{\seq{v}{1,,n}}\) and let \(x, y \in \V\).
  Since \(\beta\) is a basis for \(\V\) over \(\F\), there exist \(\seq{a}{1,,n}, \seq{b}{1,,n} \in \F\) such that \(x = \sum_{i = 1}^n a_i v_i\) and \(y = \sum_{i = 1}^n b_i v_i\).
  Then we have
  \begin{align*}
         & \psi_{\beta}(H) = A                                                                                                                   \\
    \iff & H(x, y) = H\pa{\sum_{i = 1}^n a_i v_i, \sum_{j = 1}^n b_j v_j} = \sum_{i = 1}^n \sum_{j = 1}^n a_i b_j H(v_i, v_j) &  & \by{6.8.3}[c] \\
         & = \sum_{i = 1}^n \sum_{j = 1}^n a_i b_j A_{i j}                                                                    &  & \by{6.32}     \\
         & = \sum_{i = 1}^n \sum_{j = 1}^n a_i b_j \tp{e_i} A e_j                                                             &  & \by{2.3.1}    \\
         & = \pa{\sum_{i = 1}^n a_i \tp{e_i}} A \pa{\sum_{j = 1}^n b_j e_j}                                                   &  & \by{2.3.5}    \\
         & = \tp{\pa{\sum_{i = 1}^n a_i e_i}} A \pa{\sum_{j = 1}^n b_j e_j}                                                   &  & \by{ex:1.3.3} \\
         & = \tp{(\phi_{\beta}(x))} A (\phi_{\beta}(y)).                                                                      &  & \by{2.4.11}
  \end{align*}
\end{proof}

\begin{cor}\label{6.8.8}
  Let \(\F\) be a field, \(n\) a positive integer, and \(\beta\) be the standard ordered basis for \(\vs{F}^n\) over \(\F\).
  Then for any \(H \in \bi(\vs{F}^n)\), there exists a unique matrix \(A \in \ms[n][n][\F]\), namely, \(A = \psi_{\beta}(H)\), such that
  \[
    H(x, y) = \tp{x} A y \quad \text{for all } x, y \in \vs{F}^n.
  \]
\end{cor}

\begin{proof}[\pf{6.8.8}]
  This is an immediate consequence of \cref{6.8.7}.
\end{proof}

\begin{defn}\label{6.8.9}
  Let \(A, B \in \ms[n][n][\F]\).
  Then \(B\) is said to be \textbf{congruent} to \(A\) if there exists an invertible matrix \(Q \in \ms[n][n][\F]\) such that \(B = \tp{Q} A Q\).
  Observe that the relation of congruence is an equivalence relation (see \cref{ex:6.8.12}).
\end{defn}

\begin{thm}\label{6.33}
  Let \(\V\) be a finite-dimensional vector space over \(\F\) with ordered bases \(\beta = \set{\seq{v}{1,,n}}\) and \(\gamma = \set{\seq{w}{1,,n}}\), and let \(Q\) be the change of coordinate matrix changing \(\gamma\)-coordinates into \(\beta\)-coordinates.
  Then, for any \(H \in \bi(\V)\), we have \(\psi_{\gamma}(H) = \tp{Q} \psi_{\beta}(H) Q\).
  Therefore \(\psi_{\gamma}(H)\) is congruent to \(\psi_{\beta}(H)\).
\end{thm}

\begin{proof}[\pf{6.33}]
  There are essentially two proofs of this theorem.
  One involves a direct computation, while the other follows immediately from a clever observation.
  We give the more direct proof here, leaving the other proof for the exercises (see \cref{ex:6.8.13}).

  Suppose that \(A = \psi_{\beta}(H)\) and \(B = \psi_{\gamma}(H)\).
  Then for \(i, j \in \set{1, \dots, n}\),
  \[
    w_i = \sum_{k = 1}^n Q_{k i} v_k \quad \text{and} \quad w_j = \sum_{r = 1}^n Q_{r j} v_r.
  \]
  (See \cref{2.2.4}.)
  Thus
  \begin{align*}
    B_{i j} & = H(w_i, w_j)                                                  &  & \by{6.8.5}    \\
            & = H\pa{\sum_{k = 1}^n Q_{k i} v_k, w_j}                        &  & \by{2.2.4}    \\
            & = \sum_{k = 1}^n Q_{k i} H(v_k, w_j)                           &  & \by{6.8.1}[a] \\
            & = \sum_{k = 1}^n Q_{k i} H\pa{v_k, \sum_{r = 1}^n Q_{r j} v_r} &  & \by{2.2.4}    \\
            & = \sum_{k = 1}^n Q_{k i} \sum_{r = 1}^n Q_{r j} H(v_k, v_r)    &  & \by{6.8.1}[b] \\
            & = \sum_{k = 1}^n Q_{k i} \sum_{r = 1}^n Q_{r j} A_{k r}        &  & \by{6.8.5}    \\
            & = \sum_{k = 1}^n Q_{k i} \sum_{r=  1}^n A_{k r} Q_{r j}                           \\
            & = \sum_{k = 1}^n Q_{k i} (AQ)_{k j}                            &  & \by{2.3.1}    \\
            & = \sum_{k = 1}^n (\tp{Q})_{i k} (AQ)_{k j}                     &  & \by{1.3.3}    \\
            & = (\tp{Q} A Q)_{i j}.                                          &  & \by{2.3.1}
  \end{align*}
  Hence by \cref{1.2.8} \(B = \tp{Q} A Q\).
\end{proof}

\begin{cor}\label{6.8.10}
  Let \(\V\) be an \(n\)-dimensional vector space over \(\F\) with ordered basis \(\beta\), and let \(H \in \bi(\V)\).
  For any \(B \in \ms[n][n][\F]\), if \(B\) is congruent to \(\psi_{\beta}(H)\), then there exists an ordered basis \(\gamma\) for \(\V\) over \(\F\) such that \(\psi_{\gamma}(H) = B\).
  Furthermore, if \(B = \tp{Q} \psi_{\beta}(H) Q\) for some invertible matrix \(Q\), then \(Q\) changes \(\gamma\)-coordinates into \(\beta\)-coordinates.
\end{cor}

\begin{proof}[\pf{6.8.10}]
  Suppose that \(B = \tp{Q} \psi_{\beta}(H) Q\) for some invertible matrix \(Q\) and that \(\beta = \set{\seq{v}{1,,n}}\).
  Let \(\gamma = \set{\seq{w}{1,,n}}\), where
  \[
    w_j = \sum_{i = 1}^n Q_{i j} v_i \quad \text{for } j \in \set{1, \dots, n}.
  \]
  Since \(Q\) is invertible, \(\gamma\) is an ordered basis for \(\V\) over \(\F\) (\cref{2.2,2.3}), and \(Q\) is the change of coordinate matrix that changes \(\gamma\)-coordinates into \(\beta\)-coordinates (\cref{2.5.1}).
  Therefore, by \cref{6.33},
  \[
    B = \tp{Q} \psi_{\beta}(H) Q = \psi_{\gamma}(H).
  \]
\end{proof}

\begin{note}
  \cref{6.8.10} is the converse of \cref{6.33}.
\end{note}

\begin{note}
  Like the diagonalization problem for linear operators, there is an analogous \emph{diagonalization} problem for bilinear forms, namely, the problem of determining those bilinear forms for which there are diagonal matrix representations.
  As we will see, there is a close relationship between \emph{diagonalizable} bilinear forms and those that are called \emph{symmetric}.
\end{note}

\begin{defn}\label{6.8.11}
  A bilinear form \(H\) on a vector space \(\V\) over \(\F\) is \textbf{symmetric} if \(H(x, y) = H(y, x)\) for all \(x, y \in \V\).
\end{defn}

\begin{thm}\label{6.34}
  Let \(H\) be a bilinear form on a finite-dimensional vector space \(\V\) over \(\F\), and let \(\beta\) be an ordered basis for \(\V\) over \(\F\).
  Then \(H\) is symmetric iff \(\psi_{\beta}(H)\) is symmetric.
\end{thm}

\begin{proof}[\pf{6.34}]
  Let \(\beta = \set{\seq{v}{1,,n}}\) and \(B = \psi_{\beta}(H)\).

  First assume that \(H\) is symmetric.
  Then for \(i, j \in \set{1, \dots, n}\),
  \begin{align*}
    B_{i j} & = H(v_i, v_j) &  & \by{6.8.5}  \\
            & = H(v_j, v_i) &  & \by{6.8.11} \\
            & = B_{j i},    &  & \by{6.8.5}
  \end{align*}
  and it follows that \(B\) is symmetric (\cref{1.3.4}).

  Conversely, suppose that \(B\) is symmetric.
  Let \(J : \V \times \V \to \F\) be the mapping defined by \(J(x, y) = H(y, x)\) for all \(x, y \in \V\).
  By \cref{6.8.3}(d), \(J\) is a bilinear form.
  Let \(C = \psi_{\beta}(J)\).
  Then, for \(i, j \in \set{1, \dots, n}\),
  \begin{align*}
    C_{i j} & = J(v_i, v_j) &  & \by{6.8.5}    \\
            & = H(v_j, v_i) &  & \by{6.8.3}[d] \\
            & = B_{j i}     &  & \by{6.8.5}    \\
            & = B_{i j}.    &  & \by{1.3.4}
  \end{align*}
  Thus \(C = B\) (\cref{1.2.8}).
  Since \(\psi_{\beta}\) is one-to-one (\cref{6.32}), we have \(J = H\).
  Hence \(H(y, x) = J(x, y) = H(x, y)\) for all \(x, y \in \V\), and therefore \(H\) is symmetric (\cref{6.8.11}).
\end{proof}

\begin{defn}\label{6.8.12}
  A bilinear form \(H\) on a finite-dimensional vector space \(\V\) over \(\F\) is called \textbf{diagonalizable} if there is an ordered basis \(\beta\) for \(\V\) over \(\F\) such that \(\psi_{\beta}(H)\) is a diagonal matrix.
\end{defn}

\begin{cor}\label{6.8.13}
  Let \(H\) be a diagonalizable bilinear form on a finite-dimensional vector space \(\V\) over \(\F\).
  Then \(H\) is symmetric.
\end{cor}

\begin{proof}[\pf{6.8.13}]
  Suppose that \(H\) is diagonalizable.
  Then there is an ordered basis \(\beta\) for \(\V\) over \(\F\) such that \(\psi_{\beta}(H) = D\) is a diagonal matrix (\cref{6.8.12}).
  Trivially, \(D\) is a symmetric matrix, and hence, by \cref{6.34}, \(H\) is symmetric.
\end{proof}

\begin{note}
  Unfortunately, the converse of \cref{6.8.13} is not true, as is illustrated by \cref{6.8.14}.
\end{note}

\begin{eg}\label{6.8.14}
  Let \(\F = \Z_2\), \(\V = \vs{F}^2\), and \(H : \V \times \V \to \F\) be the bilinear form defined by
  \[
    H\pa{\begin{pmatrix}
        a_1 \\
        a_2
      \end{pmatrix}, \begin{pmatrix}
        b_1 \\
        b_2
      \end{pmatrix}} = a_1 b_2 + a_2 b_1.
  \]
  Clearly \(H\) is symmetric.
  In fact, if \(\beta\) is the standard ordered basis for \(\V\) over \(\F\), then
  \[
    A = \psi_{\beta}(H) = \begin{pmatrix}
      0 & 1 \\
      1 & 0
    \end{pmatrix},
  \]
  a symmetric matrix.
  We show that \(H\) is not diagonalizable.
\end{eg}

\begin{proof}[\pf{6.8.14}]
  By way of contradiction, suppose that \(H\) is diagonalizable.
  Then there is an ordered basis \(\gamma\) for \(\V\) over \(\F\) such that \(B = \psi_{\gamma}(H)\) is a diagonal matrix (\cref{6.8.12}).
  So by \cref{6.33}, there exists an invertible matrix \(Q\) such that \(B = \tp{Q} A Q\).
  Since \(Q\) is invertible, it follows that \(\rk{B} = \rk{A} = 2\) (\cref{3.4}(c)), and consequently the diagonal entries of \(B\) are nonzero.
  Since the only nonzero scalar of \(\F\) is \(1\),
  \[
    B = \begin{pmatrix}
      1 & 0 \\
      0 & 1
    \end{pmatrix}.
  \]
  Suppose that
  \[
    Q = \begin{pmatrix}
      a & b \\
      c & d
    \end{pmatrix}.
  \]
  Then
  \begin{align*}
    \begin{pmatrix}
      1 & 0 \\
      0 & 1
    \end{pmatrix} & = B                           &  & (\F = \Z_2) \\
                    & = \tp{Q} A Q                  &  & \by{6.33} \\
                    & = \begin{pmatrix}
                          a & c \\
                          b & d
                        \end{pmatrix} \begin{pmatrix}
                                        0 & 1 \\
                                        1 & 0
                                      \end{pmatrix} \begin{pmatrix}
                                                      a & b \\
                                                      c & d
                                                    \end{pmatrix} \\
                    & = \begin{pmatrix}
                          ac + ac & bc + ad \\
                          bc + ad & bd + bd
                        \end{pmatrix}.          &  & \by{2.3.1}
  \end{align*}
  But \(p + p = 0\) for all \(p \in \F\);
  hence \(ac + ac = 0\).
  Thus, comparing the row \(1\), column \(1\) entries of the matrices in the equation above, we conclude that \(1 = 0\), a contradiction.
  Therefore \(H\) is not diagonalizable.
\end{proof}

\begin{note}
  The bilinear form of \cref{6.8.14} is an anomaly.
  Its failure to be diagonalizable is due to the fact that the scalar field \(\Z_2\) is of characteristic two.
  Recall from \cref{c.0.4} that a field \(\F\) is of \textbf{characteristic two} if \(1 + 1 = 0\) in \(\F\).
  If \(\F\) is not of characteristic two, then \(1 + 1 = 2\) has a multiplicative inverse, which we denote by \(1 / 2\).
\end{note}

\begin{lem}\label{6.8.15}
  Let \(H\) be a nonzero symmetric bilinear form on a vector space \(\V\) over a field \(\F\) not of characteristic two.
  Then there is a vector \(x \in \V\) such that \(H(x, x) \neq 0\).
\end{lem}

\begin{proof}[\pf{6.8.15}]
  Since \(H\) is nonzero, we can choose vectors \(u, v \in \V\) such that \(H(u, v) \neq 0\).
  If \(H(u, u) \neq 0\) or \(H(v, v) \neq 0\), there is nothing to prove.
  Otherwise, set \(x = u + v\).
  Then
  \begin{align*}
    H(x, x) & = H(u, u) + H(u, v) + H(v, u) + H(v, v) &  & \by{6.8.3}[c]           \\
            & = 2 H(u, v)                             &  & (H(u, u) = 0 = H(v, v)) \\
            & \neq 0
  \end{align*}
  because \(2 \neq 0\) and \(H(u, v) \neq 0\).
\end{proof}

\begin{thm}\label{6.35}
  Let \(\V\) be a finite-dimensional vector space over a field \(\F\) not of characteristic two.
  Then every symmetric bilinear form on \(\V\) is diagonalizable.
\end{thm}

\begin{proof}[\pf{6.35}]
  We use mathematical induction on \(n = \dim(\V)\).
  If \(n = 1\), then every element of \(\bi(\V)\) is diagonalizable.
  Now suppose that the theorem is valid for vector spaces of dimension less than \(n + 1\) for some fixed integer \(n \geq 1\), and suppose that \(\dim(\V) = n + 1\).
  If \(H\) is the zero bilinear form on \(\V\), then trivially \(H\) is diagonalizable;
  so suppose that \(H\) is a nonzero symmetric bilinear form on \(\V\).
  By \cref{6.8.15}, there exists a nonzero vector \(x \in \V\) such that \(H(x, x) \neq 0\).
  Recall the function \(\lt{L}_x : \V \to \F\) defined by \(\lt{L}_x(y) = H(x, y)\) for all \(y \in \V\).
  By \cref{6.8.3}(a), \(\lt{L}_x \in \ls(\V, \F)\).
  Furthermore, since \(\lt{L}_x(x) = H(x, x) \neq 0\), \(\lt{L}_x\) is nonzero.
  Consequently, \(\rk{\lt{L}_x} = 1\), and hence \(\nt{\lt{L}_x} = n\) (\cref{2.3}).

  The restriction of \(H\) to \(\ns{\lt{L}_x}\) is obviously a symmetric bilinear form on a vector space of dimension \(n\), i.e.,
  \[
    \forall y_1, y_2 \in \ns{\lt{L}_x}, H(y_1, y_2) = H(y_2, y_1).
  \]
  Thus, by the induction hypothesis, there exists an ordered basis \(\set{\seq{v}{1,,n}}\) for \(\ns{\lt{L}_x}\) such that \(H(v_i, v_j) = 0\) for \(i \neq j\) (\(i, j \in \set{1, \dots, n}\)).
  Set \(v_{n + 1} = x\).
  Then \(v_{n + 1} \notin \ns{\lt{L}_x}\), and so \(\beta = \set{\seq{v}{1,,n+1}}\) is an ordered basis for \(\V\) over \(\F\).
  In addition, \(H(v_i, v_{n + 1}) = H(v_{n + 1}, v_i) = 0\) for \(i \in \set{1, \dots, n}\) (\cref{6.8.11}).
  We conclude that \(\psi_{\beta}(H)\) is a diagonal matrix, and therefore \(H\) is diagonalizable (\cref{6.8.12}).
\end{proof}

\begin{cor}\label{6.8.16}
  Let \(\F\) be a field that is not of characteristic two.
  If \(A \in \ms[n][n][\F]\) is a symmetric matrix, then \(A\) is congruent to a diagonal matrix.
\end{cor}

\begin{proof}[\pf{6.8.16}]
  Let \(\beta\) be an ordered basis for \(\vs{F}^n\) over \(\F\).
  By \cref{6.32} there exists an unique \(H \in \bi(\V)\) such that \(\psi_{\beta}(H) = A\).
  Since \(A\) is symmetric, by \cref{6.34} we know that \(H\) is symmetric.
  Since \(\F \neq \Z_2\), by \cref{6.35} we know that \(H\) is diagonalizable.
  By \cref{6.8.12} there exists an ordered basis \(\gamma\) for \(\vs{F}^n\) over \(\F\) such that \(\psi_{\gamma}(H)\) is a diagonal matrix.
  Thus by \cref{6.33} \(A\) is congruent to \(\psi_{\gamma}(H)\).
\end{proof}

\begin{prop}\label{6.8.17}
  Let \(\F\) be a field and \(\F \neq \Z_2\).
  By means of several elementary column operations and the corresponding row operations, \(A \in \ms[n][n][\F]\) can be transformed into a diagonal matrix \(D\).
  Furthermore, if \(\seq{E}{1,,k}\) are the elementary matrices corresponding to these elementary column operations indexed in the order performed, and if \(Q = \seq[]{E}{1,,k}\), then \(\tp{Q} A Q = D\).
\end{prop}

\begin{proof}[\pf{6.8.17}]
  If \(E\) is an elementary \(n \times n\) matrix, then \(AE\) can be obtained by performing an elementary column operation on \(A\) (\cref{3.1}).
  By \cref{ex:6.8.21}, \(\tp{E} A\) can be obtained by performing the same operation on the rows of \(A\) rather than on its columns.
  Thus \(\tp{E} A E\) can be obtained from \(A\) by performing an elementary operation on the columns of \(A\) and then performing the same operation on the rows of \(AE\).
  (Note that the order of the operations can be reversed because of the associative property of matrix multiplication, see \cref{2.16}.)
  Suppose that \(Q\) is an invertible matrix and \(D\) is a diagonal matrix such that \(\tp{Q} A Q = D\) (\cref{6.8.16}).
  By \cref{3.2.6}, \(Q\) is a product of elementary matrices, say \(Q = \seq[]{E}{1,,k}\).
  Thus
  \[
    D = \tp{Q} A Q = \tp{E_k} \cdots \tp{E_1} A \seq{E}{1,,k}.
  \]
\end{proof}

\begin{prop}\label{6.8.18}
  Let \(\F\) be a field and \(\F \neq \Z_2\).
  Let \(A \in \ms[n][n][\F]\) be symmetric.
  Then \((A | I_n)\) can be transformed into \((D | \tp{Q})\) where \(D = \tp{Q} A Q\).
\end{prop}

\begin{proof}[\pf{6.8.18}]
  Since \(A\) is symmetric, by \cref{6.8.16} there exists a \(Q \in \ms[n][n][\F]\) such that \(D = \tp{Q} A Q\) is a diagonal matrix.
  Let \(Q = \seq[]{E}{1,,k}\) where \(\seq{E}{1,,k}\) are elementary column operations.
  By performing \(\seq{E}{1,,k}\) on the first \(n\) column of \((A | I_n)\) we have \((AQ | I_n)\).
  Then by \cref{ex:3.2.15} we have
  \[
    \tp{Q} (AQ | I_n) = (\tp{Q} A Q | \tp{Q}) = (D | \tp{Q}).
  \]
\end{proof}

\begin{defn}\label{6.8.19}
  Let \(\V\) be a vector space over \(\F\).
  A function \(K : \V \to \F\) is called a \textbf{quadratic form} if there exists a symmetric bilinear form \(H \in \bi(\V)\) such that
  \begin{equation}\label{eq:6.8.1}
    K(x) = H(x, x) \quad \text{for all } x \in \V.
  \end{equation}
  If \(\F \neq \Z_2\), then by \cref{6.32} there is a one-to-one correspondence between symmetric bilinear forms and quadratic forms given by \cref{eq:6.8.1}.
  In fact, if \(K\) is a quadratic form on a vector space \(\V\) over a field \(\F\) not of characteristic two, and \(K(x) = H(x, x)\) for some symmetric bilinear form \(H\) on \(\V\), then we can recover \(H\) from \(K\) because
  \begin{equation}\label{eq:6.8.2}
    H(x, y) = \frac{1}{2} (K(x + y) - K(x) - K(y))
  \end{equation}
  (see \cref{ex:6.8.16}).
\end{defn}

\begin{eg}\label{6.8.20}
  The classic example of a quadratic form is the homogeneous second-degree polynomial of several variables.
  Given the variables \(\seq{x}{1,,n}\) that take values in a field \(\F\) not of characteristic two and given (not necessarily distinct) scalars \(a_{i j} \in \F\) (\(i, j \in \set{1, \dots, n}\) and \(i \leq j\)), define the polynomial
  \[
    f\tuple{x}{1,,n} = \sum_{i = 1}^n \sum_{j = i}^n a_{i j} x_i x_j.
  \]
  Any such polynomial is a quadratic form.
  In fact, if \(\beta\) is the standard ordered basis for \(\vs{F}^n\) over \(\F\), then the symmetric bilinear form \(H\) corresponding to the quadratic form \(f\) has the matrix representation \(\psi_{\beta}(H) = A\), where
  \[
    A_{i j} = A_{j i} = \begin{dcases}
      a_{i i}             & \text{if } i = j    \\
      \frac{1}{2} a_{i j} & \text{if } i \neq j
    \end{dcases}.
  \]
\end{eg}

\begin{proof}[\pf{6.8.20}]
  Let \(A \in \ms[n][n][\F]\) be defined as above.
  Clearly \(A\) is symmetric.
  By \cref{6.32} there exists an unique \(H \in \bi(\vs{F}^n)\) such that \(\psi_{\beta}(H) = A\).
  By \cref{6.8.19} we can define \(K : \V \to \F\) to be the quadratic form associated with \(H\).
  Now we show that \(K = f\).
  This is true since
  \begin{align*}
     & \forall x \in \vs{F}^n, K(x) = H(x, x)                                                                                               &  & \by{6.8.19} \\
     & = \tp{x} A x                                                                                                                         &  & \by{6.8.8}  \\
     & = \sum_{i = 1}^n x_i (Ax)_{i}                                                                                                        &  & \by{2.3.1}  \\
     & = \sum_{i = 1}^n \sum_{j = 1}^n x_i A_{i j} x_j                                                                                      &  & \by{2.3.1}  \\
     & = \pa{\sum_{i = 1}^n x_i A_{i i} x_i} + \pa{\sum_{i = 1}^n \sum_{j = i + 1}^n x_i A_{i j} x_j + x_j A_{j i} x_i}                                      \\
     & = \pa{\sum_{i = 1}^n x_i a_{i i} x_i} + \pa{\sum_{i = 1}^n \sum_{j = i + 1}^n x_i \frac{a_{i j}}{2} x_j + x_j \frac{a_{i j}}{2} x_i}                  \\
     & = \pa{\sum_{i = 1}^n a_{i i} x_i x_i} + \pa{\sum_{i = 1}^n \sum_{j = i + 1}^n a_{i j} x_i x_j}                                                        \\
     & = \sum_{i = 1}^n \sum_{j = i}^n a_{i j} x_i x_j                                                                                                       \\
     & = f(x).
  \end{align*}
  Thus \(f\) is a quadratic form.
\end{proof}

\begin{note}
  Since symmetric matrices over \(\R\) are orthogonally diagonalizable (see \cref{6.20}), the theory of symmetric bilinear forms and quadratic forms on finite-dimensional vector spaces over \(\R\) is especially nice.
  The following theorem (\cref{6.36}) and its corollary (\cref{6.8.21}) are useful.
\end{note}

\begin{thm}\label{6.36}
  Let \(\V\) be a finite-dimensional real inner product space, and let \(H\) be a symmetric bilinear form on \(\V\).
  Then there exists an orthonormal basis \(\beta\) for \(\V\) over \(\R\) such that \(\psi_{\beta}(H)\) is a diagonal matrix.
\end{thm}

\begin{proof}[\pf{6.36}]
  Choose any orthonormal basis \(\gamma = \set{\seq{v}{1,,n}}\) for \(\V\) over \(\R\), and let \(A = \psi_{\gamma}(H)\).
  Since \(A\) is symmetric, there exists an orthogonal matrix \(Q\) and a diagonal matrix \(D\) such that \(D = \tp{Q} A Q\) by \cref{6.20}.
  Let \(\beta = \set{\seq{w}{1,,n}}\) be defined by
  \[
    w_j = \sum_{i = 1}^n Q_{i j} v_i \quad \text{for } j \in \set{1, \dots, n}.
  \]
  By \cref{6.33}, \(\psi_{\beta}(H) = D\).
  Furthermore, since \(Q\) is orthogonal and \(\gamma\) is orthonormal, \(\beta\) is orthonormal by \cref{ex:6.5.30}.
\end{proof}

\begin{cor}\label{6.8.21}
  Let \(K\) be a quadratic form on a finite-dimensional real inner product space \(\V\).
  There exists an orthonormal basis \(\beta = \set{\seq{v}{1,,n}}\) for \(\V\) over \(\R\) and scalars \(\seq{\lambda}{1,,n} \in \R\) (not necessarily distinct) such that if \(x \in \V\) and
  \[
    x = \sum_{i = 1}^n s_i v_i, \quad s_i \in \R,
  \]
  then
  \[
    K(x) = \sum_{i = 1}^n \lambda_i s_i^2.
  \]
  In fact, if \(H\) is the symmetric bilinear form determined by \(K\), then \(\beta\) can be chosen to be any orthonormal basis for \(\V\) over \(\R\) such that \(\psi_{\beta}(H)\) is a diagonal matrix.
\end{cor}

\begin{proof}[\pf{6.8.21}]
  Let \(H\) be the symmetric bilinear form for which \(K(x) = H(x, x)\) for all \(x \in \V\).
  By \cref{6.36}, there exists an orthonormal basis \(\beta = \set{\seq{v}{1,,n}}\) for \(\V\) over \(\R\) such that \(\psi_{\beta}(H)\) is the diagonal matrix
  \[
    D = \begin{pmatrix}
      \lambda_1 & 0         & \cdots & 0         \\
      0         & \lambda_2 & \cdots & 0         \\
      \vdots    & \vdots    &        & \vdots    \\
      0         & 0         & \cdots & \lambda_n
    \end{pmatrix}.
  \]
  Let \(x \in \V\), and suppose that \(x = \sum_{i = 1}^n s_i v_i\).
  Then
  \begin{align*}
    K(x) & = H(x, x)                                     &  & \by{6.8.19} \\
         & = \tp{\pa{\phi_{\beta}(x)}} D \phi_{\beta}(x) &  & \by{6.8.7}  \\
         & = \begin{pmatrix}
               s_1 & \cdots & s_n
             \end{pmatrix} D \begin{pmatrix}
                               s_1    \\
                               \vdots \\
                               s_n
                             \end{pmatrix}               &  & \by{2.4.11} \\
         & = \sum_{i = 1}^n \lambda_i s_i^2.             &  & \by{2.3.1}
  \end{align*}
\end{proof}

\begin{defn}\label{6.8.22}
  Let \(z = f\tuple{t}{1,,n}\) be a fixed real-valued function of \(n\) real variables for which all third-order partial derivatives exist and are continuous.
  The function \(f\) is said to have a \textbf{local maximum} at a point \(p \in \R^n\) if there exists a \(\delta > 0\) such that \(f(p) \geq f(x)\) whenever \(\norm{x - p} < \delta\).
  Likewise, \(f\) has a \textbf{local minimum} at \(p \in \R^n\) if there exists a \(\delta > 0\) such that \(f(p) \leq f(x)\) whenever \(\norm{x - p} < \delta\).
  If \(f\) has either a local minimum or a local maximum at \(p\), we say that \(f\) has a \textbf{local extremum} at \(p\).
  A point \(p \in \R^n\) is called a \textbf{critical point} of \(f\) if \(\frac{\partial f}{\partial t_i}(p) = 0\) for \(i \in \set{1, \dots, n}\).
  It is a well-known fact that if \(f\) has a local extremum at a point \(p \in \R^n\), then \(p\) is a critical point of \(f\).
  For, if \(f\) has a local extremum at \(p = \tuple{p}{1,,n}\), then for any \(i \in \set{1, \dots, n}\), the function \(\phi_i : \R \to \R\) defined by \(\phi_i(t) = f(\seq{p}{1,,i-1}, t, \seq{p}{i+1,,n})\) has a local extremum at \(t = p_i\).
  So, by an elementary single-variable argument,
  \[
    \frac{\partial f}{\partial t_i}(p) = \frac{d \phi_i}{dt}(p_i) = 0.
  \]
  Thus \(p\) is a critical point of \(f\).
  But critical points are not necessarily local extrema.

  The second-order partial derivatives of \(f\) at a critical point \(p\) can often be used to test for a local extremum at \(p\).
  These partials determine a matrix \(A(p)\) in which the row \(i\), column \(j\) entry is
  \[
    \frac{\partial^2 f}{\partial t_i \partial t_j}(p).
  \]
  This matrix is called the \textbf{Hessian matrix} of \(f\) at \(p\).
  Note that if the third-order partial derivatives of \(f\) are continuous, then the mixed second-order partials of \(f\) at \(p\) are independent of the order in which they are taken, and hence \(A(p)\) is a symmetric matrix.
  In this case, all of the eigenvalues of \(A(p)\) are real.
\end{defn}

\begin{thm}[The Second Derivative Test]\label{6.37}
  Let \(f\tuple{t}{1,,n}\) be a real-valued function in \(n\) real variables for which all third-order partial derivatives exist and are continuous.
  Let \(p = \tuple{p}{1,,n}\) be a critical point of \(f\), and let \(A(p)\) be the Hessian of \(f\) at \(p\).
  \begin{enumerate}
    \item If all eigenvalues of \(A(p)\) are positive, then \(f\) has a local minimum at \(p\).
    \item If all eigenvalues of \(A(p)\) are negative, then \(f\) has a local maximum at \(p\).
    \item If \(A(p)\) has at least one positive and at least one negative eigenvalue, then \(f\) has no local extremum at \(p\)
          (\(p\) is called a \textbf{saddle-point} of \(f\)).
    \item If \(\rk{A(p)} < n\) and \(A(p)\) does not have both positive and negative eigenvalues, then the second derivative test is inconclusive.
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{6.37}]
  If \(p \neq \zv\), we may define a function \(g : \R^n \to \R\) by
  \[
    g\tuple{t}{1,,n} = f(t_1 + p_1, \dots, t_n + p_n) - f(p).
  \]
  The following facts are easily verified.
  \begin{enumerate}[label=(\arabic*)]
    \item The function \(f\) has a local maximum (minimum) at \(p\) iff \(g\) has a local maximum (minimum) at \(\zv = (0, \dots, 0)\).
    \item The partial derivatives of \(g\) at \(\zv\) are equal to the corresponding partial derivatives of \(f\) at \(p\).
    \item \(\zv\) is a critical point of \(g\).
    \item \(A_{i j}(p) = \frac{\partial^2 g}{\partial t_i \partial t_j}(\zv)\) for all \(i, j \in \set{1, \dots, n}\).
  \end{enumerate}

  In view of these facts, we may assume without loss of generality that \(p = \zv\) and \(f(p) = 0\).

  Now we apply Taylor's theorem to \(f\) to obtain the first-order approximation of \(f\) around \(\zv\).
  We have
  \begin{equation}\label{eq:6.8.3}
    \begin{aligned}
      f\tuple{t}{1,,n} & = f(\zv) + \sum_{i = 1}^n \frac{\partial f}{\partial t_i}(\zv) t_i + \frac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n \frac{\partial^2 f}{\partial t_i \partial t_j}(\zv) t_i t_j + S\tuple{t}{1,,n} \\
                       & = \frac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n \frac{\partial^2 f}{\partial t_i \partial t_j}(\zv) t_i t_j + S\tuple{t}{1,,n},
    \end{aligned}
  \end{equation}
  where \(S\) is a real-valued function on \(\R^n\) such that
  \begin{equation}\label{eq:6.8.4}
    \Lim_{x \to \zv} \frac{S(x)}{\norm{x}^2} = \Lim_{\tuple{t}{1,,n} \to \zv} \frac{S\tuple{t}{1,,n}}{t_1^2 + \cdots + t_n^2} = 0.
  \end{equation}
  Let \(K : \R^n \to \R\) be the quadratic form defined by
  \begin{equation}\label{eq:6.8.5}
    K \begin{pmatrix}
      t_1    \\
      \vdots \\
      t_n
    \end{pmatrix} = \frac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n \frac{\partial^2 f}{\partial t_i \partial t_j}(\zv) t_i t_j,
  \end{equation}
  let \(H\) be the symmetric bilinear form corresponding to \(K\), and \(\beta\) be the standard ordered basis for \(\R^n\) over \(\R\).
  It is easy to verify that \(\psi_{\beta}(H) = \frac{1}{2} A(p)\).
  Since \(A(p)\) is symmetric, \cref{6.20} implies that there exists an orthogonal matrix \(Q\) such that
  \[
    \tp{Q} A(p) Q = \begin{pmatrix}
      \lambda_1 & 0         & \cdots & 0         \\
      0         & \lambda_2 & \cdots & 0         \\
      0         & 0         & \cdots & \lambda_n
    \end{pmatrix}
  \]
  is a diagonal matrix whose diagonal entries are the eigenvalues of \(A(p)\).
  Let \(\gamma = \set{\seq{v}{1,,n}}\) be the orthogonal basis for \(\R^n\) whose \(i\)th vector is the \(i\)th column of \(Q\).
  Then \(Q\) is the change of coordinate matrix changing \(\gamma\)-coordinates into \(\beta\)-coordinates, and by \cref{6.33}
  \[
    \psi_{\gamma}(H) = \tp{Q} \psi_{\beta}(H) Q = \frac{1}{2} \tp{Q} A(p) Q = \begin{pmatrix}
      \frac{\lambda_1}{2} & 0                   & \cdots & 0                   \\
      0                   & \frac{\lambda_2}{2} & \cdots & 0                   \\
      0                   & 0                   & \cdots & \frac{\lambda_n}{2}
    \end{pmatrix}.
  \]

  Suppose that \(A(p)\) is not the zero matrix.
  Then \(A(p)\) has nonzero eigenvalues.
  Choose \(\epsilon > 0\) such that \(\epsilon < \abs{\lambda_i} / 2\) for all \(\lambda_i \neq 0\).
  By \cref{eq:6.8.4}, there exists \(\delta > 0\) such that for any \(x \in \R^n\) satisfying \(0 < \norm{x} < \delta\), we have \(\abs{S(x)} < \epsilon \norm{x}^2\).
  Consider any \(x \in \R^n\) such that \(0 < \norm{x} < \delta\).
  Then, by \cref{eq:6.8.3,eq:6.8.5},
  \[
    \abs{f(x) - K(x)} = \abs{S(x)} < \epsilon \norm{x}^2,
  \]
  and hence
  \begin{equation}\label{eq:6.8.6}
    K(x) - \epsilon \norm{x}^2 < f(x) < K(x) + \epsilon \norm{x}^2.
  \end{equation}
  Suppose that \(x = \sum_{i = 1}^n s_i v_i\).
  Then
  \[
    \norm{x}^2 = \sum_{i = 1}^n s_i^2 \quad \text{and} \quad K(x) = \frac{1}{2} \sum_{i = 1}^n \lambda_i s_i^2.
  \]
  Combining these equations with \cref{eq:6.8.6}, we obtain
  \begin{equation}\label{eq:6.8.7}
    \sum_{i = 1}^n \pa{\frac{1}{2} \lambda_i - \epsilon} s_i^2 < f(x) < \sum_{i = 1}^n \pa{\frac{1}{2} \lambda_i + \epsilon} s_i^2.
  \end{equation}

  Now suppose that all eigenvalues of \(A(p)\) are positive.
  Then \(\frac{1}{2} \lambda_i - \epsilon > 0\) for all \(i \in \set{1, \dots, n}\), and hence, by the left inequality in \cref{eq:6.8.7},
  \[
    f(\zv) = 0 \leq \sum_{i = 1}^n \pa{\frac{1}{2} \lambda_i - \epsilon} s_i^2 < f(x).
  \]
  Thus \(f(\zv) \leq f(x)\) for \(\norm{x} < \delta\), and so \(f\) has a local minimum at \(\zv\).
  By a similar argument using the right inequality in \cref{eq:6.8.7}, we have that if all of the eigenvalues of \(A(p)\) are negative, then \(f\) has a local maximum at \(\zv\).
  This establishes (a) and (b) of the theorem.

  Next, suppose that \(A(p)\) has both a positive and a negative eigenvalue, say, \(\lambda_i > 0\) and \(\lambda_j < 0\) for some \(i, j \in \set{1, \dots, n}\).
  Then \(\frac{1}{2} \lambda_i - \epsilon > 0\) and \(\frac{1}{2} \lambda_j + \epsilon < 0\).
  Let \(s\) be any real number such that \(0 < \abs{s} < \delta\).
  Substituting \(x = s v_i\) and \(x = s v_j\) into the left inequality and the right inequality of \cref{eq:6.8.7}, respectively, we obtain
  \[
    f(\zv) = 0 < \pa{\frac{1}{2} \lambda_i - \epsilon} s^2 < f(s v_i) \quad \text{and} \quad f(s v_j) < \pa{\frac{1}{2} \lambda_j + \epsilon} s^2 < 0 = f(\zv).
  \]
  Thus \(f\) attains both positive and negative values arbitrarily close to \(\zv\);
  so \(f\) has neither a local maximum nor a local minimum at \(\zv\).
  This establishes (c).

  To show that the second-derivative test is inconclusive under the conditions stated in (d), consider the functions
  \[
    f\tuple{t}{1,2} = t_1^2 - t_2^4 \quad \text{and} \quad g\tuple{t}{1,2} = t_1^2 + t_2^4
  \]
  at \(p = \zv\).
  In both cases, the function has a critical point at \(p\), and
  \[
    A(p) = \begin{pmatrix}
      2 & 0 \\
      0 & 0
    \end{pmatrix}.
  \]
  However, \(f\) does not have a local extremum at \(\zv\), whereas \(g\) has a local minimum at \(\zv\).
\end{proof}

\exercisesection

\begin{ex}\label{ex:6.8.12}
  Prove that the relation of congruence is an equivalence relation.
\end{ex}

\begin{proof}[\pf{ex:6.8.12}]
  Let \(A, B, C \in \ms[n][n][\F]\).
  Since \(A = \tp{I_n} A I_n\), by \cref{6.8.9} we know that the congruence relation is reflexive.
  Since
  \begin{align*}
             & B \text{ is congruent to } A                                           \\
    \implies & \exists Q \in \ms[n][n][\F] : \begin{dcases}
                                               Q \text{ is invertible} \\
                                               B = \tp{Q} A Q
                                             \end{dcases}        &  & \by{6.8.9}      \\
    \implies & A = (\tp{Q})^{-1} B Q^{-1} = \tp{(Q^{-1})} B Q^{-1} &  & \by{ex:2.4.5} \\
    \implies & A \text{ is congruent to } B,
  \end{align*}
  by \cref{6.8.9} we know that the congruence relation is symmetric.
  Since
  \begin{align*}
             & \begin{dcases}
                 B \text{ is congruent to } A \\
                 C \text{ is congruent to } B
               \end{dcases}                                    \\
    \implies & \exists P, Q \in \ms[n][n][\F] : \begin{dcases}
                                                  P, Q \text{ are invertible} \\
                                                  B = \tp{Q} A Q              \\
                                                  C = \tp{P} B P
                                                \end{dcases} &  & \by{6.8.9}   \\
    \implies & C = \tp{P} \tp{Q} A Q P = \tp{QP} A QP          &  & \by{2.3.2} \\
    \implies & C \text{ is congruent to } A,                   &  & \by{6.8.9}
  \end{align*}
  by \cref{6.8.9} we know that the congruence relation is transitive.
  Thus the congruence relation is an equivalence relation.
\end{proof}

\begin{ex}\label{ex:6.8.13}
\end{ex}

\begin{ex}\label{ex:6.8.16}
  Let \(\V\) be a vector space over a field \(\F\) not of characteristic two, and let \(H\) be a symmetric bilinear form on \(\V\).
  Prove that if \(K(x) = H(x, x)\) is the quadratic form associated with \(H\), then, for all \(x, y \in \V\),
  \[
    H(x, y) = \frac{1}{2} (K(x + y) - K(x) - K(y)).
  \]
\end{ex}

\begin{proof}[\pf{ex:6.8.16}]
  We have
  \begin{align*}
     & \forall x, y \in \V, \frac{1}{2} (K(x + y) - K(x) - K(y))                    \\
     & = \frac{1}{2} (H(x + y, x + y) - H(x, x) - H(y, y))       &  & \by{6.8.19}   \\
     & = \frac{1}{2} (H(x, y) + H(y, x))                         &  & \by{6.8.3}[c] \\
     & = \frac{1}{2} 2 H(x, y)                                   &  & \by{6.8.11}   \\
     & = H(x, y).
  \end{align*}
\end{proof}

\begin{ex}\label{ex:6.8.21}
\end{ex}
