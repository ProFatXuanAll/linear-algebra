\section{Bilinear and Quadratic Forms}\label{sec:6.8}

\begin{defn}\label{6.8.1}
  Let \(\V\) be a vector space over a field \(\F\).
  A function \(H\) from the set \(\V \times \V\) of ordered pairs of vectors to \(\F\) is called a \textbf{bilinear form} on \(\V\) if \(H\) is linear in each variable when the other variable is held fixed;
  that is, \(H\) is a bilinear form on \(\V\) if
  \begin{enumerate}
    \item \(H(a x_1 + x_2, y) = a H(x_1, y) + H(x_2, y)\) for all \(x_1, x_2, y \in \V\) and \(a \in \F\).
    \item \(H(x, a y_1 + y_2) = a H(x, y_1) + H(x, y_2)\) for all \(x, y_1, y_2 \in \V\) and \(a \in \F\).
  \end{enumerate}
  We denote the set of all bilinear forms on \(\V\) by \(\bi(\V)\).
  Observe that an inner product on a vector space is a bilinear form if the underlying field is real, but not if the underlying field is complex (\cref{6.1}(b)).
\end{defn}

\begin{eg}\label{6.8.2}
  Let \(\V = \vs{F}^n\), where the vectors are considered as column vectors.
  For any \(A \in \ms[n][n][\F]\), define \(H : \V \times \V \to \F\) by
  \[
    H(x, y) = \tp{x} A y \quad \text{for } x, y \in \V.
  \]
  Notice that since \(x\) and \(y\) are \(n \times 1\) matrices and \(A \in \ms[n][n][\F]\), \(H(x, y)\) is a \(1 \times 1\) matrix.
  Then \(H \in \bi(\V)\).
\end{eg}

\begin{proof}[\pf{6.8.2}]
  Let \(x, y, z \in \V\) and let \(c \in \F\).
  Since
  \begin{align*}
    H(ax + y, z) & = \tp{(ax + y)} A z         &  & \by{6.8.2}     \\
                 & = (a \tp{x} + \tp{y}) A z   &  & \by{ex:1.3.3}  \\
                 & = a \tp{x} A z + \tp{y} A z &  & \by{2.12}[a,b] \\
                 & = a H(x, z) + H(y, z);      &  & \by{6.8.2}     \\
    H(z, ax + y) & = \tp{z} A (ax + y)         &  & \by{6.8.2}     \\
                 & = a \tp{z} A x + \tp{z} A y &  & \by{2.12}[a,b] \\
                 & = a H(z, x) + H(z, y),      &  & \by{6.8.2}
  \end{align*}
  by \cref{6.8.1} we know that \(H \in \bi(\V)\).
\end{proof}

\begin{prop}\label{6.8.3}
  For any bilinear form \(H\) on a vector space \(\V\) over a field \(\F\), the following properties hold.
  \begin{enumerate}
    \item If, for any \(x \in \V\), the functions \(\lt{L}_x, \lt{R}_x : \V \to \F\) are defined by
          \[
            \lt{L}_x(y) = H(x, y) \quad \text{and} \quad \lt{R}_x(y) = H(y, x) \quad \text{for all } y \in \V,
          \]
          then \(\lt{L}_x, \lt{R}_x \in \ls(\V, \F)\).
    \item \(H(\zv, x) = H(x, \zv) = 0\) for all \(x \in \V\).
    \item For all \(x, y, z, w \in \V\),
          \[
            H(x + y, z + w) = H(x, z) + H(x, w) + H(y, z) + H(y, w).
          \]
    \item If \(J : \V \times \V \to \F\) is defined by \(J(x, y) = H(y, x)\), then \(J\) is a bilinear form.
  \end{enumerate}
\end{prop}

\begin{proof}[\pf{6.8.3}(a)]
  Let \(y, z \in \V\) and let \(c \in \F\).
  Since
  \begin{align*}
    \lt{L}_x(cy + z) & = H(x, cy + z)                 &  & \by{6.8.3}[a] \\
                     & = c H(x, y) + H(x, z)          &  & \by{6.8.1}[b] \\
                     & = c \lt{L}_x(y) + \lt{L}_x(z); &  & \by{6.8.3}[a] \\
    \lt{R}_x(cy + z) & = H(cy + z, x)                 &  & \by{6.8.3}[a] \\
                     & = c H(y, x) + H(z, x)          &  & \by{6.8.1}[a] \\
                     & = c \lt{R}_x(y) + \lt{R}_x(z), &  & \by{6.8.3}[a]
  \end{align*}
  by \cref{2.1.2}(b) we know that \(\lt{L}_x, \lt{R}_x \in \ls(\V, \F)\).
\end{proof}

\begin{proof}[\pf{6.8.3}(b)]
  Since
  \begin{align*}
    H(\zv, x) & = H(\zv + \zv, x)        &  & \by{1.2.1}    \\
              & = H(\zv, x) + H(\zv, x); &  & \by{6.8.1}[a] \\
    H(x, \zv) & = H(x, \zv + \zv)        &  & \by{1.2.1}    \\
              & = H(x, \zv) + H(x, \zv), &  & \by{6.8.1}[b]
  \end{align*}
  by \cref{c.1} we know that
  \[
    H(\zv, x) = 0 = H(x, \zv).
  \]
\end{proof}

\begin{proof}[\pf{6.8.3}(c)]
  We have
  \begin{align*}
    H(x + y, z + w) & = H(x, z + w) + H(y, z + w)              &  & \by{6.8.1}[a] \\
                    & = H(x, z) + H(x, w) + H(y, z) + H(y, w). &  & \by{6.8.1}[b]
  \end{align*}
\end{proof}

\begin{proof}[\pf{6.8.3}(d)]
  Let \(x, y, z \in \V\) and \(c \in \F\).
  Since
  \begin{align*}
    J(cx + y, z) & = H(z, cx + y)         &  & \by{6.8.3}[d] \\
                 & = c H(z, x) + H(z, y)  &  & \by{6.8.1}[b] \\
                 & = c J(x, z) + J(y, z); &  & \by{6.8.3}[d] \\
    J(z, cx + y) & = H(cx + y, z)         &  & \by{6.8.3}[d] \\
                 & = c H(x, z) + H(y, z)  &  & \by{6.8.1}[a] \\
                 & = c J(z, x) + J(z, y), &  & \by{6.8.3}[d]
  \end{align*}
  by \cref{6.8.1} we know that \(J \in \bi(\V)\).
\end{proof}

\begin{defn}\label{6.8.4}
  Let \(\V\) be a vector space over \(\F\), let \(H_1, H_2 \in \bi(\V)\), and let \(a \in \F\).
  We define the \textbf{sum} \(H_1 + H_2\) and the \textbf{scalar product} \(a H_1\) by the equations
  \[
    (H_1 + H_2)(x, y) = H_1(x, y) + H_2(x, y)
  \]
  and
  \[
    (a H_1)(x, y) = a (H_1(x, y)) \quad \text{for all } x, y \in \V.
  \]
\end{defn}

\begin{thm}\label{6.31}
  For any vector space \(\V\) over \(\F\), the sum of two bilinear forms and the product of a scalar and a bilinear form on \(\V\) are again bilinear forms on \(\V\).
  Furthermore, \(\bi(\V)\) is a vector space with respect to these operations.
\end{thm}

\begin{proof}[\pf{6.31}]
  By \cref{1.2.10} we know that the set \(\fs(\V \times \V, \F)\) is a vector space and \(\bi(\V) \subseteq \fs(\V \times \V, \F)\).
  Let \(\zT\) be the zero function of \(\fs(\V \times \V, \F)\).
  Let \(f, g \in \bi(\V)\), let \(x, y, z \in \V\) and let \(a, c \in \F\).
  Since
  \begin{align*}
    (cf + g)(ax + y, z) & = c f(ax + y, z) + g(ax + y, z)                 &  & \by{6.8.4}    \\
                        & = c (a f(x, z) + f(y, z)) + a g(x, z) + g(y, z) &  & \by{6.8.1}[a] \\
                        & = a (c f(x, z) + g(x, z)) + c f(y, z) + g(y, z)                    \\
                        & = a (cf + g)(x, z) + (cf + g)(y, z);            &  & \by{6.8.4}    \\
    (cf + g)(z, ax + y) & = c f(z, ax + y) + g(z, ax + y)                 &  & \by{6.8.4}    \\
                        & = c (a f(z, x) + f(z, y)) + a g(z, x) + g(z, y) &  & \by{6.8.1}[b] \\
                        & = a (c f(z, x) + g(z, x)) + c f(z, y) + g(z, y)                    \\
                        & = a (cf + g)(z, x) + (cf + g)(z, y),            &  & \by{6.8.4}
  \end{align*}
  by \cref{6.8.1} we have \(cf + g \in \bi(\V)\).
  Since
  \begin{align*}
    \zT(ax + y, z) & = 0                        \\
                   & = a 0 + 0                  \\
                   & = a \zT(x, z) + \zT(y, z); \\
    \zT(z, ax + y) & = 0                        \\
                   & = a 0 + 0                  \\
                   & = a \zT(z, x) + \zT(z, y),
  \end{align*}
  by \cref{6.8.1} we have \(\zT \in \bi(\V)\).
  Thus by \cref{ex:1.3.17} we know that \(\bi(\V)\) is a vector space over \(\F\).
\end{proof}

\begin{defn}\label{6.8.5}
  Let \(\beta = \set{\seq{v}{1,,n}}\) be an ordered basis for an \(n\)-dimensional vector space \(\V\) over \(\F\), and let \(H \in \bi(\V)\).
  We can associate \(H\) with an \(n \times n\) matrix \(A\) whose entry in row \(i\) and column \(j\) is defined by
  \[
    A_{i j} = H(v_i, v_j) \quad \text{for } i, j \in \set{1, \dots, n}.
  \]
  The matrix \(A\) above is called the \textbf{matrix representation} of \(H\) with respect to the ordered basis \(\beta\) and is denoted by \(\psi_{\beta}(H)\).
  We can therefore regard \(\psi_{\beta}\) as a mapping from \(\bi(\V)\) to \(\ms[n][n][\F]\), where \(\F\) is the field of scalars for \(\V\), that takes a bilinear form \(H\) into its matrix representation \(\psi_{\beta}(H)\).
\end{defn}

\begin{thm}\label{6.32}
  For any \(n\)-dimensional vector space \(\V\) over \(\F\) and any ordered basis \(\beta\) for \(\V\) over \(\F\), \(\psi_{\beta} : \bi(\V) \to \ms[n][n][\F]\) is an isomorphism.
\end{thm}

\begin{proof}[\pf{6.32}]
  First we show that \(\psi_{\beta} \in \ls(\bi(\V), \ms[n][n][\F])\).
  Let \(f, g \in \bi(\V)\) and let \(c \in \F\).
  Since
  \begin{align*}
    \forall i, j \in \set{1, \dots, n}, \pa{\psi_{\beta}(cf + g)}_{i j} & = (cf + g)(v_i, v_j)                                        &  & \by{6.8.5} \\
                                                                        & = c f(v_i, v_j) + g(v_i, v_j)                               &  & \by{6.8.4} \\
                                                                        & = c \pa{\psi_{\beta}(f)}_{i j} + \pa{\psi_{\beta}(g)}_{i j} &  & \by{6.8.5} \\
                                                                        & = \pa{c \psi_{\beta}(f) + \psi_{\beta}(g)}_{i j},           &  & \by{1.2.9}
  \end{align*}
  by \cref{1.2.8} we know that \(\psi_{\beta}(cf + g) = c \psi_{\beta}(f) + \psi_{\beta}(g)\) and thus by \cref{2.1.2}(b) \(\psi_{\beta} \in \ls(\bi(\V), \ms[n][n][\F])\).

  To show that \(\psi_{\beta}\) is one-to-one, suppose that \(\psi_{\beta}(H) = \zm\) for some \(H \in \bi(\V)\).
  Fix \(v_i \in \beta\), and recall the mapping \(\lt{L}_{v_i} : \V \to \F\), which is linear by \cref{6.8.3}(a).
  By hypothesis, \(\lt{L}_{v_i}(v_j) = H(v_i, v_j) = 0\) for all \(v_j \in \beta\).
  Hence \(\lt{L}_{v_i}\) is the zero transformation from \(\V\) to \(\F\) by \cref{2.1.13}.
  So
  \[
    H(v_i, x) = \lt{L}_{v_i}(x) = 0 \quad \text{for all } x \in \V \text{ and } v_i \in \beta.
  \]
  Next fix an arbitrary \(y \in \V\), and recall the linear mapping \(\lt{R}_y : \V \to \F\) defined in \cref{6.8.3}(a).
  From the equation above we have \(\lt{R}_y(v_i) = H(v_i, y) = 0\) for all \(v_i \in \beta\), and hence \(\lt{R}_y\) is the zero transformation by \cref{2.1.13}.
  So \(H(x, y) = \lt{R}_y(x) = 0\) for all \(x, y \in \V\).
  Thus \(H\) is the zero bilinear form, and therefore \(\psi_{\beta}\) is one-to-one by \cref{2.4}.

  To show that \(\psi_{\beta}\) is onto, consider any \(A \in \ms[n][n][\F]\).
  Recall the isomorphism \(\phi_{\beta} \in \ls(\V, \vs{\F}^n)\) defined in \cref{2.4.11}.
  For \(x \in \V\), we view \(\phi_{\beta}(x) \in \vs{F}^n\) as a column vector.
  Let \(H : \V \times \V \to \F\) be the mapping defined by
  \[
    H(x, y) = \tp{(\phi_{\beta}(x))} A (\phi_{\beta}(y)) \quad \text{for all } x, y \in \V.
  \]
  By \cref{6.8.2} we know that \(H \in \bi(\V)\).
  We show that \(\psi_{\beta}(H) = A\).
  Let \(v_i, v_j \in \beta\).
  Then \(\phi_{\beta}(v_i) = e_i\) and \(\phi_{\beta}(v_j) = e_j\);
  hence, for any \(i, j \in \set{1, \dots, n}\),
  \[
    H(v_i, v_j) = \tp{(\phi_{\beta}(v_i))} A (\phi_{\beta}(v_j)) = \tp{e_i} A e_j = A_{i j}.
  \]
  We conclude that \(\psi_{\beta}(H) = A\) and \(\psi_{\beta}\) is onto.
\end{proof}

\begin{cor}\label{6.8.6}
  For any \(n\)-dimensional vector space \(\V\), \(\bi(\V)\) has dimension \(n^2\).
\end{cor}

\begin{proof}[\pf{6.8.6}]
  By \cref{1.6.11,2.19,6.32} we see that this is true.
\end{proof}

\begin{cor}\label{6.8.7}
  Let \(\V\) be an \(n\)-dimensional vector space over \(\F\) with ordered basis \(\beta\).
  If \(H \in \bi(\V)\) and \(A \in \ms[n][n][\F]\), then \(\psi_{\beta}(H) = A\) iff \(H(x, y) = \tp{(\phi_{\beta}(x))} A (\phi_{\beta}(y))\) for all \(x, y \in \V\).
\end{cor}

\begin{proof}[\pf{6.8.7}]
  Let \(\beta = \set{\seq{v}{1,,n}}\) and let \(x, y \in \V\).
  Since \(\beta\) is a basis for \(\V\) over \(\F\), there exist \(\seq{a}{1,,n}, \seq{b}{1,,n} \in \F\) such that \(x = \sum_{i = 1}^n a_i v_i\) and \(y = \sum_{i = 1}^n b_i v_i\).
  Then we have
  \begin{align*}
         & \psi_{\beta}(H) = A                                                                                                                   \\
    \iff & H(x, y) = H\pa{\sum_{i = 1}^n a_i v_i, \sum_{j = 1}^n b_j v_j} = \sum_{i = 1}^n \sum_{j = 1}^n a_i b_j H(v_i, v_j) &  & \by{6.8.3}[c] \\
         & = \sum_{i = 1}^n \sum_{j = 1}^n a_i b_j A_{i j}                                                                    &  & \by{6.32}     \\
         & = \sum_{i = 1}^n \sum_{j = 1}^n a_i b_j \tp{e_i} A e_j                                                             &  & \by{2.3.1}    \\
         & = \pa{\sum_{i = 1}^n a_i \tp{e_i}} A \pa{\sum_{j = 1}^n b_j e_j}                                                   &  & \by{2.3.5}    \\
         & = \tp{\pa{\sum_{i = 1}^n a_i e_i}} A \pa{\sum_{j = 1}^n b_j e_j}                                                   &  & \by{ex:1.3.3} \\
         & = \tp{(\phi_{\beta}(x))} A (\phi_{\beta}(y)).                                                                      &  & \by{2.4.11}
  \end{align*}
\end{proof}

\begin{cor}\label{6.8.8}
  Let \(\F\) be a field, \(n\) a positive integer, and \(\beta\) be the standard ordered basis for \(\vs{F}^n\) over \(\F\).
  Then for any \(H \in \bi(\vs{F}^n)\), there exists an unique matrix \(A \in \ms[n][n][\F]\), namely, \(A = \psi_{\beta}(H)\), such that
  \[
    H(x, y) = \tp{x} A y \quad \text{for all } x, y \in \vs{F}^n.
  \]
\end{cor}

\begin{proof}[\pf{6.8.8}]
  This is an immediate consequence of \cref{6.8.7}.
\end{proof}

\begin{defn}\label{6.8.9}
  Let \(A, B \in \ms[n][n][\F]\).
  Then \(B\) is said to be \textbf{congruent} to \(A\) if there exists an invertible matrix \(Q \in \ms[n][n][\F]\) such that \(B = \tp{Q} A Q\).
  Observe that the relation of congruence is an equivalence relation (see \cref{ex:6.8.12}).
\end{defn}

\begin{thm}\label{6.33}
  Let \(\V\) be a finite-dimensional vector space over \(\F\) with ordered bases \(\beta = \set{\seq{v}{1,,n}}\) and \(\gamma = \set{\seq{w}{1,,n}}\), and let \(Q\) be the change of coordinate matrix changing \(\gamma\)-coordinates into \(\beta\)-coordinates.
  Then, for any \(H \in \bi(\V)\), we have \(\psi_{\gamma}(H) = \tp{Q} \psi_{\beta}(H) Q\).
  Therefore \(\psi_{\gamma}(H)\) is congruent to \(\psi_{\beta}(H)\).
\end{thm}

\begin{proof}[\pf{6.33}]
  There are essentially two proofs of this theorem.
  One involves a direct computation, while the other follows immediately from a clever observation.
  We give the more direct proof here, leaving the other proof for the exercises (see \cref{ex:6.8.13}).

  Suppose that \(A = \psi_{\beta}(H)\) and \(B = \psi_{\gamma}(H)\).
  Then for \(i, j \in \set{1, \dots, n}\),
  \[
    w_i = \sum_{k = 1}^n Q_{k i} v_k \quad \text{and} \quad w_j = \sum_{r = 1}^n Q_{r j} v_r.
  \]
  (See \cref{2.2.4}.)
  Thus
  \begin{align*}
    B_{i j} & = H(w_i, w_j)                                                  &  & \by{6.8.5}    \\
            & = H\pa{\sum_{k = 1}^n Q_{k i} v_k, w_j}                        &  & \by{2.2.4}    \\
            & = \sum_{k = 1}^n Q_{k i} H(v_k, w_j)                           &  & \by{6.8.1}[a] \\
            & = \sum_{k = 1}^n Q_{k i} H\pa{v_k, \sum_{r = 1}^n Q_{r j} v_r} &  & \by{2.2.4}    \\
            & = \sum_{k = 1}^n Q_{k i} \sum_{r = 1}^n Q_{r j} H(v_k, v_r)    &  & \by{6.8.1}[b] \\
            & = \sum_{k = 1}^n Q_{k i} \sum_{r = 1}^n Q_{r j} A_{k r}        &  & \by{6.8.5}    \\
            & = \sum_{k = 1}^n Q_{k i} \sum_{r=  1}^n A_{k r} Q_{r j}                           \\
            & = \sum_{k = 1}^n Q_{k i} (AQ)_{k j}                            &  & \by{2.3.1}    \\
            & = \sum_{k = 1}^n (\tp{Q})_{i k} (AQ)_{k j}                     &  & \by{1.3.3}    \\
            & = (\tp{Q} A Q)_{i j}.                                          &  & \by{2.3.1}
  \end{align*}
  Hence by \cref{1.2.8} \(B = \tp{Q} A Q\).
\end{proof}

\begin{cor}\label{6.8.10}
  Let \(\V\) be an \(n\)-dimensional vector space over \(\F\) with ordered basis \(\beta\), and let \(H \in \bi(\V)\).
  For any \(B \in \ms[n][n][\F]\), if \(B\) is congruent to \(\psi_{\beta}(H)\), then there exists an ordered basis \(\gamma\) for \(\V\) over \(\F\) such that \(\psi_{\gamma}(H) = B\).
  Furthermore, if \(B = \tp{Q} \psi_{\beta}(H) Q\) for some invertible matrix \(Q\), then \(Q\) changes \(\gamma\)-coordinates into \(\beta\)-coordinates.
\end{cor}

\begin{proof}[\pf{6.8.10}]
  Suppose that \(B = \tp{Q} \psi_{\beta}(H) Q\) for some invertible matrix \(Q\) and that \(\beta = \set{\seq{v}{1,,n}}\).
  Let \(\gamma = \set{\seq{w}{1,,n}}\), where
  \[
    w_j = \sum_{i = 1}^n Q_{i j} v_i \quad \text{for } j \in \set{1, \dots, n}.
  \]
  Since \(Q\) is invertible, \(\gamma\) is an ordered basis for \(\V\) over \(\F\) (\cref{2.2,2.3}), and \(Q\) is the change of coordinate matrix that changes \(\gamma\)-coordinates into \(\beta\)-coordinates (\cref{2.5.1}).
  Therefore, by \cref{6.33},
  \[
    B = \tp{Q} \psi_{\beta}(H) Q = \psi_{\gamma}(H).
  \]
\end{proof}

\begin{note}
  \cref{6.8.10} is the converse of \cref{6.33}.
\end{note}

\begin{note}
  Like the diagonalization problem for linear operators, there is an analogous \emph{diagonalization} problem for bilinear forms, namely, the problem of determining those bilinear forms for which there are diagonal matrix representations.
  As we will see, there is a close relationship between \emph{diagonalizable} bilinear forms and those that are called \emph{symmetric}.
\end{note}

\begin{defn}\label{6.8.11}
  A bilinear form \(H\) on a vector space \(\V\) over \(\F\) is \textbf{symmetric} if \(H(x, y) = H(y, x)\) for all \(x, y \in \V\).
\end{defn}

\begin{thm}\label{6.34}
  Let \(H\) be a bilinear form on a finite-dimensional vector space \(\V\) over \(\F\), and let \(\beta\) be an ordered basis for \(\V\) over \(\F\).
  Then \(H\) is symmetric iff \(\psi_{\beta}(H)\) is symmetric.
\end{thm}

\begin{proof}[\pf{6.34}]
  Let \(\beta = \set{\seq{v}{1,,n}}\) and \(B = \psi_{\beta}(H)\).

  First assume that \(H\) is symmetric.
  Then for \(i, j \in \set{1, \dots, n}\),
  \begin{align*}
    B_{i j} & = H(v_i, v_j) &  & \by{6.8.5}  \\
            & = H(v_j, v_i) &  & \by{6.8.11} \\
            & = B_{j i},    &  & \by{6.8.5}
  \end{align*}
  and it follows that \(B\) is symmetric (\cref{1.3.4}).

  Conversely, suppose that \(B\) is symmetric.
  Let \(J : \V \times \V \to \F\) be the mapping defined by \(J(x, y) = H(y, x)\) for all \(x, y \in \V\).
  By \cref{6.8.3}(d), \(J\) is a bilinear form.
  Let \(C = \psi_{\beta}(J)\).
  Then, for \(i, j \in \set{1, \dots, n}\),
  \begin{align*}
    C_{i j} & = J(v_i, v_j) &  & \by{6.8.5}    \\
            & = H(v_j, v_i) &  & \by{6.8.3}[d] \\
            & = B_{j i}     &  & \by{6.8.5}    \\
            & = B_{i j}.    &  & \by{1.3.4}
  \end{align*}
  Thus \(C = B\) (\cref{1.2.8}).
  Since \(\psi_{\beta}\) is one-to-one (\cref{6.32}), we have \(J = H\).
  Hence \(H(y, x) = J(x, y) = H(x, y)\) for all \(x, y \in \V\), and therefore \(H\) is symmetric (\cref{6.8.11}).
\end{proof}

\begin{defn}\label{6.8.12}
  A bilinear form \(H\) on a finite-dimensional vector space \(\V\) over \(\F\) is called \textbf{diagonalizable} if there is an ordered basis \(\beta\) for \(\V\) over \(\F\) such that \(\psi_{\beta}(H)\) is a diagonal matrix.
\end{defn}

\begin{cor}\label{6.8.13}
  Let \(H\) be a diagonalizable bilinear form on a finite-dimensional vector space \(\V\) over \(\F\).
  Then \(H\) is symmetric.
\end{cor}

\begin{proof}[\pf{6.8.13}]
  Suppose that \(H\) is diagonalizable.
  Then there is an ordered basis \(\beta\) for \(\V\) over \(\F\) such that \(\psi_{\beta}(H) = D\) is a diagonal matrix (\cref{6.8.12}).
  Trivially, \(D\) is a symmetric matrix, and hence, by \cref{6.34}, \(H\) is symmetric.
\end{proof}

\begin{note}
  Unfortunately, the converse of \cref{6.8.13} is not true, as is illustrated by \cref{6.8.14}.
\end{note}

\begin{eg}\label{6.8.14}
  Let \(\F = \Z_2\), \(\V = \vs{F}^2\), and \(H : \V \times \V \to \F\) be the bilinear form defined by
  \[
    H\pa{\begin{pmatrix}
        a_1 \\
        a_2
      \end{pmatrix}, \begin{pmatrix}
        b_1 \\
        b_2
      \end{pmatrix}} = a_1 b_2 + a_2 b_1.
  \]
  Clearly \(H\) is symmetric.
  In fact, if \(\beta\) is the standard ordered basis for \(\V\) over \(\F\), then
  \[
    A = \psi_{\beta}(H) = \begin{pmatrix}
      0 & 1 \\
      1 & 0
    \end{pmatrix},
  \]
  a symmetric matrix.
  We show that \(H\) is not diagonalizable.
\end{eg}

\begin{proof}[\pf{6.8.14}]
  By way of contradiction, suppose that \(H\) is diagonalizable.
  Then there is an ordered basis \(\gamma\) for \(\V\) over \(\F\) such that \(B = \psi_{\gamma}(H)\) is a diagonal matrix (\cref{6.8.12}).
  So by \cref{6.33}, there exists an invertible matrix \(Q\) such that \(B = \tp{Q} A Q\).
  Since \(Q\) is invertible, it follows that \(\rk{B} = \rk{A} = 2\) (\cref{3.4}(c)), and consequently the diagonal entries of \(B\) are nonzero.
  Since the only nonzero scalar of \(\F\) is \(1\),
  \[
    B = \begin{pmatrix}
      1 & 0 \\
      0 & 1
    \end{pmatrix}.
  \]
  Suppose that
  \[
    Q = \begin{pmatrix}
      a & b \\
      c & d
    \end{pmatrix}.
  \]
  Then
  \begin{align*}
    \begin{pmatrix}
      1 & 0 \\
      0 & 1
    \end{pmatrix} & = B                           &  & (\F = \Z_2) \\
                    & = \tp{Q} A Q                  &  & \by{6.33} \\
                    & = \begin{pmatrix}
                          a & c \\
                          b & d
                        \end{pmatrix} \begin{pmatrix}
                                        0 & 1 \\
                                        1 & 0
                                      \end{pmatrix} \begin{pmatrix}
                                                      a & b \\
                                                      c & d
                                                    \end{pmatrix} \\
                    & = \begin{pmatrix}
                          ac + ac & bc + ad \\
                          bc + ad & bd + bd
                        \end{pmatrix}.          &  & \by{2.3.1}
  \end{align*}
  But \(p + p = 0\) for all \(p \in \F\);
  hence \(ac + ac = 0\).
  Thus, comparing the row \(1\), column \(1\) entries of the matrices in the equation above, we conclude that \(1 = 0\), a contradiction.
  Therefore \(H\) is not diagonalizable.
\end{proof}

\begin{note}
  The bilinear form of \cref{6.8.14} is an anomaly.
  Its failure to be diagonalizable is due to the fact that the scalar field \(\Z_2\) is of characteristic two.
  Recall from \cref{c.0.4} that a field \(\F\) is of \textbf{characteristic two} if \(1 + 1 = 0\) in \(\F\).
  If \(\F\) is not of characteristic two, then \(1 + 1 = 2\) has a multiplicative inverse, which we denote by \(1 / 2\).
\end{note}

\begin{lem}\label{6.8.15}
  Let \(H\) be a nonzero symmetric bilinear form on a vector space \(\V\) over a field \(\F\) not of characteristic two.
  Then there is a vector \(x \in \V\) such that \(H(x, x) \neq 0\).
\end{lem}

\begin{proof}[\pf{6.8.15}]
  Since \(H\) is nonzero, we can choose vectors \(u, v \in \V\) such that \(H(u, v) \neq 0\).
  If \(H(u, u) \neq 0\) or \(H(v, v) \neq 0\), there is nothing to prove.
  Otherwise, set \(x = u + v\).
  Then
  \begin{align*}
    H(x, x) & = H(u, u) + H(u, v) + H(v, u) + H(v, v) &  & \by{6.8.3}[c]           \\
            & = 2 H(u, v)                             &  & (H(u, u) = 0 = H(v, v)) \\
            & \neq 0
  \end{align*}
  because \(2 \neq 0\) and \(H(u, v) \neq 0\).
\end{proof}

\begin{thm}\label{6.35}
  Let \(\V\) be a finite-dimensional vector space over a field \(\F\) not of characteristic two.
  Then every symmetric bilinear form on \(\V\) is diagonalizable.
\end{thm}

\begin{proof}[\pf{6.35}]
  We use mathematical induction on \(n = \dim(\V)\).
  If \(n = 1\), then every element of \(\bi(\V)\) is diagonalizable.
  Now suppose that the theorem is valid for vector spaces of dimension less than \(n + 1\) for some fixed integer \(n \geq 1\), and suppose that \(\dim(\V) = n + 1\).
  If \(H\) is the zero bilinear form on \(\V\), then trivially \(H\) is diagonalizable;
  so suppose that \(H\) is a nonzero symmetric bilinear form on \(\V\).
  By \cref{6.8.15}, there exists a nonzero vector \(x \in \V\) such that \(H(x, x) \neq 0\).
  Recall the function \(\lt{L}_x : \V \to \F\) defined by \(\lt{L}_x(y) = H(x, y)\) for all \(y \in \V\).
  By \cref{6.8.3}(a), \(\lt{L}_x \in \ls(\V, \F)\).
  Furthermore, since \(\lt{L}_x(x) = H(x, x) \neq 0\), \(\lt{L}_x\) is nonzero.
  Consequently, \(\rk{\lt{L}_x} = 1\), and hence \(\nt{\lt{L}_x} = n\) (\cref{2.3}).

  The restriction of \(H\) to \(\ns{\lt{L}_x}\) is obviously a symmetric bilinear form on a vector space of dimension \(n\), i.e.,
  \[
    \forall y_1, y_2 \in \ns{\lt{L}_x}, H(y_1, y_2) = H(y_2, y_1).
  \]
  Thus, by the induction hypothesis, there exists an ordered basis \(\set{\seq{v}{1,,n}}\) for \(\ns{\lt{L}_x}\) such that \(H(v_i, v_j) = 0\) for \(i \neq j\) (\(i, j \in \set{1, \dots, n}\)).
  Set \(v_{n + 1} = x\).
  Then \(v_{n + 1} \notin \ns{\lt{L}_x}\), and so \(\beta = \set{\seq{v}{1,,n+1}}\) is an ordered basis for \(\V\) over \(\F\).
  In addition, \(H(v_i, v_{n + 1}) = H(v_{n + 1}, v_i) = 0\) for \(i \in \set{1, \dots, n}\) (\cref{6.8.11}).
  We conclude that \(\psi_{\beta}(H)\) is a diagonal matrix, and therefore \(H\) is diagonalizable (\cref{6.8.12}).
\end{proof}

\begin{cor}\label{6.8.16}
  Let \(\F\) be a field that is not of characteristic two.
  If \(A \in \ms[n][n][\F]\) is a symmetric matrix, then \(A\) is congruent to a diagonal matrix.
\end{cor}

\begin{proof}[\pf{6.8.16}]
  Let \(\beta\) be an ordered basis for \(\vs{F}^n\) over \(\F\).
  By \cref{6.32} there exists an unique \(H \in \bi(\V)\) such that \(\psi_{\beta}(H) = A\).
  Since \(A\) is symmetric, by \cref{6.34} we know that \(H\) is symmetric.
  Since \(\F \neq \Z_2\), by \cref{6.35} we know that \(H\) is diagonalizable.
  By \cref{6.8.12} there exists an ordered basis \(\gamma\) for \(\vs{F}^n\) over \(\F\) such that \(\psi_{\gamma}(H)\) is a diagonal matrix.
  Thus by \cref{6.33} \(A\) is congruent to \(\psi_{\gamma}(H)\).
\end{proof}

\begin{prop}\label{6.8.17}
  Let \(\F\) be a field and \(\F \neq \Z_2\).
  By means of several elementary column operations and the corresponding row operations, \(A \in \ms[n][n][\F]\) can be transformed into a diagonal matrix \(D\).
  Furthermore, if \(\seq{E}{1,,k}\) are the elementary matrices corresponding to these elementary column operations indexed in the order performed, and if \(Q = \seq[]{E}{1,,k}\), then \(\tp{Q} A Q = D\).
\end{prop}

\begin{proof}[\pf{6.8.17}]
  If \(E\) is an elementary \(n \times n\) matrix, then \(AE\) can be obtained by performing an elementary column operation on \(A\) (\cref{3.1}).
  By \cref{ex:6.8.21}, \(\tp{E} A\) can be obtained by performing the same operation on the rows of \(A\) rather than on its columns.
  Thus \(\tp{E} A E\) can be obtained from \(A\) by performing an elementary operation on the columns of \(A\) and then performing the same operation on the rows of \(AE\).
  (Note that the order of the operations can be reversed because of the associative property of matrix multiplication, see \cref{2.16}.)
  Suppose that \(Q\) is an invertible matrix and \(D\) is a diagonal matrix such that \(\tp{Q} A Q = D\) (\cref{6.8.16}).
  By \cref{3.2.6}, \(Q\) is a product of elementary matrices, say \(Q = \seq[]{E}{1,,k}\).
  Thus
  \[
    D = \tp{Q} A Q = \tp{E_k} \cdots \tp{E_1} A \seq{E}{1,,k}.
  \]
\end{proof}

\begin{prop}\label{6.8.18}
  Let \(\F\) be a field and \(\F \neq \Z_2\).
  Let \(A \in \ms[n][n][\F]\) be symmetric.
  Then \((A | I_n)\) can be transformed into \((D | \tp{Q})\) where \(D = \tp{Q} A Q\).
\end{prop}

\begin{proof}[\pf{6.8.18}]
  Since \(A\) is symmetric, by \cref{6.8.16} there exists a \(Q \in \ms[n][n][\F]\) such that \(D = \tp{Q} A Q\) is a diagonal matrix.
  Let \(Q = \seq[]{E}{1,,k}\) where \(\seq{E}{1,,k}\) are elementary column operations.
  By performing \(\seq{E}{1,,k}\) on the first \(n\) column of \((A | I_n)\) we have \((AQ | I_n)\).
  Then by \cref{ex:3.2.15} we have
  \[
    \tp{Q} (AQ | I_n) = (\tp{Q} A Q | \tp{Q}) = (D | \tp{Q}).
  \]
\end{proof}

\begin{defn}\label{6.8.19}
  Let \(\V\) be a vector space over \(\F\).
  A function \(K : \V \to \F\) is called a \textbf{quadratic form} if there exists a symmetric bilinear form \(H \in \bi(\V)\) such that
  \begin{equation}\label{eq:6.8.1}
    K(x) = H(x, x) \quad \text{for all } x \in \V.
  \end{equation}
  If \(\F \neq \Z_2\), then by \cref{6.32} there is a one-to-one correspondence between symmetric bilinear forms and quadratic forms given by \cref{eq:6.8.1}.
  In fact, if \(K\) is a quadratic form on a vector space \(\V\) over a field \(\F\) not of characteristic two, and \(K(x) = H(x, x)\) for some symmetric bilinear form \(H\) on \(\V\), then we can recover \(H\) from \(K\) because
  \begin{equation}\label{eq:6.8.2}
    H(x, y) = \dfrac{1}{2} (K(x + y) - K(x) - K(y))
  \end{equation}
  (see \cref{ex:6.8.16}).
\end{defn}

\begin{eg}\label{6.8.20}
  The classic example of a quadratic form is the homogeneous second-degree polynomial of several variables.
  Given the variables \(\seq{x}{1,,n}\) that take values in a field \(\F\) not of characteristic two and given (not necessarily distinct) scalars \(a_{i j} \in \F\) (\(i, j \in \set{1, \dots, n}\) and \(i \leq j\)), define the polynomial
  \[
    f\tuple{x}{1,,n} = \sum_{i = 1}^n \sum_{j = i}^n a_{i j} x_i x_j.
  \]
  Any such polynomial is a quadratic form.
  In fact, if \(\beta\) is the standard ordered basis for \(\vs{F}^n\) over \(\F\), then the symmetric bilinear form \(H\) corresponding to the quadratic form \(f\) has the matrix representation \(\psi_{\beta}(H) = A\), where
  \[
    A_{i j} = A_{j i} = \begin{dcases}
      a_{i i}              & \text{if } i = j    \\
      \dfrac{1}{2} a_{i j} & \text{if } i \neq j
    \end{dcases}.
  \]
\end{eg}

\begin{proof}[\pf{6.8.20}]
  Let \(A \in \ms[n][n][\F]\) be defined as above.
  Clearly \(A\) is symmetric.
  By \cref{6.32} there exists an unique \(H \in \bi(\vs{F}^n)\) such that \(\psi_{\beta}(H) = A\).
  By \cref{6.8.19} we can define \(K : \V \to \F\) to be the quadratic form associated with \(H\).
  Now we show that \(K = f\).
  This is true since
  \begin{align*}
     & \forall x \in \vs{F}^n, K(x) = H(x, x)                                                                                                 &  & \by{6.8.19} \\
     & = \tp{x} A x                                                                                                                           &  & \by{6.8.8}  \\
     & = \sum_{i = 1}^n x_i (Ax)_{i}                                                                                                          &  & \by{2.3.1}  \\
     & = \sum_{i = 1}^n \sum_{j = 1}^n x_i A_{i j} x_j                                                                                        &  & \by{2.3.1}  \\
     & = \pa{\sum_{i = 1}^n x_i A_{i i} x_i} + \pa{\sum_{i = 1}^n \sum_{j = i + 1}^n x_i A_{i j} x_j + x_j A_{j i} x_i}                                        \\
     & = \pa{\sum_{i = 1}^n x_i a_{i i} x_i} + \pa{\sum_{i = 1}^n \sum_{j = i + 1}^n x_i \dfrac{a_{i j}}{2} x_j + x_j \dfrac{a_{i j}}{2} x_i}                  \\
     & = \pa{\sum_{i = 1}^n a_{i i} x_i x_i} + \pa{\sum_{i = 1}^n \sum_{j = i + 1}^n a_{i j} x_i x_j}                                                          \\
     & = \sum_{i = 1}^n \sum_{j = i}^n a_{i j} x_i x_j                                                                                                         \\
     & = f(x).
  \end{align*}
  Thus \(f\) is a quadratic form.
\end{proof}

\begin{note}
  Since symmetric matrices over \(\R\) are orthogonally diagonalizable (see \cref{6.20}), the theory of symmetric bilinear forms and quadratic forms on finite-dimensional vector spaces over \(\R\) is especially nice.
  The following theorem (\cref{6.36}) and its corollary (\cref{6.8.21}) are useful.
\end{note}

\begin{thm}\label{6.36}
  Let \(\V\) be a finite-dimensional real inner product space, and let \(H\) be a symmetric bilinear form on \(\V\).
  Then there exists an orthonormal basis \(\beta\) for \(\V\) over \(\R\) such that \(\psi_{\beta}(H)\) is a diagonal matrix.
\end{thm}

\begin{proof}[\pf{6.36}]
  Choose any orthonormal basis \(\gamma = \set{\seq{v}{1,,n}}\) for \(\V\) over \(\R\), and let \(A = \psi_{\gamma}(H)\).
  Since \(A\) is symmetric, there exists an orthogonal matrix \(Q\) and a diagonal matrix \(D\) such that \(D = \tp{Q} A Q\) by \cref{6.20}.
  Let \(\beta = \set{\seq{w}{1,,n}}\) be defined by
  \[
    w_j = \sum_{i = 1}^n Q_{i j} v_i \quad \text{for } j \in \set{1, \dots, n}.
  \]
  By \cref{6.33}, \(\psi_{\beta}(H) = D\).
  Furthermore, since \(Q\) is orthogonal and \(\gamma\) is orthonormal, \(\beta\) is orthonormal by \cref{ex:6.5.30}.
\end{proof}

\begin{cor}\label{6.8.21}
  Let \(K\) be a quadratic form on a finite-dimensional real inner product space \(\V\).
  There exists an orthonormal basis \(\beta = \set{\seq{v}{1,,n}}\) for \(\V\) over \(\R\) and scalars \(\seq{\lambda}{1,,n} \in \R\) (not necessarily distinct) such that if \(x \in \V\) and
  \[
    x = \sum_{i = 1}^n s_i v_i, \quad s_i \in \R,
  \]
  then
  \[
    K(x) = \sum_{i = 1}^n \lambda_i s_i^2.
  \]
  In fact, if \(H\) is the symmetric bilinear form determined by \(K\), then \(\beta\) can be chosen to be any orthonormal basis for \(\V\) over \(\R\) such that \(\psi_{\beta}(H)\) is a diagonal matrix.
\end{cor}

\begin{proof}[\pf{6.8.21}]
  Let \(H\) be the symmetric bilinear form for which \(K(x) = H(x, x)\) for all \(x \in \V\).
  By \cref{6.36}, there exists an orthonormal basis \(\beta = \set{\seq{v}{1,,n}}\) for \(\V\) over \(\R\) such that \(\psi_{\beta}(H)\) is the diagonal matrix
  \[
    D = \begin{pmatrix}
      \lambda_1 & 0         & \cdots & 0         \\
      0         & \lambda_2 & \cdots & 0         \\
      \vdots    & \vdots    &        & \vdots    \\
      0         & 0         & \cdots & \lambda_n
    \end{pmatrix}.
  \]
  Let \(x \in \V\), and suppose that \(x = \sum_{i = 1}^n s_i v_i\).
  Then
  \begin{align*}
    K(x) & = H(x, x)                                     &  & \by{6.8.19} \\
         & = \tp{\pa{\phi_{\beta}(x)}} D \phi_{\beta}(x) &  & \by{6.8.7}  \\
         & = \begin{pmatrix}
               s_1 & \cdots & s_n
             \end{pmatrix} D \begin{pmatrix}
                               s_1    \\
                               \vdots \\
                               s_n
                             \end{pmatrix}               &  & \by{2.4.11} \\
         & = \sum_{i = 1}^n \lambda_i s_i^2.             &  & \by{2.3.1}
  \end{align*}
\end{proof}

\begin{defn}\label{6.8.22}
  Let \(z = f\tuple{t}{1,,n}\) be a fixed real-valued function of \(n\) real variables for which all third-order partial derivatives exist and are continuous.
  The function \(f\) is said to have a \textbf{local maximum} at a point \(p \in \R^n\) if there exists a \(\delta > 0\) such that \(f(p) \geq f(x)\) whenever \(\norm{x - p} < \delta\).
  Likewise, \(f\) has a \textbf{local minimum} at \(p \in \R^n\) if there exists a \(\delta > 0\) such that \(f(p) \leq f(x)\) whenever \(\norm{x - p} < \delta\).
  If \(f\) has either a local minimum or a local maximum at \(p\), we say that \(f\) has a \textbf{local extremum} at \(p\).
  A point \(p \in \R^n\) is called a \textbf{critical point} of \(f\) if \(\dfrac{\partial f}{\partial t_i}(p) = 0\) for \(i \in \set{1, \dots, n}\).
  It is a well-known fact that if \(f\) has a local extremum at a point \(p \in \R^n\), then \(p\) is a critical point of \(f\).
  For, if \(f\) has a local extremum at \(p = \tuple{p}{1,,n}\), then for any \(i \in \set{1, \dots, n}\), the function \(\phi_i : \R \to \R\) defined by \(\phi_i(t) = f(\seq{p}{1,,i-1}, t, \seq{p}{i+1,,n})\) has a local extremum at \(t = p_i\).
  So, by an elementary single-variable argument,
  \[
    \dfrac{\partial f}{\partial t_i}(p) = \dfrac{d \phi_i}{dt}(p_i) = 0.
  \]
  Thus \(p\) is a critical point of \(f\).
  But critical points are not necessarily local extrema.

  The second-order partial derivatives of \(f\) at a critical point \(p\) can often be used to test for a local extremum at \(p\).
  These partials determine a matrix \(A(p)\) in which the row \(i\), column \(j\) entry is
  \[
    \dfrac{\partial^2 f}{\partial t_i \partial t_j}(p).
  \]
  This matrix is called the \textbf{Hessian matrix} of \(f\) at \(p\).
  Note that if the third-order partial derivatives of \(f\) are continuous, then the mixed second-order partials of \(f\) at \(p\) are independent of the order in which they are taken, and hence \(A(p)\) is a symmetric matrix.
  In this case, all of the eigenvalues of \(A(p)\) are real.
\end{defn}

\begin{thm}[The Second Derivative Test]\label{6.37}
  Let \(f\tuple{t}{1,,n}\) be a real-valued function in \(n\) real variables for which all third-order partial derivatives exist and are continuous.
  Let \(p = \tuple{p}{1,,n}\) be a critical point of \(f\), and let \(A(p)\) be the Hessian of \(f\) at \(p\).
  \begin{enumerate}
    \item If all eigenvalues of \(A(p)\) are positive, then \(f\) has a local minimum at \(p\).
    \item If all eigenvalues of \(A(p)\) are negative, then \(f\) has a local maximum at \(p\).
    \item If \(A(p)\) has at least one positive and at least one negative eigenvalue, then \(f\) has no local extremum at \(p\)
          (\(p\) is called a \textbf{saddle-point} of \(f\)).
    \item If \(\rk{A(p)} < n\) and \(A(p)\) does not have both positive and negative eigenvalues, then the second derivative test is inconclusive.
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{6.37}]
  If \(p \neq \zv\), we may define a function \(g : \R^n \to \R\) by
  \[
    g\tuple{t}{1,,n} = f(t_1 + p_1, \dots, t_n + p_n) - f(p).
  \]
  The following facts are easily verified.
  \begin{enumerate}[label=(\arabic*)]
    \item The function \(f\) has a local maximum (minimum) at \(p\) iff \(g\) has a local maximum (minimum) at \(\zv = (0, \dots, 0)\).
    \item The partial derivatives of \(g\) at \(\zv\) are equal to the corresponding partial derivatives of \(f\) at \(p\).
    \item \(\zv\) is a critical point of \(g\).
    \item \(A_{i j}(p) = \dfrac{\partial^2 g}{\partial t_i \partial t_j}(\zv)\) for all \(i, j \in \set{1, \dots, n}\).
  \end{enumerate}

  In view of these facts, we may assume without loss of generality that \(p = \zv\) and \(f(p) = 0\).

  Now we apply Taylor's theorem to \(f\) to obtain the first-order approximation of \(f\) around \(\zv\).
  We have
  \begin{equation}\label{eq:6.8.3}
    \begin{aligned}
       & f\tuple{t}{1,,n}                                                                                                                                                   \\
       & = f(\zv) + \sum_{i = 1}^n \dfrac{\partial f}{\partial t_i}(\zv) t_i                                                                                                \\
       & \quad + \dfrac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n \dfrac{\partial^2 f}{\partial t_i \partial t_j}(\zv) t_i t_j + S\tuple{t}{1,,n}                                 \\
       & = \sum_{i = 1}^n \dfrac{\partial f}{\partial t_i}(\zv) t_i                                                                                                         \\
       & \quad + \dfrac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n \dfrac{\partial^2 f}{\partial t_i \partial t_j}(\zv) t_i t_j + S\tuple{t}{1,,n} &  & (f(p) = 0)                 \\
       & = \dfrac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n \dfrac{\partial^2 f}{\partial t_i \partial t_j}(\zv) t_i t_j + S\tuple{t}{1,,n},      &  & \text{(\(p\) is critical)}
    \end{aligned}
  \end{equation}
  where \(S\) is a real-valued function on \(\R^n\) such that
  \begin{equation}\label{eq:6.8.4}
    \Lim_{x \to \zv} \dfrac{S(x)}{\norm{x}^2} = \Lim_{\tuple{t}{1,,n} \to \zv} \dfrac{S\tuple{t}{1,,n}}{t_1^2 + \cdots + t_n^2} = 0.
  \end{equation}
  Let \(K : \R^n \to \R\) be the quadratic form defined by
  \begin{equation}\label{eq:6.8.5}
    K \begin{pmatrix}
      t_1    \\
      \vdots \\
      t_n
    \end{pmatrix} = \dfrac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n \dfrac{\partial^2 f}{\partial t_i \partial t_j}(\zv) t_i t_j,
  \end{equation}
  let \(H\) be the symmetric bilinear form corresponding to \(K\), and \(\beta\) be the standard ordered basis for \(\R^n\) over \(\R\).
  It is easy to verify that \(\psi_{\beta}(H) = \dfrac{1}{2} A(p)\).
  Since \(A(p)\) is symmetric, \cref{6.20} implies that there exists an orthogonal matrix \(Q\) such that
  \[
    \tp{Q} A(p) Q = \begin{pmatrix}
      \lambda_1 & 0         & \cdots & 0         \\
      0         & \lambda_2 & \cdots & 0         \\
      \vdots    & \vdots    &        & \vdots    \\
      0         & 0         & \cdots & \lambda_n
    \end{pmatrix}
  \]
  is a diagonal matrix whose diagonal entries are the eigenvalues of \(A(p)\).
  Let \(\gamma = \set{\seq{v}{1,,n}}\) be the orthogonal basis for \(\R^n\) whose \(i\)th vector is the \(i\)th column of \(Q\).
  Then \(Q\) is the change of coordinate matrix changing \(\gamma\)-coordinates into \(\beta\)-coordinates, and by \cref{6.33}
  \[
    \psi_{\gamma}(H) = \tp{Q} \psi_{\beta}(H) Q = \dfrac{1}{2} \tp{Q} A(p) Q = \begin{pmatrix}
      \dfrac{\lambda_1}{2} & 0                    & \cdots & 0                    \\
      0                    & \dfrac{\lambda_2}{2} & \cdots & 0                    \\
      \vdots               & \vdots               &        & \vdots               \\
      0                    & 0                    & \cdots & \dfrac{\lambda_n}{2}
    \end{pmatrix}.
  \]

  Suppose that \(A(p)\) is not the zero matrix.
  Then \(A(p)\) has nonzero eigenvalues.
  Choose \(\epsilon > 0\) such that \(\epsilon < \abs{\lambda_i} / 2\) for all \(\lambda_i \neq 0\).
  By \cref{eq:6.8.4}, there exists \(\delta > 0\) such that for any \(x \in \R^n\) satisfying \(0 < \norm{x} < \delta\), we have \(\abs{S(x)} < \epsilon \norm{x}^2\).
  Consider any \(x \in \R^n\) such that \(0 < \norm{x} < \delta\).
  Then, by \cref{eq:6.8.3,eq:6.8.5},
  \[
    \abs{f(x) - K(x)} = \abs{S(x)} < \epsilon \norm{x}^2,
  \]
  and hence
  \begin{equation}\label{eq:6.8.6}
    K(x) - \epsilon \norm{x}^2 < f(x) < K(x) + \epsilon \norm{x}^2.
  \end{equation}
  Suppose that \(x = \sum_{i = 1}^n s_i v_i\).
  Then by \cref{ex:6.1.10,6.8.2} we have
  \[
    \norm{x}^2 = \sum_{i = 1}^n s_i^2 \quad \text{and} \quad K(x) = \dfrac{1}{2} \sum_{i = 1}^n \lambda_i s_i^2.
  \]
  Combining these equations with \cref{eq:6.8.6}, we obtain
  \begin{equation}\label{eq:6.8.7}
    \sum_{i = 1}^n \pa{\dfrac{1}{2} \lambda_i - \epsilon} s_i^2 < f(x) < \sum_{i = 1}^n \pa{\dfrac{1}{2} \lambda_i + \epsilon} s_i^2.
  \end{equation}

  Now suppose that all eigenvalues of \(A(p)\) are positive.
  Then \(\dfrac{1}{2} \lambda_i - \epsilon > 0\) for all \(i \in \set{1, \dots, n}\), and hence, by the left inequality in \cref{eq:6.8.7},
  \[
    f(\zv) = 0 \leq \sum_{i = 1}^n \pa{\dfrac{1}{2} \lambda_i - \epsilon} s_i^2 < f(x).
  \]
  Thus \(f(\zv) \leq f(x)\) for \(\norm{x} < \delta\), and so \(f\) has a local minimum at \(\zv\).
  By a similar argument using the right inequality in \cref{eq:6.8.7}, we have that if all of the eigenvalues of \(A(p)\) are negative, then \(f\) has a local maximum at \(\zv\).
  This establishes (a) and (b) of the theorem.

  Next, suppose that \(A(p)\) has both a positive and a negative eigenvalue, say, \(\lambda_i > 0\) and \(\lambda_j < 0\) for some \(i, j \in \set{1, \dots, n}\).
  Then \(\dfrac{1}{2} \lambda_i - \epsilon > 0\) and \(\dfrac{1}{2} \lambda_j + \epsilon < 0\).
  Let \(s\) be any real number such that \(0 < \abs{s} < \delta\).
  Substituting \(x = s v_i\) and \(x = s v_j\) into the left inequality and the right inequality of \cref{eq:6.8.7}, respectively, we obtain
  \[
    f(\zv) = 0 < \pa{\dfrac{1}{2} \lambda_i - \epsilon} s^2 < f(s v_i) \quad \text{and} \quad f(s v_j) < \pa{\dfrac{1}{2} \lambda_j + \epsilon} s^2 < 0 = f(\zv).
  \]
  Thus \(f\) attains both positive and negative values arbitrarily close to \(\zv\);
  so \(f\) has neither a local maximum nor a local minimum at \(\zv\).
  This establishes (c).

  To show that the second-derivative test is inconclusive under the conditions stated in (d), consider the functions
  \[
    f\tuple{t}{1,2} = t_1^2 - t_2^4 \quad \text{and} \quad g\tuple{t}{1,2} = t_1^2 + t_2^4
  \]
  at \(p = \zv\).
  In both cases, the function has a critical point at \(p\), and
  \[
    A(p) = \begin{pmatrix}
      2 & 0 \\
      0 & 0
    \end{pmatrix}.
  \]
  However, \(f\) does not have a local extremum at \(\zv\), whereas \(g\) has a local minimum at \(\zv\).
\end{proof}

\begin{defn}\label{6.8.23}
  Any two matrix representations of a bilinear form have the same rank because rank is preserved under congruence (\cref{ex:6.8.14}).
  We can therefore define the \textbf{rank} of a bilinear form to be the rank of any of its matrix representations.
  If a matrix representation is a diagonal matrix, then the rank is equal to the number of nonzero diagonal entries of the matrix.
\end{defn}

\begin{thm}[Sylvester's Law of Inertia]\label{6.38}
  Let \(H\) be a symmetric bilinear form on a finite-dimensional real vector space \(\V\).
  Then the number of positive diagonal entries and the number of negative diagonal entries in any diagonal matrix representation of \(H\) are each independent of the diagonal representation.
\end{thm}

\begin{proof}[\pf{6.38}]
  Suppose that \(\beta\) and \(\gamma\) are ordered bases for \(\V\) over \(\R\) that determine diagonal representations of \(H\).
  Without loss of generality, we may assume that \(\beta\) and \(\gamma\) are ordered so that on each diagonal the entries are in the order of positive, negative, and zero.
  It suffices to show that both representations have the same number of positive entries because the number of negative entries is equal to the difference between the rank and the number of positive entries.
  Let \(p\) and \(q\) be the number of positive diagonal entries in the matrix representations of \(H\) with respect to \(\beta\) and \(\gamma\), respectively.
  We suppose that \(p \neq q\) and arrive at a contradiction.
  Without loss of generality, assume that \(p < q\).
  Let
  \[
    \beta = \set{\seq{v}{1,,p,,r,,n}} \quad \text{and} \quad \gamma = \set{\seq{w}{1,,q,,r,,n}},
  \]
  where \(r\) is the rank of \(H\) and \(n = \dim(\V)\).
  Let \(\lt{L} : \V \to \R^{p + r - q}\) be the mapping defined by
  \[
    \lt{L}(x) = (H(x, v_1), \dots, H(x, v_p), H(x, w_{q + 1}), \dots, H(x, w_r)).
  \]
  It is easily verified that \(\lt{L}\) is linear (\cref{6.8.3}(a)) and \(\rk{\lt{L}} \leq p + r - q\).
  Hence
  \[
    \nt{\lt{L}} \geq n - (p + r - q) > n - r.
  \]
  So there exists a nonzero vector \(v_0\) such that \(v_0 \notin \spn{\set{\seq{v}{r+1,,n}}}\), but \(v_0 \in \ns{\lt{L}}\).
  Since \(v_0 \in \ns{\lt{L}}\), it follows that \(H(v_0, v_i) = 0\) for \(i \in \set{1, \dots, p}\) and \(H(v_0, w_i) = 0\) for \(i \in \set{q + 1, \dots, r}\).
  Suppose that
  \[
    v_0 = \sum_{j = 1}^n a_j v_j = \sum_{j = 1}^n b_j w_j.
  \]
  For any \(i \in \set{1, \dots, p}\),
  \begin{align*}
    H(v_0, v_i) & = H\pa{\sum_{j = 1}^n a_j v_j, v_i} &  & \by{1.6.1}    \\
                & = \sum_{j = 1}^n a_j H(v_j, v_i)    &  & \by{6.8.1}[a] \\
                & = a_i H(v_i, v_i).                  &  & \by{6.8.5}
  \end{align*}
  But for \(i \in \set{1, \dots, p}\), we have \(H(v_i, v_i) > 0\) and \(H(v_0, v_i) = 0\), so that \(a_i = 0\).
  Similarly, \(b_i = 0\) for \(i \in \set{q + 1, \dots, r}\).
  Since \(v_0\) is not in the span of \(\set{\seq{v}{r+1,,n}}\), it follows that \(a_i \neq 0\) for some \(i \in \set{p + 1, \dots, r}\).
  Thus
  \begin{align*}
    H(v_0, v_0) & = H\pa{\sum_{j = 1}^n a_j v_j, \sum_{i = 1}^n a_i v_i}                                          \\
                & = \sum_{j = 1}^n \sum_{i = 1}^n a_j a_i H(v_j, v_i)    &  & \by{6.8.1}                          \\
                & = \sum_{j = 1}^n a_j^2 H(v_j, v_j)                     &  & \by{6.8.5}                          \\
                & = \sum_{j = p + 1}^r a_j^2 H(v_j, v_j)                                                          \\
                & < 0.                                                   &  & \text{(by the definition of \(p\))}
  \end{align*}
  Furthermore,
  \begin{align*}
    H(v_0, v_0) & = H\pa{\sum_{j = 1}^n b_j w_j, \sum_{i = 1}^n b_i w_i}                 \\
                & = \sum_{j = 1}^n \sum_{i = 1}^n b_j b_i H(w_j, w_i)    &  & \by{6.8.1} \\
                & = \sum_{j = 1}^n b_j^2 H(w_j, w_j)                     &  & \by{6.8.5} \\
                & = \sum_{j = p + 1}^r b_j^2 H(w_j, w_j)                                 \\
                & \geq 0.                                                &  & (p < q)
  \end{align*}
  So \(H(v_0, v_0) < 0\) and \(H(v_0, v_0) \geq 0\), which is a contradiction.
  We conclude that \(p = q\).
\end{proof}

\begin{defn}\label{6.8.24}
  The number of positive diagonal entries in a diagonal representation of a symmetric bilinear form on a real vector space is called the \textbf{index} of the form.
  The difference between the number of positive and the number of negative diagonal entries in a diagonal representation of a symmetric bilinear form is called the \textbf{signature} of the form.
  The three terms \emph{rank}, \emph{index}, and \emph{signature} are called the \textbf{invariants} of the bilinear form because they are invariant with respect to matrix representations.
  These same terms apply to the associated quadratic form.
  Notice that the values of any two of these invariants determine the value of the third.
\end{defn}

\begin{eg}\label{6.8.25}
  The matrix representation of the bilinear form corresponding to the quadratic form \(K(x, y) = x^2 - y^2\) on \(\R^2\) with respect to the standard ordered basis is the diagonal matrix with diagonal entries of \(1\) and \(-1\).
  Therefore the rank of \(K\) is \(2\), the index of \(K\) is \(1\), and the signature of \(K\) is \(0\).
\end{eg}

\begin{proof}[\pf{6.8.25}]
  Let \(\beta = \set{\seq{e}{1,2}}\) and let \(H \in \bi(\R^2)\) such that
  \[
    \forall u, v \in \R^2, H(u, v) = \dfrac{1}{2} (K(u + v) - K(u) - K(v)).
  \]
  By \cref{6.8.19}, \(H\) is the unique bilinear form associated with \(K\).
  Since
  \begin{align*}
    H(e_1, e_1) & = \dfrac{1}{2} (K(2 e_1) - K(e_1) - K(e_1))                         \\
                & = \dfrac{1}{2} (2^2 - 0^2 - 1^2 + 0^2 - 1^2 + 0^2)                  \\
                & = 1;                                                                \\
    H(e_1, e_2) & = H(e_2, e_1)                                      &  & \by{6.8.19} \\
                & = \dfrac{1}{2} (K(e_1 + e_2) - K(e_1) - K(e_2))                     \\
                & = \dfrac{1}{2} (1^2 - 1^2 - 1^2 + 0^2 - 0^2 + 1^2)                  \\
                & = 0;                                                                \\
    H(e_2, e_2) & = \dfrac{1}{2} (K(2 e_2) - K(e_2) - K(e_2))                         \\
                & = \dfrac{1}{2} (0^2 - 2^2 - 0^2 + 1^2 - 0^2 + 1^2)                  \\
                & = -1,
  \end{align*}
  by \cref{6.8.5} we have
  \[
    \psi_{\beta}(H) = \begin{pmatrix}
      1 & 0  \\
      0 & -1
    \end{pmatrix}.
  \]
\end{proof}

\begin{cor}[Sylvester's Law of Inertia for Matrices]\label{6.8.26}
  Let \(A\) be a real symmetric matrix.
  Then the number of positive diagonal entries and the number of negative diagonal entries in any diagonal matrix congruent to \(A\) is independent of the choice of the diagonal matrix.
\end{cor}

\begin{proof}[\pf{6.8.26}]
  Let \(A \in \ms[n][n][\R]\) be symmetric, and suppose that \(D\) and \(E\) are each diagonal matrices congruent to \(A\).
  By \cref{6.8.8}, \(A\) is the matrix representation of the bilinear form \(H\) on \(\R^n\) defined by \(H(x, y) = \tp{x} A y\) with respect to the standard ordered basis for \(\R^n\).
  Therefore Sylvester's law of inertia (\cref{6.38}) tells us that \(D\) and \(E\) have the same number of positive and negative diagonal entries.
\end{proof}

\begin{defn}\label{6.8.27}
  Let \(A\) be a real symmetric matrix, and let \(D\) be a diagonal matrix that is congruent to \(A\).
  The number of positive diagonal entries of \(D\) is called the \textbf{index} of \(A\).
  The difference between the number of positive diagonal entries and the number of negative diagonal entries of \(D\) is called the \textbf{signature} of \(A\).
  As before, the rank, index, and signature of a matrix are called the \textbf{invariants} of the matrix, and the values of any two of these invariants determine the value of the third.
\end{defn}

\begin{cor}\label{6.8.28}
  Two real symmetric \(n \times n\) matrices are congruent iff they have the same invariants.
\end{cor}

\begin{proof}[\pf{6.8.28}]
  If \(A\) and \(B\) are congruent \(n \times n\) real symmetric matrices, then by \cref{6.33,6.35} they are both congruent to the same diagonal matrix, and it follows from \cref{6.8.26} that they have the same invariants.

  Conversely, suppose that \(A\) and \(B\) are \(n \times n\) symmetric matrices with the same invariants.
  Let \(D\) and \(E\) be diagonal matrices congruent to \(A\) and \(B\), respectively, chosen so that the diagonal entries are in the order of positive, negative, and zero.
  (\cref{ex:6.8.23} allows us to do this.)
  Since \(A\) and \(B\) have the same invariants, so do \(D\) and \(E\).
  Let \(p\) and \(r\) denote the index and the rank, respectively, of both \(D\) and \(E\).
  Let \(d_i\) denote the \(i\)th diagonal entry of \(D\), and let \(Q\) be the \(n \times n\) diagonal matrix whose \(i\)th diagonal entry \(q_i\) is given by
  \[
    q_i = \begin{dcases}
      \dfrac{1}{\sqrt{d_i}}  & \text{if } i \in \set{1, \dots, p}     \\
      \dfrac{1}{\sqrt{-d_i}} & \text{if } i \in \set{p + 1, \dots, r} \\
      1                      & \text{if } i \in \set{r + 1, \dots, n}
    \end{dcases}.
  \]
  Then \(\tp{Q} D Q = J_{pr}\), where
  \[
    J_{pr} = \begin{pmatrix}
      I_p & \zm         & \zm \\
      \zm & - I_{r - p} & \zm \\
      \zm & \zm         & \zm
    \end{pmatrix}.
  \]
  It follows that \(A\) is congruent to \(J_{pr}\).
  Similarly, \(B\) is congruent to \(J_{pr}\), and hence \(A\) is congruent to \(B\).
\end{proof}

\begin{note}
  By \cref{6.8.27,6.8.28}, any two of the invariants can be used to determine an equivalence class of congruent real symmetric matrices.

  The matrix \(J_{pr}\) in \cref{6.28} acts as a \emph{canonical form} for the theory of real symmetric matrices.
  \cref{6.8.29} describes the role of \(J_{pr}\).
\end{note}

\begin{cor}\label{6.8.29}
  A real symmetric \(n \times n\) matrix \(A\) has index \(p\) and rank \(r\) iff \(A\) is congruent to \(J_{pr}\) (as defined in \cref{6.8.28}).
\end{cor}

\begin{proof}[\pf{6.8.29}]
  See the proof of \cref{6.8.28}.
\end{proof}

\exercisesection

\setcounter{ex}{5}
\begin{ex}\label{ex:6.8.6}
  Let \(H : \R^2 \times \R^2 \to \R\) be the function defined by
  \[
    H\pa{\begin{pmatrix}
        a_1 \\
        a_2
      \end{pmatrix}, \begin{pmatrix}
        b_1 \\
        b_2
      \end{pmatrix}} = a_1 b_2 + a_2 b_1 \quad \text{for } \begin{pmatrix}
      a_1 \\
      a_2
    \end{pmatrix}, \begin{pmatrix}
      b_1 \\
      b_2
    \end{pmatrix} \in \R^2.
  \]
  \begin{enumerate}
    \item Prove that \(H\) is a bilinear form.
    \item Find the \(2 \times 2\) matrix \(A\) such that \(H(x, y) = \tp{x} A y\) for all \(x, y \in \R^2\).
  \end{enumerate}
  For a \(2 \times 2\) matrix \(M\) with columns \(x\) and \(y\), the bilinear form \(H(M) = H(x, y)\) is called the \textbf{permanent} of \(M\).
\end{ex}

\begin{proof}[\pf{ex:6.8.6}(a)]
  Let \(x, y, z \in \R^2\) and let \(c \in \R\).
  Since
  \begin{align*}
    H(cx + y, z) & = (cx + y)_1 z_2 + (cx + y)_2 z_1           &  & \by{ex:6.8.6} \\
                 & = c x_1 z_2 + y_1 z_2 + c x_2 z_1 + y_2 z_1 &  & \by{1.2.5}    \\
                 & = c (x_1 z_2 + x_2 z_1) + y_1 z_2 + y_2 z_1                    \\
                 & = c H(x, z) + H(y, z);                      &  & \by{ex:6.8.6} \\
    H(z, cx + y) & = z_1 (cx + y)_2 + z_2 (cx + y)_1           &  & \by{ex:6.8.6} \\
                 & = z_1 c x_2 + z_1 y_2 + z_2 c x_1 + z_2 y_1 &  & \by{1.2.5}    \\
                 & = c (z_1 x_2 + z_2 x_1) + z_1 y_2 + z_2 y_1                    \\
                 & = c H(z, x) + H(z, y),                      &  & \by{ex:6.8.6}
  \end{align*}
  by \cref{6.8.1} we know that \(H \in \bi(\R^2)\).
\end{proof}

\begin{proof}[\pf{ex:6.8.6}(b)]
  By \cref{6.8.8} we see that
  \[
    A = \begin{pmatrix}
      H(e_1, e_1) & H(e_1, e_2) \\
      H(e_2, e_1) & H(e_2, e_2)
    \end{pmatrix} = \begin{pmatrix}
      0 & 1 \\
      1 & 0
    \end{pmatrix}.
  \]
\end{proof}

\begin{ex}\label{ex:6.8.7}
  Let \(\V\) and \(\W\) be vector spaces over the same field \(\F\), and let \(\T \in \ls(\V, \W)\).
  For any \(H \in \bi(\W)\), define \(\widehat{\T}(H) : \V \times \V \to \F\) by \(\widehat{\T}(H)(x, y) = H(\T(x), \T(y))\) for all \(x, y \in \V\).
  Prove the following results.
  \begin{enumerate}
    \item If \(H \in \bi(\W)\), then \(\widehat{\T}(H) \in \bi(\V)\).
    \item \(\widehat{\T} \in \ls(\bi(\W), \bi(\V))\).
    \item If \(\T\) is an isomorphism, then so is \(\widehat{\T}\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.8.7}(a)]
  Let \(x, y, z \in \V\) and let \(c \in \F\).
  Since
  \begin{align*}
    \widehat{\T}(H)(cx + y, z) & = H(\T(cx + y), \T(z))                             &  & \by{ex:6.8.7} \\
                               & = H(c \T(x) + \T(y), \T(z))                        &  & \by{2.1.2}[b] \\
                               & = c H(\T(x), \T(z)) + H(\T(y), \T(z))              &  & \by{6.8.1}[a] \\
                               & = c \widehat{\T}(H)(x, z) + \widehat{\T}(H)(y, z); &  & \by{ex:6.8.7} \\
    \widehat{\T}(H)(z, cx + y) & = H(\T(z), \T(cx + y))                             &  & \by{ex:6.8.7} \\
                               & = H(\T(z), c \T(x) + \T(y))                        &  & \by{2.1.2}[b] \\
                               & = c H(\T(z), \T(x)) + H(\T(z), \T(y))              &  & \by{6.8.1}[a] \\
                               & = c \widehat{\T}(H)(z, x) + \widehat{\T}(H)(z, y), &  & \by{ex:6.8.7}
  \end{align*}
  by \cref{6.8.1} we see that \(\widehat{\T}(H) \in \bi(\V)\).
\end{proof}

\begin{proof}[\pf{ex:6.8.7}(b)]
  Let \(f, g \in \bi(\W)\) and let \(c \in \F\).
  Since
  \begin{align*}
    \forall x, y \in \V, \widehat{\T}(cf + g)(x, y) & = (cf + g)(\T(x), \T(y))                           &  & \by{ex:6.8.7} \\
                                                    & = c f(\T(x), \T(y)) + g(\T(x), \T(y))              &  & \by{6.31}     \\
                                                    & = c \widehat{\T}(f)(x, y) + \widehat{\T}(g)(x, y), &  & \by{ex:6.8.7}
  \end{align*}
  we have \(\widehat{\T}(cf + g) = c \widehat{\T}(f) + \widehat{\T}(g)\).
  Thus by \cref{2.1.2}(b) and \cref{ex:6.8.7}(a) we know that \(\widehat{\T} \in \ls(\bi(\W), \bi(\V))\).
\end{proof}

\begin{proof}[\pf{ex:6.8.7}(c)]
  By \cref{ex:6.8.7}(b) we know that \(\widehat{\T} \in \ls(\bi(\W), \bi(\V))\), thus by \cref{2.4.8} we only need to show that \(\widehat{\T}\) is one-to-one and onto.

  First we show that \(\widehat{\T}\) is one-to-one.
  Let \(H \in \ns{\widehat{\T}}\).
  Then \(\widehat{\T}(H)\) is the zero bilinear transformation in \(\bi(\V)\).
  Since
  \begin{align*}
             & \forall x, y \in \V, \widehat{\T}(H)(x, y) = H(\T(x), \T(y)) = 0 &  & \by{ex:6.8.7} \\
    \implies & \forall u, v \in \W, H(u, v) = 0,                                &  & \by{2.4.8}
  \end{align*}
  we know that \(H\) is the zero bilinear transformation in \(\bi(\W)\).
  Thus \(\ns{\widehat{\T}} = \set{\zv}\) and by \cref{2.4} \(\widehat{\T}\) is one-to-one.

  Now we show that \(\widehat{\T}\) is onto.
  Let \(g \in \bi(\V)\).
  Now we define \(f \in \W \times \W \to \F\) by \(f = \widehat{\T^{-1}}(g)\).
  By \cref{ex:6.8.7}(a) we know that \(f \in \bi(\W)\).
  Since
  \begin{align*}
    \forall x, y \in \V, \widehat{\T}(f)(x, y) & = f\pa{\T(x), \T(y)}                   &  & \by{ex:6.8.7} \\
                                               & = \widehat{\T^{-1}}(g)(\T(x), \T(y))                      \\
                                               & = g\pa{\T^{-1}(\T(x)), \T^{-1}(\T(y))} &  & \by{ex:6.8.7} \\
                                               & = g(x, y),
  \end{align*}
  we know that \(\widehat{\T}(f) = g\).
  Thus \(\widehat{\T}\) is onto, and we conclude that \(\widehat{\T}\) is an isomorphism from \(\bi(\W)\) to \(\bi(\V)\).
\end{proof}

\begin{ex}\label{ex:6.8.8}
  Assume the notation of \cref{6.32}.
  \begin{enumerate}
    \item Prove that for any ordered basis \(\beta\), \(\psi_{\beta}\) is linear.
    \item Let \(\beta\) be an ordered basis for an \(n\)-dimensional space \(\V\) over \(\F\), and let \(\phi_{\beta} : \V \to \vs{F}^n\) be the standard representation of \(\V\) with respect to \(\beta\).
          For \(A \in \ms[n][n][\F]\), define \(H : \V \times \V \to \F\) by \(H(x, y) = \tp{(\phi_{\beta}(x))} A \phi_{\beta}(y)\).
          Prove that \(H \in \bi(V)\).
          Can you establish this as a corollary to \cref{ex:6.8.7}?
    \item Prove the converse of (b):
          Let \(H\) be a bilinear form on \(\V\).
          If \(A = \psi_{\beta}(H)\), then \(H(x, y) = \tp{(\phi_{\beta}(x))} A \phi_{\beta}(y)\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.8.8}(a)]
  See \cref{6.32}.
\end{proof}

\begin{proof}[\pf{ex:6.8.8}(b)]
  Define \(G : \vs{F}^n \times \vs{F}^n \to \F\) by
  \[
    \forall x, y \in \vs{F}^n, G(x, y) = \tp{x} A y.
  \]
  By \cref{6.8.2} we know that \(G \in \bi(\vs{F}^n)\).
  By \cref{ex:6.8.7} we see that \(H = \widehat{\phi_{\beta}}(G)\) and by \cref{ex:6.8.7}(a) we have \(H \in \bi(\V)\).
\end{proof}

\begin{proof}[\pf{ex:6.8.8}(c)]
  See \cref{6.8.7}.
\end{proof}

\begin{ex}\label{ex:6.8.9}
  \begin{enumerate}
    \item Prove \cref{6.8.6}.
    \item For a finite-dimensional vector space \(\V\) over \(\F\), describe a method for finding an ordered basis for \(\bi(\V)\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.8.9}(a)]
  See \cref{6.8.6}.
\end{proof}

\begin{proof}[\pf{ex:6.8.9}(b)]
  Let \(n = \dim(\V)\) and let \(\beta\) be an ordered basis for \(\V\) over \(\F\).
  By \cref{6.32} we know that \(\psi_{\beta}\) is an isomorphism from \(\bi(\V)\) to \(\ms[n][n][\F]\).
  Now we define \(\gamma = \set{E^{i j} \in \ms[n][n][\F] : i, j \in \set{1, \dots, n}}\) as in \cref{1.6.4}.
  Then by \cref{2.2,2.3} we know that \(\psi_{\beta}^{-1}(\gamma)\) is an ordered basis for \(\bi(\V)\) over \(\F\).
\end{proof}

\begin{ex}\label{ex:6.8.10}
  Prove \cref{6.8.7}.
\end{ex}

\begin{proof}[\pf{ex:6.8.10}]
  See \cref{6.8.7}.
\end{proof}

\begin{ex}\label{ex:6.8.11}
  Prove \cref{6.8.8}.
\end{ex}

\begin{proof}[\pf{ex:6.8.11}]
  See \cref{6.8.8}.
\end{proof}

\begin{ex}\label{ex:6.8.12}
  Prove that the relation of congruence is an equivalence relation.
\end{ex}

\begin{proof}[\pf{ex:6.8.12}]
  Let \(A, B, C \in \ms[n][n][\F]\).
  Since \(A = \tp{I_n} A I_n\), by \cref{6.8.9} we know that the congruence relation is reflexive.
  Since
  \begin{align*}
             & B \text{ is congruent to } A                                           \\
    \implies & \exists Q \in \ms[n][n][\F] : \begin{dcases}
                                               Q \text{ is invertible} \\
                                               B = \tp{Q} A Q
                                             \end{dcases}        &  & \by{6.8.9}      \\
    \implies & A = (\tp{Q})^{-1} B Q^{-1} = \tp{(Q^{-1})} B Q^{-1} &  & \by{ex:2.4.5} \\
    \implies & A \text{ is congruent to } B,
  \end{align*}
  by \cref{6.8.9} we know that the congruence relation is symmetric.
  Since
  \begin{align*}
             & \begin{dcases}
                 B \text{ is congruent to } A \\
                 C \text{ is congruent to } B
               \end{dcases}                                    \\
    \implies & \exists P, Q \in \ms[n][n][\F] : \begin{dcases}
                                                  P, Q \text{ are invertible} \\
                                                  B = \tp{Q} A Q              \\
                                                  C = \tp{P} B P
                                                \end{dcases} &  & \by{6.8.9}   \\
    \implies & C = \tp{P} \tp{Q} A Q P = \tp{QP} A QP          &  & \by{2.3.2} \\
    \implies & C \text{ is congruent to } A,                   &  & \by{6.8.9}
  \end{align*}
  by \cref{6.8.9} we know that the congruence relation is transitive.
  Thus the congruence relation is an equivalence relation.
\end{proof}

\begin{ex}\label{ex:6.8.13}
  The following outline provides an alternative proof to \cref{6.33}.
  \begin{enumerate}
    \item Suppose that \(\beta\) and \(\gamma\) are ordered bases for a finite-dimensional vector space \(\V\) over \(\F\), and let \(Q\) be the change of coordinate matrix changing \(\gamma\)-coordinates into \(\beta\)-coordinates.
          Prove that \(\phi_{\beta} = \L_Q \phi_{\gamma}\), where \(\phi_{\beta}\) and \(\phi_{\gamma}\) are the standard representations of \(\V\) with respect to \(\beta\) and \(\gamma\), respectively.
    \item Apply \cref{6.8.7} to (a) to obtain an alternative proof of \cref{6.33}.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.8.13}(a)]
  Suppose that \(n = \dim(\V)\).
  Then we have
  \begin{align*}
    \forall x \in \V, (\L_Q \phi_{\gamma})(x) & = \L_Q(\phi_{\gamma}(x))                                        \\
                                              & = \L_Q([x]_{\gamma})                            &  & \by{2.2.3} \\
                                              & = Q [x]_{\gamma}                                &  & \by{2.3.8} \\
                                              & = [\IT[\vs{F}^n]]_{\gamma}^{\beta} [x]_{\gamma} &  & \by{2.5.1} \\
                                              & = [x]_{\beta}                                   &  & \by{2.14}  \\
                                              & = \phi_{\beta}(x)                               &  & \by{2.2.3}
  \end{align*}
  and thus \(\L_Q \phi_{\gamma} = \phi_{\beta}\).
\end{proof}

\begin{proof}[\pf{ex:6.8.13}(b)]
  Continue the proof of \cref{ex:6.8.13}(a).
  Let \(H \in \bi(\V)\).
  Then we have
  \begin{align*}
    \forall x, y \in \V, H(x, y) & = \tp{(\phi_{\beta}(x))} \psi_{\beta}(H) \phi_{\beta}(y)    &  & \by{6.8.7}        \\
                                 & = \tp{([x]_{\beta})} \psi_{\beta}(H) [y]_{\beta}            &  & \by{2.2.3}        \\
                                 & = \tp{(Q [x]_{\gamma})} \psi_{\beta}(H) Q [y]_{\gamma}      &  & \by{ex:6.8.13}[a] \\
                                 & = \tp{([x]_{\gamma})} \tp{Q} \psi_{\beta}(H) Q [y]_{\gamma} &  & \by{2.3.2}        \\
                                 & = \tp{([x]_{\gamma})} \psi_{\gamma}(H) [y]_{\gamma}.        &  & \by{6.8.7}
  \end{align*}
  Thus by \cref{2.15}(b) we have \(\psi_{\gamma}(H) = \tp{Q} \psi_{\beta}(H) Q\).
\end{proof}

\begin{ex}\label{ex:6.8.14}
  Let \(\V\) be a finite-dimensional vector space over \(\F\) and \(H \in \bi(\V)\).
  Prove that, for any ordered bases \(\beta\) and \(\gamma\) of \(\V\) over \(\F\), \(\rk{\psi_{\beta}(H)} = \rk{\psi_{\gamma}(H)}\).
\end{ex}

\begin{proof}[\pf{ex:6.8.14}]
  Let \(n = \dim(\V)\).
  By \cref{6.33} there exists a \(Q \in \ms[n][n][\F]\) such that \(Q\) is invertible and \(\psi_{\gamma}(H) = \tp{Q} \psi_{\beta}(H) Q\).
  Then we have
  \begin{align*}
    \rk{\psi_{\gamma}(H)} & = \rk{\tp{Q} \psi_{\beta}(H) Q} &  & \by{6.33}   \\
                          & = \rk{\psi_{\beta}(H)}.         &  & \by{3.4}[c]
  \end{align*}
\end{proof}

\begin{ex}\label{ex:6.8.15}
  Prove the following results.
  \begin{enumerate}
    \item Any square diagonal matrix is symmetric.
    \item Any matrix congruent to a diagonal matrix is symmetric.
    \item \cref{6.8.16}.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.8.15}(a)]
  See \cref{ex:1.3.7}.
\end{proof}

\begin{proof}[\pf{ex:6.8.15}(b)]
  Let \(A, D, Q \in \ms[n][n][\F]\) such that \(Q\) is invertible and \(D = \tp{Q} A Q\) is a diagonal matrix.
  Then we have
  \begin{align*}
    \tp{A} & = \tp{\pa{\pa{\tp{Q}}^{-1} D Q^{-1}}}                &  & \by{2.4.3}    \\
           & = \tp{\pa{Q^{-1}}} \tp{D} \tp{\pa{\pa{\tp{Q}}^{-1}}} &  & \by{2.3.2}    \\
           & = \pa{\tp{Q}}^{-1} \tp{D} \tp{\pa{\tp{\pa{Q^{-1}}}}} &  & \by{ex:2.4.5} \\
           & = \pa{\tp{Q}}^{-1} \tp{D} Q^{-1}                     &  & \by{ex:1.3.4} \\
           & = \pa{\tp{Q}}^{-1} D Q^{-1}                          &  & \by{ex:1.3.7} \\
           & = A.
  \end{align*}
  Thus by \cref{1.3.4} \(A\) is symmetric.
\end{proof}

\begin{proof}[\pf{ex:6.8.15}(c)]
  See \cref{6.8.16}.
\end{proof}

\begin{ex}\label{ex:6.8.16}
  Let \(\V\) be a vector space over a field \(\F\) not of characteristic two, and let \(H\) be a symmetric bilinear form on \(\V\).
  Prove that if \(K(x) = H(x, x)\) is the quadratic form associated with \(H\), then, for all \(x, y \in \V\),
  \[
    H(x, y) = \dfrac{1}{2} (K(x + y) - K(x) - K(y)).
  \]
\end{ex}

\begin{proof}[\pf{ex:6.8.16}]
  We have
  \begin{align*}
     & \forall x, y \in \V, \dfrac{1}{2} (K(x + y) - K(x) - K(y))                    \\
     & = \dfrac{1}{2} (H(x + y, x + y) - H(x, x) - H(y, y))       &  & \by{6.8.19}   \\
     & = \dfrac{1}{2} (H(x, y) + H(y, x))                         &  & \by{6.8.3}[c] \\
     & = \dfrac{1}{2} 2 H(x, y)                                   &  & \by{6.8.11}   \\
     & = H(x, y).
  \end{align*}
\end{proof}

\setcounter{ex}{18}
\begin{ex}\label{ex:6.8.19}
  Prove the following refinement of \cref{6.37}(d).
  \begin{enumerate}
    \item If \(0 < \rk{A} < n\) and \(A\) has no negative eigenvalues, then \(f\) has no local maximum at \(p\).
    \item If \(0 < \rk{A} < n\) and \(A\) has no positive eigenvalues, then \(f\) has no local minimum at \(p\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.8.19}(a)]
  Continue the proof of \cref{6.37}.
  Since \(A\) has no negative eigenvalues and \(\rk{A} \in \set{1, \dots, n - 1}\), there exist an \(i \in \set{1, \dots, n}\) such that \(\lambda_i > 0\) and
  \[
    \dfrac{1}{2} \lambda_i - \epsilon > 0.
  \]
  Let \(s \in \R\) such that \(\abs{s} \in (0, \delta)\).
  Substituting \(x = s v_i\) into the left inequality of \cref{ex:6.8.7} we obtain
  \[
    f(\zv) = 0 < \pa{\dfrac{1}{2} \lambda_i - \epsilon} s^2 < f(s v_i).
  \]
  Thus \(f\) has no local maximum at \(p\).
\end{proof}

\begin{proof}[\pf{ex:6.8.19}(b)]
  Continue the proof of \cref{6.37}.
  Since \(A\) has no positive eigenvalues and \(\rk{A} \in \set{1, \dots, n - 1}\), there exist an \(i \in \set{1, \dots, n}\) such that \(\lambda_i < 0\) and
  \[
    \dfrac{1}{2} \lambda_i + \epsilon < 0.
  \]
  Let \(s \in \R\) such that \(\abs{s} \in (0, \delta)\).
  Substituting \(x = s v_i\) into the right inequality of \cref{ex:6.8.7} we obtain
  \[
    f(s v_i) < \pa{\dfrac{1}{2} \lambda_i + \epsilon} s^2 < \zv = f(\zv).
  \]
  Thus \(f\) has no local minimum at \(p\).
\end{proof}

\begin{ex}\label{ex:6.8.20}
  Prove the following variation of the second-derivative test (\cref{6.37}) for the case \(n = 2\):
  Define
  \[
    D = \pa{\dfrac{\partial^2 f}{\partial t_1^2}(p)} \pa{\dfrac{\partial^2 f}{\partial t_2^2}(p)} - \pa{\dfrac{\partial^2 f}{\partial t_1 \partial t_2}(p)}^2.
  \]
  \begin{enumerate}
    \item If \(D > 0\) and \(\dfrac{\partial^2 f}{\partial t_1^2}(p) > 0\), then \(f\) has a local minimum at \(p\).
    \item If \(D > 0\) and \(\dfrac{\partial^2 f}{\partial t_1^2}(p) < 0\), then \(f\) has a local maximum at \(p\).
    \item If \(D < 0\), then \(f\) has no local extremum at \(p\).
    \item If \(D = 0\), then the test is inconclusive.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.8.20}]
  Define
  \[
    A(p) = \begin{pmatrix}
      \dfrac{\partial^2 f}{\partial t_1^2}(p)            & \dfrac{\partial^2 f}{\partial t_1 \partial t_2}(p) \\
      \dfrac{\partial^2 f}{\partial t_2 \partial t_1}(p) & \dfrac{\partial^2 f}{\partial t_2^2}(p)
    \end{pmatrix}
  \]
  to be the Hessian of \(f : \R^2 \to \R\).
  Since \(f \in \cfs[3]\), we know that \(A(p)\) is symmetric.
  Observe that \(D = \det(A(p))\).

  First suppose that \(D > 0\).
  By \cref{ex:6.5.12} this means either all eigenvalues of \(A(p)\) are positive or negative.
  If \(\dfrac{\partial^2 f}{\partial t_1^2}(p) > 0\), then by the definition of \(D\) we know that \(\dfrac{\partial^2 f}{\partial t_2^2}(p) > 0\).
  Thus by \cref{ex:6.5.10} we know that all eigenvalues are positive and by \cref{6.37}(a) we know that \(f\) has a local minimum at \(p\).
  If \(\dfrac{\partial^2 f}{\partial t_1^2}(p) < 0\), then by the definition of \(D\) we know that \(\dfrac{\partial^2 f}{\partial t_2^2}(p) < 0\).
  Thus by \cref{ex:6.5.10} we know that all eigenvalues are negative and by \cref{6.37}(b) we know that \(f\) has a local maximum at \(p\).

  Next suppose that \(D < 0\).
  By \cref{ex:6.5.12} this means \(A(p)\) have exactly one positive and one negative eigenvalues.
  Thus by \cref{6.37}(c) \(f\) has no local extremum at \(p\).

  Now suppose that \(D = 0\).
  By \cref{4.3.1} this means \(\rk{A(p)} < 2\).
  This means \(A(p)\) has a least one eigenvalue equals to \(0\) and \(A(p)\) cannot have both positive and negative eigenvalues.
  Thus by \cref{6.37}(d) the second derivative test is inconclusive.
\end{proof}

\begin{ex}\label{ex:6.8.21}
  Let \(A, E \in \ms[n][n][\F]\), with \(E\) be an elementary matrix.
  In \cref{3.1}, it was shown that \(AE\) can be obtained from \(A\) by means of an elementary column operation.
  Prove that \(\tp{E} A\) can be obtained by means of the same elementary operation performed on the rows rather than on the columns of \(A\).
\end{ex}

\begin{proof}[\pf{ex:6.8.21}]
  See \cref{ex:3.1.5}.
\end{proof}

\setcounter{ex}{22}
\begin{ex}\label{ex:6.8.23}
  Prove that if the diagonal entries of a diagonal matrix are permuted, then the resulting diagonal matrix is congruent to the original one.
\end{ex}

\begin{proof}[\pf{ex:6.8.23}]
  Let \(D \in \ms[n][n][\F]\) be a diagonal matrix and let \(\beta\) be the standard ordered basis for \(\vs{F}^n\) over \(\F\).
  Let \(D' \in \ms[n][n][\F]\) be a diagonal matrix such that the diagonal entries of \(D'\) are the same as \(D\) but permuted.
  Let \(f : \set{1, \dots, n} \to \set{1, \dots, n}\) be a permutation, i.e.,
  \[
    \forall i \in \set{1, \dots, n}, D_{i i}' = D_{f(i) f(i)}.
  \]
  Then we can define an ordered basis \(\gamma = \set{e_{f(i)} : i \in \set{1, \dots, n}}\) for \(\vs{F}^n\) over \(\F\).
  We claim that \([\L_D]_{\gamma} = D'\).
  Since
  \begin{align*}
    \forall i \in \set{1, \dots, n}, \L_D(e_{f(i)}) & = D e_{f(i)}             &  & \by{2.15}[a] \\
                                                    & = D_{f(i) f(i)} e_{f(i)} &  & \by{2.3.1}   \\
                                                    & = D_{i i}' e_{f(i)},
  \end{align*}
  by \cref{2.2.4} we know that \([\L_D]_{\gamma} = D'\).
  Thus
  \begin{align*}
    D & = [\L_D]_{\beta}                                                                    &  & \by{2.15}[a]                  \\
      & = [\IT[\vs{F}^n]]_{\gamma}^{\beta} [\L_D]_{\gamma} [\IT[\vs{F}^n]]_{\beta}^{\gamma} &  & \by{2.23}                     \\
      & = [\IT[\vs{F}^n]]_{\gamma}^{\beta} D' [\IT[\vs{F}^n]]_{\beta}^{\gamma}              &  & \text{(from the claim above)} \\
      & = \tp{\pa{[\IT[\vs{F}^n]]_{\beta}^{\gamma}}} D' [\IT[\vs{F}^n]]_{\beta}^{\gamma}    &  & \by{ex:6.5.30}
  \end{align*}
  and by \cref{6.8.9} \(D\) and \(D'\) are congruent.
\end{proof}

\begin{ex}\label{ex:6.8.24}
  Let \(\T\) be a linear operator on a real inner product space \(\V\), and define \(H : \V \times \V \to \R\) by \(H(x, y) = \inn{x, \T(y)}\) for all \(x, y \in \V\).
  \begin{enumerate}
    \item Prove that \(H\) is a bilinear form.
    \item Prove that \(H\) is symmetric iff \(\T\) is self-adjoint.
    \item What properties must \(\T\) have for \(H\) to be an inner product on \(\V\) over \(\R\)?
    \item Explain why \(H\) may fail to be a bilinear form if \(\V\) is a complex inner product space.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.8.24}(a)]
  Let \(x, y, z \in \V\) and let \(a \in \R\).
  Since
  \begin{align*}
    H(ax + y, z) & = \inn{ax + y, \T(z)}               &  & \by{ex:6.8.24}  \\
                 & = a \inn{x, \T(z)} + \inn{y, \T(z)} &  & \by{6.1.1}[a,b] \\
                 & = a H(x, z) + H(y, z);              &  & \by{ex:6.8.24}  \\
    H(z, ax + y) & = \inn{z, \T(ax + y)}               &  & \by{ex:6.8.24}  \\
                 & = \inn{z, a \T(x) + \T(y)}          &  & \by{2.1.2}[b]   \\
                 & = a \inn{z, \T(x)} + \inn{z, \T(y)} &  & \by{6.1}[a,b]   \\
                 & = a H(z, x) + H(z, y),              &  & \by{ex:6.8.24}
  \end{align*}
  by \cref{6.8.1} we know that \(H \in \bi(\V)\).
\end{proof}

\begin{proof}[\pf{ex:6.8.24}(b)]
  First suppose that \(H\) is symmetric.
  Then we have
  \begin{align*}
             & \forall x, y \in \V, H(x, y) = \inn{x, \T(y)} = \inn{y, \T(x)} = H(y, x) &  & \by{6.8.11}   \\
    \implies & \forall x, y \in \V, \inn{\T^*(x), y} = \inn{y, \T(x)}                   &  & \by{6.9}      \\
             & = \conj{\inn{\T(x), y}} = \inn{\T(x), y}                                 &  & \by{6.1.1}[c] \\
    \implies & \T^* = \T                                                                &  & \by{6.1}[e]   \\
    \implies & \T \text{ is self-adjoint}.                                              &  & \by{6.4.8}
  \end{align*}

  Now suppose that \(\T\) is self-adjoint.
  Then we have
  \begin{align*}
    H(x, y) & = \inn{x, \T(y)}        &  & \by{ex:6.8.24} \\
            & = \inn{x, \T^*(y)}      &  & \by{6.4.8}     \\
            & = \inn{\T(x), y}        &  & \by{6.9}       \\
            & = \conj{\inn{y, \T(x)}} &  & \by{6.1.1}[c]  \\
            & = \inn{y, \T(x)}                            \\
            & = H(y, x).              &  & \by{ex:6.8.24}
  \end{align*}
  Thus by \cref{6.8.11} \(H\) is symmetric.
\end{proof}

\begin{proof}[\pf{ex:6.8.24}(c)]
  By \cref{ex:6.4.22} \(\T\) must be positive definite.
\end{proof}

\begin{proof}[\pf{ex:6.8.24}(d)]
  If \(x, y \in \V\), then we have
  \begin{align*}
    H(x, iy) & = \inn{x, \T(iy)}   &  & \by{ex:6.8.24} \\
             & = \inn{x, i \T(y)}  &  & \by{2.1.1}[b]  \\
             & = -i \inn{x, \T(y)} &  & \by{6.1}[b]    \\
             & \neq i H(x, y).     &  & \by{ex:6.8.24}
  \end{align*}
  Thus \(H\) may fail to be a bilinear form if \(\V\) is a complex inner product space.
\end{proof}

\begin{ex}\label{ex:6.8.25}
  Prove the converse to \cref{ex:6.8.24}(a):
  Let \(\V\) be a finite-dimensional real inner product space, and let \(H \in \bi(\V)\).
  Then there exists a unique \(\T \in \ls(\V)\) such that \(H(x, y) = \inn{x, \T(y)}\) for all \(x, y \in \V\).
\end{ex}

\begin{proof}[\pf{ex:6.8.25}]
  Let \(\beta\) be an orthonormal basis (with respect to \(\inn{\cdot, \cdot}\)) for \(\V\) over \(\R\), and let \(A = \psi_{\beta}(H)\).
  By \cref{2.6} we can define a \(\T \in \ls(\V)\) such that \([\T]_{\beta} = A\).
  Let \(\inn{\cdot, \cdot}'\) be the standard ordered basis for \(\R^n\) over \(\R\).
  Then we have
  \begin{align*}
    \forall x, y \in \V, H(x, y) & = \tp{(\phi_{\beta}(x))} A \phi_{\beta}(y)    &  & \by{6.8.7}        \\
                                 & = \tp{([x]_{\beta})} A [y]_{\beta}            &  & \by{2.2.3}        \\
                                 & = \tp{([x]_{\beta})} [\T]_{\beta} [y]_{\beta} &  & \by{2.6}          \\
                                 & = \tp{([x]_{\beta})} [\T(y)]_{\beta}          &  & \by{2.14}         \\
                                 & = \inn{[x]_{\beta}, [\T(y)]_{\beta}}'         &  & \by{6.1.2}        \\
                                 & = \inn{x, \T(y)}.                             &  & \by{ex:6.2.15}[b]
  \end{align*}
\end{proof}

\begin{ex}\label{ex:6.8.26}
  Prove that the number of distinct equivalence classes of congruent \(n \times n\) real symmetric matrices is
  \[
    \dfrac{(n + 1)(n + 2)}{2}.
  \]
\end{ex}

\begin{proof}[\pf{ex:6.8.26}]
  By \cref{6.8.27,6.8.28} the equivalent classes of congruent \(n \times n\) real symmetric matrices is determined by the index and signature, or equivalently, the number of positive and negative diagonal entries \(p\) and \(q\).
  We want to find all possible combinations of \(p, q \in \set{0, \dots, n}\) such that \(0 \leq p + q \leq n\).
  By enumerating all possible outcomes, namely \(p + q = 0, p + q = 1, \dots, p + q = n\), we see that the number of possible combinations are
  \[
    1 + 2 + \cdots + n + (n + 1) = \dfrac{(n + 1 + 1) (n + 1)}{2} = \dfrac{(n + 1)(n + 2)}{2}.
  \]
\end{proof}
