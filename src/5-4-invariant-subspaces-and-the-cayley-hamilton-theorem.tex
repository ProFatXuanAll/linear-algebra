\section{Invariant Subspaces and the Cayley-Hamilton Theorem}\label{sec:5.4}

\begin{defn}\label{5.4.1}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\).
  A subspace \(\W\) of \(\V\) over \(\F\) is called a \textbf{\(\T\)-invariant subspace} of \(\V\) over \(\F\) if \(\T(\W) \subseteq \W\), that is, if \(\T(v) \in \W\) for all \(v \in \W\).
\end{defn}

\begin{eg}\label{5.4.2}
  Suppose that \(\T\) is a linear operator on a vector space \(\V\) over \(\F\).
  Then the following subspaces of \(\V\) are \(\T\)-invariant:
  \begin{enumerate}
    \item \(\set{\zv}\)
    \item \(\V\)
    \item \(\rg{\T}\)
    \item \(\ns{\T}\)
    \item \(E_{\lambda}\), for any eigenvalue \(\lambda\) of \(\T\).
  \end{enumerate}
\end{eg}

\begin{proof}[\pf{5.4.2}]
  We have
  \begin{align*}
    \T(\set{\zv}) & = \set{\zv} \subseteq \set{\zv} &  & \text{(by \cref{2.1.2}(a))} \\
    \T(\V)        & = \rg{\T} \subseteq \V          &  & \text{(by \cref{2.5.2})}    \\
    \T(\rg{\T})   & \subseteq \T(\V) = \rg{\T}      &  & \text{(by \cref{2.1.10})}   \\
    \T(\ns{\T})   & = \set{\zv} \subseteq \ns{\T}   &  & \text{(by \cref{2.1.10})}
  \end{align*}
  and
  \begin{align*}
             & \forall v \in E_{\lambda}, \T(v) = \lambda v \in E_{\lambda} &  & \text{(by \cref{5.2.4})} \\
    \implies & \T(E_{\lambda}) \subseteq E_{\lambda}.
  \end{align*}
  Thus by \cref{5.4.1} \(\set{\zv}, \V, \rg{\T}, \ns{\T}, E_{\lambda}\) are \(\T\)-invariant subspace of \(\V\) over \(\F\).
\end{proof}

\begin{defn}\label{5.4.3}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\), and let \(x\) be a nonzero vector in \(\V\).
  The subspace
  \[
    \W = \spn{\set{x, \T(x), \T^2(x), \dots}}
  \]
  is called the \textbf{\(\T\)-cyclic subspace of \(\V\) generated by \(x\)}.
  It is a simple matter to show that \(\W\) is \(\T\)-invariant.
  In fact, \(\W\) is the ``smallest'' \(\T\)-invariant subspace of \(\V\) containing \(x\).
  That is, any \(\T\)-invariant subspace of \(\V\) containing \(x\) must also contain \(\W\)
  (see \cref{ex:5.4.11}).
\end{defn}

\begin{thm}\label{5.21}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and let \(\W\) be a \(\T\)-invariant subspace of \(\V\) over \(\F\).
  Then the characteristic polynomial of \(\T_{\W}\) divides the characteristic polynomial of \(\T\).
\end{thm}

\begin{proof}[\pf{5.21}]
  Choose an ordered basis \(\gamma = \set{\seq{v}{1,,k}}\) for \(\W\) over \(\F\), and extend it to an ordered basis \(\beta = \set{\seq{v}{1,,k,k+1,n}}\) for \(\V\) over \(\F\).
  Let \(A = [\T]_{\beta}\) and \(B_1 = [\T_{\W}]_{\gamma}\).
  Then, by \cref{ex:5.4.12}, \(A\) can be written in the form
  \[
    A = \begin{pmatrix}
      B_1 & B_2 \\
      \zm & B_3
    \end{pmatrix}.
  \]
  Let \(f\) be the characteristic polynomial of \(\T\) and \(g\) the characteristic polynomial of \(\T_{\W}\).
  Then
  \[
    f(t) = \det(A - t I_n) = \det\begin{pmatrix}
      B_1 - t I_k & B_2               \\
      \zm         & B_3 - t I_{n - k}
    \end{pmatrix} = g(t) \cdot \det(B_3 - t I_{n - k})
  \]
  by \cref{ex:4.3.21}.
  Thus \(g\) divides \(f\).
\end{proof}

\begin{thm}\label{5.22}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and let \(\W\) denote the \(\T\)-cyclic subspace of \(\V\) generated by a nonzero vector \(v \in \V\).
  Let \(k = \dim(\W)\).
  Then
  \begin{enumerate}
    \item \(\set{v, \T(v), \T^2(v), \dots, \T^{k - 1}(v)}\) is a basis for \(\W\) over \(\F\).
    \item If \(a_0 v + a_1 \T(v) + \cdots + a_{k - 1} \T^{k - 1}(v) + \T^k(v) = \zv\), then the characteristic polynomial of \(\T_{\W}\) is \(f(t) = (-1)^k (a_0 + a_1 t + \cdots + a_{k - 1} t^{k - 1} + t^k)\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{5.22}(a)]
  Since \(v \neq \zv\), the set \(\set{v}\) is linearly independent.
  Let \(j\) be the largest positive integer for which
  \[
    \beta = \set{v, \T(v), \dots, \T^{j - 1}(v)}
  \]
  is linearly independent.
  Such a \(j\) must exist because \(\V\) is finite-dimensional.
  Let \(\vs{Z} = \spn{\beta}\).
  Then \(\beta\) is a basis for \(\vs{Z}\) over \(\F\).
  Furthermore, \(\T^j(v) \in \vs{Z}\) by \cref{1.7}.
  We use this information to show that \(\vs{Z}\) is a \(\T\)-invariant subspace of \(\V\) over \(\F\).
  Let \(w \in \vs{Z}\).
  Since \(w\) is a linear combination of the vectors of \(\beta\), there exist scalars \(\seq{b}{0,,j-1} \in \F\) such that
  \[
    w = b_0 v + b_1 \T(v) + \cdots + b_{j - 1} \T^{j - 1}(v),
  \]
  and hence
  \[
    \T(w) = b_0 \T(v) + b_1 \T^2(v) + \cdots + b_{j - 1} \T^j(v).
  \]
  Thus \(\T(w)\) is a linear combination of vectors in \(\vs{Z}\), and hence belongs to \(\vs{Z}\).
  So \(\vs{Z}\) is \(\T\)-invariant.
  Furthermore, \(v \in \vs{Z}\).
  By \cref{ex:5.4.11}, \(\W\) is the smallest \(\T\)-invariant subspace of \(\V\) that contains \(v\), so that \(\W \subseteq \vs{Z}\).
  Clearly, \(\vs{Z} \subseteq \W\), and so we conclude that \(\vs{Z} = \W\).
  It follows that \(\beta\) is a basis for \(\W\) over \(\F\), and therefore \(\dim(\W) = j\).
  Thus \(j = k\).
  This proves (a).
\end{proof}

\begin{proof}[\pf{5.22}(b)]
  Now view \(\beta\) (from (a)) as an ordered basis for \(\W\).
  Let \(\seq{a}{0,,k-1} \in \F\) such that
  \[
    a_0 v + a_1 \T(v) + \cdots + a_{k - 1} \T^{k - 1}(v) + \T^k(v) = \zv.
  \]
  Observe that
  \[
    [\T_{\W}]_{\beta} = \begin{pmatrix}
      0      & 0      & \cdots & 0      & -a_0       \\
      1      & 0      & \cdots & 0      & -a_1       \\
      \vdots & \vdots &        & \vdots & \vdots     \\
      0      & 0      & \cdots & 1      & -a_{k - 1}
    \end{pmatrix}
  \]
  which has the characteristic polynomial
  \[
    f(t) = (-1)^k (a_0 + a_1 t + \cdots + a_{k - 1} t^{k - 1} + t^k)
  \]
  by \cref{ex:5.4.19}.
  Thus \(f\) is the characteristic polynomial of \(\T_{\W}\), proving (b).
\end{proof}

\begin{thm}[Cayley--Hamilton theorem]\label{5.23}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and let \(f\) be the characteristic polynomial of \(\T\).
  Then \(f(\T) = \zT\), the zero transformation.
  That is, \(\T\) ``satisfies'' its characteristic equation.
\end{thm}

\begin{proof}[\pf{5.23}]
  We show that \(f(\T)(v) = \zv\) for all \(v \in \V\).
  This is obvious if \(v = 0\) because \(f(\T)\) is linear;
  so suppose that \(v \neq \zv\).
  Let \(\W\) be the \(\T\)-cyclic subspace generated by \(v\), and suppose that \(\dim(\W) = k\).
  By \cref{5.22}(a), there exist scalars \(\seq{a}{0,,k-1} \in \F\) such that
  \[
    a_0 v + a_1 \T(v) + \cdots + a_{k - 1} \T^{k - 1}(v) + \T^k(v) = \zv.
  \]
  Hence \cref{5.22}(b) implies that
  \[
    g(t) = (-1)^k (a_0 + a_1 t + \cdots + a_{k - 1} t^{k - 1} + t^k)
  \]
  is the characteristic polynomial of \(\T_{\W}\).
  Combining these two equations yields
  \[
    g(\T)(v) = (-1)^k (a_0 \IT[\V] + a_1 \T + \cdots + a_{k - 1} \T^{k - 1} + \T^k)(v) = \zv.
  \]
  By \cref{5.21}, \(g\) divides \(f\);
  hence there exists a polynomial \(q\) such that \(f\) = \(qg\).
  So
  \[
    f(\T)(v) = (q(\T) g(\T))(v) = q(\T)(g(\T)(v)) = q(\T)(\zv) = \zv.
  \]
\end{proof}

\begin{cor}[Cayley--Hamilton Theorem for Matrices]\label{5.4.4}
  Let \(A \in \ms{n}{n}{\F}\), and let \(f\) be the characteristic polynomial of \(A\).
  Then \(f(A) = \zm\), the \(n \times n\) zero matrix.
\end{cor}

\begin{proof}[\pf{5.4.4}]
  Let \(\beta\) be the standard ordered basis for \(\vs{F}^n\) over \(\F\).
  Then we have
  \begin{align*}
             & A = [\L_A]_{\beta}                                  &  & \text{(by \cref{2.15}(a))} \\
    \implies & f \text{ is the characteristic polynomial of } \L_A &  & \text{(by \cref{5.1.6})}   \\
    \implies & f(\L_A) = \det([\L_A]_{\beta} - t I_n)(\L_A) = \zT  &  & \text{(by \cref{5.23})}    \\
    \implies & f(A) = \det(A - t I_n)(A) = \zm.                    &  & \text{(by \cref{2.3.8})}
  \end{align*}
\end{proof}

\begin{note}
  It is useful to decompose a finite-dimensional vector space \(\V\) over \(\F\) into a direct sum of as many \(\T\)-invariant subspaces as possible because the behavior of \(\T\) on \(\V\) can be inferred from its behavior on the direct summands.
  For example, \(\T\) is diagonalizable iff \(\V\) can be decomposed into a direct sum of one-dimensional \(\T\)-invariant subspaces (see \cref{ex:5.4.36}).
  In \cref{ch:7}, we consider alternate ways of decomposing \(\V\) into direct sums of \(\T\)-invariant subspaces if \(\T\) is not diagonalizable.
\end{note}

\begin{thm}\label{5.24}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and suppose that \(\V = \seq[\oplus]{\W}{1,,k}\), where \(\W_i\) is a \(\T\)-invariant subspace of \(\V\) over \(\F\) for each \(i \in \set{1, \dots, k}\).
  Suppose that \(f_i\) is the characteristic polynomial of \(\T_{\W}\) for each \(i \in \set{1, \dots, k}\).
  Then \(\prod_{i = 1}^k f_i\) is the characteristic polynomial of \(\T\).
\end{thm}

\begin{proof}[\pf{5.24}]
  The proof is by mathematical induction on \(k\).
  In what follows, \(f\) denotes the characteristic polynomial of \(\T\).
  Suppose first that \(k = 2\).
  Let \(\beta_1\) be an ordered basis for \(\W_1\) over \(\F\), \(\beta_2\) an ordered basis for \(\W_2\) over \(\F\), and \(\beta = \beta_1 \cup \beta_2\).
  Then \(\beta\) is an ordered basis for \(\V\) over \(\F\) by \cref{5.10}(d).
  Let \(A = [\T]_{\beta}\), \(B_1 = [\T_{\W_1}]_{\beta_1}\), and \(B_2 = [\T_{\W_2}]_{\beta_2}\).
  By \cref{ex:5.4.34}, it follows that
  \[
    A = \begin{pmatrix}
      B_1  & \zm \\
      \zm' & B_2
    \end{pmatrix},
  \]
  where \(\zm\) and \(\zm'\) are zero matrices of the appropriate sizes.
  Then
  \[
    f(t) = \det(A - tI) = \det(B_1 - tI) \cdot \det(B_2 - tI) = f_1(t) \cdot f_2(t)
  \]
  as in the proof of \cref{5.21}, proving the result for \(k = 2\).
  Now assume that the theorem is valid for \(k\) summands, where \(k \geq 2\), and suppose that \(\V\) is a direct sum of \(k + 1\) subspaces, say,
  \[
    \V = \seq[\oplus]{\W}{1,,k+1}.
  \]
  Let \(\W = \seq[+]{\W}{1,,k}\).
  It is easily verified that \(\W\) is \(\T\)-invariant and that \(\V = \W \oplus \W_{k + 1}\).
  So by the case for \(k = 2\), \(f = g \cdot f_{k + 1}\), where \(g\) is the characteristic polynomial of \(\T_{\W}\).
  Clearly \(\W = \seq[\oplus]{\W}{1,,k}\), and therefore \(g = \prod_{i = 1}^k f_i\) by the induction hypothesis.
  We conclude that \(f = g \cdot f_{k + 1} = \prod_{i = 1}^{k + 1} f_i\).
\end{proof}

\begin{note}
  As an illustration of \cref{5.24}, suppose that \(\T\) is a diagonalizable linear operator on a finite-dimensional vector space \(\V\) over \(\F\) with distinct eigenvalues \(\seq{\lambda}{1,,k}\).
  By \cref{5.11}, \(\V\) is a direct sum of the eigenspaces of \(\T\).
  Since each eigenspace is \(\T\)-invariant, we may view this situation in the context of \cref{5.24}.
  For each eigenvalue \(\lambda_i\), the restriction of \(\T\) to \(E_{\lambda_i}\) has characteristic polynomial \((\lambda_i - t)^{m_i}\), where \(m_i\) is the dimension of \(E_{\lambda_i}\).
  By \cref{5.24}, the characteristic polynomial \(f\) of \(\T\) is the product
  \[
    f(t) = (\lambda_1 - t)^{m_1} (\lambda_2 - t)^{m_2} \cdots (\lambda_k - t)^{m_k}.
  \]
  It follows that the multiplicity of each eigenvalue is equal to the dimension of the corresponding eigenspace, as expected.
\end{note}

\begin{defn}\label{5.4.5}
  Let \(B_1 \in \ms{m}{m}{\F}\), and let \(B_2 \in \ms{n}{n}{\F}\).
  We define the \textbf{direct sum} of \(B_1\) and \(B_2\), denoted \(B_1 \oplus B_2\), as the \((m + n) \times (m + n)\) matrix \(A\) such that
  \[
    A_{i j} = \begin{dcases}
      (B_1)_{i j}             & \text{for } i, j \in \set{1, \dots, m}         \\
      (B_2)_{(i - m) (j - m)} & \text{for } i, j \in \set{m + 1, \dots, m + n} \\
      0                       & \text{otherwise}
    \end{dcases}.
  \]
  If \(\seq{B}{1,,k}\) are square matrices with entries from \(\F\), then we define the \textbf{direct sum} of \(\seq{B}{1,,k}\) recursively by
  \[
    \seq[\oplus]{B}{1,,k} = (\seq[\oplus]{B}{1,,k-1}) \oplus B_k.
  \]
  If \(A = \seq[\oplus]{B}{1,,k}\), then we often write
  \[
    A = \begin{pmatrix}
      B_1    & \zm    & \cdots & \zm    \\
      \zm    & B_2    & \cdots & \zm    \\
      \vdots & \vdots &        & \vdots \\
      \zm    & \zm    & \cdots & B_k
    \end{pmatrix}.
  \]
\end{defn}

\begin{thm}\label{5.25}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and let \(\seq{\W}{1,,k}\) be \(\T\)-invariant subspaces of \(\V\) over \(\F\) such that \(\V = \seq[\oplus]{\W}{1,,k}\).
  For each \(i \in \set{1, \dots, k}\), let \(\beta_i\) be an ordered basis for \(\W_i\), and let \(\beta = \bigcup_{i = 1}^k \beta_i\).
  Let \(A = [\T]_{\beta}\) and \(B_i = [\T_{\W_i}]_{\beta_i}\) for \(i \in \set{1, \dots, k}\).
  Then \(A = \seq[\oplus]{B}{1,,k}\).
\end{thm}

\exercisesection

\setcounter{ex}{3}
\begin{ex}\label{ex:5.4.4}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\), and let \(\W\) be a \(\T\)-invariant subspace of \(\V\) over \(\F\).
  Prove that \(\W\) is \(g(\T)\)-invariant for any polynomial \(g\).
\end{ex}

\begin{proof}[\pf{ex:5.4.4}]
  Let \(g(x) = a_0 + a_1 x + \cdots + a_n x^n\) for some \(\seq{a}{0,,n} \in \F\).
  Then we have
  \begin{align*}
    \forall w \in \W, g(\T)(w) & = (a_0 \IT[\V] + a_1 \T + \cdots + a_n \T^n)(w) &  & \text{(by \cref{e.0.7})} \\
                               & = a_0 w + a_1 \T(w) + \cdots + a_n \T^n(w)                                    \\
                               & \subseteq \W                                    &  & \text{(by \cref{5.4.1})}
  \end{align*}
  and thus by \cref{5.4.1} \(\W\) is \(g(\T)\)-invariant.
\end{proof}

\begin{ex}\label{ex:5.4.5}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\).
  Prove that the intersection of any collection of \(\T\)-invariant subspaces of \(\V\) over \(\F\) is a \(\T\)-invariant subspace of \(\V\) over \(\F\).
\end{ex}

\begin{proof}[\pf{ex:5.4.5}]
  Let \(K\) be an index set and let \(\set{U_{\alpha} : \alpha \in K}\) be a set of \(\T\)-invariant subspaces of \(\V\) over \(\F\).
  Then we have
  \begin{align*}
             & \forall \alpha \in K, \begin{dcases}
                                       U_{\alpha} \text{ is a subspace of } \V \text{ over } \F \\
                                       \T(U_{\alpha}) \subseteq U_{\alpha}
                                     \end{dcases}                                               &  & \text{(by \cref{5.4.1})}                          \\
    \implies & \begin{dcases}
                 \bigcap_{\alpha \in K} U_{\alpha} \text{ is a subspace of } \V \text{ over } \F \\
                 \T\pa{\bigcap_{\alpha \in K} U_{\alpha}} \subseteq \bigcap_{\alpha \in K} U_{\alpha}
               \end{dcases} &  & \text{(by \cref{1.4})}                                \\
    \implies & \bigcap_{\alpha \in K} U_{\alpha} \text{ is } \T\text{-invariant}.                                        &  & \text{(by \cref{5.4.1})}
  \end{align*}
\end{proof}

\setcounter{ex}{6}
\begin{ex}\label{ex:5.4.7}
  Prove that the restriction of a linear operator \(\T\) to a \(\T\)-invariant subspace is a linear operator on that subspace.
\end{ex}

\begin{proof}[\pf{ex:5.4.7}]
  Let \(\V\) be a vector space over \(\F\), let \(\T \in \ls(\V)\) and let \(\W\) be a \(\T\)-invariant subspace of \(\V\) over \(\F\).
  Let \(x, y \in \W\) and let \(c \in \F\).
  Since
  \begin{align*}
    \T_{\W}(cx + y) & = \T(cx + y)                &  & \text{(by \cref{b.0.4})}    \\
                    & = c \T(x) + \T(y)           &  & \text{(by \cref{2.1.2}(b))} \\
                    & = c \T_{\W}(x) + \T_{\W}(y) &  & \text{(by \cref{b.0.4})}    \\
                    & \in \W,                     &  & \text{(by \cref{1.3})}
  \end{align*}
  by \cref{2.1.2}(b) we know that \(\T_{\W} \in \ls(\W)\).
\end{proof}

\begin{ex}\label{ex:5.4.8}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\) with a \(\T\)-invariant subspace \(\W\) over \(\F\).
  Prove that if \(v\) is an eigenvector of \(\T_{\W}\) with corresponding eigenvalue \(\lambda\), then the same is true for \(\T\).
\end{ex}

\begin{proof}[\pf{ex:5.4.8}]
  We have
  \begin{align*}
             & \T_{\W}(v) = \lambda v &  & \text{(by \cref{5.1.2})} \\
    \implies & \T(v) = \lambda v.     &  & \text{(by \cref{b.0.4})}
  \end{align*}
\end{proof}

\setcounter{ex}{10}
\begin{ex}\label{ex:5.4.11}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\), let \(v\) be a nonzero vector in \(\V\), and let \(\W\) be the \(\T\)-cyclic subspace of \(\V\) generated by \(v\).
  Prove that
  \begin{enumerate}
    \item \(\W\) is \(\T\)-invariant.
    \item Any \(\T\)-invariant subspace of \(\V\) over \(\F\) containing \(v\) also contains \(\W\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:5.4.11}(a)]
  Let \(x \in \W\).
  Then we have
  \begin{align*}
             & \begin{dcases}
                 \exists \seq{v}{1,,k} \in \W \\
                 \exists \seq{a}{1,,k} \in \F
               \end{dcases} : x = \seq[+]{a,v}{1,,k}                                     &  & \text{(by \cref{1.4.3})} \\
    \implies & \begin{dcases}
                 \exists \seq{i}{1,,k} \in \N \\
                 \exists \seq{a}{1,,k} \in \F
               \end{dcases} : x = a_1 \T^{i_1}(v) + \cdots + a_k \T^{i_k}(v)             &  & \text{(by \cref{5.4.3})} \\
    \implies & \begin{dcases}
                 \exists \seq{i}{1,,k} \in \N \\
                 \exists \seq{a}{1,,k} \in \F
               \end{dcases} : \T(x) = a_1 \T^{i_1 + 1}(v) + \cdots + a_k \T^{i_k + 1}(v) &  & \text{(by \cref{2.10})}  \\
    \implies & \T(x) \in \W.                                                             &  & \text{(by \cref{5.4.3})}
  \end{align*}
  Since \(x\) is arbitrary, we see that \(\T(\W) \subseteq \W\) and by \cref{5.4.1} \(\W\) is \(\T\)-invariant.
\end{proof}

\begin{proof}[\pf{ex:5.4.11}(b)]
  Let \(\vs{X}\) be a \(\T\)-invariant subspace of \(\V\) over \(\F\) containing \(v\).
  Then we have
  \begin{align*}
             & v \in \vs{X}                                                         \\
    \implies & \T(v) \in \vs{X}                       &  & \text{(by \cref{5.4.1})} \\
    \implies & \forall k \in \Z^+, \T^k(v) \in \vs{X} &  & \text{(by \cref{5.4.1})} \\
    \implies & \W \subseteq \vs{X}.                   &  & \text{(by \cref{1.3})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:5.4.12}
  Prove that \(A = \begin{pmatrix}
    B_1 & B_2 \\
    \zm & B_3
  \end{pmatrix}\) in the proof of \cref{5.21}.
\end{ex}

\begin{proof}[\pf{ex:5.4.12}]
  Since \(\W\) is a \(\T\)-invariant subspace of \(\V\) over \(\F\), we know that \(\T(\gamma) \subseteq \W = \spn{\gamma}\).
  Thus if \(A = [\T]_{\beta}\) and \(B_1 = [\T_{\W}]_{\gamma}\), then the first \(\#(\gamma)\) columns of \(A\) can be uniquely express as linear combinations of \(\gamma\) (\cref{1.8}), with coefficients precisely those in the corresponding column of \(B_1\) (\cref{2.2.4}).
\end{proof}

\begin{ex}\label{ex:5.4.13}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\), let \(v\) be a nonzero vector in \(\V\), and let \(\W\) be the \(\T\)-cyclic subspace of \(\V\) generated by \(v\).
  For any \(w \in \V\), prove that \(w \in \W\) iff there exists a polynomial \(g\) such that \(w = g(\T)(v)\).
\end{ex}

\begin{proof}[\pf{ex:5.4.13}]
  We have
  \begin{align*}
         & w \in \W                                                                                    \\
    \iff & \begin{dcases}
             \exists \seq{i}{1,,k} \in \N \\
             \exists \seq{a}{1,,k} \in \F
           \end{dcases} : w = a_1 \T^{i_1}(v) + \cdots + a_k \T^{i_k}(v) &  & \text{(by \cref{5.4.3})} \\
    \iff & \begin{dcases}
             \exists k \in \N \\
             \exists \seq{a}{0,,k} \in \F
           \end{dcases} : w = a_0 v + a_1 \T(v) + \cdots + a_k \T^k(v)                                 \\
    \iff & \begin{dcases}
             \exists k \in \N \\
             \exists \seq{a}{0,,k} \in \F
           \end{dcases} : \begin{dcases}
                            g(t) = a_0 + a_1 t + \cdots + a_k t^k \\
                            w = g(\T)(v)
                          \end{dcases}.                      &  & \text{(by \cref{e.0.7})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:5.4.14}
  Prove that the polynomial \(g\) of \cref{ex:5.4.13} can always be chosen so that its degree is less than \(\dim(\W)\).
\end{ex}

\begin{proof}[\pf{ex:5.4.14}]
  By \cref{5.22}(a) we see that if \(k = \dim(\W)\) then
  \[
    \set{v, \T(v), \dots, \T^{k - 1}(v)}
  \]
  is a basis for \(\W\) over \(\F\).
  Thus
  \[
    w \in \W \iff \exists \seq{a}{0,,k-1} : \begin{dcases}
      g(t) = a_0 + a_1 t + \cdots + a_{k - 1} t^{k - 1} \\
      w = g(\T)(v)
    \end{dcases}.
  \]
\end{proof}

\setcounter{ex}{15}
\begin{ex}\label{ex:5.4.16}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\).
  \begin{enumerate}
    \item Prove that if the characteristic polynomial of \(\T\) splits, then so does the characteristic polynomial of the restriction of \(\T\) to any \(\T\)-invariant subspace of \(\V\).
    \item Deduce that if the characteristic polynomial of \(\T\) splits, then any nontrivial \(\T\)-invariant subspace of \(\V\) contains an eigenvector of \(\T\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:5.4.16}(a)]
  Let \(\W\) be a \(\T\)-invariant subspace of \(\V\) over \(\F\), and let \(f, g\) be the characteristic polynomials of \(\T, \T_{\W}\), respectively.
  By \cref{5.21} we see that \(g\) divides \(f\), thus there exists some polynomial \(q\) such that \(f = g \cdot q\).
  Since \(f\) splits, we see that \(g\) and \(q\) must split.
\end{proof}

\begin{proof}[\pf{ex:5.4.16}(b)]
  Let \(\W\) be a nontrivial \(\T\)-invariant subspace of \(\V\) over \(\F\), and let \(f, g\) be the characteristic polynomials of \(\T, \T_{\W}\), respectively.
  By \cref{ex:5.4.16}(a) we know that \(f, g\) split and \(f = g \cdot q\) for some polynomial \(q\), thus there exists some \(\lambda \in \F\) such that \(g(\lambda) = f(\lambda) = 0\).
  By \cref{5.2} we see that \(\lambda\) is an eigenvalue of \(\T\) and \(\T_{\W}\).
  Since \(\W \neq \set{\zv}\), by \cref{5.7} we can find some \(v \in \W \setminus \set{\zv}\) such that \(\T_{\W}(v) = \lambda v\).
  Thus \(\T(v) = \lambda v\) and by \cref{5.1.2} \(v\) is an eigenvector of \(\T\).
\end{proof}

\begin{ex}\label{ex:5.4.17}
  Let \(A \in \ms{n}{n}{\F}\).
  Prove that
  \[
    \dim\pa{\spn{\set{I_n, A, A^2, \dots}}} \leq n.
  \]
\end{ex}

\begin{proof}[\pf{ex:5.4.17}]
  Let \(f\) be the characteristic polynomial of \(A\).
  By \cref{5.3} there exist some \(\seq{a}{0,,n-1} \in \F\) such that
  \[
    f(t) = a_0 + a_1 t + \cdots + a_{n - 1} t^{n - 1} + (-1)^n t^n.
  \]
  By Cayley--Hamilton theorem (\cref{5.4.4}) we see that
  \[
    f(A) = a_0 I_n + a_1 A + \cdots + a_{n - 1} A^{n - 1} + (-1)^n A^n = \zm.
  \]
  Thus \(A^n \in \spn{\set{I_n, A, A^2, \dots, A^{n - 1}}}\).
  Since
  \[
    a_0 A + a_1 A^2 + \cdots + a_{n - 1} A^n + (-1)^n A^{n + 1} = \zm,
  \]
  we see that \(A^{n + 1} \in \spn{\set{A, A^2, \dots, A^{n - 1}, A^n}} = \spn{\set{I_n, A, A^2, \dots, A^{n - 1}}}\).
  Thus for all \(m \geq n\), we have \(A^m \in \spn{\set{I_n, A, A^2, \dots, A^{n - 1}}}\).
  By \cref{1.6.8} this means \(\dim\pa{\spn{\set{I_n, A, A^2, \dots}}} = \dim\pa{\spn{\set{I_n, A, A^2, \dots, A^{n - 1}}}} \leq n\).
\end{proof}

\begin{ex}\label{ex:5.4.18}
  Let \(A \in \ms{n}{n}{\F}\) with characteristic polynomial
  \[
    f(t) = (-1)^n t^n + a_{n - 1} t^{n - 1} + \cdots + a_1 t + a_0.
  \]
  \begin{enumerate}
    \item Prove that \(A\) is invertible iff \(a_0 \neq 0\).
    \item Prove that if \(A\) is invertible, then
          \[
            A^{-1} = \frac{-1}{a_0} \pa{(-1)^n A^{n - 1} + a_{n - 1} A^{n - 2} + \cdots + a_1 I_n}.
          \]
    \item Use (b) to compute \(A^{-1}\) for
          \[
            A = \begin{pmatrix}
              1 & 2 & 1  \\
              0 & 2 & 3  \\
              0 & 0 & -1
            \end{pmatrix}.
          \]
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:5.4.18}(a)]
  See \cref{ex:5.1.20}
\end{proof}

\begin{proof}[\pf{ex:5.4.18}(b)]
  We have
  \begin{align*}
             & f(A) = (-1)^n A^n + a_{n - 1} A^{n - 1} + \cdots + a_1 A + a_0 I_n = \zm                   &  & \text{(by \cref{5.4.4})} \\
    \implies & (-1)^n A^n + a_{n - 1} A^{n - 1} + \cdots + a_1 A = -a_0 I_n                                                             \\
    \implies & A \pa{(-1)^n A^{n - 1} + a_{n - 1} A^{n - 2} + \cdots + a_1 I_n} = -a_0 I_n                &  & \text{(by \cref{2.3.5})} \\
    \implies & A \pa{\frac{-1}{a_0} \pa{(-1)^n A^{n - 1} + a_{n - 1} A^{n - 2} + \cdots + a_1 I_n}} = I_n &  & (a_0 \neq 0)             \\
    \implies & \frac{-1}{a_0} \pa{(-1)^n A^{n - 1} + a_{n - 1} A^{n - 2} + \cdots + a_1 I_n} = A^{-1}.    &  & \text{(by \cref{2.4.3})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:5.4.18}(c)]
  We have
  \begin{align*}
    \det(A - t I_3) & = \begin{pmatrix}
                          1 - t & 2     & 1      \\
                          0     & 2 - t & 3      \\
                          0     & 0     & -1 - t
                        \end{pmatrix} &  & \text{(by \cref{4.2.2})}               \\
                    & = (1 - t)(2 - t)(-1 - t)  &  & \text{(by \cref{ex:4.2.23})} \\
                    & = -t^3 + 2t^2 + t - 2
  \end{align*}
  and
  \begin{align*}
    A^{-1} & = \frac{-1}{-2} \pa{-A^2 + 2A + I_3} &  & \text{(by \cref{ex:5.4.18})} \\
           & = \frac{1}{2} \pa{-\begin{pmatrix}
                                    1 & 6 & 6 \\
                                    0 & 4 & 3 \\
                                    0 & 0 & 1
                                  \end{pmatrix} + 2\begin{pmatrix}
                                                     1 & 2 & 1  \\
                                                     0 & 2 & 3  \\
                                                     0 & 0 & -1
                                                   \end{pmatrix} + I_3}               \\
           & = \begin{pmatrix}
                 1 & -1          & -2          \\
                 0 & \frac{1}{2} & \frac{3}{2} \\
                 0 & 0           & -1
               \end{pmatrix}.
  \end{align*}
\end{proof}

\begin{ex}\label{ex:5.4.19}
  Let \(A \in \ms{k}{k}{\F}\)
  \[
    A = \begin{pmatrix}
      0      & 0      & \cdots & 0      & -a_0       \\
      1      & 0      & \cdots & 0      & -a_1       \\
      0      & 1      & \cdots & 0      & -a_2       \\
      \vdots & \vdots &        & \vdots & \vdots     \\
      0      & 0      & \cdots & 0      & -a_{k - 2} \\
      0      & 0      & \cdots & 1      & -a_{k - 1}
    \end{pmatrix}
  \]
  where \(\seq{a}{0,,k-1} \in \F\).
  Prove that the characteristic polynomial of \(A\) is
  \[
    (-1)^k (a_0 + a_1 t + \cdots + a_{k - 1} t^{k - 1} + t^k).
  \]
\end{ex}

\begin{proof}[\pf{ex:5.4.19}]
  We use induction on \(k\).
  For \(k = 1\), we have
  \begin{align*}
    \det(-a_0 - t) & = -a_0 - t         &  & \text{(by \cref{4.2.2})} \\
                   & = (-1)^1 (a_0 + t)
  \end{align*}
  and thus the base case holds.
  Suppose inductively that for some \(k \geq 1\) the statement is true.
  We need to show that the statement is also true for \(k + 1\).
  Let \(A \in \ms{(k + 1)}{(k + 1)}{\F}\) and let \(\seq{a}{0,,k} \in \F\) such that
  \[
    A = \begin{pmatrix}
      0      & 0      & \cdots & 0      & -a_0       \\
      1      & 0      & \cdots & 0      & -a_1       \\
      \vdots & \vdots &        & \vdots & \vdots     \\
      0      & 0      & \cdots & 0      & -a_{k - 1} \\
      0      & 0      & \cdots & 1      & -a_k
    \end{pmatrix}.
  \]
  Let \(B = A - t I_{k + 1}\).
  Then we have
  \begin{align*}
    \det(B) & = \sum_{j = 1}^{k + 1} (-1)^{1 + j} B_{1 j} \det(\tilde{B}_{1 j})                        &  & \text{(by \cref{4.2.2})}         \\
            & = B_{1 1} \det(\tilde{B}_{1 1}) + (-1)^{k + 2} B_{1 (k + 1)} \det(\tilde{B}_{1 (k + 1)})                                       \\
            & = (-t) \begin{pmatrix}
                       -t     & 0      & \cdots & 0  & -a_1     \\
                       1      & -t     & \cdots & 0  & -a_2     \\
                       \vdots & \vdots &        & -t & \vdots   \\
                       0      & 0      & \cdots & 1  & -a_k - t
                     \end{pmatrix}                                                                                \\
            & \quad + (-1)^{k + 2} (-a_0) \begin{pmatrix}
                                            1      & -t     & 0      & \cdots & 0      & 0      \\
                                            0      & 1      & -t     & \cdots & 0      & 0      \\
                                            \vdots & \vdots & \vdots &        & \vdots & \vdots \\
                                            0      & 0      & 0      & \cdots & 1      & -t     \\
                                            0      & 0      & 0      & \cdots & 0      & 1
                                          \end{pmatrix}                                                \\
            & = (-t) (-1)^k (a_1 + a_2 t + \cdots + a_k t^{k - 1} + t^k)                               &  & \text{(by induction hypothesis)} \\
            & \quad + (-1)^{k + 3} a_0                                                                 &  & \text{(by \cref{ex:4.2.23})}     \\
            & = (-1)^{k + 1} (a_0 + a_1 t + \cdots + a_k t^k + t^{k + 1}).
  \end{align*}
  This closes the induction.
\end{proof}

\begin{ex}\label{ex:5.4.20}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\), and suppose that \(\V\) is a \(\T\)-cyclic subspace of itself.
  Prove that if \(\U\) is a linear operator on \(\V\) over \(\F\), then \(\U \T = \T \U\) iff \(\U = g(\T)\) for some polynomial \(g\).
\end{ex}

\begin{proof}[\pf{ex:5.4.20}]
  Since \(\V\) is a \(\T\)-cyclic subspace of itself, by \cref{5.4.3} there exists some \(v \in \V\) such that \(\V = \spn{\set{v, \T(v), \T^2(v), \dots,}}\).

  First suppose that \(\U \T = \T \U\).
  Since \(\U(v) \in \V\), by \cref{ex:5.4.13} there exists some \(g \in \ps{\F}\) such that \(\U(v) = g(\T)(v)\).
  Thus
  \[
    \exists \seq{a}{0,,n} \in \F : \begin{dcases}
      g(t) = a_0 + a_1 t + \cdots + a_n t^n            \\
      g(\T) = a_0 \IT[\V] + a_1 \T + \cdots + a_n \T^n \\
      g(\T)(v) = \U(v)
    \end{dcases}.
  \]
  Then we have
  \begin{align*}
             & \forall w \in \V, \begin{dcases}
                                   \exists k \in \N \\
                                   \exists \seq{b}{0,,k} \in \F
                                 \end{dcases} :                                                               \\
             & w = b_0 v + b_1 \T(v) + \cdots + b_k \T^k(v)                               &  & \text{(by \cref{5.4.3})}    \\
    \implies & \forall w \in \V, \begin{dcases}
                                   \exists k \in \N \\
                                   \exists \seq{b}{0,,k} \in \F
                                 \end{dcases} :                                                               \\
             & \U(w) = \U\pa{b_0 v + b_1 \T(v) + \cdots + b_k \T^k(v)}                                                     \\
             & = b_0 \U(v) + b_1 \U(\T(v)) \cdots + b_k \U(\T^k(v))                       &  & \text{(by \cref{2.1.2}(b))} \\
             & = b_0 \U(v) + b_1 \T(\U(v)) \cdots + b_k \T^k(\U(v))                       &  & (\U \T = \T \U)             \\
             & = b_0 (a_0 v + a_1 \T(v) + \cdots + a_n \T^n(v))                           &  & (g(\T)(v) = \U(v))          \\
             & \quad + b_1 (a_0 \T(v) + a_1 \T^2(v) + \cdots + a_n \T^{n + 1}(v))         &  & \text{(by \cref{2.1.2}(b))} \\
             & \quad + \cdots                                                                                              \\
             & \quad + b_k (a_0 \T^k(v) + a_1 \T^{k + 1}(v) + \cdots + a_n \T^{n + k}(v)) &  & \text{(by \cref{2.1.2}(b))} \\
             & = a_0 (b_0 v + b_1 \T(v) + \cdots + b_k \T^k(v))                           &  & \text{(by \cref{1.2.1})}    \\
             & \quad + a_1 \T\pa{b_0 v + b_1 \T(v) + \cdots + b_k \T^k(v)}                &  & \text{(by \cref{2.1.2}(b))} \\
             & \quad + \cdots                                                                                              \\
             & \quad + a_n \T^n\pa{b_0 v + b_1 \T(v) + \cdots + b_k \T^k(v)}              &  & \text{(by \cref{2.1.2}(b))} \\
             & = (a_0 + a_1 \T + \cdots + a_n \T^n)(w)                                                                     \\
             & = g(\T)(w)                                                                                                  \\
    \implies & \U = g(\T).
  \end{align*}

  Now suppose that \(\U = g(\T)\) for some \(g \in \ps{\F}\).
  Let \(g(t) = a_0 + a_1 t + \cdots + a_n t^n\) for some \(\seq{a}{0,,n} \in \F\).
  Then we have
  \begin{align*}
    \forall w \in \V, (\U \T)(w) & = (g(\T)(\T))(w)                                                                    \\
                                 & = (a_0 \T + a_1 \T^2 + \cdots + a_n \T^{n + 1})(w)  &  & \text{(by \cref{2.10}(a))} \\
                                 & = (\T(a_0 \IT[\V] + a_1 \T + \cdots + a_n \T^n))(w) &  & \text{(by \cref{2.10}(a))} \\
                                 & = ((\T) g(\T))(w)                                                                   \\
                                 & = (\T \U)(w)
  \end{align*}
  and thus \(\U \T = \T \U\).
\end{proof}

\begin{ex}\label{ex:5.4.21}
  Let \(\T\) be a linear operator on a two-dimensional vector space \(\V\) over \(\F\).
  Prove that either \(\V\) is a \(\T\)-cyclic subspace of itself or \(\T = c \IT[\V]\) for some \(c \in \F\).
\end{ex}

\begin{proof}[\pf{ex:5.4.21}]
  If \(\T = \zT\), then we see that \(\T = 0 \IT[\V]\).
  So suppose that \(\T \neq \zT\).
  Now we split into two cases:
  \begin{itemize}
    \item If there exists some \(v \in \V\) such that \(\dim(\spn{\set{v, \T(v)}}) = 2\), then by \cref{1.6.15}(b) we have \(\V = \spn{\set{v, \T(v)}}\).
          By \cref{5.4.3} we see that \(\V\) is a \(\T\)-cyclic subspace of itself generated by \(v\).
    \item If there does not exist such \(v \in \V\), then we have \(\dim(\spn{\set{v, \T(v)}}) < 2\) for all \(v \in \V\).
          Since \(\T \neq \zT\), there exist some \(w_1 \in \V\) such that \(\T(w_1) \neq \zv\).
          Fix such \(w_1\).
          Since \(\T(w_1) \in \spn{\set{w_1}} \setminus \set{\zv}\), there exists some \(c_1 \in \F \setminus \set{0}\) such that \(\T(w_1) = c_1 w_1\).
          Now we extend \(\set{w_1}\) to a basis \(\set{\seq{w}{1,2}}\) for \(\V\) over \(\F\).
          By our hypothesis we have
          \[
            \T(w_2) \in \spn{\set{w_2}} \quad \text{and} \quad \T(w_1 + w_2) \in \spn{\set{w_1 + w_2}}.
          \]
          This means \(\T(w_2) = c_2 w_2\) and \(\T(w_1 + w_2) = c_3 (w_1 + w_2)\) for some \(c_2, c_3 \in \F\).
          Now observe that
          \begin{align*}
            \T(w_1 + w_2) & = c_3 (w_1 + w_2)                                  \\
                          & = c_3 w_1 + c_3 w_2  &  & \text{(by \cref{1.2.1})} \\
                          & = \T(w_1) + \T(w_2)  &  & \text{(by \cref{2.1.1})} \\
                          & = c_1 w_1 + c_2 w_2.
          \end{align*}
          Since \(\set{w_1, w_2}\) is linearly independent, we know that
          \[
            (c_3 - c_1) w_1 + (c_3 - c_2) w_2 = \zv \implies c_1 = c_3 = c_2.
          \]
          Thus by setting \(c = c_1\) we see that \(\T = c \IT[\V]\).
  \end{itemize}
  From all cases above we see that \cref{ex:5.4.21} is true.
\end{proof}

\begin{ex}\label{ex:5.4.22}
  Let \(\T\) be a linear operator on a two-dimensional vector space \(\V\) over \(\F\) and suppose that \(\T \neq c \IT[\V]\) for any scalar \(c \in \F\).
  Show that if \(\U \in \ls(\V)\) such that \(\U \T = \T \U\), then \(\U = g(\T)\) for some polynomial \(g\).
\end{ex}

\begin{proof}[\pf{ex:5.4.22}]
  By \cref{ex:5.4.21} we see that \(\V\) is a \(\T\)-cyclic subspace of itself.
  By \cref{ex:5.4.20} we conclude that \(\U = g(\T)\) for some \(g \in \ps{\F}\).
\end{proof}

\begin{ex}\label{ex:5.4.23}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and let \(\W\) be a \(\T\)-invariant subspace of \(\V\) over \(\F\).
  Suppose that \(\seq{v}{1,,k}\) are eigenvectors of \(\T\) corresponding to distinct eigenvalues.
  Prove that if \(\seq[+]{v}{1,,k} \in \W\), then \(v_i \in \W\) for all \(i \in \set{1, \dots, k}\).
\end{ex}

\begin{proof}[\pf{ex:5.4.23}]
  We use induction on \(k\).
  The case for \(k = 1\) is trivial.
  So suppose inductively that for some \(k \geq 1\) the statement is true.
  We need to show that for \(k + 1\) the statement is true.
  For each \(i \in \set{1, \dots, k + 1}\), let \(v_i\) be an eigenvector of \(\T\) with corresponding eigenvalue \(\lambda_i\) such that \(\seq{\lambda}{1,,k+1}\) are distinct and \(\seq[+]{v}{1,,k+1} \in \W\).
  Since \(\W\) is \(\T\)-invariant, by \cref{1.3} we know that \(\W\) is also \((\T - \lambda_{k + 1} \IT[\V])\)-invariant.
  Thus we see that
  \begin{align*}
             & \seq[+]{v}{1,,k+1} \in \W                                                                                            \\
    \implies & (\T - \lambda_{k + 1} \IT[\V])(\seq[+]{v}{1,,k+1}) \in \W                              &  & \text{(by \cref{5.4.1})} \\
    \implies & (\lambda_1 - \lambda_{k + 1}) v_1 + \cdots + (\lambda_k - \lambda_{k + 1}) v_k \in \W. &  & \text{(by \cref{5.1.2})}
  \end{align*}
  Since \(\lambda_i - \lambda_{k + 1} \neq 0\) for all \(i \in \set{1, \dots, k}\), by \cref{5.2.4} we see that \((\lambda_i - \lambda_{k + 1}) v_i\) is an eigenvector of \(\T\) and \((\lambda_i - \lambda_{k + 1}) v_i \in E_{\lambda_i}\).
  Thus by induction hypothesis we see that \((\lambda_i - \lambda_{k + 1}) v_i \in \W\) for all \(i \in \set{1, \dots, k}\).
  Then we have
  \begin{align*}
             & \forall i \in \set{1, \dots, k}, \lambda_i - \lambda_{k + 1} \neq 0                                                                           \\
    \implies & \forall i \in \set{1, \dots, k}, \frac{\lambda_i - \lambda_{k + 1}}{\lambda_i - \lambda_{k + 1}} v_i = v_i \in \W &  & \text{(by \cref{1.3})} \\
    \implies & \seq[+]{v}{1,,k} \in \W                                                                                           &  & \text{(by \cref{1.3})} \\
    \implies & \seq[+]{v}{1,,k+1} + -(\seq[+]{v}{1,,k}) = v_{k + 1} \in \W                                                       &  & \text{(by \cref{1.3})}
  \end{align*}
  and this closes the induction.
\end{proof}

\begin{ex}\label{ex:5.4.24}
  Prove that the restriction of a diagonalizable linear operator \(\T\) to any nontrivial \(\T\)-invariant subspace is also diagonalizable.
\end{ex}

\begin{proof}[\pf{ex:5.4.24}]
  Let \(\V\) be a finite-dimensional vector space over \(\F\), let \(\T \in \ls(\V)\) such that \(\T\) is diagonalizable and let \(\W\) be a nontrivial \(\T\)-invariant subspace of \(\V\) over \(\F\).
  By \cref{5.1.1} there exist an ordered basis \(\beta\) such that \([\T]_{\beta}\) is diagonal matrix.
  We claim that \(\W \cap \beta \neq \varnothing\) and \(\W \cap \beta\) is an ordered basis for \(\W\).
  If not, then we must have \(\W = \set{\zv}\), which contradict to our hypothesis that \(\W\) is nontrivial.
  Let \(\beta_{\W} = \W \cap \beta\).
  Since \(\beta\) is consist of eigenvectors of \(\T\) and \(\W\) is \(\T\)-invariant, by \cref{5.1.2,5.4.1} we know that \(\T(\beta_{\W}) = \T_{\W}(\beta_{\W}) \subseteq \W\) and \(\beta_{\W}\) is consist of eigenvectors of \(\T_{\W}\).
  Thus \([\T_{\W}]_{\beta_{\W}}\) is diagonal matrix and by \cref{5.1.1} \(\T_{\W}\) is diagonalizable.
\end{proof}

\begin{ex}\label{ex:5.4.25}

\end{ex}

\begin{ex}\label{ex:5.4.32}

\end{ex}

\begin{ex}\label{ex:5.4.34}

\end{ex}

\begin{ex}\label{ex:5.4.36}

\end{ex}
