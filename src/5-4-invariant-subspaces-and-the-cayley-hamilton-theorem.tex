\section{Invariant Subspaces and the Cayley-Hamilton Theorem}\label{sec:5.4}

\begin{defn}\label{5.4.1}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\).
  A subspace \(\W\) of \(\V\) over \(\F\) is called a \textbf{\(\T\)-invariant subspace} of \(\V\) over \(\F\) if \(\T(\W) \subseteq \W\), that is, if \(\T(v) \in \W\) for all \(v \in \W\).
\end{defn}

\begin{eg}\label{5.4.2}
  Suppose that \(\T\) is a linear operator on a vector space \(\V\) over \(\F\).
  Then the following subspaces of \(\V\) are \(\T\)-invariant:
  \begin{enumerate}
    \item \(\set{\zv}\)
    \item \(\V\)
    \item \(\rg{\T}\)
    \item \(\ns{\T}\)
    \item \(E_{\lambda}\), for any eigenvalue \(\lambda\) of \(\T\).
  \end{enumerate}
\end{eg}

\begin{proof}[\pf{5.4.2}]
  We have
  \begin{align*}
    \T(\set{\zv}) & = \set{\zv} \subseteq \set{\zv} &  & \text{(by \cref{2.1.2}(a))} \\
    \T(\V)        & = \rg{\T} \subseteq \V          &  & \text{(by \cref{2.5.2})}    \\
    \T(\rg{\T})   & \subseteq \T(\V) = \rg{\T}      &  & \text{(by \cref{2.1.10})}   \\
    \T(\ns{\T})   & = \set{\zv} \subseteq \ns{\T}   &  & \text{(by \cref{2.1.10})}
  \end{align*}
  and
  \begin{align*}
             & \forall v \in E_{\lambda}, \T(v) = \lambda v \in E_{\lambda} &  & \text{(by \cref{5.2.4})} \\
    \implies & \T(E_{\lambda}) \subseteq E_{\lambda}.
  \end{align*}
  Thus by \cref{5.4.1} \(\set{\zv}, \V, \rg{\T}, \ns{\T}, E_{\lambda}\) are \(\T\)-invariant subspace of \(\V\) over \(\F\).
\end{proof}

\begin{defn}\label{5.4.3}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\), and let \(x\) be a nonzero vector in \(\V\).
  The subspace
  \[
    \W = \spn{\set{x, \T(x), \T^2(x), \dots}}
  \]
  is called the \textbf{\(\T\)-cyclic subspace of \(\V\) generated by \(x\)}.
  It is a simple matter to show that \(\W\) is \(\T\)-invariant.
  In fact, \(\W\) is the ``smallest'' \(\T\)-invariant subspace of \(\V\) containing \(x\).
  That is, any \(\T\)-invariant subspace of \(\V\) containing \(x\) must also contain \(\W\)
  (see \cref{ex:5.4.11}).
\end{defn}

\begin{thm}\label{5.21}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and let \(\W\) be a \(\T\)-invariant subspace of \(\V\) over \(\F\).
  Then the characteristic polynomial of \(\T_{\W}\) divides the characteristic polynomial of \(\T\).
\end{thm}

\begin{proof}[\pf{5.21}]
  Choose an ordered basis \(\gamma = \set{\seq{v}{1,,k}}\) for \(\W\) over \(\F\), and extend it to an ordered basis \(\beta = \set{\seq{v}{1,,k,k+1,n}}\) for \(\V\) over \(\F\).
  Let \(A = [\T]_{\beta}\) and \(B_1 = [\T_{\W}]_{\gamma}\).
  Then, by \cref{ex:5.4.12}, \(A\) can be written in the form
  \[
    A = \begin{pmatrix}
      B_1 & B_2 \\
      \zm & B_3
    \end{pmatrix}.
  \]
  Let \(f\) be the characteristic polynomial of \(\T\) and \(g\) the characteristic polynomial of \(\T_{\W}\).
  Then
  \[
    f(t) = \det(A - t I_n) = \det\begin{pmatrix}
      B_1 - t I_k & B_2               \\
      \zm         & B_3 - t I_{n - k}
    \end{pmatrix} = g(t) \cdot \det(B_3 - t I_{n - k})
  \]
  by \cref{ex:4.3.21}.
  Thus \(g\) divides \(f\).
\end{proof}

\begin{thm}\label{5.22}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and let \(\W\) denote the \(\T\)-cyclic subspace of \(\V\) generated by a nonzero vector \(v \in \V\).
  Let \(k = \dim(\W)\).
  Then
  \begin{enumerate}
    \item \(\set{v, \T(v), \T^2(v), \dots, \T^{k - 1}(v)}\) is a basis for \(\W\) over \(\F\).
    \item If \(a_0 v + a_1 \T(v) + \cdots + a_{k - 1} \T^{k - 1}(v) + \T^k(v) = \zv\), then the characteristic polynomial of \(\T_{\W}\) is \(f(t) = (-1)^k (a_0 + a_1 t + \cdots + a_{k - 1} t^{k - 1} + t^k)\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{5.22}(a)]
  Since \(v \neq \zv\), the set \(\set{v}\) is linearly independent.
  Let \(j\) be the largest positive integer for which
  \[
    \beta = \set{v, \T(v), \dots, \T^{j - 1}(v)}
  \]
  is linearly independent.
  Such a \(j\) must exist because \(\V\) is finite-dimensional.
  Let \(\vs{Z} = \spn{\beta}\).
  Then \(\beta\) is a basis for \(\vs{Z}\) over \(\F\).
  Furthermore, \(\T^j(v) \in \vs{Z}\) by \cref{1.7}.
  We use this information to show that \(\vs{Z}\) is a \(\T\)-invariant subspace of \(\V\) over \(\F\).
  Let \(w \in \vs{Z}\).
  Since \(w\) is a linear combination of the vectors of \(\beta\), there exist scalars \(\seq{b}{0,,j-1} \in \F\) such that
  \[
    w = b_0 v + b_1 \T(v) + \cdots + b_{j - 1} \T^{j - 1}(v),
  \]
  and hence
  \[
    \T(w) = b_0 \T(v) + b_1 \T^2(v) + \cdots + b_{j - 1} \T^j(v).
  \]
  Thus \(\T(w)\) is a linear combination of vectors in \(\vs{Z}\), and hence belongs to \(\vs{Z}\).
  So \(\vs{Z}\) is \(\T\)-invariant.
  Furthermore, \(v \in \vs{Z}\).
  By \cref{ex:5.4.11}, \(\W\) is the smallest \(\T\)-invariant subspace of \(\V\) that contains \(v\), so that \(\W \subseteq \vs{Z}\).
  Clearly, \(\vs{Z} \subseteq \W\), and so we conclude that \(\vs{Z} = \W\).
  It follows that \(\beta\) is a basis for \(\W\) over \(\F\), and therefore \(\dim(\W) = j\).
  Thus \(j = k\).
  This proves (a).
\end{proof}

\begin{proof}[\pf{5.22}(b)]
  Now view \(\beta\) (from (a)) as an ordered basis for \(\W\).
  Let \(\seq{a}{0,,k-1} \in \F\) such that
  \[
    a_0 v + a_1 \T(v) + \cdots + a_{k - 1} \T^{k - 1}(v) + \T^k(v) = \zv.
  \]
  Observe that
  \[
    [\T_{\W}]_{\beta} = \begin{pmatrix}
      0      & 0      & \cdots & 0      & -a_0       \\
      1      & 0      & \cdots & 0      & -a_1       \\
      \vdots & \vdots &        & \vdots & \vdots     \\
      0      & 0      & \cdots & 1      & -a_{k - 1}
    \end{pmatrix}
  \]
  which has the characteristic polynomial
  \[
    f(t) = (-1)^k (a_0 + a_1 t + \cdots + a_{k - 1} t^{k - 1} + t^k)
  \]
  by \cref{ex:5.4.19}.
  Thus \(f\) is the characteristic polynomial of \(\T_{\W}\), proving (b).
\end{proof}

\begin{thm}[Cayley--Hamilton theorem]\label{5.23}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and let \(f\) be the characteristic polynomial of \(\T\).
  Then \(f(\T) = \zT\), the zero transformation.
  That is, \(\T\) ``satisfies'' its characteristic equation.
\end{thm}

\begin{proof}[\pf{5.23}]
  We show that \(f(\T)(v) = \zv\) for all \(v \in \V\).
  This is obvious if \(v = 0\) because \(f(\T)\) is linear;
  so suppose that \(v \neq \zv\).
  Let \(\W\) be the \(\T\)-cyclic subspace generated by \(v\), and suppose that \(\dim(\W) = k\).
  By \cref{5.22}(a), there exist scalars \(\seq{a}{0,,k-1} \in \F\) such that
  \[
    a_0 v + a_1 \T(v) + \cdots + a_{k - 1} \T^{k - 1}(v) + \T^k(v) = \zv.
  \]
  Hence \cref{5.22}(b) implies that
  \[
    g(t) = (-1)^k (a_0 + a_1 t + \cdots + a_{k - 1} t^{k - 1} + t^k)
  \]
  is the characteristic polynomial of \(\T_{\W}\).
  Combining these two equations yields
  \[
    g(\T)(v) = (-1)^k (a_0 \IT[\V] + a_1 \T + \cdots + a_{k - 1} \T^{k - 1} + \T^k)(v) = \zv.
  \]
  By \cref{5.21}, \(g\) divides \(f\);
  hence there exists a polynomial \(q\) such that \(f\) = \(qg\).
  So
  \[
    f(\T)(v) = (q(\T) g(\T))(v) = q(\T)(g(\T)(v)) = q(\T)(\zv) = \zv.
  \]
\end{proof}

\begin{cor}[Cayley--Hamilton Theorem for Matrices]\label{5.4.4}
  Let \(A \in \ms{n}{n}{\F}\), and let \(f\) be the characteristic polynomial of \(A\).
  Then \(f(A) = \zm\), the \(n \times n\) zero matrix.
\end{cor}

\begin{proof}[\pf{5.4.4}]
  Let \(\beta\) be the standard ordered basis for \(\vs{F}^n\) over \(\F\).
  Then we have
  \begin{align*}
             & A = [\L_A]_{\beta}                                  &  & \text{(by \cref{2.15}(a))} \\
    \implies & f \text{ is the characteristic polynomial of } \L_A &  & \text{(by \cref{5.1.6})}   \\
    \implies & f(\L_A) = \det([\L_A]_{\beta} - t I_n)(\L_A) = \zT  &  & \text{(by \cref{5.23})}    \\
    \implies & f(A) = \det(A - t I_n)(A) = \zm.                    &  & \text{(by \cref{2.3.8})}
  \end{align*}
\end{proof}

\begin{note}
  It is useful to decompose a finite-dimensional vector space \(\V\) over \(\F\) into a direct sum of as many \(\T\)-invariant subspaces as possible because the behavior of \(\T\) on \(\V\) can be inferred from its behavior on the direct summands.
  For example, \(\T\) is diagonalizable iff \(\V\) can be decomposed into a direct sum of one-dimensional \(\T\)-invariant subspaces (see \cref{ex:5.4.36}).
  In \cref{ch:7}, we consider alternate ways of decomposing \(\V\) into direct sums of \(\T\)-invariant subspaces if \(\T\) is not diagonalizable.
\end{note}

\begin{thm}\label{5.24}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and suppose that \(\V = \seq[\oplus]{\W}{1,,k}\), where \(\W_i\) is a \(\T\)-invariant subspace of \(\V\) over \(\F\) for each \(i \in \set{1, \dots, k}\).
  Suppose that \(f_i\) is the characteristic polynomial of \(\T_{\W}\) for each \(i \in \set{1, \dots, k}\).
  Then \(\prod_{i = 1}^k f_i\) is the characteristic polynomial of \(\T\).
\end{thm}

\begin{proof}[\pf{5.24}]
  The proof is by mathematical induction on \(k\).
  In what follows, \(f\) denotes the characteristic polynomial of \(\T\).
  Suppose first that \(k = 2\).
  Let \(\beta_1\) be an ordered basis for \(\W_1\) over \(\F\), \(\beta_2\) an ordered basis for \(\W_2\) over \(\F\), and \(\beta = \beta_1 \cup \beta_2\).
  Then \(\beta\) is an ordered basis for \(\V\) over \(\F\) by \cref{5.10}(d).
  Let \(A = [\T]_{\beta}\), \(B_1 = [\T_{\W_1}]_{\beta_1}\), and \(B_2 = [\T_{\W_2}]_{\beta_2}\).
  By \cref{ex:5.4.34}, it follows that
  \[
    A = \begin{pmatrix}
      B_1  & \zm \\
      \zm' & B_2
    \end{pmatrix},
  \]
  where \(\zm\) and \(\zm'\) are zero matrices of the appropriate sizes.
  Then
  \[
    f(t) = \det(A - tI) = \det(B_1 - tI) \cdot \det(B_2 - tI) = f_1(t) \cdot f_2(t)
  \]
  as in the proof of \cref{5.21}, proving the result for \(k = 2\).
  Now assume that the theorem is valid for \(k\) summands, where \(k \geq 2\), and suppose that \(\V\) is a direct sum of \(k + 1\) subspaces, say,
  \[
    \V = \seq[\oplus]{\W}{1,,k+1}.
  \]
  Let \(\W = \seq[+]{\W}{1,,k}\).
  It is easily verified that \(\W\) is \(\T\)-invariant and that \(\V = \W \oplus \W_{k + 1}\).
  So by the case for \(k = 2\), \(f = g \cdot f_{k + 1}\), where \(g\) is the characteristic polynomial of \(\T_{\W}\).
  Clearly \(\W = \seq[\oplus]{\W}{1,,k}\), and therefore \(g = \prod_{i = 1}^k f_i\) by the induction hypothesis.
  We conclude that \(f = g \cdot f_{k + 1} = \prod_{i = 1}^{k + 1} f_i\).
\end{proof}

\begin{note}
  As an illustration of \cref{5.24}, suppose that \(\T\) is a diagonalizable linear operator on a finite-dimensional vector space \(\V\) over \(\F\) with distinct eigenvalues \(\seq{\lambda}{1,,k}\).
  By \cref{5.11}, \(\V\) is a direct sum of the eigenspaces of \(\T\).
  Since each eigenspace is \(\T\)-invariant, we may view this situation in the context of \cref{5.24}.
  For each eigenvalue \(\lambda_i\), the restriction of \(\T\) to \(E_{\lambda_i}\) has characteristic polynomial \((\lambda_i - t)^{m_i}\), where \(m_i\) is the dimension of \(E_{\lambda_i}\).
  By \cref{5.24}, the characteristic polynomial \(f\) of \(\T\) is the product
  \[
    f(t) = (\lambda_1 - t)^{m_1} (\lambda_2 - t)^{m_2} \cdots (\lambda_k - t)^{m_k}.
  \]
  It follows that the multiplicity of each eigenvalue is equal to the dimension of the corresponding eigenspace, as expected.
\end{note}

\begin{defn}\label{5.4.5}
  Let \(B_1 \in \ms{m}{m}{\F}\), and let \(B_2 \in \ms{n}{n}{\F}\).
  We define the \textbf{direct sum} of \(B_1\) and \(B_2\), denoted \(B_1 \oplus B_2\), as the \((m + n) \times (m + n)\) matrix \(A\) such that
  \[
    A_{i j} = \begin{dcases}
      (B_1)_{i j}             & \text{for } i, j \in \set{1, \dots, m}         \\
      (B_2)_{(i - m) (j - m)} & \text{for } i, j \in \set{m + 1, \dots, m + n} \\
      0                       & \text{otherwise}
    \end{dcases}.
  \]
  If \(\seq{B}{1,,k}\) are square matrices with entries from \(\F\), then we define the \textbf{direct sum} of \(\seq{B}{1,,k}\) recursively by
  \[
    \seq[\oplus]{B}{1,,k} = (\seq[\oplus]{B}{1,,k-1}) \oplus B_k.
  \]
  If \(A = \seq[\oplus]{B}{1,,k}\), then we often write
  \[
    A = \begin{pmatrix}
      B_1    & \zm    & \cdots & \zm    \\
      \zm    & B_2    & \cdots & \zm    \\
      \vdots & \vdots &        & \vdots \\
      \zm    & \zm    & \cdots & B_k
    \end{pmatrix}.
  \]
\end{defn}

\begin{thm}\label{5.25}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and let \(\seq{\W}{1,,k}\) be \(\T\)-invariant subspaces of \(\V\) over \(\F\) such that \(\V = \seq[\oplus]{\W}{1,,k}\).
  For each \(i \in \set{1, \dots, k}\), let \(\beta_i\) be an ordered basis for \(\W_i\), and let \(\beta = \bigcup_{i = 1}^k \beta_i\).
  Let \(A = [\T]_{\beta}\) and \(B_i = [\T_{\W_i}]_{\beta_i}\) for \(i \in \set{1, \dots, k}\).
  Then \(A = \seq[\oplus]{B}{1,,k}\).
\end{thm}

\begin{proof}[\pf{5.25}]
  We use induction on \(k\).
  The case for \(k = 1\) is trivial.
  Suppose inductively that the statement is true for some \(k \geq 1\).
  We need to show that it is also true for \(k + 1\).
  So let \(\V\) be a vector space over \(\F\), let \(\T \in \ls(\V)\) and let \(\seq{\W}{1,,k+1}\) be \(\T\)-invariant subspaces of \(\V\) over \(\F\) such that \(\V = \seq[\oplus]{\W}{1,,k+1}\).
  For each \(i \in \set{1, \dots, k + 1}\), let \(\beta_i\) be an ordered basis for \(\W_i\) over \(\F\).
  By \cref{5.10}(a)(d) we know that \(\beta = \bigcup_{i = 1}^{k + 1} \beta_i\) is an ordered basis for \(\V\) over \(\F\).
  If we let \(\W = \seq[\oplus]{\W}{2,,k+1}\), then we have \(\V = \W_1 \oplus \W\) and by \cref{ex:5.4.34} we have
  \[
    [\T]_{\beta} = \begin{pmatrix}
      [\T_{\W_1}]_{\beta_1} & \zm_1                                       \\
      \zm_2                 & [\T_{\W}]_{\bigcup_{i = 2}^{k + 1} \beta_i}
    \end{pmatrix}
  \]
  By induction hypothesis we have
  \[
    [\T_{\W}]_{\bigcup_{i = 2}^{k + 1} \beta_i} = \begin{pmatrix}
      [\T_{\W_2}]_{\beta_2} & \zm                   & \cdots & \zm                               \\
      \zm                   & [\T_{\W_3}]_{\beta_3} & \cdots & \zm                               \\
      \vdots                & \vdots                &        & \vdots                            \\
      \zm                   & \zm                   & \cdots & [\T_{\W_{k + 1}}]_{\beta_{k + 1}}
    \end{pmatrix}.
  \]
  Thus by \cref{5.4.5} we see that
  \[
    [\T]_{\beta} = \begin{pmatrix}
      [\T_{\W_1}]_{\beta_1} & \zm                   & \cdots & \zm                               \\
      \zm                   & [\T_{\W_2}]_{\beta_2} & \cdots & \zm                               \\
      \vdots                & \vdots                &        & \vdots                            \\
      \zm                   & \zm                   & \cdots & [\T_{\W_{k + 1}}]_{\beta_{k + 1}}
    \end{pmatrix} = [\T_{\W_1}]_{\beta_1} \oplus \cdots \oplus [\T_{\W_{k + 1}}]_{\beta_{k + 1}}
  \]
  and this closes the induction.
\end{proof}

\exercisesection

\setcounter{ex}{3}
\begin{ex}\label{ex:5.4.4}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\), and let \(\W\) be a \(\T\)-invariant subspace of \(\V\) over \(\F\).
  Prove that \(\W\) is \(g(\T)\)-invariant for any polynomial \(g\).
\end{ex}

\begin{proof}[\pf{ex:5.4.4}]
  Let \(g(x) = a_0 + a_1 x + \cdots + a_n x^n\) for some \(\seq{a}{0,,n} \in \F\).
  Then we have
  \begin{align*}
    \forall w \in \W, g(\T)(w) & = (a_0 \IT[\V] + a_1 \T + \cdots + a_n \T^n)(w) &  & \text{(by \cref{e.0.7})} \\
                               & = a_0 w + a_1 \T(w) + \cdots + a_n \T^n(w)                                    \\
                               & \subseteq \W                                    &  & \text{(by \cref{5.4.1})}
  \end{align*}
  and thus by \cref{5.4.1} \(\W\) is \(g(\T)\)-invariant.
\end{proof}

\begin{ex}\label{ex:5.4.5}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\).
  Prove that the intersection of any collection of \(\T\)-invariant subspaces of \(\V\) over \(\F\) is a \(\T\)-invariant subspace of \(\V\) over \(\F\).
\end{ex}

\begin{proof}[\pf{ex:5.4.5}]
  Let \(K\) be an index set and let \(\set{U_{\alpha} : \alpha \in K}\) be a set of \(\T\)-invariant subspaces of \(\V\) over \(\F\).
  Then we have
  \begin{align*}
             & \forall \alpha \in K, \begin{dcases}
                                       U_{\alpha} \text{ is a subspace of } \V \text{ over } \F \\
                                       \T(U_{\alpha}) \subseteq U_{\alpha}
                                     \end{dcases}                                               &  & \text{(by \cref{5.4.1})}                          \\
    \implies & \begin{dcases}
                 \bigcap_{\alpha \in K} U_{\alpha} \text{ is a subspace of } \V \text{ over } \F \\
                 \T\pa{\bigcap_{\alpha \in K} U_{\alpha}} \subseteq \bigcap_{\alpha \in K} U_{\alpha}
               \end{dcases} &  & \text{(by \cref{1.4})}                                \\
    \implies & \bigcap_{\alpha \in K} U_{\alpha} \text{ is } \T\text{-invariant}.                                        &  & \text{(by \cref{5.4.1})}
  \end{align*}
\end{proof}

\setcounter{ex}{6}
\begin{ex}\label{ex:5.4.7}
  Prove that the restriction of a linear operator \(\T\) to a \(\T\)-invariant subspace is a linear operator on that subspace.
\end{ex}

\begin{proof}[\pf{ex:5.4.7}]
  Let \(\V\) be a vector space over \(\F\), let \(\T \in \ls(\V)\) and let \(\W\) be a \(\T\)-invariant subspace of \(\V\) over \(\F\).
  Let \(x, y \in \W\) and let \(c \in \F\).
  Since
  \begin{align*}
    \T_{\W}(cx + y) & = \T(cx + y)                &  & \text{(by \cref{b.0.4})}    \\
                    & = c \T(x) + \T(y)           &  & \text{(by \cref{2.1.2}(b))} \\
                    & = c \T_{\W}(x) + \T_{\W}(y) &  & \text{(by \cref{b.0.4})}    \\
                    & \in \W,                     &  & \text{(by \cref{1.3})}
  \end{align*}
  by \cref{2.1.2}(b) we know that \(\T_{\W} \in \ls(\W)\).
\end{proof}

\begin{ex}\label{ex:5.4.8}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\) with a \(\T\)-invariant subspace \(\W\) over \(\F\).
  Prove that if \(v\) is an eigenvector of \(\T_{\W}\) with corresponding eigenvalue \(\lambda\), then the same is true for \(\T\).
\end{ex}

\begin{proof}[\pf{ex:5.4.8}]
  We have
  \begin{align*}
             & \T_{\W}(v) = \lambda v &  & \text{(by \cref{5.1.2})} \\
    \implies & \T(v) = \lambda v.     &  & \text{(by \cref{b.0.4})}
  \end{align*}
\end{proof}

\setcounter{ex}{10}
\begin{ex}\label{ex:5.4.11}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\), let \(v\) be a nonzero vector in \(\V\), and let \(\W\) be the \(\T\)-cyclic subspace of \(\V\) generated by \(v\).
  Prove that
  \begin{enumerate}
    \item \(\W\) is \(\T\)-invariant.
    \item Any \(\T\)-invariant subspace of \(\V\) over \(\F\) containing \(v\) also contains \(\W\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:5.4.11}(a)]
  Let \(x \in \W\).
  Then we have
  \begin{align*}
             & \begin{dcases}
                 \exists k \in \N \\
                 \exists \seq{a}{0,,k} \in \F
               \end{dcases} : x = a_0 v + a_1 \T(v) + \cdots + a_k \T^k(v)  &  & \text{(by \cref{5.4.3})} \\
    \implies & \begin{dcases}
                 \exists k \in \N \\
                 \exists \seq{a}{0,,k} \in \F
               \end{dcases} :                                                                \\
             & \T(x) = a_0 \T(x) + a_1 \T^2(v) + \cdots + a_k \T^{k + 1}(v) &  & \text{(by \cref{2.10})}  \\
    \implies & \T(x) \in \W.                                                &  & \text{(by \cref{5.4.3})}
  \end{align*}
  Since \(x\) is arbitrary, we see that \(\T(\W) \subseteq \W\) and by \cref{5.4.1} \(\W\) is \(\T\)-invariant.
\end{proof}

\begin{proof}[\pf{ex:5.4.11}(b)]
  Let \(\vs{X}\) be a \(\T\)-invariant subspace of \(\V\) over \(\F\) containing \(v\).
  Then we have
  \begin{align*}
             & v \in \vs{X}                                                         \\
    \implies & \T(v) \in \vs{X}                       &  & \text{(by \cref{5.4.1})} \\
    \implies & \forall k \in \Z^+, \T^k(v) \in \vs{X} &  & \text{(by \cref{5.4.1})} \\
    \implies & \W \subseteq \vs{X}.                   &  & \text{(by \cref{1.3})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:5.4.12}
  Prove that \(A = \begin{pmatrix}
    B_1 & B_2 \\
    \zm & B_3
  \end{pmatrix}\) in the proof of \cref{5.21}.
\end{ex}

\begin{proof}[\pf{ex:5.4.12}]
  Since \(\W\) is a \(\T\)-invariant subspace of \(\V\) over \(\F\), we know that \(\T(\gamma) \subseteq \W = \spn{\gamma}\).
  Thus if \(A = [\T]_{\beta}\) and \(B_1 = [\T_{\W}]_{\gamma}\), then the first \(\#(\gamma)\) columns of \(A\) can be uniquely express as linear combinations of \(\gamma\) (\cref{1.8}), with coefficients precisely those in the corresponding column of \(B_1\) (\cref{2.2.4}).
\end{proof}

\begin{ex}\label{ex:5.4.13}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\), let \(v\) be a nonzero vector in \(\V\), and let \(\W\) be the \(\T\)-cyclic subspace of \(\V\) generated by \(v\).
  For any \(w \in \V\), prove that \(w \in \W\) iff there exists a polynomial \(g\) such that \(w = g(\T)(v)\).
\end{ex}

\begin{proof}[\pf{ex:5.4.13}]
  We have
  \begin{align*}
         & w \in \W                                                                                  \\
    \iff & \begin{dcases}
             \exists k \in \N \\
             \exists \seq{a}{0,,k} \in \F
           \end{dcases} : w = a_0 v + a_1 \T(v) + \cdots + a_k \T^k(v) &  & \text{(by \cref{5.4.3})} \\
    \iff & \begin{dcases}
             \exists k \in \N \\
             \exists \seq{a}{0,,k} \in \F
           \end{dcases} : \begin{dcases}
                            g(t) = a_0 + a_1 t + \cdots + a_k t^k \\
                            w = g(\T)(v)
                          \end{dcases}.                    &  & \text{(by \cref{e.0.7})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:5.4.14}
  Prove that the polynomial \(g\) of \cref{ex:5.4.13} can always be chosen so that its degree is less than \(\dim(\W)\).
\end{ex}

\begin{proof}[\pf{ex:5.4.14}]
  By \cref{5.22}(a) we see that if \(k = \dim(\W)\) then
  \[
    \set{v, \T(v), \dots, \T^{k - 1}(v)}
  \]
  is a basis for \(\W\) over \(\F\).
  Thus
  \[
    w \in \W \iff \exists \seq{a}{0,,k-1} : \begin{dcases}
      g(t) = a_0 + a_1 t + \cdots + a_{k - 1} t^{k - 1} \\
      w = g(\T)(v)
    \end{dcases}.
  \]
\end{proof}

\setcounter{ex}{15}
\begin{ex}\label{ex:5.4.16}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\).
  \begin{enumerate}
    \item Prove that if the characteristic polynomial of \(\T\) splits, then so does the characteristic polynomial of the restriction of \(\T\) to any \(\T\)-invariant subspace of \(\V\).
    \item Deduce that if the characteristic polynomial of \(\T\) splits, then any nontrivial \(\T\)-invariant subspace of \(\V\) contains an eigenvector of \(\T\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:5.4.16}(a)]
  Let \(\W\) be a \(\T\)-invariant subspace of \(\V\) over \(\F\), and let \(f, g\) be the characteristic polynomials of \(\T, \T_{\W}\), respectively.
  By \cref{5.21} we see that \(g\) divides \(f\), thus there exists some polynomial \(q\) such that \(f = g \cdot q\).
  Since \(f\) splits, we see that \(g\) and \(q\) must split.
\end{proof}

\begin{proof}[\pf{ex:5.4.16}(b)]
  Let \(\W\) be a nontrivial \(\T\)-invariant subspace of \(\V\) over \(\F\), and let \(f, g\) be the characteristic polynomials of \(\T, \T_{\W}\), respectively.
  By \cref{ex:5.4.16}(a) we know that \(f, g\) split and \(f = g \cdot q\) for some polynomial \(q\), thus there exists some \(\lambda \in \F\) such that \(g(\lambda) = f(\lambda) = 0\).
  By \cref{5.2} we see that \(\lambda\) is an eigenvalue of \(\T\) and \(\T_{\W}\).
  Since \(\W \neq \set{\zv}\), by \cref{5.7} we can find some \(v \in \W \setminus \set{\zv}\) such that \(\T_{\W}(v) = \lambda v\).
  Thus \(\T(v) = \lambda v\) and by \cref{5.1.2} \(v\) is an eigenvector of \(\T\).
\end{proof}

\begin{ex}\label{ex:5.4.17}
  Let \(A \in \ms{n}{n}{\F}\).
  Prove that
  \[
    \dim\pa{\spn{\set{I_n, A, A^2, \dots}}} \leq n.
  \]
\end{ex}

\begin{proof}[\pf{ex:5.4.17}]
  Let \(f\) be the characteristic polynomial of \(A\).
  By \cref{5.3} there exist some \(\seq{a}{0,,n-1} \in \F\) such that
  \[
    f(t) = a_0 + a_1 t + \cdots + a_{n - 1} t^{n - 1} + (-1)^n t^n.
  \]
  By Cayley--Hamilton theorem (\cref{5.4.4}) we see that
  \[
    f(A) = a_0 I_n + a_1 A + \cdots + a_{n - 1} A^{n - 1} + (-1)^n A^n = \zm.
  \]
  Thus \(A^n \in \spn{\set{I_n, A, A^2, \dots, A^{n - 1}}}\).
  Since
  \[
    a_0 A + a_1 A^2 + \cdots + a_{n - 1} A^n + (-1)^n A^{n + 1} = \zm,
  \]
  we see that \(A^{n + 1} \in \spn{\set{A, A^2, \dots, A^{n - 1}, A^n}} = \spn{\set{I_n, A, A^2, \dots, A^{n - 1}}}\).
  Thus for all \(m \geq n\), we have \(A^m \in \spn{\set{I_n, A, A^2, \dots, A^{n - 1}}}\).
  By \cref{1.6.8} this means \(\dim\pa{\spn{\set{I_n, A, A^2, \dots}}} = \dim\pa{\spn{\set{I_n, A, A^2, \dots, A^{n - 1}}}} \leq n\).
\end{proof}

\begin{ex}\label{ex:5.4.18}
  Let \(A \in \ms{n}{n}{\F}\) with characteristic polynomial
  \[
    f(t) = (-1)^n t^n + a_{n - 1} t^{n - 1} + \cdots + a_1 t + a_0.
  \]
  \begin{enumerate}
    \item Prove that \(A\) is invertible iff \(a_0 \neq 0\).
    \item Prove that if \(A\) is invertible, then
          \[
            A^{-1} = \frac{-1}{a_0} \pa{(-1)^n A^{n - 1} + a_{n - 1} A^{n - 2} + \cdots + a_1 I_n}.
          \]
    \item Use (b) to compute \(A^{-1}\) for
          \[
            A = \begin{pmatrix}
              1 & 2 & 1  \\
              0 & 2 & 3  \\
              0 & 0 & -1
            \end{pmatrix}.
          \]
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:5.4.18}(a)]
  See \cref{ex:5.1.20}
\end{proof}

\begin{proof}[\pf{ex:5.4.18}(b)]
  We have
  \begin{align*}
             & f(A) = (-1)^n A^n + a_{n - 1} A^{n - 1} + \cdots + a_1 A + a_0 I_n = \zm                   &  & \text{(by \cref{5.4.4})} \\
    \implies & (-1)^n A^n + a_{n - 1} A^{n - 1} + \cdots + a_1 A = -a_0 I_n                                                             \\
    \implies & A \pa{(-1)^n A^{n - 1} + a_{n - 1} A^{n - 2} + \cdots + a_1 I_n} = -a_0 I_n                &  & \text{(by \cref{2.3.5})} \\
    \implies & A \pa{\frac{-1}{a_0} \pa{(-1)^n A^{n - 1} + a_{n - 1} A^{n - 2} + \cdots + a_1 I_n}} = I_n &  & (a_0 \neq 0)             \\
    \implies & \frac{-1}{a_0} \pa{(-1)^n A^{n - 1} + a_{n - 1} A^{n - 2} + \cdots + a_1 I_n} = A^{-1}.    &  & \text{(by \cref{2.4.3})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:5.4.18}(c)]
  We have
  \begin{align*}
    \det(A - t I_3) & = \begin{pmatrix}
                          1 - t & 2     & 1      \\
                          0     & 2 - t & 3      \\
                          0     & 0     & -1 - t
                        \end{pmatrix} &  & \text{(by \cref{4.2.2})}               \\
                    & = (1 - t)(2 - t)(-1 - t)  &  & \text{(by \cref{ex:4.2.23})} \\
                    & = -t^3 + 2t^2 + t - 2
  \end{align*}
  and
  \begin{align*}
    A^{-1} & = \frac{-1}{-2} \pa{-A^2 + 2A + I_3} &  & \text{(by \cref{ex:5.4.18})} \\
           & = \frac{1}{2} \pa{-\begin{pmatrix}
                                    1 & 6 & 6 \\
                                    0 & 4 & 3 \\
                                    0 & 0 & 1
                                  \end{pmatrix} + 2\begin{pmatrix}
                                                     1 & 2 & 1  \\
                                                     0 & 2 & 3  \\
                                                     0 & 0 & -1
                                                   \end{pmatrix} + I_3}               \\
           & = \begin{pmatrix}
                 1 & -1          & -2          \\
                 0 & \frac{1}{2} & \frac{3}{2} \\
                 0 & 0           & -1
               \end{pmatrix}.
  \end{align*}
\end{proof}

\begin{ex}\label{ex:5.4.19}
  Let \(A \in \ms{k}{k}{\F}\)
  \[
    A = \begin{pmatrix}
      0      & 0      & \cdots & 0      & -a_0       \\
      1      & 0      & \cdots & 0      & -a_1       \\
      0      & 1      & \cdots & 0      & -a_2       \\
      \vdots & \vdots &        & \vdots & \vdots     \\
      0      & 0      & \cdots & 0      & -a_{k - 2} \\
      0      & 0      & \cdots & 1      & -a_{k - 1}
    \end{pmatrix}
  \]
  where \(\seq{a}{0,,k-1} \in \F\).
  Prove that the characteristic polynomial of \(A\) is
  \[
    (-1)^k (a_0 + a_1 t + \cdots + a_{k - 1} t^{k - 1} + t^k).
  \]
\end{ex}

\begin{proof}[\pf{ex:5.4.19}]
  We use induction on \(k\).
  For \(k = 1\), we have
  \begin{align*}
    \det(-a_0 - t) & = -a_0 - t         &  & \text{(by \cref{4.2.2})} \\
                   & = (-1)^1 (a_0 + t)
  \end{align*}
  and thus the base case holds.
  Suppose inductively that for some \(k \geq 1\) the statement is true.
  We need to show that the statement is also true for \(k + 1\).
  Let \(A \in \ms{(k + 1)}{(k + 1)}{\F}\) and let \(\seq{a}{0,,k} \in \F\) such that
  \[
    A = \begin{pmatrix}
      0      & 0      & \cdots & 0      & -a_0       \\
      1      & 0      & \cdots & 0      & -a_1       \\
      \vdots & \vdots &        & \vdots & \vdots     \\
      0      & 0      & \cdots & 0      & -a_{k - 1} \\
      0      & 0      & \cdots & 1      & -a_k
    \end{pmatrix}.
  \]
  Let \(B = A - t I_{k + 1}\).
  Then we have
  \begin{align*}
    \det(B) & = \sum_{j = 1}^{k + 1} (-1)^{1 + j} B_{1 j} \det(\tilde{B}_{1 j})                        &  & \text{(by \cref{4.2.2})}         \\
            & = B_{1 1} \det(\tilde{B}_{1 1}) + (-1)^{k + 2} B_{1 (k + 1)} \det(\tilde{B}_{1 (k + 1)})                                       \\
            & = (-t) \begin{pmatrix}
                       -t     & 0      & \cdots & 0  & -a_1     \\
                       1      & -t     & \cdots & 0  & -a_2     \\
                       \vdots & \vdots &        & -t & \vdots   \\
                       0      & 0      & \cdots & 1  & -a_k - t
                     \end{pmatrix}                                                                                \\
            & \quad + (-1)^{k + 2} (-a_0) \begin{pmatrix}
                                            1      & -t     & 0      & \cdots & 0      & 0      \\
                                            0      & 1      & -t     & \cdots & 0      & 0      \\
                                            \vdots & \vdots & \vdots &        & \vdots & \vdots \\
                                            0      & 0      & 0      & \cdots & 1      & -t     \\
                                            0      & 0      & 0      & \cdots & 0      & 1
                                          \end{pmatrix}                                                \\
            & = (-t) (-1)^k (a_1 + a_2 t + \cdots + a_k t^{k - 1} + t^k)                               &  & \text{(by induction hypothesis)} \\
            & \quad + (-1)^{k + 3} a_0                                                                 &  & \text{(by \cref{ex:4.2.23})}     \\
            & = (-1)^{k + 1} (a_0 + a_1 t + \cdots + a_k t^k + t^{k + 1}).
  \end{align*}
  This closes the induction.
\end{proof}

\begin{ex}\label{ex:5.4.20}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\), and suppose that \(\V\) is a \(\T\)-cyclic subspace of itself.
  Prove that if \(\U\) is a linear operator on \(\V\) over \(\F\), then \(\U \T = \T \U\) iff \(\U = g(\T)\) for some polynomial \(g\).
\end{ex}

\begin{proof}[\pf{ex:5.4.20}]
  Since \(\V\) is a \(\T\)-cyclic subspace of itself, by \cref{5.4.3} there exists some \(v \in \V\) such that \(\V = \spn{\set{v, \T(v), \T^2(v), \dots,}}\).

  First suppose that \(\U \T = \T \U\).
  Since \(\U(v) \in \V\), by \cref{ex:5.4.13} there exists some \(g \in \ps{\F}\) such that \(\U(v) = g(\T)(v)\).
  Thus
  \[
    \exists \seq{a}{0,,n} \in \F : \begin{dcases}
      g(t) = a_0 + a_1 t + \cdots + a_n t^n            \\
      g(\T) = a_0 \IT[\V] + a_1 \T + \cdots + a_n \T^n \\
      g(\T)(v) = \U(v)
    \end{dcases}.
  \]
  Then we have
  \begin{align*}
             & \forall w \in \V, \begin{dcases}
                                   \exists k \in \N \\
                                   \exists \seq{b}{0,,k} \in \F
                                 \end{dcases} :                                                               \\
             & w = b_0 v + b_1 \T(v) + \cdots + b_k \T^k(v)                               &  & \text{(by \cref{5.4.3})}    \\
    \implies & \forall w \in \V, \begin{dcases}
                                   \exists k \in \N \\
                                   \exists \seq{b}{0,,k} \in \F
                                 \end{dcases} :                                                               \\
             & \U(w) = \U\pa{b_0 v + b_1 \T(v) + \cdots + b_k \T^k(v)}                                                     \\
             & = b_0 \U(v) + b_1 \U(\T(v)) \cdots + b_k \U(\T^k(v))                       &  & \text{(by \cref{2.1.2}(b))} \\
             & = b_0 \U(v) + b_1 \T(\U(v)) \cdots + b_k \T^k(\U(v))                       &  & (\U \T = \T \U)             \\
             & = b_0 (a_0 v + a_1 \T(v) + \cdots + a_n \T^n(v))                           &  & (g(\T)(v) = \U(v))          \\
             & \quad + b_1 (a_0 \T(v) + a_1 \T^2(v) + \cdots + a_n \T^{n + 1}(v))         &  & \text{(by \cref{2.1.2}(b))} \\
             & \quad + \cdots                                                                                              \\
             & \quad + b_k (a_0 \T^k(v) + a_1 \T^{k + 1}(v) + \cdots + a_n \T^{n + k}(v)) &  & \text{(by \cref{2.1.2}(b))} \\
             & = a_0 (b_0 v + b_1 \T(v) + \cdots + b_k \T^k(v))                           &  & \text{(by \cref{1.2.1})}    \\
             & \quad + a_1 \T\pa{b_0 v + b_1 \T(v) + \cdots + b_k \T^k(v)}                &  & \text{(by \cref{2.1.2}(b))} \\
             & \quad + \cdots                                                                                              \\
             & \quad + a_n \T^n\pa{b_0 v + b_1 \T(v) + \cdots + b_k \T^k(v)}              &  & \text{(by \cref{2.1.2}(b))} \\
             & = (a_0 + a_1 \T + \cdots + a_n \T^n)(w)                                                                     \\
             & = g(\T)(w)                                                                                                  \\
    \implies & \U = g(\T).
  \end{align*}

  Now suppose that \(\U = g(\T)\) for some \(g \in \ps{\F}\).
  Let \(g(t) = a_0 + a_1 t + \cdots + a_n t^n\) for some \(\seq{a}{0,,n} \in \F\).
  Then we have
  \begin{align*}
    \forall w \in \V, (\U \T)(w) & = (g(\T)(\T))(w)                                                                    \\
                                 & = (a_0 \T + a_1 \T^2 + \cdots + a_n \T^{n + 1})(w)  &  & \text{(by \cref{2.10}(a))} \\
                                 & = (\T(a_0 \IT[\V] + a_1 \T + \cdots + a_n \T^n))(w) &  & \text{(by \cref{2.10}(a))} \\
                                 & = ((\T) g(\T))(w)                                                                   \\
                                 & = (\T \U)(w)
  \end{align*}
  and thus \(\U \T = \T \U\).
\end{proof}

\begin{ex}\label{ex:5.4.21}
  Let \(\T\) be a linear operator on a two-dimensional vector space \(\V\) over \(\F\).
  Prove that either \(\V\) is a \(\T\)-cyclic subspace of itself or \(\T = c \IT[\V]\) for some \(c \in \F\).
\end{ex}

\begin{proof}[\pf{ex:5.4.21}]
  If \(\T = \zT\), then we see that \(\T = 0 \IT[\V]\).
  So suppose that \(\T \neq \zT\).
  Now we split into two cases:
  \begin{itemize}
    \item If there exists some \(v \in \V\) such that \(\dim(\spn{\set{v, \T(v)}}) = 2\), then by \cref{1.6.15}(b) we have \(\V = \spn{\set{v, \T(v)}}\).
          By \cref{5.4.3} we see that \(\V\) is a \(\T\)-cyclic subspace of itself generated by \(v\).
    \item If there does not exist such \(v \in \V\), then we have \(\dim(\spn{\set{v, \T(v)}}) < 2\) for all \(v \in \V\).
          Since \(\T \neq \zT\), there exist some \(w_1 \in \V\) such that \(\T(w_1) \neq \zv\).
          Fix such \(w_1\).
          Since \(\T(w_1) \in \spn{\set{w_1}} \setminus \set{\zv}\), there exists some \(c_1 \in \F \setminus \set{0}\) such that \(\T(w_1) = c_1 w_1\).
          Now we extend \(\set{w_1}\) to a basis \(\set{\seq{w}{1,2}}\) for \(\V\) over \(\F\).
          By our hypothesis we have
          \[
            \T(w_2) \in \spn{\set{w_2}} \quad \text{and} \quad \T(w_1 + w_2) \in \spn{\set{w_1 + w_2}}.
          \]
          This means \(\T(w_2) = c_2 w_2\) and \(\T(w_1 + w_2) = c_3 (w_1 + w_2)\) for some \(c_2, c_3 \in \F\).
          Now observe that
          \begin{align*}
            \T(w_1 + w_2) & = c_3 (w_1 + w_2)                                  \\
                          & = c_3 w_1 + c_3 w_2  &  & \text{(by \cref{1.2.1})} \\
                          & = \T(w_1) + \T(w_2)  &  & \text{(by \cref{2.1.1})} \\
                          & = c_1 w_1 + c_2 w_2.
          \end{align*}
          Since \(\set{w_1, w_2}\) is linearly independent, we know that
          \[
            (c_3 - c_1) w_1 + (c_3 - c_2) w_2 = \zv \implies c_1 = c_3 = c_2.
          \]
          Thus by setting \(c = c_1\) we see that \(\T = c \IT[\V]\).
  \end{itemize}
  From all cases above we see that \cref{ex:5.4.21} is true.
\end{proof}

\begin{ex}\label{ex:5.4.22}
  Let \(\T\) be a linear operator on a two-dimensional vector space \(\V\) over \(\F\) and suppose that \(\T \neq c \IT[\V]\) for any scalar \(c \in \F\).
  Show that if \(\U \in \ls(\V)\) such that \(\U \T = \T \U\), then \(\U = g(\T)\) for some polynomial \(g\).
\end{ex}

\begin{proof}[\pf{ex:5.4.22}]
  By \cref{ex:5.4.21} we see that \(\V\) is a \(\T\)-cyclic subspace of itself.
  By \cref{ex:5.4.20} we conclude that \(\U = g(\T)\) for some \(g \in \ps{\F}\).
\end{proof}

\begin{ex}\label{ex:5.4.23}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and let \(\W\) be a \(\T\)-invariant subspace of \(\V\) over \(\F\).
  Suppose that \(\seq{v}{1,,k}\) are eigenvectors of \(\T\) corresponding to distinct eigenvalues.
  Prove that if \(\seq[+]{v}{1,,k} \in \W\), then \(v_i \in \W\) for all \(i \in \set{1, \dots, k}\).
\end{ex}

\begin{proof}[\pf{ex:5.4.23}]
  We use induction on \(k\).
  The case for \(k = 1\) is trivial.
  So suppose inductively that for some \(k \geq 1\) the statement is true.
  We need to show that for \(k + 1\) the statement is true.
  For each \(i \in \set{1, \dots, k + 1}\), let \(v_i\) be an eigenvector of \(\T\) with corresponding eigenvalue \(\lambda_i\) such that \(\seq{\lambda}{1,,k+1}\) are distinct and \(\seq[+]{v}{1,,k+1} \in \W\).
  Since \(\W\) is \(\T\)-invariant, by \cref{1.3} we know that \(\W\) is also \((\T - \lambda_{k + 1} \IT[\V])\)-invariant.
  Thus we see that
  \begin{align*}
             & \seq[+]{v}{1,,k+1} \in \W                                                                                            \\
    \implies & (\T - \lambda_{k + 1} \IT[\V])(\seq[+]{v}{1,,k+1}) \in \W                              &  & \text{(by \cref{5.4.1})} \\
    \implies & (\lambda_1 - \lambda_{k + 1}) v_1 + \cdots + (\lambda_k - \lambda_{k + 1}) v_k \in \W. &  & \text{(by \cref{5.1.2})}
  \end{align*}
  Since \(\lambda_i - \lambda_{k + 1} \neq 0\) for all \(i \in \set{1, \dots, k}\), by \cref{5.2.4} we see that \((\lambda_i - \lambda_{k + 1}) v_i\) is an eigenvector of \(\T\) and \((\lambda_i - \lambda_{k + 1}) v_i \in E_{\lambda_i}\).
  Thus by induction hypothesis we see that \((\lambda_i - \lambda_{k + 1}) v_i \in \W\) for all \(i \in \set{1, \dots, k}\).
  Then we have
  \begin{align*}
             & \forall i \in \set{1, \dots, k}, \lambda_i - \lambda_{k + 1} \neq 0                                                                           \\
    \implies & \forall i \in \set{1, \dots, k}, \frac{\lambda_i - \lambda_{k + 1}}{\lambda_i - \lambda_{k + 1}} v_i = v_i \in \W &  & \text{(by \cref{1.3})} \\
    \implies & \seq[+]{v}{1,,k} \in \W                                                                                           &  & \text{(by \cref{1.3})} \\
    \implies & \seq[+]{v}{1,,k+1} + -(\seq[+]{v}{1,,k}) = v_{k + 1} \in \W                                                       &  & \text{(by \cref{1.3})}
  \end{align*}
  and this closes the induction.
\end{proof}

\begin{ex}\label{ex:5.4.24}
  Prove that the restriction of a diagonalizable linear operator \(\T\) to any nontrivial \(\T\)-invariant subspace is also diagonalizable.
\end{ex}

\begin{proof}[\pf{ex:5.4.24}]
  Let \(\V\) be a finite-dimensional vector space over \(\F\), let \(\T \in \ls(\V)\) such that \(\T\) is diagonalizable and let \(\W\) be a nontrivial \(\T\)-invariant subspace of \(\V\) over \(\F\).
  By \cref{5.1.1} there exist an ordered basis \(\beta\) such that \([\T]_{\beta}\) is diagonal matrix.
  We claim that \(\W \cap \beta \neq \varnothing\) and \(\W \cap \beta\) is an ordered basis for \(\W\).
  If not, then we must have \(\W = \set{\zv}\), which contradict to our hypothesis that \(\W\) is nontrivial.
  Let \(\beta_{\W} = \W \cap \beta\).
  Since \(\beta\) is consist of eigenvectors of \(\T\) and \(\W\) is \(\T\)-invariant, by \cref{5.1.2,5.4.1} we know that \(\T(\beta_{\W}) = \T_{\W}(\beta_{\W}) \subseteq \W\) and \(\beta_{\W}\) is consist of eigenvectors of \(\T_{\W}\).
  Thus \([\T_{\W}]_{\beta_{\W}}\) is diagonal matrix and by \cref{5.1.1} \(\T_{\W}\) is diagonalizable.
\end{proof}

\begin{ex}\label{ex:5.4.25}
  \begin{enumerate}
    \item Prove the converse to \cref{ex:5.2.18}(a):
          If \(\T\) and \(\U\) are diagonalizable linear operators on a finite-dimensional vector space \(\V\) over \(\F\) such that \(\U \T = \T \U\), then \(\T\) and \(\U\) are simultaneously diagonalizable.
          (See the definitions in the \cref{5.2.8}.)
    \item State and prove a matrix version of (a).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:5.4.25}(a)]
  Let \(\lambda\) be an eigenvalue of \(\T\) and let \(E_{\lambda} = \ns{\T - \lambda \IT[\V]}\).
  We claim that \(E_{\lambda}\) is \(\U\)-invariant.
  This is true since
  \begin{align*}
             & \forall v \in E_{\lambda}, \T(v) = \lambda v                                     &  & \text{(by \cref{5.1.2})} \\
    \implies & \forall v \in E_{\lambda}, \U(\T(v)) = \U(\lambda v) = \lambda \U(v) = \T(\U(v)) &  & (\U \T = \T \U)          \\
    \implies & \forall v \in E_{\lambda}, \U(v) \in E_{\lambda}                                 &  & \text{(by \cref{5.2.4})} \\
    \implies & \U(E_{\lambda}) \subseteq E_{\lambda}.
  \end{align*}
  Since \(\U\) is diagonalizable and \(E_{\lambda}\) is \(\U\)-invariant, by \cref{ex:5.4.24} we see that \(\U_{E_{\lambda}}\) is diagonalizable.
  Thus we can find an ordered basis \(\beta_{\lambda}\) for \(E_{\lambda}\) over \(\F\) such that \([\U]_{\beta_{\lambda}}\) is a diagonal matrix.
  Note that since \(\beta_{\lambda}\) is an ordered basis for \(E_{\lambda}\) over \(\F\), \([\T]_{\beta_{\lambda}}\) is also a diagonal matrix.
  Applying the above argument to every eigenvalue of \(\T\) we can find an ordered basis \(\beta\) over \(\F\) such that \(\beta\) is an union of bases of eigenspaces (by \cref{5.9}(b)) and both \([\U]_{\beta}\) and \([\T]_{\beta}\) are diagonal matrix.
  Thus by \cref{5.2.8} \(\T, \U\) are simultaneously diagonalizable.
\end{proof}

\begin{proof}[\pf{ex:5.4.25}(b)]
  Let \(A, B \in \ms{n}{n}{\F}\) be diagonalizable matrix.
  We claim that if \(AB = BA\), then \(A, B\) are simultaneously diagonalizable.
  This is true since
  \begin{align*}
             & AB = BA                                                                                   \\
    \implies & \L_A \L_B = \L_{AB} = \L_{BA} = \L_B \L_A            &  & \text{(by \cref{2.15}(e))}      \\
    \implies & \L_A, \L_B \text{ are simultaneously diagonalizable} &  & \text{(by \cref{ex:5.4.25}(a))} \\
    \implies & A, B \text{ are simultaneously diagonalizable}.      &  & \text{(by \cref{ex:5.2.17}(a))}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:5.4.26}
  Let \(\T\) be a linear operator on an \(n\)-dimensional vector space \(\V\) over \(\F\) such that \(\T\) has \(n\) distinct eigenvalues.
  Prove that \(\V\) is a \(\T\)-cyclic subspace of itself.
\end{ex}

\begin{proof}[\pf{ex:5.4.26}]
  For each \(i \in \set{1, \dots, n}\), let \(v_i\) be an eigenvector of \(\T\) corresponding to eigenvalue \(\lambda_i \in \F\).
  Let \(v = \seq[+]{v}{1,,n}\) and let \(\W\) be the \(\T\)-cyclic subspace generated by \(v\).
  By \cref{ex:5.4.11}(a) we know that \(\W\) is \(\T\)-invariant, thus by \cref{ex:5.4.23} we see that \(v_i \in \W\) for all \(i \in \set{1, \dots, n}\).
  But by \cref{5.5} we know that \(\set{\seq{v}{1,,n}}\) is a basis for \(\V\) over \(\F\), thus \(\W = \V\) and by \cref{5.4.3} \(\V\) is a \(\T\)-cyclic subspace of itself.
\end{proof}

\begin{defn}\label{5.4.6}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\), and let \(\W\) be a \(\T\)-invariant subspace of \(\V\) over \(\F\).
  Define \(\overline{\T} : \V / \W \to \V / \W\) by
  \[
    \overline{\T}(v + \W) = \T(v) + \W \quad \text{for any } v + \W \in \V / \W.
  \]
\end{defn}

\begin{ex}\label{ex:5.4.27}
  Using the symbols in \cref{5.4.6}.
  \begin{enumerate}
    \item Prove that \(\overline{\T}\) is well defined.
          That is, show that \(\overline{\T}(v + \W) = \overline{\T}(v' + \W)\) whenever \(v + \W = v' + \W\).
    \item Prove that \(\overline{\T}\) is a linear operator on \(\V / \W\).
    \item Let \(\eta : \V \to \V / \W\) be the linear transformation defined in \cref{ex:2.1.40} by \(\eta(v) = v + \W\).
          prove that \(\eta \T = \overline{\T} \eta\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:5.4.27}(a)]
  We have
  \begin{align*}
             & v + \W = v' + \W                                                                     \\
    \implies & v - v' \in \W                                   &  & \text{(by \cref{ex:1.3.31}(b))} \\
    \implies & \T(v - v') \in \W                               &  & \text{(by \cref{5.4.1})}        \\
    \implies & \T(v) - \T(v') \in \W                           &  & \text{(by \cref{2.1.2}(c))}     \\
    \implies & \T(v) + \W = \T(v') + \W                        &  & \text{(by \cref{ex:1.3.31}(b))} \\
    \implies & \overline{\T}(v + \W) = \overline{\T}(v' + \W). &  & \text{(by \cref{5.4.6})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:5.4.27}(b)]
  Let \(x + \W, y + \W \in \V / \W\) and let \(c \in \F\).
  Then we have
  \begin{align*}
    \overline{\T}(c(x + \W) + (y + \W)) & = \overline{\T}((cx + y) + \W)                    &  & \text{(by \cref{ex:1.3.31}(c))} \\
                                        & = \T(cx + y) + \W                                 &  & \text{(by \cref{5.4.6})}        \\
                                        & = (c \T(x) + \T(y)) + \W                          &  & \text{(by \cref{2.1.2}(b))}     \\
                                        & = c (\T(x) + \W) + (\T(y) + \W)                   &  & \text{(by \cref{ex:1.3.31}(c))} \\
                                        & = c \overline{\T}(x + \W) + \overline{\T}(y + \W) &  & \text{(by \cref{5.4.6})}
  \end{align*}
  and thus by \cref{2.1.2}(b) \(\overline{\T} \in \ls(\V / \W)\).
\end{proof}

\begin{proof}[\pf{ex:5.4.27}(c)]
  We have
  \begin{align*}
    \forall v \in \V, (\eta \T)(v) & = \eta(\T(v))                                               \\
                                   & = \T(v) + \W              &  & \text{(by \cref{ex:2.1.40})} \\
                                   & = \overline{\T}(v + \W)   &  & \text{(by \cref{5.4.6})}     \\
                                   & = \overline{\T}(\eta(v))  &  & \text{(by \cref{ex:2.1.40})} \\
                                   & = (\overline{\T} \eta)(v)
  \end{align*}
  and thus \(\eta \T = \overline{\T} \eta\).
\end{proof}

\begin{ex}\label{ex:5.4.28}
  Let \(\V\) be an \(n\)-dimensional vector space over \(\F\), let \(\T \in \ls(\V)\) and let \(\W\) be a nonzero \(\T\)-invariant subspace of \(\V\) over \(\F\).
  Let \(f, g, h\) be the characteristic polynomials of \(\T, \T_{\W}, \overline{\T}\), respectively.
  Prove that \(f = gh\).
\end{ex}

\begin{proof}[\pf{ex:5.4.28}]
  Let \(\gamma = \set{\seq{v}{1,,k}}\) be an ordered basis for \(\W\) over \(\F\).
  By \cref{1.6.19} we can extend \(\gamma\) to an ordered basis \(\beta = \set{\seq{v}{1,,k,k+1,,n}}\) for \(\V\) over \(\F\).
  Let \(\alpha = \set{v_{k + 1} + \W, \dots, v_n + \W}\).
  By \cref{ex:1.6.35}(a) we know that \(\alpha\) is an ordered basis for \(\V / \W\) over \(\F\).
  Since \(\W\) is \(\T\)-invariant, by \cref{ex:5.4.12} we see that
  \[
    [\T]_{\beta} = \begin{pmatrix}
      [\T_{\W}]_{\gamma} & B_1 \\
      \zm                & B_2
    \end{pmatrix}
  \]
  where \(B_1 \in \ms{k}{(n - k)}{\F}\) and \(B_2 \in \ms{(n - k)}{(n - k)}{\F}\).
  If we can show that \(B_2 = [\overline{\T}]_{\alpha}\), then by \cref{ex:4.3.21} we see that \(f = gh\).

  Let \(i \in \set{1, \dots, k}\).
  Since
  \begin{align*}
    \overline{\T}(v_i + \W) & = \T(v_i) + \W                                                                  &  & \text{(by \cref{5.4.6})}                \\
                            & = \pa{\sum_{j = 1}^n ([\T]_{\beta})_{j i} v_j} + \W                             &  & \text{(by \cref{2.2.4})}                \\
                            & = \pa{\sum_{j = 1}^k (B_1)_{j i} v_j + \sum_{j = k + 1}^n (B_2)_{j i} v_j} + \W                                              \\
                            & = \pa{\sum_{j = k + 1}^n (B_2)_{j i} v_j} + \W                                  &  & (\sum_{j = 1}^k (B_1)_{j i} v_j \in \W) \\
                            & = \sum_{j = k + 1}^n (B_2)_{j i} (v_j + \W),                                    &  & \text{(by \cref{ex:1.3.31}(c))}
  \end{align*}
  by \cref{2.2.4} we see that \([\overline{\T}]_{\alpha} = B_2\).
  Thus we have \(f = gh\).
\end{proof}

\begin{ex}\label{ex:5.4.29}
  Let \(\V\) be a finite-dimensional vector space over \(\F\), let \(\T \in \ls(\V)\) and let \(\W\) be a nonzero \(\T\)-invariant subspace of \(\V\) over \(\F\).
  Prove that if \(\T\) is diagonalizable, then so is \(\overline{\T}\).
\end{ex}

\begin{proof}[\pf{ex:5.4.29}]
  By \cref{ex:5.4.24} we know that \(\T_{\W}\) is diagonalizable, thus there exists an ordered basis \(\gamma\) for \(\W\) over \(\F\) such that \([\T_{\W}]_{\gamma}\) is a diagonal matrix.
  By \cref{1.6.19} we can extend \(\gamma\) to an ordered basis \(\beta\) for \(\V\) over \(\F\) such that \([\T]_{\beta}\) is a diagonal matrix.
  If we let \(\alpha = \eta(\beta \setminus \gamma)\) (\cref{ex:5.4.27}(c)), then by \cref{ex:1.6.35}(a) we know that \(\alpha\) is an ordered basis for \(\V / \W\) over \(\F\).
  By \cref{ex:5.4.28} we see that
  \[
    [\T]_{\beta} = \begin{pmatrix}
      [\T_{\W}]_{\gamma} & \zm_1                    \\
      \zm_2              & [\overline{\T}]_{\alpha}
    \end{pmatrix}
  \]
  where \(\seq{\zm}{1,2}\) are zero matrix.
  Thus \([\overline{\T}]_{\alpha}\) is a diagonal matrix and by \cref{5.1.1} \(\overline{\T}\) is diagonalizable.
\end{proof}

\begin{ex}\label{ex:5.4.30}
  Let \(\V\) be a finite-dimensional vector space over \(\F\), let \(\T \in \ls(\V)\) and let \(\W\) be a nonzero \(\T\)-invariant subspace of \(\V\) over \(\F\).
  Prove that if both \(\T_{\W}\) and \(\overline{\T}\) are diagonalizable and have no common eigenvalues, then \(\T\) is diagonalizable.
\end{ex}

\begin{proof}[\pf{ex:5.4.30}]
  Since \(\T_{\W}\) is diagonalizable, by \cref{5.1.1} there exists an ordered basis \(\gamma = \set{\seq{v}{1,,k}}\) for \(\W\) over \(\F\) such that \([\T_{\W}]_{\gamma}\) is a diagonal matrix.
  Similarly, there exists an ordered basis \(\alpha' = \set{v_{k + 1} + \W, \dots, v_n + \W}\) for \(\V / \W\) over \(\F\) such that \([\overline{\T}]_{\alpha'}\) is a diagonal matrix.
  Let \(\alpha = \set{\seq{v}{k+1,,n}}\).
  We fix one such \(\gamma\) and \(\alpha\).

  Now we find an ordered basis for \(\V\) over \(\F\) consist of eigenvectors of \(\T\).
  For each \(i \in \set{1, \dots, k}\), let \(\lambda_i\) be the eigenvalue of \(v_i\).
  For each \(j \in \set{k + 1, \dots, n}\), let \(\lambda_j\) be the eigenvalue of \(v_j + \W\).
  Since
  \begin{align*}
             & \forall v_j \in \alpha, \overline{\T}(v_j + \W) = \lambda_j (v_j + \W)                                                 &  & \text{(by \cref{5.1.2})}        \\
    \implies & \forall v_j \in \alpha, \T(v_j) + \W = (\lambda_j v_j) + \W                                                            &  & \text{(by \cref{5.4.6})}        \\
    \implies & \forall v_j \in \alpha, \T(v_j) - \lambda_j v_j \in \W = \spn{\gamma}                                                  &  & \text{(by \cref{ex:1.3.31}(b))} \\
    \implies & \forall v_j \in \alpha, \exists a_{j 1}, \dots, a_{j k} \in \F : \T(v_j) - \lambda_j v_j = \sum_{i = 1}^k a_{j i} v_i, &  & \text{(by \cref{1.4.3})}
  \end{align*}
  if we define
  \[
    \forall j \in \set{k + 1, \dots, n}, v_j' = v_j + \sum_{i = 1}^k \frac{a_{j i}}{\lambda_j - \lambda_i} v_i,
  \]
  (note that \(\lambda_j - \lambda_i \neq 0\) by hypothesis)
  then we have
  \begin{align*}
    \T(v_j') & = \T\pa{v_j + \sum_{i = 1}^k \frac{a_{j i}}{\lambda_j - \lambda_i} v_i}                                                                            \\
             & = \T(v_j) + \sum_{i = 1}^k \frac{a_{j i}}{\lambda_j - \lambda_i} \T(v_i)                                          &  & \text{(by \cref{2.1.2}(d))} \\
             & = \lambda_j v_j + \sum_{i = 1}^k a_{j i} v_i + \sum_{i = 1}^k \frac{a_{j i} \lambda_i}{\lambda_j - \lambda_i} v_i &  & \text{(by \cref{5.1.2})}    \\
             & = \lambda_j v_j + \sum_{i = 1}^k \frac{a_{j i} \lambda_j}{\lambda_j - \lambda_i} v_i                                                               \\
             & = \lambda_j \pa{v_j + \sum_{i = 1}^k \frac{a_{j i}}{\lambda_j - \lambda_i} v_i}                                                                    \\
             & = \lambda_j v_j'.
  \end{align*}
  Thus the set \(\omega = \set{v_{k + 1}', \dots, v_n'}\) is consist of eigenvectors of \(\T\).
  Now we show that \(\beta = \gamma \cup \omega\) is an ordered basis for \(\V\) over \(\F\) which is consist of eigenvectors of \(\T\).
  Since \(\gamma\) is consist of eigenvectors of \(\T_{\W}\), by \cref{b.0.4} we know that \(\gamma\) is consist of eigenvectors of \(\T\).
  From the proof above we see that \(\beta\) is consist of eigenvectors of \(\T\).
  So we only need to show that \(\beta\) is an ordered basis for \(\V\) over \(\F\).

  We start by showing that \(\spn{\beta} = \V\).
  Clearly we have \(\spn{\beta} \subseteq \V\), so we show that \(\V \subseteq \spn{\beta}\).
  Let \(x \in \V\).
  Then \(x + \W \in \V / \W = \spn{\alpha'}\) and there exist some \(\seq{b}{k+1,,n} \in \F\) such that
  \begin{align*}
    x + \W & = \sum_{j = k + 1}^n b_j (v_j + \W)                                                                                                                        \\
           & = \pa{\sum_{j = k + 1}^n b_j v_j} + \W                                                                  &  & \text{(by \cref{ex:1.3.31}(c))}               \\
           & = \pa{\sum_{j = k + 1}^n b_j \pa{v_j' - \sum_{i = 1}^k \frac{a_{j i}}{\lambda_j - \lambda_i} v_i}} + \W                                                    \\
           & = \pa{\sum_{j = k + 1}^n b_j v_j'} + \W.                                                                &  & (\forall i \in \set{1, \dots, k}, v_i \in \W)
  \end{align*}
  By \cref{ex:1.3.31}(b) we know that \(x - \sum_{j = k + 1}^n b_j v_j' \in \W\).
  Thus there exist some \(\seq{b}{1,,k} \in \F\) such that
  \[
    \sum_{i = 1}^k b_i v_i = x - \sum_{j = k + 1}^n b_j v_j.
  \]
  Therefore \(x \in \spn{\beta}\).
  Since \(x\) is arbitrary, we have \(\V \subseteq \spn{\beta}\) and thus \(\V = \spn{\beta}\).

  Now we show that \(\beta\) is linearly independent.
  Clearly \(\gamma\) is linearly independent.
  If we can show that \(\omega\) is linearly independent, then by \cref{5.8} (and the fact that \(\set{\lambda_i : i \in \set{1, \dots k}} \cap \set{\lambda_j : j \in \set{k + 1, \dots, n}} = \varnothing\)) we see that \(\beta\) is linearly independent.
  Let \(\seq{b}{k+1,,n} \in \F\) such that \(\sum_{j = k + 1}^n b_j v_j' = \zv\).
  Since
  \begin{align*}
    \zv + \W & = \pa{\sum_{j = k + 1}^n b_j v_j'} + \W                                                                                                                   \\
             & = \pa{\sum_{j = k + 1}^n b_j \pa{v_j + \sum_{i = 1}^n \frac{a_{j i}}{\lambda_j - \lambda_i} v_i}} + \W                                                    \\
             & = \sum_{j = k + 1}^n b_j v_j + \W                                                                      &  & (\forall i \in \set{1, \dots, k}, v_i \in \W) \\
             & = \sum_{j = k + 1}^n b_j (v_j + \W)                                                                    &  & \text{(by \cref{ex:1.3.31}(c))}
  \end{align*}
  implies \(\seq[=]{b}{k+1,,n} = 0\), by \cref{1.5.3} we know that \(\omega\) is linearly independent.
  Thus we conclude that \(\beta\) is an ordered basis for \(\V\) over \(\F\) consist of eigenvectors of \(\T\), therefore by \cref{5.1.1} \(\T\) is diagonalizable.
\end{proof}

\setcounter{ex}{31}
\begin{ex}\label{ex:5.4.32}
  Prove the converse to \cref{ex:5.2.9}(a):
  If the characteristic polynomial of \(\T\) splits, then there is an ordered basis \(\beta\) for \(\V\) over \(\F\) such that \([\T]_{\beta}\) is an upper triangular matrix.
\end{ex}

\begin{proof}[\pf{ex:5.4.32}]
  We use induction on \(n\).
  For \(n = 1\), since \(\dim(\V) = 1\), there exists some \(v \in \V \setminus \set{\zv}\) such that \(\beta = \set{v}\) is an ordered basis for \(\V\) over \(\F\).
  Clearly \([\T]_{\beta}\) is an upper triangular matrix, thus the base case holds.
  Suppose inductively that the statement is true for some \(n \geq 1\).
  We need to show that the statement is also true for \(n + 1\).
  So let \(\dim(\V) = n + 1\) and let \(\T \in \ls(\V)\) such that the characteristic polynomial of \(\T\) splits.
  Let \(f\) be the characteristic polynomial of \(\T\).
  Since \(f\) splits, by \cref{5.2.2} there exists some \(\lambda \in \F\) such that \(f(\lambda) = 0\).
  By \cref{5.7} we have \(\dim(\ns{\T - \lambda \IT[\V]}) \geq 1\), thus there exists a \(v_1 \in \V \setminus \set{\zv}\) such that \(\T(v_1) = \lambda v_1\).
  Let \(\W = \spn{\set{v_1}}\).
  Clearly \(\W\) is a \(\T\)-invariant subspace of \(\V\) over \(\F\).
  If we define \(\overline{\T}\) as in \cref{5.4.6} and let \(g\) and \(h\) be the characteristic polynomial of \(\T_{\W}\) and \(\overline{\T}\), then by \cref{ex:5.4.28} we know that \(f = g h\).
  Since \(f\) splits, we know that \(g, h\) splits.
  By \cref{ex:1.6.35}(b) we know that \(\dim(\V / \W) = \dim(\V) - \dim(\W) = n + 1 - 1 = n\), thus by induction hypothesis there exists an ordered basis \(\alpha = \set{v_2 + \W, \dots, v_{n + 1} + \W}\) for \(\V / \W\) over \(\F\) such that \([\overline{\T}]_{\alpha}\) is an upper triangular matrix.
  Now we claim that \(\beta = \set{\seq{v}{1,,n+1}}\) is an ordered basis for \(\V\) over \(\F\).
  Since
  \begin{align*}
             & \forall \seq{a}{2,,n+1} \in \F, \sum_{i = 2}^{n + 1} a_i v_i = \zv                                                        \\
    \implies & \zv + \W = \sum_{i = 2}^{n + 1} (a_i v_i) + \W = \sum_{i = 2}^{n + 1} a_i (v_i + \W) &  & \text{(by \cref{ex:1.3.31}(c))} \\
    \implies & \seq[=]{a}{2,,n+1} = 0,                                                              &  & \text{(by \cref{1.5.3})}
  \end{align*}
  we know that \(\set{\seq{v}{2,,n+1}}\) is linearly independent.
  Since
  \begin{align*}
             & \zv + \W \notin \alpha                                         &  & \text{(by \cref{1.5.2})}        \\
    \implies & \forall i \in \set{2, \dots, n + 1}, v_i + \W \neq \zv + \W    &  & \text{(by \cref{ex:1.3.31}(d))} \\
    \implies & \forall i \in \set{2, \dots, n + 1}, v_i - \zv = v_i \notin \W &  & \text{(by \cref{ex:1.3.31}(b))} \\
    \implies & \set{\seq{v}{2,,n+1}} \cap \W = \varnothing,
  \end{align*}
  by \cref{1.7} we know that \(\beta\) is linearly independent.
  Thus by \cref{1.6.15}(b) \(\beta\) is an ordered basis for \(\V\) over \(\F\).
  By \cref{ex:5.4.28} we see that
  \[
    [\T]_{\beta} = \begin{pmatrix}
      [\T_{\W}]_{\set{v_1}} & B                        \\
      \zm                   & [\overline{\T}]_{\alpha}
    \end{pmatrix}
  \]
  where \(B \in \ms{1}{n}{\F}\) and \(\zm \in \ms{n}{1}{\F}\) is a zero matrix.
  Since \([\overline{\T}]_{\alpha}\) is an upper triangular matrix, we see that \([\T]_{\beta}\) is also an upper triangular matrix.
  This closes the induction.
\end{proof}

\begin{ex}\label{ex:5.4.33}
  Let \(\T\) be a linear operator on a vector space \(\V\) over \(\F\), and let \(\seq{\W}{1,,k}\) be \(\T\)-invariant subspaces of \(\V\) over \(\F\).
  Prove that \(\seq[+]{\W}{1,,k}\) is also a \(\T\)-invariant subspace of \(\V\) over \(\F\).
\end{ex}

\begin{proof}[\pf{ex:5.4.33}]
  Let \(\sum_{i = 1}^k v_i \in \sum_{i = 1}^k \W_i\).
  Since \(v_i \in \W_i\) for each \(i \in \set{1, \dots, k}\), by \cref{5.4.1} we know that \(\T(v_i) \in \W_i\).
  Thus by \cref{2.1.2}(d) we have \(\T\pa{\sum_{i = 1} v_i} = \sum_{i = 1}^k \T(v_i) \in \sum_{i = 1}^k \W_i\).
  By \cref{5.4.1} this means \(\sum_{i = 1}^k \W_i\) is \(\T\)-invariant.
\end{proof}

\begin{ex}\label{ex:5.4.34}
  Give a direct proof of \cref{5.25} for the case \(k = 2\).
  (This result is used in the proof of \cref{5.24}.)
\end{ex}

\begin{proof}[\pf{ex:5.4.34}]
  Let \(\V\) be an \(n\)-dimensional vector space over \(\F\) and let \(\W_1, \W_2\) be nontrivial \(\T\)-invariant subspaces of \(\V\) over \(\F\) such that \(\V = \W_1 \oplus \W_2\).
  Let \(\beta_1 = \set{\seq{v}{1,,k}}, \beta_2 = \set{\seq{v}{k+1,,n}}\) be ordered basis for \(\W_1, \W_2\) over \(\F\), respectively.
  By \cref{5.10}(a)(d) we know that \(\beta = \beta_1 \cup \beta_2\) is an ordered basis for \(\V\) over \(\F\).
  Then we have
  \begin{align*}
             & \forall i \in \set{1, \dots, k}, \T(v_i) = \T_{\W_1}(v_i)                               &  & \text{(by \cref{5.4.1})} \\
             & = \sum_{j = 1}^k ([\T_{\W_1}]_{\beta_1})_{j i} v_j                                      &  & \text{(by \cref{2.2.4})} \\
             & = \sum_{j = 1}^k ([\T_{\W_1}]_{\beta_1})_{j i} v_j + \sum_{j = k + 1}^n 0 v_j           &  & \text{(by \cref{1.2.1})} \\
    \implies & \forall (i, j) \in \set{1, \dots, k} \times \set{1, \dots, n},                                                        \\
             & ([\T]_{\beta})_{j i} = \begin{dcases}
                                        ([\T_{\W_1}]_{\beta_1})_{j i} & \text{if } j \in \set{1, \dots, k}     \\
                                        0                             & \text{if } j \in \set{k + 1, \dots, n}
                                      \end{dcases}
  \end{align*}
  and
  \begin{align*}
             & \forall i \in \set{k + 1, \dots, n}, \T(v_i) = \T_{\W_2}(v_i)                         &  & \text{(by \cref{5.4.1})} \\
             & = \sum_{j = k + 1}^n ([\T_{\W_2}]_{\beta_2})_{j i} v_j                                &  & \text{(by \cref{2.2.4})} \\
             & = \sum_{j = 1}^k 0 v_j + \sum_{j = k + 1}^n ([\T_{\W_2}]_{\beta_2})_{j i} v_j         &  & \text{(by \cref{1.2.1})} \\
    \implies & \forall (i, j) \in \set{k + 1, \dots, n} \times \set{1, \dots, n},                                                  \\
             & ([\T]_{\beta})_{j i} = \begin{dcases}
                                        0                             & \text{if } j \in \set{1, \dots, k}     \\
                                        ([\T_{\W_1}]_{\beta_1})_{j i} & \text{if } j \in \set{k + 1, \dots, n}
                                      \end{dcases}.
  \end{align*}
  Thus by \cref{5.4.5} we see that
  \[
    [\T]_{\beta} = \begin{pmatrix}
      [\T_{\W_1}]_{\beta_1} & \zm_1                 \\
      \zm_2                 & [\T_{\W_2}]_{\beta_2}
    \end{pmatrix}
  \]
  where \(\zm_1 \in \ms{k}{(n - k)}{\F}\) and \(\zm_2 \in \ms{(n - k)}{k}{\F}\) are zero matrices.
\end{proof}

\setcounter{ex}{35}
\begin{ex}\label{ex:5.4.36}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\).
  Prove that \(\T\) is diagonalizable iff \(\V\) is the direct sum of one-dimensional \(\T\)-invariant subspaces.
\end{ex}

\begin{proof}[\pf{ex:5.4.36}]
  First suppose that \(\T\) is diagonalizable.
  By \cref{5.1.1} there exists an ordered basis \(\beta\) for \(\V\) over \(\F\) such that \([\T]_{\beta}\) is a diagonal matrix.
  For each \(v \in \beta\), we know that \(\spn{\set{v}}\) is a one-dimensional \(\T\)-invariant subspace of \(\V\) over \(\F\).
  Thus by \cref{5.10}(a)(d) we know that \(\V\) is a direct sum of one-dimensional \(\T\)-invariant subspaces.

  Now suppose that \(\V\) is the direct sum of one-dimensional \(\T\)-invariant subspaces.
  Let \(\seq{\W}{1,,n}\) be \(\T\)-invariant subspaces of \(\V\) over \(\F\) such that \(\V = \seq[\oplus]{\W}{1,,n}\).
  If we let \(\beta_i\) be an ordered basis for \(\W_i\) over \(\F\) for each \(i \in \set{1, \dots, n}\), then by \cref{5.10}(a)(d) we know that \(\beta = \bigcup_{i = 1}^n \beta_i\) is an ordered basis for \(\V\) over \(\F\).
  By \cref{5.25} we have \([\T]_{\beta} = [\T_{\W_1}]_{\beta_1} \oplus \cdots \oplus [\T_{\W_n}]_{\beta_n}\).
  Therefore by \cref{5.4.5} \([\T]_{\beta}\) is a diagonal matrix and by \cref{5.1.1} \(\T\) is diagonalizable.
\end{proof}

\begin{ex}\label{ex:5.4.37}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and let \(\seq{\W}{1,,k}\) be \(\T\)-invariant subspaces of \(\V\) such that \(\V = \seq[\oplus]{\W}{1,,k}\).
  Prove that
  \[
    \det(\T) = \det(\T_{\W_1}) \cdots \det(\T_{\W_k}).
  \]
\end{ex}

\begin{proof}[\pf{ex:5.4.37}]
  For each \(i \in \set{1, \dots, k}\), let \(\beta_i\) be an ordered basis for \(\W_i\) over \(\F\).
  Let \(\beta = \bigcup_{i = 1}^k \beta_i\).
  Then we have
  \begin{align*}
    \det(\T) & = \det([\T]_{\beta})                                                                            &  & \text{(by \cref{ex:5.1.7})}  \\
             & = \det\begin{pmatrix}
                       [\T_{\W_1}]_{\beta_1} & \zm                   & \cdots & \zm                   \\
                       \zm                   & [\T_{\W_2}]_{\beta_2} & \cdots & \zm                   \\
                       \vdots                & \vdots                &        & \vdots                \\
                       \zm                   & \zm                   & \cdots & [\T_{\W_k}]_{\beta_k}
                     \end{pmatrix} &  & \text{(by \cref{5.25})}                                \\
             & = \det([\T_{\W_1}]_{\beta_1}) \cdots \det([\T_{\W_k}]_{\beta_k})                                &  & \text{(by \cref{ex:4.3.21})} \\
             & = \det(\T_{\W_1}) \cdot \det(\T_{\W_k}).                                                        &  & \text{(by \cref{ex:5.1.7})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:5.4.38}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and let \(\seq{\W}{1,,k}\) be \(\T\)-invariant subspaces of \(\V\) over \(\F\) such that \(\V = \seq[\oplus]{\W}{1,,k}\).
  Prove that \(\T\) is diagonalizable iff \(\T_{\W_i}\) is diagonalizable for all \(i \in \set{1, \dots, k}\).
\end{ex}

\begin{proof}[\pf{ex:5.4.38}]
  If \(\T\) is diagonalizable, then by \cref{ex:5.4.24} \(\T_{\W_i}\) is diagonalizable for all \(i \in \set{1, \dots, k}\).
  So suppose that \(\T_{\W_i}\) is diagonalizable for all \(i \in \set{1, \dots, k}\).
  By \cref{5.1.1}, for each \(i \in \set{1, \dots, k}\), there exists an ordered basis \(\beta_i\) for \(\W_i\) over \(\F\) such that \([\T_{\W_i}]_{\beta_i}\) is an diagonal matrix.
  By \cref{5.10}(a)(d) we know that \(\beta = \bigcup_{i = 1}^k \beta_i\) is an ordered basis for \(\V\) over \(\F\).
  Thus by \cref{5.25} we see that \([\T]_{\beta}\) is a diagonal matrix and by \cref{5.1.1} \(\T\) is diagonalizable.
\end{proof}

\begin{ex}\label{ex:5.4.39}
  Let \(\mathcal{C}\) be a collection of diagonalizable linear operators on a finite-dimensional vector space \(\V\).
  Prove that there is an ordered basis \(\beta\) such that \([\T]_{\beta}\) is a diagonal matrix for all \(\T \in \mathcal{C}\) iff the operators of \(\mathcal{C}\) commute under composition.
  (This is an extension of \cref{ex:5.4.25}.)
\end{ex}

\begin{proof}[\pf{ex:5.4.39}]
  First suppose that every \(\T \in \mathcal{C}\) are simultaneously diagonalizable.
  Then by \cref{ex:5.2.18}(a) we see that operators of \(\mathcal{C}\) commute under composition.

  Now suppose that operators of \(\mathcal{C}\) commute under composition.
  Let \(k = \dim(\V)\).
  We use induction on \(k\) to show that every \(\T \in \mathcal{C}\) are simultaneously diagonalizable.
  For \(k = 1\), for any ordered basis \(\beta\) for \(\V\) over \(\F\), \([\T]_{\beta}\) is a diagonal matrix for every \(\T \in \mathcal{C}\).
  Thus the base case holds.
  Suppose inductively that the statement is true for some \(k \geq 1\).
  We need to show that it is true for \(k + 1\).
  Let \(\V\) be a vector space over \(\F\) such that \(\dim(\V) = k + 1\).
  Let \(\mathcal{C}\) be a collection of diagonalizable linear operators on \(\V\).
  By \cref{5.11} we know that \(\V\) is a direct sum of eigenspaces of any \(\T \in \mathcal{C}\).
  If every \(\T \in \mathcal{C}\) has only one eigenvalue, then every \(\T \in \mathcal{C}\) are simultaneously diagonalizable with any ordered basis for \(\V\) over \(\F\) and we are done.
  So suppose that there exists a \(\T \in \mathcal{C}\) which has at least two eigenvalues.
  Fix one such \(\T\) and let \(\seq{\W}{1,,m}\) be eigenspaces of \(\T\) such that \(m \geq 2\) and \(\V = \seq[\oplus]{\W}{1,,m}\).
  We define
  \[
    \forall i \in \set{1, \dots, m}, \mathcal{C}_i = \set{\U_{\W_i} : \U \in \mathcal{C}}.
  \]
  Clearly \(\T_{\W_i} \in \mathcal{C}_i\) and \(\U_{\W_i} \T_{\W_i} = \T_{\W_i} \U_{\W_i}\) for all \(i \in \set{1, \dots, m}\) and \(\U_{\W_i} \in \mathcal{C}_i\).
  By \cref{5.4.2}(e) we know that \(\W_i\) is a \(\T\)-invariant subspace of \(\V\) over \(\F\) for all \(i \in \set{1, \dots, m}\), thus by \cref{ex:5.4.25}(a) we know that every \(\U_{\W_i} \in \mathcal{C}_i\) is diagonalizable for all \(i \in \set{1, \dots, m}\).
  Since \(m \geq 2\), by \cref{5.7} we know that \(\dim(\W_i) \leq k\) for all \(i \in \set{1, \dots, m}\).
  Thus by induction hypothesis we know that for each \(i \in \set{1, \dots, m}\), there exists an ordered basis for \(\W_i\) over \(\F\) such that \([\U_{\W_i}]_{\beta_i}\) is a diagonal matrix for all \(\U_{\W_i} \in \mathcal{C}_i\).
  Since \(\V = \seq[\oplus]{\W}{1,,m}\), by \cref{5.10}(a)(d) we know that \(\beta = \bigcup_{i = 1}^m \beta_i\) is an ordered basis for \(\V\).
  By \cref{5.25} this means \([\U]_{\beta}\) is a diagonal matrix for all \(\U \in \mathcal{C}\).
  Thus every \(\T \in \mathcal{C}\) are simultaneously diagonalizable, and this closes the induction.
\end{proof}
