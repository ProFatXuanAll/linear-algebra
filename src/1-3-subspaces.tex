\section{Subspaces}\label{sec:1.3}

\begin{defn}\label{1.3.1}
  A subset \(\W\) of a vector space \(\V\) over a field \(\F\) is called a \textbf{subspace} of \(\V\) over \(\F\) if \(\W\) is a vector space over \(\F\) with the operations of addition and scalar multiplication defined on \(\V\).
\end{defn}

\begin{eg}\label{1.3.2}
  In any vector space \(\V\) over \(\F\), note that \(\V\) and \(\set{\zv}\) are subspaces.
  The latter is called the \textbf{zero subspace} of \(\V\) over \(\F\).
\end{eg}

\begin{proof}
  Since \(\V \subseteq \V\) and \(\V\) is a vector space over \(\F\) with the operations of addition and scalar multiplication defined on \(\V\), by \cref{1.3.1} we know that \(\V\) is a subspace of \(\V\) over \(\F\).
  Since \(\zv \in \V\) (by \ref{vs3}), we know that \(\set{\zv} \subseteq \V\).
  Thus by \cref{ex:1.2.11} and \cref{1.3.1} \(\set{\zv}\) is a subspace of \(\V\) over \(\F\).
\end{proof}

\begin{thm}\label{1.3}
  Let \(\V\) be a vector space over \(\F\) and \(\W\) a subset of \(\V\).
  Then \(\W\) is a subspace of \(\V\) over \(\F\) if and only if the following three conditions hold for the operations defined in \(\V\).
  \begin{enumerate}
    \item \(\zv \in \W\).
    \item (\(\W\) is \textbf{closed under addition}.)
          \(x + y \in \W\) whenever \(x \in \W\) and \(y \in \W\).
    \item (\(\W\) is \textbf{closed under scalar multiplication}.)
          \(cx \in \W\) whenever \(c \in \F\) and \(x \in \W\).
  \end{enumerate}
\end{thm}

\begin{proof}
  If \(\W\) is a subspace of \(\V\) over \(\F\), then \(\W\) is a vector space over \(\F\) with the operations of addition and scalar multiplication defined on \(\V\).
  Hence conditions (b) and (c) hold, and there exists a vector \(\zv' \in \W\) such that \(x + \zv' = x\) for each \(x \in \W\).
  But also \(x + \zv = x\), and thus \(\zv' = \zv\) by \cref{1.1}.
  So condition (a) holds.

  Since properties \ref{vs1}, \ref{vs2}, \ref{vs5}, \ref{vs6}, \ref{vs7}, and \ref{vs8} hold for all vectors in the vector space, these properties automatically hold for the vectors in any subset.
  Thus if conditions (a), (b), and (c) hold, then \(\W\) is a subspace of \(\V\) over \(\F\) if the additive inverse of each vector in \(\W\) lies in \(\W\).
  But if \(x \in W\), then \(\p{-1} x \in \W\) by condition (c), and \(-x = \p{-1} x\) by \cref{1.2}.
  Hence \(\W\) is a subspace of \(\V\) over \(\F\).
\end{proof}

\begin{defn}\label{1.3.3}
  The \textbf{transpose} \(\tp{A}\) of an \(m \times n\) matrix \(A\) is the \(n \times m\) matrix obtained from \(A\) by interchanging the rows with the columns;
  that is, \(\p{\tp{A}}_{i j} = A_{j i}\).
\end{defn}

\begin{defn}\label{1.3.4}
  A \textbf{symmetric matrix} is a matrix \(A\) such that \(\tp{A} = A\).
  Clearly, a symmetric matrix must be square.
\end{defn}

\begin{eg}\label{1.3.5}
  The set \(\W\) of all symmetric matrices in \(\ms{n}{n}{\F}\) is a subspace of \(\ms{n}{n}{\F}\) over \(\F\).
\end{eg}

\begin{proof}
  Observe that
  \begin{itemize}
    \item The zero matrix is equal to its transpose and hence belongs to \(\W\).
    \item If \(A \in \W\) and \(B \in \W\), then \(\tp{A} = A\) and \(\tp{B} = B\).
          Thus by \cref{ex:1.3.3} \(\tp{\p{A + B}} = \tp{A} + \tp{B} = A + B\), so that \(A + B \in \W\).
    \item If \(A \in \W\), then \(\tp{A} = A\).
          So for any \(a \in \F\), we have \(\tp{\p{aA}} = a\tp{A} = aA\) by \cref{ex:1.3.3}
          Thus \(aA \in W\).
  \end{itemize}
  Thus by \cref{1.3} \(\W\) is a subspace of \(\ms{n}{n}{\F}\) over \(\F\).
\end{proof}

\begin{eg}\label{1.3.6}
  Let \(n\) be a nonnegative integer, and let \(\ps[n]{\F}\) consist of all polynomials in \(\ps{\F}\) having degree less than or equal to \(n\).
  Then \(\ps[n]{\F}\) is a subspace of \(\ps{\F}\) over \(\F\).
\end{eg}

\begin{proof}
  Since the zero polynomial has degree \(-1\), it is in \(\ps[n]{\F}\).
  Moreover, the sum of two polynomials with degrees less than or equal to \(n\) is another polynomial of degree less than or equal to \(n\), and the product of a scalar and a polynomial of degree less than or equal to \(n\) is a polynomial of degree less than or equal to \(n\).
  So \(\ps[n]{\F}\) is closed under addition and scalar multiplication.
  It therefore follows from \cref{1.3} that \(\ps[n]{\F}\) is a subspace of \(\ps{\F}\) over \(\F\).
\end{proof}

\begin{eg}\label{1.3.7}
  Let \(\cfs{\R}\) denote the set of all continuous real-valued functions defined on \(\R\).
  Clearly \(\cfs{\R}\) is a subset of the vector space \(\fs{\R}{\R}\) defined in \cref{1.2.10} of \cref{sec:1.2}.
  We claim that \(\cfs{\R}\) is a subspace of \(\fs{\R}{\R}\) over \(\R\).
\end{eg}

\begin{proof}
  First note that the zero of \(\fs{\R}{\R}\) is the constant function defined by \(f\p{t} = 0\) for all \(t \in \R\).
  Since constant functions are continuous, we have \(f \in \cfs{\R}\).
  Moreover, the sum of two continuous functions is continuous, and the product of a real number and a continuous function is continuous.
  So \(\cfs{\R}\) is closed under addition and scalar multiplication and hence is a subspace of \(\fs{\R}{\R}\) over \(\R\) by \cref{1.3}.
\end{proof}

\begin{eg}\label{1.3.8}
  An \(n \times n\) matrix \(M\) is called a \textbf{diagonal matrix} if \(M_{i j} = 0\) whenever \(i \neq j\), that is, if all its nondiagonal entries are zero.
  Then the set of diagonal matrices is a subspace of \(\ms{n}{n}{\F}\) over \(\F\).
\end{eg}

\begin{proof}
  Clearly the zero matrix is a diagonal matrix because all of its entries are \(0\).
  Moreover, if \(A\) and \(B\) are diagonal \(n \times n\) matrices, then whenever \(i \neq j\),
  \[
    \p{A + B}_{i j} = A_{i j} + B_{i j} = 0 + 0 = 0 \quad \text{and} \quad \p{cA}_{i j} = cA_{i j} = c0 = 0
  \]
  for any scalar \(c \in \F\).
  Hence \(A + B\) and \(cA\) are diagonal matrices for any scalar \(c \in \F\).
  Therefore the set of diagonal matrices is a subspace of \(\ms{n}{n}{\F}\) over \(\F\) by \cref{1.3}.
\end{proof}

\begin{eg}\label{1.3.9}
  The \textbf{trace} of an \(n \times n\) matrix \(M\), denoted \(\tr{M}\), is the sum of the diagonal entries of \(M\);
  that is,
  \[
    \tr{M} = M_{1 1} + M_{2 2} + \dots + M_{n n}.
  \]
  The set \(\W\) of \(n \times n\) matrices having trace equal to \(0\) is a subspace of \(\ms{n}{n}{\F}\) over \(\F\).
\end{eg}

\begin{proof}
  Clearly we have \(\W \subseteq \ms{n}{n}{\F}\), \(\tr{\zm} = 0\) and \(\zm \in \W\).
  Moreover, if \(A, B \in \W\), then
  \begin{align*}
    \tr{A + B} & = \p{A + B}_{1 1} + \p{A + B}_{2 2} + \dots + \p{A + B}_{n n}               &  & \text{(by \cref{1.3.9})}  \\
               & = A_{1 1} + B_{1 1} + A_{2 2} + B_{2 2} + \dots + A_{n n} + B_{n n}         &  & \text{(by \cref{1.2.9})}  \\
               & = A_{1 1} + A_{2 2} + \dots + A_{n n} + B_{1 1} + B_{2 2} + \dots + B_{n n} &  & (A_{i i}, B_{i i} \in \F) \\
               & = 0                                                                         &  & (A, B \in \W)
  \end{align*}
  and
  \begin{align*}
    \tr{cA} & = \p{cA}_{1 1} + \p{cA}_{2 2} + \dots + \p{cA}_{n n} &  & \text{(by \cref{1.3.9})} \\
            & = cA_{1 1} + cA_{2 2} + \dots + cA_{n n}             &  & \text{(by \cref{1.2.9})} \\
            & = c\p{A_{1 1} + A_{2 2} + A_{n n}}                   &  & (c, A_{i i} \in \F)      \\
            & = c0                                                 &  & (A \in \W)               \\
            & = 0                                                  &  & (c \in \F)
  \end{align*}
  for any scalar \(c \in \F\).
  Therefore \(\W\) is a subspace of \(\ms{n}{n}{\F}\) over \(\F\) by \cref{1.3}.
\end{proof}

\begin{thm}\label{1.4}
  Any intersection of subspaces of a vector space \(\V\) is a subspace of \(\V\).
\end{thm}

\begin{proof}
  Let \(\cvs\) be a collection of subspaces of \(\V\) over \(\F\), and let \(\W\) denote the intersection of the subspaces in \(\cvs\).
  Since every subspace contains the zero vector, \(\zv \in \W\).
  Let \(a \in \F\) and \(x, y \in \W\).
  Then \(x\) and \(y\) are contained in each subspace in \(\cvs\).
  Because each subspace in \(\cvs\) is closed under addition and scalar multiplication, it follows that \(x + y\) and \(ax\) are contained in each subspace in \(\cvs\).
  Hence \(x + y\) and \(ax\) are also contained in \(\W\), so that \(\W\) is a subspace of \(\V\) over \(\F\) by \cref{1.3}.
\end{proof}

\exercisesection

\setcounter{ex}{2}
\begin{ex}\label{ex:1.3.3}
  Prove that \(\tp{\p{aA + bB}} = a\tp{A} + b\tp{B}\) for any \(A, B \in \ms{m}{n}{\F}\) and any \(a, b \in \F\).
\end{ex}

\begin{proof}
  Let \(i = 1, \dots, n\) and \(j = 1, \dots, m\).
  Since
  \begin{align*}
    \p{a\tp{A} + b\tp{B}}_{i j} & = a\p{\tp{A}}_{i j} + b\p{\tp{B}}_{i j} &  & \text{(by \cref{1.2.9})} \\
                                & = aA_{j i} + bB_{j i}                   &  & \text{(by \cref{1.3.3})} \\
                                & = \p{aA}_{j i} + \p{bB}_{j i}           &  & \text{(by \cref{1.2.9})} \\
                                & = \p{aA + bB}_{j i}                     &  & \text{(by \cref{1.2.9})} \\
                                & = \tp{\p{aA + bB}}_{i j},               &  & \text{(by \cref{1.3.3})}
  \end{align*}
  by \cref{1.2.7} we know that \(\tp{\p{aA + bB}} = a\tp{A} + b\tp{B}\).
\end{proof}

\begin{ex}\label{ex:1.3.4}
  Prove that \(\tp{\p{\tp{A}}} = A\) for each \(A \in \MS\).
\end{ex}

\begin{proof}
  Let \(i = 1, \dots, m\) and \(j = 1, \dots, n\).
  Since
  \begin{align*}
    A_{i j} & = \p{\tp{A}}_{j i}           &  & \text{(by \cref{1.3.3})} \\
            & = \p{\tp{\p{\tp{A}}}}_{i j}, &  & \text{(by \cref{1.3.3})}
  \end{align*}
  by \cref{1.2.8} we know that \(\tp{\p{\tp{A}}} = A\).
\end{proof}

\begin{ex}\label{ex:1.3.5}
  Prove that \(A + \tp{A}\) is symmetric for any square matrix \(A \in \ms{n}{n}{\F}\).
\end{ex}

\begin{proof}
  Let \(i, j = 1, \dots, n\).
  Since
  \begin{align*}
    \p{\tp{\p{A + \tp{A}}}}_{i j} & = \p{A + \tp{A}}_{j i}       &  & \text{(by \cref{1.3.3})}  \\
                                  & = A_{j i} + \p{\tp{A}}_{j i} &  & \text{(by \cref{1.2.9})}  \\
                                  & = A_{j i} + A_{i j}          &  & \text{(by \cref{1.3.3})}  \\
                                  & = A_{i j} + A_{j i}          &  & (A_{i j}, A_{j i} \in \F) \\
                                  & = A_{i j} + \p{\tp{A}}_{i j} &  & \text{(by \cref{1.3.3})}  \\
                                  & = \p{A + \tp{A}}_{i j},      &  & \text{(by \cref{1.2.9})}
  \end{align*}
  by \cref{1.2.8} we know that \(\tp{\p{A + \tp{A}}} = A + \tp{A}\).
  Thus by \cref{1.3.4} \(A + \tp{A}\) is symmetric.
\end{proof}

\begin{ex}\label{ex:1.3.6}
  Prove that \(\tr{aA + bB} = a\tr{A} + b\tr{B}\) for any \(A, B \in \ms{n}{n}{\F}\).
\end{ex}

\begin{proof}
  We have
  \begin{align*}
     & \tr{aA + bB}                                                                                                         \\
     & = (aA + bB)_{1 1} + (aA + bB)_{2 2} + \dots + (aA + bB)_{n n}                       &  & \text{(by \cref{1.3.9})}    \\
     & = aA_{1 1} + bB_{1 1} + aA_{2 2} + bB_{2 2} + \dots + aA_{n n} + bB_{n n}           &  & \text{(by \cref{1.2.9})}    \\
     & = a\p{A_{1 1} + A_{2 2} + \dots + A_{n n}} + b\p{B_{1 1} + B_{2 2} + \dots B_{n n}} &  & (aA_{i i}, bB_{i i} \in \F) \\
     & = a\tr{A} + b\tr{B}.                                                                &  & \text{(by \cref{1.3.9})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:1.3.7}
  Prove that diagonal matrices are symmetric matrices.
\end{ex}

\begin{proof}
  Let \(A \in \ms{n}{n}{\F}\) be a diagonal matrices and let \(i, j = 1, \dots, n\).
  Then we have
  \begin{align*}
             & \begin{dcases}
      A_{i j} = A_{j i} = 0 & \text{if } i \neq j \\
      A_{i i} = A_{i i}     & \text{otherwise}
    \end{dcases}           &  & \text{(by \cref{1.3.8})} \\
    \implies & A_{i j} = A_{j i} = \p{\tp{A}}_{i j} &  & \text{(by \cref{1.3.3})} \\
    \implies & A = \tp{A}                           &  & \text{(by \cref{1.2.8})}
  \end{align*}
  and thus by \cref{1.3.4} diagonal matrices are symmetric.
\end{proof}