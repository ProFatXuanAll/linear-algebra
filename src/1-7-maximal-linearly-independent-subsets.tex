\section{Maximal Linearly Independent Subsets}\label{sec:1.7}

\begin{note}
  The difficulty that arises in extending the theorems of the preceding section to infinite-dimensional vector spaces is that the principle of mathematical induction, which played a crucial role in many of the proofs of \cref{sec:1.6}, is no longer adequate.
  Instead, a more general result called the \emph{maximal principle} is needed.
  Before stating this principle, we need to introduce some terminology.
\end{note}

\begin{defn}\label{1.7.1}
  Let \(\mathcal{F}\) be a family of sets.
  A member \(M\) of \(\mathcal{F}\) is called \textbf{maximal} (with respect to set inclusion) if \(M\) is contained in no member of \(\mathcal{F}\) other than \(M\) itself.
\end{defn}

\begin{eg}\label{1.7.2}
  Let \(\mathcal{F}\) be the family of all subsets of a nonempty set \(S\).
  (This family \(\mathcal{F}\) is called the \textbf{power set} of \(S\).)
  The set \(S\) is easily seen to be a maximal element of \(\mathcal{F}\).
\end{eg}

\begin{proof}[\pf{1.7.2}]
  Suppose for sake of contradiction that there exists a set \(S' \in 2^S\) such that \(S \subseteq S'\) and \(S \neq S'\).
  But \(S' \in 2^S\) implies \(S' \subseteq S\), thus we have \(S = S'\), a contradiction.
\end{proof}

\begin{eg}\label{1.7.3}
  Let \(S\) and \(T\) be disjoint nonempty sets, and let \(\mathcal{F}\) be the union of their power sets.
  Then \(S\) and \(T\) are both maximal elements of \(\mathcal{F}\).
\end{eg}

\begin{proof}[\pf{1.7.3}]
  Since \(2^S \cap 2^T = \varnothing\), we know that a maximal element of \(2^S \cup 2^T\) cannot be in \(S \cap T\).
  Thus by \cref{1.7.2} we know that \(S, T\) are both maximal elements of \(\mathcal{F}\).
\end{proof}

\begin{eg}\label{1.7.4}
  Let \(\mathcal{F}\) be the family of all finite subsets of an infinite set \(S\).
  Then \(\mathcal{F}\) has no maximal element.
\end{eg}

\begin{proof}[\pf{1.7.4}]
  Suppose for sake of contradiction that there exists a maximal element \(M\) in \(\mathcal{F}\).
  But then we have
  \begin{align*}
             & \begin{dcases}
      M \text{ is finite} \\
      S \text{ is infinite}
    \end{dcases}                   \\
    \implies & S \setminus M \neq \varnothing              \\
    \implies & \exists s \in S : s \notin M                \\
    \implies & \exists s \in S : \begin{dcases}
      M \subseteq M \cup \set{s} \\
      M \cup \set{s} \in \mathcal{F}
    \end{dcases}
  \end{align*}
  which contradict the fact that \(M\) is a maximal element of \(\mathcal{F}\).
\end{proof}

\begin{defn}\label{1.7.5}
  A collection of sets \(\mathcal{C}\) is called a \textbf{chain} (or \textbf{nest} or \textbf{tower}) if for each pair of sets \(A\) and \(B\) in \(\mathcal{C}\), either \(A \subseteq B\) or \(B \subseteq A\).
\end{defn}

\begin{eg}\label{1.7.6}
  For each positive integer \(n\) let \(A_n = \set{1, 2, \dots, n}\).
  Then the collection of sets \(\mathcal{C} = \set{A_n : n = 1, 2, 3, \dots}\) is a chain.
  In fact, \(A_m \subseteq A_n\) if and only if \(m \leq n\).
\end{eg}

\begin{proof}[\pf{1.7.6}]
  We have
  \begin{align*}
             & \forall \seq{A}{m,n} \in \mathcal{C}, \begin{dcases}
      A_m = \set{1, 2, \dots, m} \\
      A_n = \set{1, 2, \dots, n}
    \end{dcases}                               \\
    \implies & \forall \seq{A}{m,n} \in \mathcal{C}, \begin{dcases}
      A_m \subseteq A_n & \text{if } m \leq n \\
      A_n \subseteq A_m & \text{if } m > n
    \end{dcases}                               \\
    \implies & \mathcal{C} \text{ is a chain}.                                  &  & \text{(by \cref{1.7.5})}
  \end{align*}
\end{proof}

\begin{ax}[Maximal Principle]\label{1.7.7}
  Let \(\mathcal{F}\) be a family of sets.
  If, for each chain \(\mathcal{C} \subseteq \mathcal{F}\), there exists a member of \(\mathcal{F}\) that contains each member of \(\mathcal{C}\), then \(\mathcal{F}\) contains a maximal member.
\end{ax}

\begin{note}
  The \emph{Maximal Principle} is logically equivalent to the \emph{Axiom of Choice}, which is an assumption in most axiomatic developments of set theory.
  For a treatment of set theory using the Maximal Principle, see John L. Kelley, General Topology, Graduate Texts in Mathematics Series, Vol. 27, Springer-Verlag, 1991.
\end{note}

\begin{defn}\label{1.7.8}
  Let \(S\) be a subset of a vector space \(\V\) over \(\F\).
  A \textbf{maximal linearly independent subset} of \(S\) is a subset \(B\) of \(S\) satisfying both of the following conditions.
  \begin{enumerate}
    \item \(B\) is linearly independent.
    \item The only linearly independent subset of \(S\) that contains \(B\) is \(B\) itself.
  \end{enumerate}
\end{defn}

\begin{note}
  Maximal linearly independent subsets of a set need not be unique.
\end{note}

\begin{eg}\label{1.7.9}
  A basis \(\beta\) for a vector space \(\V\) over \(\F\) is a maximal linearly independent subset of \(\V\).
\end{eg}

\begin{proof}[\pf{1.7.9}]
  \cref{1.7.9} is true since
  \begin{itemize}
    \item \(\beta\) is linearly independent by definition.
    \item If \(v \in V\) and \(v \notin \beta\), then \(\beta \cup \set{v}\) is linearly dependent by \cref{1.7} because \(\spn{\beta} = \V\).
  \end{itemize}
\end{proof}

\begin{thm}\label{1.12}
  Let \(\V\) be a vector space over \(\F\) and \(S\) a subset that generates \(\V\).
  If \(\beta\) is a maximal linearly independent subset of \(S\), then \(\beta\) is a basis for \(\V\) over \(\F\).
\end{thm}

\begin{proof}[\pf{1.12}]
  Let \(\beta\) be a maximal linearly independent subset of \(S\).
  Because \(\beta\) is linearly independent, it suffices to prove that \(\beta\) generates \(\V\).
  We claim that \(S \subseteq \spn{\beta}\), for otherwise there exists a \(v \in S\) such that \(v \notin \spn{\beta}\).
  Since \cref{1.7} implies that \(\beta \cup \set{v}\) is linearly independent, we have contradicted the maximality of \(\beta\).
  Therefore \(S \subseteq \spn{\beta}\).
  Because \(\spn{S} = \V\), it follows from \cref{1.5} that \(\spn{\beta} = \V\).
\end{proof}

\begin{note}
  From \cref{1.7.9} and \cref{1.12} we see that a subset of a vector space is a basis if and only if it is a maximal linearly independent subset of the vector space.
\end{note}

\begin{thm}\label{1.13}
  Let \(S\) be a linearly independent subset of a vector space \(\V\) over \(\F\).
  There exists a maximal linearly independent subset of \(\V\) that contains \(S\).
\end{thm}

\begin{proof}[\pf{1.13}]
  Let \(\mathcal{F}\) denote the family of all linearly independent subsets of \(\V\) that contain \(S\).
  In order to show that \(\mathcal{F}\) contains a maximal element, we must show that if \(\mathcal{C}\) is a chain in \(\mathcal{F}\), then there exists a member \(U\) of \(\mathcal{F}\) that contains each member of \(\mathcal{C}\).
  We claim that \(U\), the union of the members of \(\mathcal{C}\), is the desired set.
  Clearly \(U\) contains each member of \(\mathcal{C}\), and so it suffices to prove that \(U \in \mathcal{F}\)
  (i.e., that \(U\) is a linearly independent subset of \(\V\) that contains \(S\)).
  Because each member of \(\mathcal{C}\) is a subset of \(\V\) containing \(S\), we have \(S \subseteq U \subseteq \V\).
  Thus we need only prove that \(U\) is linearly independent.
  Let \(\seq{u}{1,2,,n}\) be in \(U\) and \(\seq{a}{1,2,,n}\) be scalars in \(\F\) such that \(\seq[+]{a,u}{1,2,,n} = 0\).
  Because \(u_i \in U\) for \(i = 1, 2, \dots, n\), there exists a set \(A_i\) in \(\mathcal{C}\) such that \(u_i \in A_i\).
  But since \(\mathcal{C}\) is a chain, one of these sets, say \(A_k\), contains all the others.
  Thus \(u_i \in A_k\) for \(i = 1, 2, \dots, n\).
  However, \(A_k\) is a linearly independent set;
  so \(\seq[+]{a,u}{1,2,,n} = 0\) implies that \(\seq[=]{a}{1,2,,n} = 0\).
  It follows that \(U\) is linearly independent.

  The maximal principle implies that \(\mathcal{F}\) has a maximal element.
  This element is easily seen to be a maximal linearly independent subset of \(\V\) that contains \(S\).
\end{proof}

\begin{cor}\label{1.7.10}
  Every vector space has a basis.
\end{cor}

\begin{proof}[\pf{1.7.10}]
  Let \(\V\) be a vector space over \(\F\).
  If \(\V = \set{\zv}\), then by \cref{1.6.2} \(\varnothing\) is the basis for \(\set{\zv}\).
  So suppose that \(\V \neq \set{\zv}\).
  Then there exists a \(v \in \V \setminus \set{\zv}\) and by \cref{1.5.4}(b) \(\set{v}\) is linearly independent.
  From \cref{1.13} we see that there exists a maximal linearly independent subset \(\beta\) of \(\V\) that contains \(\set{v}\).
  By \cref{1.7.8} we know that \(\spn{\beta} = \V\).
  Thus by \cref{1.12} \(\beta\) is a basis for \(\V\) over \(\F\).
\end{proof}

\begin{note}
  It can be shown, analogously to \cref{1.6.7}, that every basis for an infinite-dimensional vector space has the same \emph{cardinality}.
  (Sets have the same cardinality if there is a one-to-one and onto mapping between them.)
  (See, for example, N. Jacobson, Lectures in Ab- stract Algebra, vol. 2, Linear Algebra, D. Van Nostrand Company, New York, 1953, p. 240.)
\end{note}

\exercisesection
