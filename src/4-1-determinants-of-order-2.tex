\section{Determinants of Order \textrm{2}}\label{sec:4.1}

\begin{defn}\label{4.1.1}
  If
  \[
    A = \begin{pmatrix}
      a & b \\
      c & d
    \end{pmatrix}
  \]
  is a \(2 \times 2\) matrix with entries from a field \(\F\), then we define the \textbf{determinant} of \(A\), denoted \(\det(A)\) or \(\abs{A}\), to be the scalar \(ad - bc\).
\end{defn}

\begin{note}
  There exist \(A, B \in \ms{2}{2}{\F}\) such that \(\det(A + B) \neq \det(A) + \det(B)\), the function \(\det : \ms{2}{2}{\F} \to \F\) is \emph{not} a linear transformation.
\end{note}

\begin{thm}\label{4.1}
  The function \(\det : \ms{2}{2}{\F} \to \F\) is a linear function of each row of a \(2 \times 2\) matrix when the other row is held fixed.
  That is, if \(u, v, w \in \vs{F}^2\) and \(k \in \F\), then
  \[
    \det\begin{pmatrix}
      u + kv \\
      w
    \end{pmatrix} = \det\begin{pmatrix}
      u \\
      w
    \end{pmatrix} + k \det\begin{pmatrix}
      v \\
      w
    \end{pmatrix}
  \]
  and
  \[
    \det\begin{pmatrix}
      w \\
      u + kv
    \end{pmatrix} = \det\begin{pmatrix}
      w \\
      u
    \end{pmatrix} + k \det\begin{pmatrix}
      w \\
      v
    \end{pmatrix}.
  \]
\end{thm}

\begin{proof}[\pf{4.1}]
  Let \(u = \tuple{a}{1,2}, v = \tuple{b}{1,2}, w = \tuple{c}{1,2} \in \vs{F}^2\) and let \(k \in \F\).
  Then
  \begin{align*}
    \det\begin{pmatrix}
          u \\
          w
        \end{pmatrix} + k \det\begin{pmatrix}
                                v \\
                                w
                              \end{pmatrix} & = \det\begin{pmatrix}
                                                      a_1 & a_2 \\
                                                      c_1 & c_2
                                                    \end{pmatrix} + k \det\begin{pmatrix}
                                                                            b_1 & b_2 \\
                                                                            c_1 & c_2
                                                                          \end{pmatrix}  \\
                                          & = (a_1 c_2 - a_2 c_1) + k (b_1 c_2 - b_2 c_1) \\
                                          & = (a_1 + k b_1) c_2 - (a_2 + k b_2) c_1       \\
                                          & = \det\begin{pmatrix}
                                                    a_1 + k b_1 & a_2 + k b_2 \\
                                                    c_1         & c_2
                                                  \end{pmatrix}               \\
                                          & = \det\begin{pmatrix}
                                                    u + kv \\
                                                    w
                                                  \end{pmatrix}.
  \end{align*}
  A similar calculation shows that
  \[
    \det\begin{pmatrix}
      w \\
      u
    \end{pmatrix} + k \det\begin{pmatrix}
      w \\
      v
    \end{pmatrix} = \det\begin{pmatrix}
      w \\
      u + kv
    \end{pmatrix}.
  \]
\end{proof}

\begin{thm}\label{4.2}
  Let \(A \in \ms{2}{2}{\F}\).
  Then the determinant of \(A\) is nonzero iff \(A\) is invertible.
  Moreover, if \(A\) is invertible, then
  \[
    A^{-1} = \frac{1}{\det(A)} \begin{pmatrix}
      A_{2 2}  & -A_{1 2} \\
      -A_{2 1} & A_{1 1}
    \end{pmatrix}.
  \]
\end{thm}

\begin{proof}[\pf{4.2}]
  If \(\det(A) \neq 0\), then we can define a matrix
  \[
    M = \frac{1}{\det(A)} \begin{pmatrix}
      A_{2 2}  & -A_{1 2} \\
      -A_{2 1} & A_{1 1}
    \end{pmatrix}.
  \]
  A straightforward calculation shows that \(AM = MA = I\), and so \(A\) is invertible and \(M = A^{-1}\).

  Conversely, suppose that \(A\) is invertible.
  \cref{3.2.2} shows that the rank of
  \[
    A = \begin{pmatrix}
      A_{1 1} & A_{1 2} \\
      A_{2 1} & A_{2 2}
    \end{pmatrix}
  \]
  must be \(2\).
  Hence \(A_{1 1} \neq 0\) or \(A_{2 1} \neq 0\).
  If \(A_{1 1} \neq 0\), add \(-A_{2 1} / A_{1 1}\) times row \(1\) of \(A\) to row \(2\) to obtain the matrix
  \[
    \begin{pmatrix}
      A_{1 1} & A_{1 2}                                   \\
      0       & A_{2 2} - \frac{A_{1 2} A_{2 1}}{A_{1 1}}
    \end{pmatrix}.
  \]
  Because elementary row operations are rank-preserving by \cref{3.2.3}, it follows that
  \[
    A_{2 2} - \frac{A_{1 2} A_{2 1}}{A_{1 1}} \neq 0.
  \]
  Therefore \(\det(A) = A_{1 1} A_{2 2} - A_{1 2} A_{2 1} \neq 0\).
  On the other hand, if \(A_{2 1} \neq 0\), we see that \(\det(A) \neq 0\) by adding \(-A_{1 1} / A_{2 1}\) times row \(2\) of \(A\) to row \(1\) and applying a similar argument.
  Thus, in either case, \(\det(A) \neq 0\).
\end{proof}

\begin{defn}\label{4.1.2}
  By the \textbf{angle} between two vectors in \(\R^2\), we mean the angle with measure \(\theta\) (\(0 \leq \theta < \pi\)) that is formed by the vectors having the same magnitude and direction as the given vectors but emanating from the origin.

  If \(\beta = \set{u, v}\) is an ordered basis for \(\R^2\), we define the \textbf{orientation} of \(\beta\) to be the real number
  \[
    \mathbf{O}\begin{pmatrix}
      u \\
      v
    \end{pmatrix} = \frac{\det\begin{pmatrix}
        u \\
        v
      \end{pmatrix}}{\abs{\det\begin{pmatrix}
          u \\
          v
        \end{pmatrix}}}.
  \]
  (The denominator of this fraction is nonzero by \cref{4.2}.)
  Clearly
  \[
    \mathbf{O}\begin{pmatrix}
      u \\
      v
    \end{pmatrix} = \pm 1.
  \]
  Notice that
  \[
    \mathbf{O}\begin{pmatrix}
      e_1 \\
      e_2
    \end{pmatrix} = 1 \quad \text{and} \quad \mathbf{O}\begin{pmatrix}
      e_1 \\
      -e_2
    \end{pmatrix} = -1.
  \]

  Recall that a coordinate system \(\set{u, v}\) is called \textbf{right-handed} if \(u\) can be rotated in a counterclockwise direction through an angle \(\theta\) (\(0 < \theta < \pi\)) to coincide with \(v\).
  Otherwise \(\set{u, v}\) is called a \textbf{left-handed} system.
  In general (see \cref{ex:4.1.12}),
  \[
    \mathbf{O}\begin{pmatrix}
      u \\
      v
    \end{pmatrix} = 1
  \]
  iff the ordered basis \(\set{u, v}\) forms a right-handed coordinate system.
  For convenience, we also define
  \[
    \mathbf{O}\begin{pmatrix}
      u \\
      v
    \end{pmatrix} = 1
  \]
  if \(\set{u, v}\) is linearly dependent.
\end{defn}

\begin{defn}\label{4.1.3}
  Any ordered set \(\set{u, v}\) in \(\R^2\) determines a parallelogram in the following manner.
  Regarding \(u\) and \(v\) as arrows emanating from the origin of \(\R^2\), we call the parallelogram having \(u\) and \(v\) as adjacent sides the \textbf{parallelogram determined by \(u\) and \(v\)}.
  Observe that if the set \(\set{u, v}\) is linearly dependent (i.e., if \(u\) and \(v\) are parallel), then the ``parallelogram'' determined by \(u\) and \(v\) is actually a line segment, which we consider to be a degenerate parallelogram having area zero.
\end{defn}

\begin{prop}\label{4.1.4}
  Let \(\set{u, v} \subseteq \R^2\).
  If we define
  \[
    \mathbf{A}\begin{pmatrix}
      u \\
      v
    \end{pmatrix}
  \]
  as the area of the parallelogram determined by \(u\) and \(v\), then
  \[
    \mathbf{A}\begin{pmatrix}
      u \\
      v
    \end{pmatrix} = \mathbf{O}\begin{pmatrix}
      u \\
      v
    \end{pmatrix} \cdot \det\begin{pmatrix}
      u \\
      v
    \end{pmatrix} = \abs{\det\begin{pmatrix}
        u \\
        v
      \end{pmatrix}}.
  \]
\end{prop}

\begin{proof}[\pf{4.1.4}]
  First, since
  \[
    \mathbf{O}\begin{pmatrix}
      u \\
      v
    \end{pmatrix} = \pm 1,
  \]
  we may multiply both sides of the desired equation by
  \[
    \mathbf{O}\begin{pmatrix}
      u \\
      v
    \end{pmatrix}
  \]
  to obtain the equivalent form
  \[
    \mathbf{O}\begin{pmatrix}
      u \\
      v
    \end{pmatrix} \cdot \mathbf{A}\begin{pmatrix}
      u \\
      v
    \end{pmatrix} = \det\begin{pmatrix}
      u \\
      v
    \end{pmatrix}.
  \]
  We establish this equation by verifying that the three conditions of \cref{ex:4.1.11} are satisfied by the function
  \[
    \delta\begin{pmatrix}
      u \\
      v
    \end{pmatrix} = \mathbf{O}\begin{pmatrix}
      u \\
      v
    \end{pmatrix} \cdot \mathbf{A}\begin{pmatrix}
      u \\
      v
    \end{pmatrix}.
  \]
  \begin{enumerate}
    \item We begin by showing that for any real number \(c\)
          \[
            \delta\begin{pmatrix}
              u \\
              cv
            \end{pmatrix} = c \cdot \delta\begin{pmatrix}
              u \\
              v
            \end{pmatrix}.
          \]
          Observe that this equation is valid if \(c = 0\) because
          \[
            \delta\begin{pmatrix}
              u \\
              cv
            \end{pmatrix} = \mathbf{O}\begin{pmatrix}
              u \\
              \zv
            \end{pmatrix} \cdot \mathbf{A}\begin{pmatrix}
              u \\
              \zv
            \end{pmatrix} = 1 \cdot 0 = 0.
          \]
          So assume that \(c \neq 0\).
          Regarding \(cv\) as the base of the parallelogram determined by \(u\) and \(cv\), we see that
          \[
            \mathbf{A}\begin{pmatrix}
              u \\
              cv
            \end{pmatrix} = \text{base } \times \text{ altitude} = \abs{c} (\text{length of } v) (\text{altitude}) = \abs{c} \cdot \mathbf{A}\begin{pmatrix}
              u \\
              v
            \end{pmatrix},
          \]
          since the altitude of the parallelogram determined by \(u\) and \(cv\) is the same as that in the parallelogram determined by \(u\) and \(v\).
          Hence
          \begin{align*}
            \delta\begin{pmatrix}
                    u \\
                    cv
                  \end{pmatrix} & = \mathbf{O}\begin{pmatrix}
                                                u \\
                                                cv
                                              \end{pmatrix} \cdot \mathbf{A}\begin{pmatrix}
                                                                              u \\
                                                                              cv
                                                                            \end{pmatrix}                                                            \\
                                  & = \pa{\frac{c}{\abs{c}} \mathbf{O}\begin{pmatrix}
                                                                          u \\
                                                                          v
                                                                        \end{pmatrix}} \pa{\abs{c} \mathbf{A}\begin{pmatrix}
                                                                                                               u \\
                                                                                                               v
                                                                                                             \end{pmatrix}} &  & \text{(by \cref{4.1})} \\
                                  & = c \cdot \mathbf{O}\begin{pmatrix}
                                                          u \\
                                                          v
                                                        \end{pmatrix} \cdot \mathbf{A}\begin{pmatrix}
                                                                                        u \\
                                                                                        v
                                                                                      \end{pmatrix}                                                  \\
                                  & = c \cdot \delta\begin{pmatrix}
                                                      u \\
                                                      v
                                                    \end{pmatrix}.
          \end{align*}
          A similar argument shows that
          \[
            \delta\begin{pmatrix}
              cu \\
              v
            \end{pmatrix} = c \cdot \delta\begin{pmatrix}
              u \\
              v
            \end{pmatrix}.
          \]

          We next prove that
          \[
            \delta\begin{pmatrix}
              u \\
              au + bw
            \end{pmatrix} = b \cdot \delta\begin{pmatrix}
              u \\
              w
            \end{pmatrix}
          \]
          for any \(u, w \in \R^2\) and any real numbers \(a\) and \(b\).
          Because the parallelograms determined by \(u\) and \(w\) and by \(u\) and \(u + w\) have a common base \(u\) and the same altitude, it follows that
          \[
            \mathbf{A}\begin{pmatrix}
              u \\
              w
            \end{pmatrix} = \mathbf{A}\begin{pmatrix}
              u \\
              u + w
            \end{pmatrix}.
          \]
          If \(a = 0\), then
          \[
            \delta\begin{pmatrix}
              u \\
              au + bw
            \end{pmatrix} = \delta\begin{pmatrix}
              u \\
              bw
            \end{pmatrix} = b \cdot \delta\begin{pmatrix}
              u \\
              w
            \end{pmatrix}
          \]
          by the first paragraph of (a).
          Otherwise, if \(a \neq 0\), then
          \[
            \delta\begin{pmatrix}
              u \\
              au + bw
            \end{pmatrix} = a \cdot \delta\begin{pmatrix}
              u \\
              u + \frac{b}{a} w
            \end{pmatrix} = a \cdot \delta\begin{pmatrix}
              u \\
              \frac{b}{a} w
            \end{pmatrix} = b \cdot \delta\begin{pmatrix}
              u \\
              w
            \end{pmatrix}.
          \]
          So the desired conclusion is obtained in either case.

          We are now able to show that
          \[
            \delta\begin{pmatrix}
              u \\
              v_1 + v_2
            \end{pmatrix} = \delta\begin{pmatrix}
              u \\
              v_1
            \end{pmatrix} + \delta\begin{pmatrix}
              u \\
              v_2
            \end{pmatrix}
          \]
          for all \(u, v_1, v_2 \in \R^2\).
          Since the result is immediate if \(u = 0\), we assume that \(u \neq 0\).
          Choose any vector \(w \in \R^2\) such that \(\set{u, w}\) is linearly independent.
          Then for any vectors \(v_1, v_2 \in \R^2\) there exist scalars \(a_i\) and \(b_i\) such that \(v_i = a_i u + b_i w\) (\(i \in \set{1, 2}\)).
          Thus
          \begin{align*}
            \delta\begin{pmatrix}
                    u \\
                    v_1 + v_2
                  \end{pmatrix} & = \delta\begin{pmatrix}
                                            u \\
                                            (a_1 + a_2) u + (b_1 + b_2) w
                                          \end{pmatrix}           \\
                                  & = (b_1 + b_2) \delta\begin{pmatrix}
                                                          u \\
                                                          w
                                                        \end{pmatrix}           \\
                                  & = \delta\begin{pmatrix}
                                              u \\
                                              a_1 u + b_1 w
                                            \end{pmatrix} + \delta\begin{pmatrix}
                                                                    u \\
                                                                    a_2 u + b_2 w
                                                                  \end{pmatrix} \\
                                  & = \delta\begin{pmatrix}
                                              u \\
                                              v_1
                                            \end{pmatrix} + \delta\begin{pmatrix}
                                                                    u \\
                                                                    v_2
                                                                  \end{pmatrix}.
          \end{align*}
          A similar argument shows that
          \[
            \delta\begin{pmatrix}
              u_1 + u_2 \\
              v
            \end{pmatrix} = \delta\begin{pmatrix}
              u_1 \\
              v
            \end{pmatrix} + \delta\begin{pmatrix}
              u_2 \\
              v
            \end{pmatrix}
          \]
          for all \(u_1, u_2, v \in \R^2\).
    \item Since
          \[
            \mathbf{A}\begin{pmatrix}
              u \\
              u
            \end{pmatrix} = 0,
          \]
          it follows that
          \[
            \delta\begin{pmatrix}
              u \\
              u
            \end{pmatrix} = \mathbf{O}\begin{pmatrix}
              u \\
              u
            \end{pmatrix} \cdot \mathbf{A}\begin{pmatrix}
              u \\
              u
            \end{pmatrix} = 0
          \]
          for any \(u \in \R^2\).
    \item Because the parallelogram determined by \(e_1\) and \(e_2\) is the unit square,
          \[
            \delta\begin{pmatrix}
              e_1 \\
              e_2
            \end{pmatrix} = \mathbf{O}\begin{pmatrix}
              e_1 \\
              e_2
            \end{pmatrix} \cdot \mathbf{A}\begin{pmatrix}
              e_1 \\
              e_2
            \end{pmatrix} = 1 \cdot 1 = 1.
          \]
          Therefore \(\delta\) satisfies the three conditions of \cref{ex:4.1.11}, and hence \(\delta = \det\).
          So the area of the parallelogram determined by \(u\) and \(v\) equals
          \[
            \mathbf{O}\begin{pmatrix}
              u \\
              v
            \end{pmatrix} \cdot \det\begin{pmatrix}
              u \\
              v
            \end{pmatrix}.
          \]
  \end{enumerate}
\end{proof}

\exercisesection

\setcounter{ex}{4}
\begin{ex}\label{ex:4.1.5}
  Prove that if \(B\) is the matrix obtained by interchanging the rows of \(A \in \ms{2}{2}{\F}\), then \(\det(B) = -\det(A)\).
\end{ex}

\begin{proof}[\pf{ex:4.1.5}]
  We have
  \begin{align*}
    \det(B) & = \det\begin{pmatrix}
                      A_{2 1} & A_{2 2} \\
                      A_{1 1} & A_{1 2}
                    \end{pmatrix}                                              \\
            & = A_{2 1} A_{1 2} - A_{2 2} A_{1 1}    &  & \text{(by \cref{4.1.1})} \\
            & = -(A_{1 1} A_{2 2} - A_{1 2} A_{2 1})                               \\
            & = -\det(A).                            &  & \text{(by \cref{4.1.1})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:4.1.6}
  Prove that if the two columns of \(A \in \ms{2}{2}{\F}\) are identical, then \(\det(A) = 0\).
\end{ex}

\begin{proof}[\pf{ex:4.1.6}]
  We have
  \begin{align*}
             & (A_{1 1}, A_{2 1}) = (A_{1 2}, A_{2 2})                                                                            \\
    \implies & \det(A) = A_{1 1} A_{2 2} - A_{1 2} A_{2 1} = A_{1 1} A_{2 2} - A_{1 1} A_{2 2} = 0. &  & \text{(by \cref{4.1.1})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:4.1.7}
  Prove that \(\det(\tp{A}) = \det(A)\) for any \(A \in \ms{2}{2}{\F}\).
\end{ex}

\begin{proof}[\pf{ex:4.1.7}]
  We have
  \begin{align*}
    \det(\tp{A}) & = \det\begin{pmatrix}
                           A_{1 1} & A_{2 1} \\
                           A_{1 2} & A_{2 2}
                         \end{pmatrix}               &  & \text{(by \cref{1.3.3})}   \\
                 & = A_{1 1} A_{2 2} - A_{2 1} A_{1 2} &  & \text{(by \cref{4.1.1})} \\
                 & = A_{1 1} A_{2 2} - A_{1 2} A_{2 1}                               \\
                 & = \det(A).                          &  & \text{(by \cref{4.1.1})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:4.1.8}
  Prove that if \(A \in \ms{2}{2}{\F}\) is upper triangular, then \(\det(A)\) equals the product of the diagonal entries of \(A\).
\end{ex}

\begin{proof}[\pf{ex:4.1.8}]
  We have
  \begin{align*}
             & A_{2 1} = 0                                                    &  & \text{(by \cref{ex:1.3.12})} \\
    \implies & \det(A) = A_{1 1} A_{2 2} - A_{1 2} A_{2 1} = A_{1 1} A_{2 2}. &  & \text{(by \cref{4.1.1})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:4.1.9}
  Prove that \(\det(AB) = \det(A) \cdot \det(B)\) for any \(A, B \in \ms{2}{2}{\F}\).
\end{ex}

\begin{proof}[\pf{ex:4.1.9}]
  We have
  \begin{align*}
     & \det(AB)                                                                                                                                      \\
     & = \det\begin{pmatrix}
               A_{1 1} B_{1 1} + A_{1 2} B_{2 1} & A_{1 1} B_{1 2} + A_{1 2} B_{2 2} \\
               A_{2 1} B_{1 1} + A_{2 2} B_{2 1} & A_{2 1} B_{1 2} + A_{2 2} B_{2 2}
             \end{pmatrix}                                        &  & \text{(by \cref{2.3.1})}                                                      \\
     & = (A_{1 1} B_{1 1} + A_{1 2} B_{2 1}) (A_{2 1} B_{1 2} + A_{2 2} B_{2 2})                                                                     \\
     & \quad - (A_{1 1} B_{1 2} + A_{1 2} B_{2 2}) (A_{2 1} B_{1 1} + A_{2 2} B_{2 1})                                 &  & \text{(by \cref{4.1.1})} \\
     & = (A_{1 1} A_{2 2}) (B_{1 1} B_{2 2} - B_{1 2} B_{2 1}) - (A_{1 2} A_{2 1}) (B_{1 1} B_{2 2} - B_{1 2} B_{2 1})                               \\
     & = (A_{1 1} A_{2 2} - A_{1 2} A_{2 1}) (B_{1 1} B_{2 2} - B_{1 2} B_{2 1})                                                                     \\
     & = \det(A) \det(B).                                                                                              &  & \text{(by \cref{4.1.1})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:4.1.10}
  The \textbf{classical adjoint} of a matrix \(A \in \ms{2}{2}{\F}\) is the matrix
  \[
    C = \begin{pmatrix}
      A_{2 2}  & -A_{1 2} \\
      -A_{2 1} & A_{1 1}
    \end{pmatrix}.
  \]
  Prove that
  \begin{enumerate}
    \item \(CA = AC = \det(A) I\).
    \item \(\det(C) = \det(A)\).
    \item The classical adjoint of \(\tp{A}\) is \(\tp{C}\).
    \item If \(A\) is invertible, then \(A^{-1} = (\det(A))^{-1} C\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:4.1.10}(a)]
  We have
  \begin{align*}
    CA & = \begin{pmatrix}
             C_{1 1} A_{1 1} + C_{1 2} A_{2 1} & C_{1 1} A_{1 2} + C_{1 2} A_{2 2} \\
             C_{2 1} A_{1 1} + C_{2 2} A_{2 1} & C_{2 1} A_{1 2} + C_{2 2} A_{2 2}
           \end{pmatrix}   &  & \text{(by \cref{2.3.1})}                                  \\
       & = \begin{pmatrix}
             A_{2 2} A_{1 1} - A_{1 2} A_{2 1}  & A_{2 2} A_{1 2} - A_{1 2} A_{2 2}  \\
             -A_{2 1} A_{1 1} + A_{1 1} A_{2 1} & -A_{2 1} A_{1 2} + A_{1 1} A_{2 2}
           \end{pmatrix} &  & \text{(by \cref{ex:4.1.10})}                                \\
       & = \begin{pmatrix}
             \det(A) & 0       \\
             0       & \det(A)
           \end{pmatrix}                                                       &  & \text{(by \cref{4.1.1})}      \\
       & = \det(A) I                                                                &  & \text{(by \cref{1.2.9})}
  \end{align*}
  and
  \begin{align*}
    AC & = \begin{pmatrix}
             A_{1 1} C_{1 1} + A_{1 2} C_{2 1} & A_{1 1} C_{1 2} + A_{1 2} C_{2 2} \\
             A_{2 1} C_{1 1} + A_{2 2} C_{2 1} & A_{2 1} C_{1 2} + A_{2 2} C_{2 2}
           \end{pmatrix}  &  & \text{(by \cref{2.3.1})}                                 \\
       & = \begin{pmatrix}
             A_{1 1} A_{2 2} - A_{1 2} A_{2 1} & -A_{1 1} A_{1 2} + A_{1 2} A_{1 1} \\
             A_{2 1} A_{2 2} - A_{2 2} A_{2 1} & -A_{2 1} A_{1 2} + A_{2 2} A_{1 1}
           \end{pmatrix} &  & \text{(by \cref{ex:4.1.10})}                                \\
       & = \begin{pmatrix}
             \det(A) & 0       \\
             0       & \det(A)
           \end{pmatrix}                                                      &  & \text{(by \cref{4.1.1})}      \\
       & = \det(A) I.                                                              &  & \text{(by \cref{1.2.9})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:4.1.10}(b)]
  We have
  \begin{align*}
    \det(C) & = A_{2 2} A_{1 1} - (-A_{1 2}) (-A_{2 1}) &  & \text{(by \cref{4.1.1})} \\
            & = A_{1 1} A_{2 2} - A_{1 2} A_{2 1}                                     \\
            & = \det(A).                                &  & \text{(by \cref{4.1.1})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:4.1.10}(c)]
  We have
  \begin{align*}
     & \text{the classical adjoint of } \tp{A}                                   \\
     & = \begin{pmatrix}
           (\tp{A})_{2 2}  & -(\tp{A})_{1 2} \\
           -(\tp{A})_{2 1} & (\tp{A})_{1 1}
         \end{pmatrix}    &  & \text{(by \cref{ex:4.1.10})}                      \\
     & = \begin{pmatrix}
           A_{2 2}  & -A_{2 1} \\
           -A_{1 2} & A_{1 1}
         \end{pmatrix}                  &  & \text{(by \cref{1.3.3})}            \\
     & = \tp{C}.                               &  & \text{(by \cref{ex:4.1.10})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:4.1.10}(d)]
  We have
  \begin{align*}
    \frac{1}{\det(A)} C & = \frac{1}{\det(A)} \begin{pmatrix}
                                                A_{2 2}  & -A_{1 2} \\
                                                -A_{2 1} & A_{1 1}
                                              \end{pmatrix} &  & \text{(by \cref{ex:4.1.10})} \\
                        & = A^{-1}.                           &  & \text{(by \cref{4.2})}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:4.1.11}
  Let \(\delta : \ms{2}{2}{\F} \to \F\) be a function with the following three properties.
  \begin{enumerate}
    \item \(\delta\) is a linear function of each row of the matrix when the other row is held fixed.
    \item If the two rows of \(A \in \ms{2}{2}{\F}\) are identical, then \(\delta(A) = 0\).
    \item \(\delta(I_2) = 1\).
  \end{enumerate}
  Prove that \(\delta(A) = \det(A)\) for all \(A \in \ms{2}{2}{\F}\).
\end{ex}

\begin{proof}[\pf{ex:4.1.11}]
  Let \(A \in \ms{n}{n}{\F}\), let \(e_1 = (1, 0)\) and let \(e_2 = (0, 1)\).
  Since
  \begin{align*}
    0 & = \delta\begin{pmatrix}
                  e_1 + e_2 \\
                  e_1 + e_2
                \end{pmatrix}               &  & \text{(by \cref{ex:4.1.11}(b))}                                                     \\
      & = \delta\begin{pmatrix}
                  e_1 \\
                  e_1 + e_2
                \end{pmatrix} + \delta\begin{pmatrix}
                                        e_2 \\
                                        e_1 + e_2
                                      \end{pmatrix} &  & \text{(by \cref{ex:4.1.11}(a))}                                             \\
      & = \delta\begin{pmatrix}
                  e_1 \\
                  e_1
                \end{pmatrix} + \delta\begin{pmatrix}
                                        e_1 \\
                                        e_2
                                      \end{pmatrix} + \delta\begin{pmatrix}
                                                              e_2 \\
                                                              e_1
                                                            \end{pmatrix} + \delta\begin{pmatrix}
                                                                                    e_2 \\
                                                                                    e_2
                                                                                  \end{pmatrix} &  & \text{(by \cref{ex:4.1.11}(a))} \\
      & = 0 + 1 + \delta\begin{pmatrix}
                          e_2 \\
                          e_1
                        \end{pmatrix} + 0,       &  & \text{(by \cref{ex:4.1.11}(b)(c))}
  \end{align*}
  we know that
  \[
    \delta\begin{pmatrix}
      e_2 \\
      e_1
    \end{pmatrix} = -1
  \]
  and thus
  \begin{align*}
    \delta(A) & = \delta\begin{pmatrix}
                          A_{1 1} & A_{1 2} \\
                          A_{2 1} & A_{2 2}
                        \end{pmatrix}                                                                                                   \\
              & = \delta\begin{pmatrix}
                          A_{1 1} e_1 + A_{1 2} e_2 \\
                          A_{2 1} e_1 + A_{2 2} e_2
                        \end{pmatrix}                                                                                           \\
              & = A_{1 1} \delta\begin{pmatrix}
                                  e_1 \\
                                  A_{2 1} e_1 + A_{2 2} e_2
                                \end{pmatrix} + A_{1 2} \delta\begin{pmatrix}
                                                                e_2 \\
                                                                A_{2 1} e_1 + A_{2 2} e_2
                                                              \end{pmatrix}             &  & \text{(by \cref{ex:4.1.11}(a))}                \\
              & = A_{1 1} \pa{A_{2 1} \delta\begin{pmatrix}
                                                e_1 \\
                                                e_1
                                              \end{pmatrix} + A_{2 2} \delta\begin{pmatrix}
                                                                              e_1 \\
                                                                              e_2
                                                                            \end{pmatrix}}             &  & \text{(by \cref{ex:4.1.11}(a))}   \\
              & \quad + A_{1 2} \pa{A_{2 1} \delta\begin{pmatrix}
                                                      e_2 \\
                                                      e_1
                                                    \end{pmatrix} + A_{2 2} \delta\begin{pmatrix}
                                                                                    e_2 \\
                                                                                    e_2
                                                                                  \end{pmatrix}}         &  & \text{(by \cref{ex:4.1.11}(a))} \\
              & = A_{1 1} A_{2 2} \delta\begin{pmatrix}
                                          e_1 \\
                                          e_2
                                        \end{pmatrix} + A_{1 2} A_{2 1} \delta\begin{pmatrix}
                                                                                e_2 \\
                                                                                e_1
                                                                              \end{pmatrix}     &  & \text{(by \cref{ex:4.1.11}(b))}        \\
              & = A_{1 1} A_{2 2} + A_{1 2} A_{2 1} \delta\begin{pmatrix}
                                                            e_2 \\
                                                            e_1
                                                          \end{pmatrix} &  & \text{(by \cref{ex:4.1.11}(c))}                                \\
              & = A_{1 1} A_{2 2} - A_{1 2} A_{2 1}                       &  & \text{(from the proof above)}                                \\
              & = \det(A).                                                &  & \text{(by \cref{4.1.1})}
  \end{align*}
  Since \(A\) is arbitrary, we conclude that \(\delta = \det\).
\end{proof}

\begin{ex}\label{ex:4.1.12}
  Let \(\set{u, v}\) be an ordered basis for \(\R^2\).
  Prove that
  \[
    \mathbf{O}\begin{pmatrix}
      u \\
      v
    \end{pmatrix} = 1
  \]
  iff \(\set{u, v}\) forms a right-handed coordinate system.
\end{ex}

\begin{proof}[\pf{ex:4.1.12}]
  First suppose that \(\set{u, v}\) forms a right-handed coordinate system.
  By \cref{4.1.2} there exists a \(\theta \in (0, \pi)\) such that by rotating \(u\) with angle \(\theta\) and scaling \(u\) with some \(t > 0\) we get \(v\).
  Note that \(\theta \neq 0\) since \(\set{u, v}\) is a basis for \(\R^2\) over \(\R\).
  By \cref{2.1.3} this means
  \[
    v = t (u_1 \cos(\theta) - u_2 \sin(\theta), u_1 \sin(\theta) + u_2 \cos(\theta)).
  \]
  Thus we have
  \begin{align*}
    \det\begin{pmatrix}
          u \\
          v
        \end{pmatrix} & = \det\begin{pmatrix}
                                u_1                                     & u_2                                     \\
                                t u_1 \cos(\theta) - t u_2 \sin(\theta) & t u_1 \sin(\theta) + t u_2 \cos(\theta)
                              \end{pmatrix}                              \\
                        & = t \det\begin{pmatrix}
                                    u_1                                 & u_2                                 \\
                                    u_1 \cos(\theta) - u_2 \sin(\theta) & u_1 \sin(\theta) + u_2 \cos(\theta)
                                  \end{pmatrix}         &  & \text{(by \cref{4.1})}                                  \\
                        & = t (u_1^2 \sin(\theta) + u_2^2 \sin(\theta))                                        &  & \text{(by \cref{4.1.1})} \\
                        & > 0.                                                                                 &  & (\theta \in (0, \pi))
  \end{align*}
  By \cref{4.1.2} we have
  \[
    \mathbf{O}\begin{pmatrix}
      u \\
      v
    \end{pmatrix} = \frac{\det\begin{pmatrix}
        u \\
        v
      \end{pmatrix}}{\abs{\det\begin{pmatrix}
          u \\
          v
        \end{pmatrix}}} = \frac{\det\begin{pmatrix}
        u \\
        v
      \end{pmatrix}}{\det\begin{pmatrix}
        u \\
        v
      \end{pmatrix}} = 1.
  \]

  Now suppose that
  \[
    \mathbf{O}\begin{pmatrix}
      u \\
      v
    \end{pmatrix} = 1.
  \]
  Since \(\set{u, v}\) is a basis for \(\R^2\), by rotating \(u\) with angle \(\theta > 0\) and scaling \(u\) with \(t > 0\) we get \(v\).
  By \cref{2.1.3} this means
  \[
    v = t (u_1 \cos(\theta) - u_2 \sin(\theta), u_1 \sin(\theta) + u_2 \cos(\theta)).
  \]
  Then we have
  \begin{align*}
             & \mathbf{O}\begin{pmatrix}
                           u \\
                           v
                         \end{pmatrix} = 1                                                        \\
    \implies & \det\begin{pmatrix}
                     u \\
                     v
                   \end{pmatrix} > 0                             &  & \text{(by \cref{4.1.2})}    \\
    \implies & u_1 v_2 - u_2 v_1 > 0                           &  & \text{(by \cref{4.1.1})}      \\
    \implies & t u_1^2 \sin(\theta) + t u_2^2 \sin(\theta) > 0 &  & \text{(from the proof above)} \\
    \implies & \sin(\theta) > 0                                &  & (t > 0)                       \\
    \implies & \theta \in (0, \pi)
  \end{align*}
  and thus by \cref{4.1.2} \(\set{u, v}\) forms a right-handed coordinate system.
\end{proof}
