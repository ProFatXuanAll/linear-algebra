\section{Linear Dependence and Linear Independence}\label{sec:1.5}

\begin{note}
  Suppose that \(\V\) is a vector space over an infinite field and that \(\W\) is a subspace of \(\V\).
  Unless \(\W\) is the zero subspace, \(\W\) is an infinite set.
  It is desirable to find a ``small'' finite subset \(S\) that generates \(\W\) because we can then describe each vector in \(\W\) as a linear combination of the finite number of vectors in \(S\).
  Indeed, the smaller that \(S\) is, the fewer computations that are required to represent vectors in \(\W\).

  Checking that some vector in \(S\) is a linear combination of the other vectors in \(S\) could require that we solve several different systems of linear equations before we determine which, if any, vectors in \(S\) is a linear combination of the others.

  Because some vector in \(S\) is a linear combination of the others, the zero vector can be expressed as a linear combination of the vectors in \(S\) using coefficients that are not all zero.
  The converse of this statement is also true:
  If the zero vector can be written as a linear combination of the vectors in \(S\) in which not all the coefficients are zero, then some vector in \(S\) is a linear combination of the others.
  Thus, rather than asking whether some vector in \(S\) is a linear combination of the other vectors in \(S\), it is more efficient to ask whether the zero vector can be expressed as a linear combination of the vectors in \(S\) with coefficients that are not all zero.
\end{note}

\begin{defn}\label{1.5.1}
  A subset \(S\) of a vector space \(\V\) over \(\F\) is called \textbf{linearly dependent} if there exist a finite number of distinct vectors \(\seq{u}{1,2,,n}\) in \(S\) and scalars \(\seq{a}{1,2,,n}\) in \(\F\), not all zero, such that
  \[
    \seq[+]{a,u}{1,2,,n} = \zv.
  \]
  In this case we also say that the vectors of \(S\) are linearly dependent.
\end{defn}

\begin{defn}\label{1.5.2}
  For any vectors \(\seq{u}{1,2,,n}\), we have \(\seq[+]{a,u}{1,2,,n} = \zv\) if \(\seq[=]{a}{1,2,,n} = 0\).
  We call this the \textbf{trivial representation} of \(\zv\) as a linear combination of \(\seq{u}{1,2,,n}\).
  Thus, for a set to be linearly dependent, there must exist a nontrivial representation of \(\zv\) as a linear combination of vectors in the set.
  Consequently, any subset of a vector space that contains the zero vector is linearly dependent, because \(\zv = 1 \cdot \zv\) is a nontrivial representation of \(\zv\) as a linear combination of vectors in the set.
\end{defn}

\begin{defn}\label{1.5.3}
  A subset \(S\) of a vector space over \(\F\) that is not linearly dependent is called \textbf{linearly independent}.
  As before, we also say that the vectors of \(S\) are linearly independent.
\end{defn}

\begin{eg}\label{1.5.4}
  The following facts about linearly independent sets are true in any vector space.
  \begin{enumerate}
    \item The empty set is linearly independent, for linearly dependent sets must be nonempty.
    \item A set consisting of a single nonzero vector is linearly independent.
          For if \(\set{u}\) is linearly dependent, then \(au = \zv\) for some nonzero scalar \(a\).
          Thus
          \[
            u = a^{-1} (au) = a^{-1} \zv = \zv.
          \]
    \item A set is linearly independent if and only if the only representations of \(\zv\) as linear combinations of its vectors are trivial representations.
  \end{enumerate}
\end{eg}

\begin{eg}\label{1.5.5}
  For \(k = 0, 1, \dots, n\) let \(p_k(x) = x^k + x^{k + 1} + \cdots + x^n\).
  The set
  \[
    \set{p_0(x), p_1(x), \dots, p_n(x)}
  \]
  is linearly independent in \(\ps[n]{\F}\).
  For if
  \[
    a_0 p_0(x) + a_1 p_1(x) + \cdots + a_n p_n(x) = \zv
  \]
  for some scalars \(\seq{a}{0,1,,n}\), then
  \[
    a_0 + (a_0 + a_1) x + (a_0 + a_1 + a_2) x^2 + \cdots + (a_0 + a_1 + \cdots + a_n) x^n = \zv.
  \]
  By equating the coefficients of \(x^k\) on both sides of this equation for \(k = 0, 1, \dots, n\), we obtain
  \[
    \begin{matrix*}[l]
      & \seq[+]{a}{0}      & = 0 \\
      & \seq[+]{a}{0,1}    & = 0 \\
      & \seq[+]{a}{0,1,2}  & = 0 \\
      & \vdots & \\
      & \seq[+]{a}{0,1,2,,n} & = 0
    \end{matrix*}
  \]
  Clearly the only solution to this system of linear equations is \(\seq[=]{a}{0,1,,n} = 0\).
\end{eg}

\begin{thm}\label{1.6}
  Let \(\V\) be a vector space over \(\F\), and let \(S_1 \subseteq S_2 \subseteq \V\).
  If \(S_1\) is linearly dependent, then \(S_2\) is linearly dependent.
\end{thm}

\begin{proof}[\pf{1.6}]
  We have
  \begin{align*}
             & \begin{dcases}
      S_1 \subseteq S_2 \\
      S_1 \text{ is linearly dependent}
    \end{dcases}                                        \\
    \implies & \begin{dcases}
      \exists \seq{u}{1,2,,n} \in S_1 \subseteq S_2 \\
      \exists \seq{a}{1,2,,n} \in \F
    \end{dcases} :                                      \\
             & \begin{dcases}
      \seq[+]{a,u}{1,2,,n} = \zv \\
      \lnot (\seq[=]{a}{1,2,,n} = 0)
    \end{dcases}         &  & \text{(by \cref{1.5.1})} \\
    \implies & S_2 \text{ is linearly dependent}. &  & \text{(by \cref{1.5.1})}
  \end{align*}
\end{proof}

\begin{cor}\label{1.5.6}
  Let \(\V\) be a vector space, and let \(S_1 \subseteq S_2 \subseteq \V\).
  If \(S_2\) is linearly independent, then \(S_1\) is linearly independent.
\end{cor}

\begin{proof}[\pf{1.5.6}]
  Suppose for sake of contradiction that \(S_1\) is linearly dependent.
  But by \cref{1.6} \(S_1 \subseteq S_2\) implies \(S_2\) is linearly dependent, which contradict to the fact that \(S_2\) is linearly independent.
  Thus \(S_1\) is linearly independent.
\end{proof}

\begin{note}
  Earlier in this section, we noted that the issue of whether \(S\) is the smallest generating set for its span is related to the question of whether some vector in \(S\) is a linear combination of the other vectors in \(S\).
  Thus the issue of whether \(S\) is the smallest generating set for its span is related to the question of whether \(S\) is linearly dependent.

  More generally, suppose that \(S\) is any linearly dependent set containing two or more vectors.
  Then some vector \(v \in S\) can be written as a linear combination of the other vectors in \(S\), and the subset obtained by removing \(v\) from \(S\) has the same span as \(S\).
  It follows that \emph{if no proper subset of \(S\) generates the span of \(S\), then \(S\) must be linearly independent.}
\end{note}

\begin{thm}\label{1.7}
  Let \(S\) be a linearly independent subset of a vector space \(\V\) over \(\F\), and let \(v\) be a vector in \(\V\) that is not in \(S\).
  Then \(S \cup \set{v}\) is linearly dependent if and only if \(v \in \spn{S}\).
\end{thm}

\begin{proof}[\pf{1.7}]
  By \cref{1.5.2} we see that when \(v = \zv\) the statement holds.
  So suppose that \(v \neq \zv\).

  If \(S \cup \set{v}\) is linearly dependent, then there are vectors \\
  \(\seq{u}{1,2,,n}\) in \(S\) such that \(a_0 v + \seq[+]{a,u}{1,2,,n} = \zv\) for some nonzero scalars \(\seq{a}{0,1,2,,n}\) in \(\F\).
  We claim that \(a_0 \neq 0\).
  So suppose for sake of contradiction that \(a_0 = 0\).
  Because \(S\) is linearly independent, we know that
  \begin{align*}
             & a_0 v + \seq[+]{a,u}{1,2,,n}                                \\
             & = 0v + \seq[+]{a,u}{1,2,,n}                                 \\
             & = \zv + \seq[+]{a,u}{1,2,,n} &  & \text{(by \cref{1.2}(a))} \\
             & = \seq[+]{a,u}{1,2,,n}       &  & \text{(by \ref{vs3})}     \\
             & = \zv                                                       \\
    \implies & \seq[=]{a}{1,2,,n} = 0.      &  & \text{(by \cref{1.5.3})}
  \end{align*}
  But this means \(\seq[=]{a}{0,1,2,,n} = 0\), a contradiction.
  Thus we must have \(a_0 \neq 0\), and so
  \[
    v = a_0^{-1} (-\seq[-]{a,u}{1,2,,n}) = -(a_0^{-1} a_1) u_1 - (a_0^{-1} a_2) u_2 - \cdots - (a_0^{-1} a_n) u_n.
  \]
  Since \(v\) is a linear combination of \(\seq{u}{1,2,,n}\), which are in \(S\), we have \(v \in \spn{S}\).

  Conversely, let \(v \in \spn{S}\).
  Then there exist vectors \(\seq{v}{1,2,,m}\) in \(S\) and scalars \(\seq{b}{1,2,,m}\) such that \(v = \seq[+]{b,v}{1,2,,m}\).
  Hence
  \[
    \zv = \seq[+]{b,v}{1,2,,m} + (-1)v.
  \]
  Since \(v \neq v_i\) for \(i = 1, 2, \dots, m\), the coefficient of \(v\) in this linear combination is nonzero, and so the set \(\set{\seq{v}{1,2,,m}, v}\) is linearly dependent.
  Therefore \(S \cup \set{v}\) is linearly dependent by \cref{1.6}.
\end{proof}

\exercisesection

\setcounter{ex}{3}
\begin{ex}\label{ex:1.5.4}
  In \(\vs{F}^n\), let \(e_j\) denote the vector whose \(j\)th coordinate is \(1\) and whose other coordinates are \(0\).
  Prove that \(\set{\seq{e}{1,2,,n}}\) is linearly independent.
\end{ex}

\begin{proof}[\pf{ex:1.5.4}]
  Let \(\seq{a}{1,2,,n} \in \F\).
  Since
  \begin{align*}
             & \seq[+]{a,e}{1,2,,n} = \zv                                                      \\
    \implies & \begin{dcases}
      a_1 1 + a_2 0 + \cdots + a_n 0 = 0 \\
      a_1 0 + a_2 1 + \cdots + a_n 0 = 0 \\
      \vdots                             \\
      a_1 0 + a_2 0 + \cdots + a_n 1 = 0
    \end{dcases}                        &  & \text{(by \cref{1.2.4})} \\
    \implies & \forall i \in \set{1, \dots, n}, a_i 1 = a_i = 0,
  \end{align*}
  by \cref{1.5.3} we know that \(\set{\seq{e}{1,2,,n}}\) is linearly independent.
\end{proof}

\begin{ex}\label{ex:1.5.5}
  Show that the set \(\set{1, x, x^2, \dots, x^n}\) is linearly independent in \(\ps[n]{\F}\).
\end{ex}

\begin{proof}[\pf{ex:1.5.5}]
  Let \(\seq{a}{0,1,2,,n} \in \F\).
  Since
  \begin{align*}
             & a_0 + a_1 x^1 + a_2 x^2 + \cdots + a_n x^n = \zv = 0 + 0x + 0x^2 + \cdots + 0x^n \\
    \implies & \seq[=]{a}{0,1,2,,n} = 0,
  \end{align*}
  by \cref{1.5.3} we know that \(\set{1, x, x^2, \dots, x^n}\) is linearly independent.
\end{proof}

\begin{ex}\label{ex:1.5.6}
  In \(\MS\), let \(E_{i j}\) denote the matrix whose only nonzero entry is \(1\) in the \(i\)th row and \(j\)th column.
  Prove that \(\set{E_{i j} : 1 \leq i \leq m, 1 \leq j \leq n}\) is linearly independent.
\end{ex}

\begin{proof}[\pf{ex:1.5.6}]
  Let \(a_{i j} \in \F\) where \(1 \leq i \leq m\) and \(1 \leq j \leq n\).
  Since
  \begin{align*}
             & \sum_{i = 1}^m \sum_{j = 1}^{n} a_{i j} E_{i j} = \zm                               \\
    \implies & \begin{pmatrix}
      a_{1 1} 1 & a_{1 2} 1 & \cdots & a_{1 n} 1 \\
      a_{2 1} 1 & a_{2 2} 1 & \cdots & a_{2 n} 1 \\
      \vdots    & \vdots    & \ddots & \vdots    \\
      a_{m 1} 1 & a_{m 2} 1 & \cdots & a_{m n} 1
    \end{pmatrix}                                                          \\
             & = \begin{pmatrix}
      a_{1 1} & a_{1 2} & \cdots & a_{1 n} \\
      a_{2 1} & a_{2 2} & \cdots & a_{2 n} \\
      \vdots  & \vdots  & \ddots & \vdots  \\
      a_{m 1} & a_{m 2} & \cdots & a_{m n}
    \end{pmatrix} = \zm                                                  \\
    \implies & a_{i j} = 0,                                          &  & \text{(by \cref{1.2.8})}
  \end{align*}
  by \cref{1.5.3} we know that \(\set{E_{i j} : 1 \leq i \leq m, 1 \leq j \leq n}\) is linearly independent.
\end{proof}

\setcounter{ex}{7}
\begin{ex}\label{ex:1.5.8}
  Let \(S = \set{(1, 1, 0), (1, 0, 1), (0, 1, 1)}\) be a subset of the vector space \(\vs{F}^3\) over \(\F\).
  \begin{enumerate}
    \item Prove that if \(\F = \R\), then \(S\) is linearly independent.
    \item Prove that if \(\F\) has characteristic \(2\), then \(S\) is linearly dependent.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:1.5.8}(a)]
  Let \(\seq{a}{1,2,3} \in \R\).
  Since
  \begin{align*}
             & a_1 (1, 1, 0) + a_2 (1, 0, 1) + a_3 (0, 1, 1) = (0, 0, 0)                               \\
    \implies & \begin{dcases}
      a_1 1 + a_2 1 + a_3 0 = 0 \\
      a_1 1 + a_2 0 + a_3 1 = 0 \\
      a_1 0 + a_2 1 + a_3 1 = 0
    \end{dcases}                                &  & \text{(by \cref{1.2.4})} \\
    \implies & \seq[=]{a}{1,2,3} = 0,
  \end{align*}
  by \cref{1.5.3} we know that \(S\) is linearly independent.
\end{proof}

\begin{proof}[\pf{ex:1.5.8}(b)]
  Observe that
  \begin{align*}
    (1, 1, 0) + (1, 0, 1) + (0, 1, 1) & = (1 + 1, 1 + 0, 0 + 1) + (0, 1, 1) \\
                                      & = (0, 1, 1) + (0, 1, 1)             \\
                                      & = (0 + 0, 1 + 1, 1 + 1)             \\
                                      & = (0, 0, 0).
  \end{align*}
  Thus by \cref{1.5.1} \(S\) is linearly dependent.
\end{proof}

\begin{ex}\label{ex:1.5.9}
  Let \(u\) and \(v\) be distinct vectors in a vector space \(\V\) over \(\F\).
  Show that \(\set{u, v}\) is linearly dependent if and only if \(u\) or \(v\) is a multiple of the other.
\end{ex}

\begin{proof}[\pf{ex:1.5.9}]
  We have
  \begin{align*}
         & \set{u, v} \text{ is linearly dependent}                                       \\
    \iff & \exists a, b \in \F : \begin{dcases}
      au + bv = \zv \\
      \lnot (a = b = 0)
    \end{dcases} &  & \text{(by \cref{1.5.1})} \\
    \iff & \exists a, b \in \F : \begin{dcases}
      au = -bv \\
      \lnot (a = b = 0)
    \end{dcases}                               \\
    \iff & \exists a, b \in \F : \begin{dcases}
      u = -\frac{b}{a} v & \text{if } a \neq 0 \\
      v = -\frac{a}{b} u & \text{if } b \neq 0
    \end{dcases}                               \\
    \iff & \exists c \in \F : u = cv.
  \end{align*}
\end{proof}

\setcounter{ex}{10}
\begin{ex}\label{ex:1.5.11}
  Let \(S = \set{\seq{u}{1,2,,n}}\) be a linearly independent subset of a vector space \(\V\) over the field \(\Z_2\).
  How many vectors are there in \(\spn{S}\)?
  Justify your answer.
\end{ex}

\begin{proof}[\pf{ex:1.5.11}]
  We have
  \[
    \forall v \in \spn{S}, \exists \seq{a}{1,2,,n} \in \Z_2 : \seq[+]{a,u}{1,2,,n} = v
  \]
  and thus there are \(2^n\) vectors in \(\spn{S}\).
\end{proof}
