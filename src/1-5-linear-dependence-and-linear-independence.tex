\section{Linear Dependence and Linear Independence}\label{sec:1.5}

\begin{note}
  Suppose that \(\V\) is a vector space over an infinite field and that \(\W\) is a subspace of \(\V\).
  Unless \(\W\) is the zero subspace, \(\W\) is an infinite set.
  It is desirable to find a ``small'' finite subset \(S\) that generates \(\W\) because we can then describe each vector in \(\W\) as a linear combination of the finite number of vectors in \(S\).
  Indeed, the smaller that \(S\) is, the fewer computations that are required to represent vectors in \(\W\).

  Checking that some vector in \(S\) is a linear combination of the other vectors in \(S\) could require that we solve several different systems of linear equations before we determine which, if any, vectors in \(S\) is a linear combination of the others.

  Because some vector in \(S\) is a linear combination of the others, the zero vector can be expressed as a linear combination of the vectors in \(S\) using coefficients that are not all zero.
  The converse of this statement is also true:
  If the zero vector can be written as a linear combination of the vectors in \(S\) in which not all the coefficients are zero, then some vector in \(S\) is a linear combination of the others.
  Thus, rather than asking whether some vector in \(S\) is a linear combination of the other vectors in \(S\), it is more efficient to ask whether the zero vector can be expressed as a linear combination of the vectors in \(S\) with coefficients that are not all zero.
\end{note}

\begin{defn}\label{1.5.1}
  A subset \(S\) of a vector space \(\V\) over \(\F\) is called \textbf{linearly dependent} if there exist a finite number of distinct vectors \(\seq{u}{1,2,,n}\) in \(S\) and scalars \(\seq{a}{1,2,,n}\) in \(\F\), not all zero, such that
  \[
    \seq[+]{a,u}{1,2,,n} = \zv.
  \]
  In this case we also say that the vectors of \(S\) are linearly dependent.
\end{defn}

\begin{defn}\label{1.5.2}
  For any vectors \(\seq{u}{1,2,,n}\), we have \(\seq[+]{a,u}{1,2,,n} = \zv\) if \(\seq[=]{a}{1,2,,n} = 0\).
  We call this the \textbf{trivial representation} of \(\zv\) as a linear combination of \(\seq{u}{1,2,,n}\).
  Thus, for a set to be linearly dependent, there must exist a nontrivial representation of \(\zv\) as a linear combination of vectors in the set.
  Consequently, any subset of a vector space that contains the zero vector is linearly dependent, because \(\zv = 1 \cdot \zv\) is a nontrivial representation of \(\zv\) as a linear combination of vectors in the set.
\end{defn}

\begin{defn}\label{1.5.3}
  A subset \(S\) of a vector space over \(\F\) that is not linearly dependent is called \textbf{linearly independent}.
  As before, we also say that the vectors of \(S\) are linearly independent.
\end{defn}

\begin{eg}\label{1.5.4}
  The following facts about linearly independent sets are true in any vector space.
  \begin{enumerate}
    \item The empty set is linearly independent, for linearly dependent sets must be nonempty.
    \item A set consisting of a single nonzero vector is linearly independent.
          For if \(\set{u}\) is linearly dependent, then \(au = \zv\) for some nonzero scalar \(a\).
          Thus
          \[
            u = a^{-1} (au) = a^{-1} \zv = \zv.
          \]
    \item A set is linearly independent if and only if the only representations of \(\zv\) as linear combinations of its vectors are trivial representations.
  \end{enumerate}
\end{eg}

\begin{eg}\label{1.5.5}
  For \(k = 0, 1, \dots, n\) let \(p_k(x) = x^k + x^{k + 1} + \cdots + x^n\).
  The set
  \[
    \set{p_0(x), p_1(x), \dots, p_n(x)}
  \]
  is linearly independent in \(\ps[n]{\F}\).
  For if
  \[
    a_0 p_0(x) + a_1 p_1(x) + \cdots + a_n p_n(x) = \zv
  \]
  for some scalars \(\seq{a}{0,1,,n}\), then
  \[
    a_0 + (a_0 + a_1) x + (a_0 + a_1 + a_2) x^2 + \cdots + (a_0 + a_1 + \cdots + a_n) x^n = \zv.
  \]
  By equating the coefficients of \(x^k\) on both sides of this equation for \(k = 0, 1, \dots, n\), we obtain
  \[
    \begin{matrix*}[l]
      & \seq[+]{a}{0}      & = 0 \\
      & \seq[+]{a}{0,1}    & = 0 \\
      & \seq[+]{a}{0,1,2}  & = 0 \\
      & \vdots & \\
      & \seq[+]{a}{0,1,2,,n} & = 0
    \end{matrix*}
  \]
  Clearly the only solution to this system of linear equations is \(\seq[=]{a}{0,1,,n} = 0\).
\end{eg}
