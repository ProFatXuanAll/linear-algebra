\section{Composition of Linear Transformations and Matrix Multiplication}\label{sec:2.3}

\begin{note}
  We use the more convenient notation of \(\U \T\) rather than \(\U \circ \T\) for the composite of linear transformations \(\U\) and \(\T\).
\end{note}

\begin{thm}\label{2.9}
  Let \(\V\), \(\W\), and \(\vs{Z}\) be vector spaces over the same field \(\F\), and let \(\T : \V \to \W\) and \(\U : \W \to \vs{Z}\) be linear.
  Then \(\U \T : \V \to \vs{Z}\) is linear.
\end{thm}

\begin{proof}[\pf{2.9}]
  Let \(x, y \in \V\) and \(a \in \F\).
  Then
  \begin{align*}
    \U \T(ax + y) & = \U(\T(ax + y))                                            \\
                  & = \U(a \T(x) + \T(y))      &  & \text{(by \cref{2.1.2}(b))} \\
                  & = a \U(\T(x)) + \U(\T(y))  &  & \text{(by \cref{2.1.2}(b))} \\
                  & = a (\U \T)(x) + \U \T(y).
  \end{align*}
\end{proof}

\begin{thm}\label{2.10}
  Let \(\V\) be a vector space over \(\F\).
  Let \(\T, \U_1, \U_2 \in \ls(\V)\).
  Then
  \begin{enumerate}
    \item \(\T(\U_1 + \U_2) = \T \U_1 + \T \U_2\) and \((\U_1 + \U_2) \T = \U_1 \T + \U_2 \T\).
    \item \(\T(\U_1 \U_2) = (\T \U_1) \U_2\).
    \item \(\T \IT = \IT \T = \T\).
    \item \(a(\U_1 \U_2) = (a \U_1) \U_2 = \U_1 (a \U_2)\) for all scalars \(a\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{2.10}(a)]
  For all \(x \in \V\), we have
  \begin{align*}
    (\T(\U_1 + \U_2))(x) & = \T((\U_1 + \U_2)(x))                                      \\
                         & = \T(\U_1(x) + \U_2(x))       &  & \text{(by \cref{2.2.5})} \\
                         & = \T(\U_1(x)) + \T(\U_2(x))   &  & \text{(by \cref{2.1.1})} \\
                         & = (\T \U_1)(x) + (\T \U_2)(x)                               \\
                         & = (\T \U_1 + \T \U_2)(x)      &  & \text{(by \cref{2.2.5})}
  \end{align*}
  and
  \begin{align*}
    ((\U_1 + \U_2) \T)(x) & = (\U_1 + \U_2)(\T(x))                                      \\
                          & = \U_1(\T(x)) + \U_2(\T(x))   &  & \text{(by \cref{2.2.5})} \\
                          & = (\U_1 \T)(x) + (\U_2 \T)(x)                               \\
                          & = (\U_1 \T + \U_2 \T)(x).     &  & \text{(by \cref{2.2.5})}
  \end{align*}
  Thus \(\T(\U_1 + \U_2) = \T \U_1 + \T \U_2\) and \((\U_1 + \U_2) \T = \U_1 \T + \U_2 \T\).
\end{proof}

\begin{proof}[\pf{2.10}(b)]
  For all \(x \in \V\), we have
  \begin{align*}
    (\T (\U_1 \U_2))(x) & = \T((\U_1 \U_2)(x))  \\
                        & = \T(\U_1(\U_2(x)))   \\
                        & = (\T \U_1)(\U_2(x))  \\
                        & = ((\T \U_1) \U_2)(x)
  \end{align*}
  and thus \(\T (\U_1 \U_2) = (\T \U_1) \U_2\).
\end{proof}

\begin{proof}[\pf{2.10}(c)]
  For all \(x \in \V\), we have
  \begin{align*}
    (\T \IT)(x) & = \T(\IT(x))                                \\
                & = \T(x)       &  & \text{(by \cref{2.1.9})} \\
                & = \IT(\T(x))  &  & \text{(by \cref{2.1.9})} \\
                & = (\IT \T)(x)
  \end{align*}
  and thus \(\T \IT = \T = \IT \T\).
\end{proof}

\begin{proof}[\pf{2.10}(d)]
  For all \(x \in \V\), we have
  \begin{align*}
    (a (\U_1 \U_2))(x) & = a (\U_1 \U_2)(x)                                 \\
                       & = a \U_1(\U_2(x))                                  \\
                       & = (a \U_1)(\U_2(x))  &  & \text{(by \cref{2.2.5})} \\
                       & = ((a \U_1) \U_2)(x)                               \\
                       & = \U_1(a \U_2(x))    &  & \text{(by \cref{2.1.1})} \\
                       & = \U_1((a \U_2)(x))                                \\
                       & = (\U_1 (a \U_2))(x)
  \end{align*}
  and thus \(a (\U_1 \U_2) = (a \U_1) \U_2 = \U_1 (a \U_2)\).
\end{proof}

\begin{defn}\label{2.3.1}
  Let \(A \in \MS\) matrix and \(B \in \ms{n}{p}{\F}\).
  We define the \textbf{product} of \(A\) and \(B\), denoted \(AB\), to be the \(m \times p\) matrix such that
  \[
    (AB)_{i j} = \sum_{k = 1}^n A_{i k} B_{k j} \quad \text{for } 1 \leq i \leq m, 1 \leq j \leq p.
  \]
  Note that \((AB)_{i j}\) is the sum of products of corresponding entries from the \(i\)th row of \(A\) and the \(j\)th column of \(B\).
\end{defn}

\begin{note}
  The reader should observe that in order for the product \(AB\) to be defined, there are restrictions regarding the relative sizes of \(A\) and \(B\).
  The following mnemonic device is helpful:
  ``\((m \times n) \cdot (n \times p) = (m \times p)\)'';
  that is, in order for the product \(AB\) to be defined, the two ``inner'' dimensions must be equal, and the two ``outer'' dimensions yield the size of the product.
\end{note}

\begin{note}
  As in the case with composition of functions, we have that matrix multiplication is not commutative. Consider the following two products:
  \[
    \begin{pmatrix}
      1 & 1 \\
      0 & 0
    \end{pmatrix} \begin{pmatrix}
      0 & 1 \\
      1 & 0
    \end{pmatrix} = \begin{pmatrix}
      1 & 1 \\
      0 & 0
    \end{pmatrix} \quad \text{and} \quad \begin{pmatrix}
      0 & 1 \\
      1 & 0
    \end{pmatrix} \begin{pmatrix}
      1 & 1 \\
      0 & 0
    \end{pmatrix} = \begin{pmatrix}
      0 & 0 \\
      1 & 1
    \end{pmatrix}.
  \]
  Hence we see that even if both of the matrix products \(AB\) and \(BA\) are defined, it need not be true that \(AB = BA\).
\end{note}

\begin{eg}\label{2.3.2}
  If \(A \in \MS\) and \(B \in \ms{n}{p}{\F}\), then \(\tp{(AB)} = \tp{B} \tp{A}\).
\end{eg}

\begin{proof}[\pf{2.3.2}]
  Since
  \[
    \tp{(AB)}_{i j} = (AB)_{j i} = \sum_{k = 1}^n A_{j k} B_{k i}
  \]
  and
  \[
    (\tp{B} \tp{A})_{i j} = \sum_{k = 1}^n \tp{B}_{i k} \tp{A}_{k j} = \sum_{k = 1}^n B_{k i} A_{j k},
  \]
  we are finished.
  Therefore the transpose of a product is the product of the transposes in the \emph{opposite order}.
\end{proof}

\begin{thm}\label{2.11}
  Let \(\V\), \(\W\), and \(\vs{Z}\) be finite-dimensional vector spaces over \(\F\) with ordered bases \(\alpha\), \(\beta\), and \(\gamma\) over \(\F\), respectively.
  Let \(\T : \V \to \W\) and \(\U : \W \to \vs{Z}\) be linear transformations.
  Then
  \[
    [\U \T]_{\alpha}^{\gamma} = [\U]_{\beta}^{\gamma} [\T]_{\alpha}^{\beta}.
  \]
\end{thm}

\begin{proof}[\pf{2.11}]
  Define
  \begin{align*}
    \alpha & = \set{\seq{v}{1,2,,n}}; \\
    \beta  & = \set{\seq{w}{1,2,,m}}; \\
    \gamma & = \set{\seq{z}{1,2,,p}}.
  \end{align*}
  For \(1 \leq j \leq n\), we have
  \begin{align*}
    (\U \T)(v_j) & = \sum_{i = 1}^p ([\U \T]_{\alpha}^{\gamma})_{i j} \cdot z_i                                                      &  & \text{(by \cref{2.2.4})}    \\
                 & = \U(\T(v_j))                                                                                                                                      \\
                 & = \U\pa{\sum_{k = 1}^m ([\T]_{\alpha}^{\beta})_{k j} \cdot w_k}                                                   &  & \text{(by \cref{2.2.4})}    \\
                 & = \sum_{k = 1}^m ([\T]_{\alpha}^{\beta})_{k j} \cdot \U(w_k)                                                      &  & \text{(by \cref{2.1.2}(d))} \\
                 & = \sum_{k = 1}^m ([\T]_{\alpha}^{\beta})_{k j} \cdot \pa{\sum_{i = 1}^p ([\U]_{\beta}^{\gamma})_{i k} \cdot z_i}  &  & \text{(by \cref{2.2.4})}    \\
                 & = \sum_{k = 1}^m \sum_{i = 1}^p \pa{([\T]_{\alpha}^{\beta})_{k j}  \cdot ([\U]_{\beta}^{\gamma})_{i k} \cdot z_i} &  & \text{(by \cref{1.2.1})}    \\
                 & = \sum_{k = 1}^m \sum_{i = 1}^p \pa{([\U]_{\beta}^{\gamma})_{i k} \cdot ([\T]_{\alpha}^{\beta})_{k j} \cdot z_i}  &  & \text{(by \cref{1.2.1})}    \\
                 & = \sum_{i = 1}^p \sum_{k = 1}^m \pa{([\U]_{\beta}^{\gamma})_{i k} \cdot ([\T]_{\alpha}^{\beta})_{k j} \cdot z_i}  &  & \text{(by \cref{1.2.1})}    \\
                 & = \sum_{i = 1}^p \pa{\sum_{k = 1}^m ([\U]_{\beta}^{\gamma})_{i k} \cdot ([\T]_{\alpha}^{\beta})_{k j}} \cdot z_i. &  & \text{(by \cref{1.2.1})}
  \end{align*}
  Thus by \cref{2.3.1} we see that \([\U \T]_{\alpha}^{\gamma} = [\U]_{\beta}^{\gamma} [\T]_{\alpha}^{\beta}\).
\end{proof}

\begin{cor}\label{2.3.3}
  Let \(\V\) be a finite-dimensional vector space over \(\F\) with an ordered basis \(\beta\).
  Let \(\T, \U \in \ls(\V)\).
  Then \([\U \T]_{\beta} = [\U]_{\beta} [\T]_{\beta}\).
\end{cor}

\begin{proof}[\pf{2.3.3}]
  We have
  \begin{align*}
    [\U \T]_{\beta} & = [\U \T]_{\beta}^{\beta}                   &  & \text{(by \cref{2.2.4})} \\
                    & = [\U]_{\beta}^{\beta} [\T]_{\beta}^{\beta} &  & \text{(by \cref{2.11})}  \\
                    & = [\U]_{\beta} [\T]_{\beta}.                &  & \text{(by \cref{2.2.4})}
  \end{align*}
\end{proof}

\begin{defn}\label{2.3.4}
  We define the \textbf{Kronecker delta} \(\delta_{i j}\) by \(\delta_{i j} = 1\) if \(i = j\) and \(\delta_{i j} = 0\) if \(i \neq j\).
  The \(n \times n\) \textbf{identity matrix} \(I_n\) is defined by \((I_n)_{i j} = \delta_{i j}\).
\end{defn}

\begin{thm}\label{2.12}
  Let \(A \in \MS\), let \(B, C \in \ms{n}{p}{\F}\) and let \(D, E \in \ms{q}{m}{\F}\).
  Then
  \begin{enumerate}
    \item \(A (B + C) = AB + AC\) and \((D + E) A = DA + EA\).
    \item \(a (AB) = (aA) B = A (aB)\) for any \(a \in \F\).
    \item \(I_m A = A = A I_n\).
    \item If \(\V\) is an \(n\)-dimensional vector space over \(\F\) with an ordered basis \(\beta\), then \([\IT[\V]]_{\beta} = I_n\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{2.12}(a)]
  We have
  \begin{align*}
    (A (B + C))_{i j} & = \sum_{k = 1}^n A_{i k} (B + C)_{k j}                            &  & \text{(by \cref{2.3.1})}           \\
                      & = \sum_{k = 1}^n A_{i k} (B_{k j} + C_{k j})                      &  & \text{(by \cref{1.2.9})}           \\
                      & = \sum_{k = 1}^n (A_{i k} B_{k j} + A_{i k} C_{k j})              &  & (A_{i k}, B_{k j}, C_{k j} \in \F) \\
                      & = \sum_{k = 1}^n A_{i k} B_{k j} + \sum_{k = 1}^n A_{i k} C_{k j} &  & (A_{i k}, B_{k j}, C_{k j} \in \F) \\
                      & = (AB)_{i j} + (AC)_{i j}                                         &  & \text{(by \cref{2.3.1})}           \\
                      & = (AB + AC)_{i j}                                                 &  & \text{(by \cref{1.2.9})}
  \end{align*}
  and
  \begin{align*}
    ((D + E) A)_{i j} & = \sum_{k = 1}^m (D + E)_{i k} A_{k j}                            &  & \text{(by \cref{2.3.1})}           \\
                      & = \sum_{k = 1}^m (D_{i k} + E_{i k}) A_{k j}                      &  & \text{(by \cref{1.2.9})}           \\
                      & = \sum_{k = 1}^m (D_{i k} A_{k j} + E_{i k} A_{k j})              &  & (A_{k j}, D_{i k}, E_{i k} \in \F) \\
                      & = \sum_{k = 1}^m D_{i k} A_{k j} + \sum_{k = 1}^m E_{i k} A_{k j} &  & (A_{k j}, D_{i k}, E_{i k} \in \F) \\
                      & = (DA)_{i j} + (EA)_{i j}                                         &  & \text{(by \cref{2.3.1})}           \\
                      & = (DA + EA)_{i j}.                                                &  & \text{(by \cref{1.2.9})}
  \end{align*}
  Thus by \cref{1.2.8} \(A (B + C) = AB + AC\) and \((D + E) A = DA + EA\).
\end{proof}

\begin{proof}[\pf{2.12}(b)]
  We have
  \begin{align*}
    (a (AB))_{i j} & = a (AB)_{i j}                          &  & \text{(by \cref{1.2.9})} \\
                   & = a \pa{\sum_{k = 1}^n A_{i k} B_{k j}} &  & \text{(by \cref{2.3.1})} \\
                   & = \sum_{k = 1}^n (a A_{i k}) B_{k j}    &  & \text{(by \cref{1.2.9})} \\
                   & = \sum_{k = 1}^n (a A)_{i k} B_{k j}    &  & \text{(by \cref{1.2.9})} \\
                   & = ((aA) B)_{i j}                        &  & \text{(by \cref{2.3.1})} \\
                   & = \sum_{k = 1}^n A_{i k} (a B_{k j})    &  & \text{(by \cref{1.2.9})} \\
                   & = \sum_{k = 1}^n A_{i k} (a B)_{k j}    &  & \text{(by \cref{1.2.9})} \\
                   & = (A (aB))_{i j}                        &  & \text{(by \cref{2.3.1})}
  \end{align*}
  thus by \cref{1.2.8} \(a (AB) = (aA) B = A (aB)\).
\end{proof}

\begin{proof}[\pf{2.12}(c)]
  We have
  \begin{align*}
    (I_m A)_{i j} & = \sum_{k = 1}^m (I_m)_{i k} A_{k j}  &  & \text{(by \cref{2.3.1})} \\
                  & = \sum_{k = 1}^m \delta_{i k} A_{k j} &  & \text{(by \cref{2.3.4})} \\
                  & = A_{i j}                             &  & \text{(by \cref{2.3.4})} \\
                  & = \sum_{k = 1}^n A_{i k} \delta_{k j} &  & \text{(by \cref{2.3.4})} \\
                  & = \sum_{k = 1}^n A_{i k} (I_n)_{k j}  &  & \text{(by \cref{2.3.4})} \\
                  & = (A I_n)_{i j}                       &  & \text{(by \cref{2.3.1})}
  \end{align*}
  thus by \cref{1.2.8} \(I_m A = A = A I_n\).
\end{proof}

\begin{proof}[\pf{2.12}(d)]
  Let \(\beta = \set{\seq{v}{1,2,,n}}\).
  For all \(j \in \set{1, 2, \dots, n}\), we have
  \begin{align*}
             & \IT[\V](v_j) = v_j                                                                          &  & \text{(by \cref{2.1.9})} \\
             & = \sum_{i = 1}^n ([\IT[\V]]_{\beta})_{i j} v_i                                              &  & \text{(by \cref{2.2.4})} \\
    \implies & \forall i \in \set{1, 2, \dots, n}, ([\IT[\V]]_{\beta})_{i j} = \delta_{i j} = (I_n)_{i j}. &  & \text{(by \cref{2.3.4})}
  \end{align*}
  Thus by \cref{1.2.8} \([\IT[\V]]_{\beta} = I_n\).
\end{proof}

\begin{note}
  \cref{2.12} provides analogs of (a), (c), and (d) of \cref{2.10}.
  \cref{2.10}(b) has its analog in \cref{2.16}.
  Observe also that part (c) of the \cref{2.12} illustrates that the identity matrix acts as a multiplicative identity in \(\ms{n}{n}{\F}\).
  When the context is clear, we sometimes omit the subscript \(n\) from \(I_n\).
\end{note}

\begin{cor}\label{2.3.5}
  Let
  \begin{align*}
    A               & \in \MS;           \\
    \seq{B}{1,2,,k} & \in \ms{n}{p}{\F}; \\
    \seq{C}{1,2,,k} & \in \ms{q}{m}{\F}; \\
    \seq{a}{1,2,,k} & \in \F.
  \end{align*}
  Then
  \begin{align*}
    A \pa{\sum_{i = 1}^k a_i B_i} & = \sum_{i = 1}^k a_i A B_i, \\
    \pa{\sum_{i = 1}^k a_i C_i} A & = \sum_{i = 1}^k a_i C_i A.
  \end{align*}
\end{cor}

\begin{proof}[\pf{2.3.5}]
  We have
  \begin{align*}
    A \pa{\sum_{i = 1}^k a_i B_i} & = \sum_{i = 1}^k (A a_i B_i)  &  & \text{(by \cref{2.12}(a))} \\
                                  & = \sum_{i = 1}^k (a_i A B_i), &  & \text{(by \cref{2.12}(b))} \\
    \pa{\sum_{i = 1}^k a_i C_i} A & = \sum_{i = 1}^k (a_i C_i A). &  & \text{(by \cref{2.12}(a))}
  \end{align*}
\end{proof}

\begin{defn}\label{2.3.6}
  For an \(A \in \ms{n}{n}{\F}\), we define \(A^1 = A\), \(A^2 = AA\), \(A^3 = A^2 A\), and, in general, \(A^k = A^{k - 1} A\) for \(k = 2, 3, \dots\).
  We define \(A^0 = I_n\).
\end{defn}

\begin{eg}\label{2.3.7}
  If
  \[
    A = \begin{pmatrix}
      0 & 0 \\
      1 & 0
    \end{pmatrix},
  \]
  then \(A^2 = \zm\) (the zero matrix) even though \(A \neq \zm\).
  Thus the cancellation property for multiplication in fields is not valid for matrices.
  To see why, assume that the cancellation law is valid.
  Then, from \(A \cdot A = A^2 = \zm = A \cdot \zm\), we would conclude that \(A = \zm\), which is false.
\end{eg}

\begin{thm}\label{2.13}
  Let \(A \in \MS\) matrix and \(B \in \ms{n}{p}{\F}\).
  For each \(j\) (\(1 \leq j \leq p\)) let \(u_j\) and \(v_j\) denote the \(j\)th columns of \(AB\) and \(B\), respectively.
  Then
  \begin{enumerate}
    \item \(u_j = A v_j\).
    \item \(v_j = B e_j\), where \(e_j\) is the \(j\)th standard vector of \(\vs{F}^p\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{2.13}(a)]
  We have
  \begin{align*}
    u_j & = \begin{pmatrix}
              (AB)_{1 j} \\
              (AB)_{2 j} \\
              \vdots     \\
              (AB)_{m j}
            \end{pmatrix}                 &  & \text{(by \cref{2.2.4})}   \\
        & = \begin{pmatrix}
              \sum_{k = 1}^n A_{1 k} B_{k j} \\
              \sum_{k = 1}^n A_{2 k} B_{k j} \\
              \vdots                         \\
              \sum_{k = 1}^n A_{m k} B_{k j}
            \end{pmatrix} &  & \text{(by \cref{2.3.1})}                   \\
        & = A \begin{pmatrix}
                B_{1 j} \\
                B_{2 j} \\
                \vdots  \\
                B_{n j}
              \end{pmatrix}               &  & \text{(by \cref{2.3.1})}   \\
        & = A v_j.                          &  & \text{(by \cref{2.2.4})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{2.13}(b)]
  We have
  \begin{align*}
    v_j & = \begin{pmatrix}
              B_{1 j} \\
              B_{2 j} \\
              \vdots  \\
              B_{m j}
            \end{pmatrix}                     &  & \text{(by \cref{2.2.4})}   \\
        & = \begin{pmatrix}
              (B I_p)_{1 j} \\
              (B I_p)_{2 j} \\
              \vdots        \\
              (B I_p)_{m j}
            \end{pmatrix}                     &  & \text{(by \cref{2.12}(c))} \\
        & = \begin{pmatrix}
              \sum_{k = 1}^p B_{1 k} (I_p)_{k j} \\
              \sum_{k = 1}^p B_{2 k} (I_p)_{k j} \\
              \vdots                             \\
              \sum_{k = 1}^p B_{m k} (I_p)_{k j}
            \end{pmatrix} &  & \text{(by \cref{2.3.1})}                       \\
        & = B \begin{pmatrix}
                (I_p)_{1 j} \\
                (I_p)_{2 j} \\
                \vdots      \\
                (I_p)_{n j}
              \end{pmatrix}                   &  & \text{(by \cref{2.3.1})}   \\
        & = B e_j.                              &  & \text{(by \cref{2.3.4})}
  \end{align*}
\end{proof}

\begin{note}
  It follows (see \cref{ex:2.3.14}) from \cref{2.13} that column \(j\) of \(AB\) is a linear combination of the columns of \(A\) with the coefficients in the linear combination being the entries of column \(j\) of \(B\).
  An analogous result holds for rows;
  that is, row \(i\) of \(AB\) is a linear combination of the rows of \(B\) with the coefficients in the linear combination being the entries of row \(i\) of \(A\).
\end{note}

\begin{thm}\label{2.14}
  Let \(\V\) and \(\W\) be finite-dimensional vector spaces over \(\F\) having ordered bases \(\beta\) and \(\gamma\) over \(\F\), respectively, and let \(\T : \V \to \W\) be linear.
  Then, for each \(u \in \V\), we have
  \[
    [\T(u)]_{\gamma} = [\T]_{\beta}^{\gamma} [u]_{\beta}.
  \]
\end{thm}

\begin{proof}[\pf{2.14}]
  Fix \(u \in \V\), and define the linear transformations \(f : \F \to \V\) by \(f(a) = au\) and \(g : \F \to \W\) by \(g(a) = a \T(u)\) for all \(a \in \F\).
  Let \(\alpha = \set{1}\) be the standard ordered basis for \(\F\).
  Notice that \(g = \T f\).
  Identifying column vectors as matrices and using \cref{2.11}, we obtain
  \[
    [\T(u)]_{\gamma} = [g(1)]_{\gamma} = [g]_{\alpha}^{\gamma} = [\T f]_{\alpha}^{\gamma} = [\T]_{\beta}^{\gamma} [f]_{\alpha}^{\beta} = [\T]_{\beta}^{\gamma} [f(1)]_{\beta} = [\T]_{\beta}^{\gamma} [u]_{\beta}.
  \]
\end{proof}

\begin{defn}\label{2.3.8}
  Let \(A \in \ms{m}{n}{\F}\).
  We denote by \(\L_A\) the mapping \(\L_A : \vs{F}^n \to \vs{F}^m\) defined by \(\L_A(x) = Ax\) (the matrix product of \(A\) and \(x\)) for each column vector \(x \in \vs{F}^n\).
  We call \(\L_A\) a \textbf{left-multiplication transformation}.
\end{defn}

\begin{thm}\label{2.15}
  Let \(A \in \MS\).
  Then the left-multiplication transformation \(\L_A : \vs{F}^n \to \vs{F}^m\) is linear.
  Furthermore, if \(B \in \MS\) and \(\beta\) and \(\gamma\) are the standard ordered bases for \(\vs{F}^n\) and \(\vs{F}^m\) over \(\F\), respectively, then we have the following properties.
  \begin{enumerate}
    \item \([\L_A]_{\beta}^{\gamma} = A\).
    \item \(\L_A = \L_B\) iff \(A = B\).
    \item \(\L_{A + B} = \L_A + \L_B\) and \(\L_{aA} = a \L_A\) for all \(a \in \F\).
    \item If \(\T : \vs{F}^n \to \vs{F}^m\) is linear, then there exists a unique \(C \in \MS\) such that \(\T = \L_C\).
          In fact, \(C = [\T]_{\beta}^{\gamma}\).
    \item If \(E \in \ms{n}{p}{\F}\), then \(\L_{AE} = \L_A \L_E\).
    \item If \(m = n\), then \(\L_{I_n} = \IT[\vs{F}^n]\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{2.15}(a)]
  The fact that \(\L_A\) is linear follows immediately from \cref{2.12}(a)(b).
  By \cref{2.2.4} the \(j\)th column of \([\L_A]_{\beta}^{\gamma}\) is equal to \(\L_A(e_j)\).
  However by \cref{2.3.8} we have \(\L_A(e_j) = A e_j\), which is also the \(j\)th column of \(A\) by \cref{2.13}(b).
  So \([\L_A]_{\beta}^{\gamma} = A\).
\end{proof}

\begin{proof}[\pf{2.15}(b)]
  If \(\L_A = \L_B\), then we may use \cref{2.15}(a) to write \(A = [\L_A]_{\beta}^{\gamma} = [\L_B]_{\beta}^{\gamma} = B\).
  Hence \(A = B\).
  The proof of the converse is trivial.
\end{proof}

\begin{proof}[\pf{2.15}(c)]
  For all \(x \in \vs{F}^n\), we have
  \begin{align*}
    \L_{A + B}(x) & = (A + B) x         &  & \text{(by \cref{2.3.8})}   \\
                  & = Ax + Bx           &  & \text{(by \cref{2.12}(a))} \\
                  & = \L_A(x) + \L_B(x) &  & \text{(by \cref{2.3.8})}   \\
                  & = (\L_A + \L_B)(x)  &  & \text{(by \cref{2.2.5})}
  \end{align*}
  and
  \begin{align*}
    \L_{aA}(x) & = (aA) x       &  & \text{(by \cref{2.3.8})}   \\
               & = a (Ax)       &  & \text{(by \cref{2.12}(b))} \\
               & = a \L_A(x)    &  & \text{(by \cref{2.3.8})}   \\
               & = (a \L_A)(x). &  & \text{(by \cref{2.2.5})}
  \end{align*}
  Thus \(\L_{A + B} = \L_A + \L_B\) and \(\L_{aA} = a \L_A\).
\end{proof}

\begin{proof}[\pf{2.15}(d)]
  Let \(C = [\T]_{\beta}^{\gamma}\).
  By \cref{2.14}, we have \([\T(x)]_{\gamma} = [\T]_{\beta}^{\gamma} [x]_{\beta}\), or \(\T(x) = Cx = \L_C(x)\) for all \(x \in \vs{F}^n\).
  So \(\T = \L_C\).
  The uniqueness of \(C\) follows from \cref{2.15}(b).
\end{proof}

\begin{proof}[\pf{2.15}(e)]
  For any \(j\) (\(1 \leq j \leq p\)), we may apply \cref{2.13} several times to note that \((AE) e_j\) is the \(j\)th column of \(AE\) and that the \(j\)th column of \(AE\) is also equal to \(A (E e_j)\).
  So \((AE) e_j = A (Ee_j)\).
  Thus
  \[
    \L_{AE}(e_j) = (AE) e_j = A (E e_j) = \\L_A(E e_j) = \L_A(L_E(e_j)).
  \]
  Hence \(\L_{AE} = \L_A \L_E\) by \cref{2.1.13}.
\end{proof}

\begin{proof}[\pf{2.15}(f)]
  For all \(x \in \vs{F}^n\), we have
  \begin{align*}
    \L_{I_n}(x) & = I_n x              &  & \text{(by \cref{2.3.8})}   \\
                & = x                  &  & \text{(by \cref{2.12}(c))} \\
                & = \IT_{\vs{F}^n}(x). &  & \text{(by \cref{2.1.9})}
  \end{align*}
  Thus \(\L_{I_n} = \IT_{\vs{F}^n}\).
\end{proof}

\begin{thm}\label{2.16}
  Let \(A\), \(B\), and \(C\) be matrices such that \(A (BC)\) is defined.
  Then \((AB) C\) is also defined and \(A (BC) = (AB) C\);
  that is, matrix multiplication is associative.
\end{thm}

\begin{proof}[\pf{2.16}]
  Since \(A (BC)\) is defined, by \cref{2.3.1} we can let \(A \in \MS\) and \(BC \in \ms{n}{p}{\F}\) such that \(A (BC) \in \ms{m}{p}{\F}\).
  Since \(BC\) is also defined, by \cref{2.3.1} again we can let \(B \in \ms{n}{k}{\F}\) and \(C \in \ms{k}{p}{\F}\).
  Then we have
  \begin{align*}
             & \begin{dcases}
                 A \in \MS           \\
                 B \in \ms{n}{k}{\F} \\
                 C \in \ms{k}{p}{\F}
               \end{dcases}                                    \\
    \implies & \begin{dcases}
                 AB \in \ms{m}{k}{\F} \\
                 C \in \ms{k}{p}{\F}
               \end{dcases}  &  & \text{(by \cref{2.3.1})}            \\
    \implies & (AB) C \in \ms{m}{p}{\F} &  & \text{(by \cref{2.3.1})}
  \end{align*}
  and thus \((AB) C\) is defined.

  Using \cref{2.15}(e) and the associativity of functional composition, we have
  \[
    \L_{A (BC)} = \L_A \L_{BC} = \L_A (\L_B \L_C) = (\L_A \L_B) \L_C = \L_{AB} \L_C = \L_{(AB) C}.
  \]
  So by \cref{2.15}(b), it follows that \(A (BC) = (AB) C\).
\end{proof}

\exercisesection

\begin{ex}\label{ex:2.3.14}

\end{ex}
