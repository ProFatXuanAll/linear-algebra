\section{Composition of Linear Transformations and Matrix Multiplication}\label{sec:2.3}

\begin{note}
  We use the more convenient notation of \(\U \T\) rather than \(\U \circ \T\) for the composite of linear transformations \(\U\) and \(\T\).
\end{note}

\begin{thm}\label{2.9}
  Let \(\V\), \(\W\), and \(\vs{Z}\) be vector spaces over the same field \(\F\), and let \(\T : \V \to \W\) and \(\U : \W \to \vs{Z}\) be linear.
  Then \(\U \T : \V \to \vs{Z}\) is linear.
\end{thm}

\begin{proof}[\pf{2.9}]
  Let \(x, y \in \V\) and \(a \in \F\).
  Then
  \begin{align*}
    \U \T(ax + y) & = \U(\T(ax + y))                                            \\
                  & = \U(a \T(x) + \T(y))      &  & \text{(by \cref{2.1.2}(b))} \\
                  & = a \U(\T(x)) + \U(\T(y))  &  & \text{(by \cref{2.1.2}(b))} \\
                  & = a (\U \T)(x) + \U \T(y).
  \end{align*}
\end{proof}

\begin{thm}\label{2.10}
  Let \(\V\) be a vector space over \(\F\).
  Let \(\T, \U_1, \U_2 \in \ls(\V)\).
  Then
  \begin{enumerate}
    \item \(\T(\U_1 + \U_2) = \T \U_1 + \T \U_2\) and \((\U_1 + \U_2) \T = \U_1 \T + \U_2 \T\).
    \item \(\T(\U_1 \U_2) = (\T \U_1) \U_2\).
    \item \(\T \IT = \IT \T = \T\).
    \item \(a(\U_1 \U_2) = (a \U_1) \U_2 = \U_1 (a \U_2)\) for all scalars \(a\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{2.10}(a)]
  For all \(x \in \V\), we have
  \begin{align*}
    (\T(\U_1 + \U_2))(x) & = \T((\U_1 + \U_2)(x))                                      \\
                         & = \T(\U_1(x) + \U_2(x))       &  & \text{(by \cref{2.2.5})} \\
                         & = \T(\U_1(x)) + \T(\U_2(x))   &  & \text{(by \cref{2.1.1})} \\
                         & = (\T \U_1)(x) + (\T \U_2)(x)                               \\
                         & = (\T \U_1 + \T \U_2)(x)      &  & \text{(by \cref{2.2.5})}
  \end{align*}
  and
  \begin{align*}
    ((\U_1 + \U_2) \T)(x) & = (\U_1 + \U_2)(\T(x))                                      \\
                          & = \U_1(\T(x)) + \U_2(\T(x))   &  & \text{(by \cref{2.2.5})} \\
                          & = (\U_1 \T)(x) + (\U_2 \T)(x)                               \\
                          & = (\U_1 \T + \U_2 \T)(x).     &  & \text{(by \cref{2.2.5})}
  \end{align*}
  Thus \(\T(\U_1 + \U_2) = \T \U_1 + \T \U_2\) and \((\U_1 + \U_2) \T = \U_1 \T + \U_2 \T\).
\end{proof}

\begin{proof}[\pf{2.10}(b)]
  For all \(x \in \V\), we have
  \begin{align*}
    (\T (\U_1 \U_2))(x) & = \T((\U_1 \U_2)(x))  \\
                        & = \T(\U_1(\U_2(x)))   \\
                        & = (\T \U_1)(\U_2(x))  \\
                        & = ((\T \U_1) \U_2)(x)
  \end{align*}
  and thus \(\T (\U_1 \U_2) = (\T \U_1) \U_2\).
\end{proof}

\begin{proof}[\pf{2.10}(c)]
  For all \(x \in \V\), we have
  \begin{align*}
    (\T \IT)(x) & = \T(\IT(x))                                \\
                & = \T(x)       &  & \text{(by \cref{2.1.9})} \\
                & = \IT(\T(x))  &  & \text{(by \cref{2.1.9})} \\
                & = (\IT \T)(x)
  \end{align*}
  and thus \(\T \IT = \T = \IT \T\).
\end{proof}

\begin{proof}[\pf{2.10}(d)]
  For all \(x \in \V\), we have
  \begin{align*}
    (a (\U_1 \U_2))(x) & = a (\U_1 \U_2)(x)                                 \\
                       & = a \U_1(\U_2(x))                                  \\
                       & = (a \U_1)(\U_2(x))  &  & \text{(by \cref{2.2.5})} \\
                       & = ((a \U_1) \U_2)(x)                               \\
                       & = \U_1(a \U_2(x))    &  & \text{(by \cref{2.1.1})} \\
                       & = \U_1((a \U_2)(x))                                \\
                       & = (\U_1 (a \U_2))(x)
  \end{align*}
  and thus \(a (\U_1 \U_2) = (a \U_1) \U_2 = \U_1 (a \U_2)\).
\end{proof}

\begin{defn}\label{2.3.1}
  Let \(A\) be an \(m \times n\) matrix and \(B\) be an \(n \times p\) matrix.
  We define the \textbf{product} of \(A\) and \(B\), denoted \(AB\), to be the \(m \times p\) matrix such that
  \[
    (AB)_{i j} = \sum_{k = 1}^n A_{i k} B_{k j} \quad \text{for } 1 \leq i \leq m, 1 \leq j \leq p.
  \]
  Note that \((AB)_{i j}\) is the sum of products of corresponding entries from the \(i\)th row of \(A\) and the \(j\)th column of \(B\).
\end{defn}

\begin{note}
  The reader should observe that in order for the product \(AB\) to be defined, there are restrictions regarding the relative sizes of \(A\) and \(B\).
  The following mnemonic device is helpful:
  ``\((m \times n) \cdot (n \times p) = (m \times p)\)'';
  that is, in order for the product \(AB\) to be defined, the two ``inner'' dimensions must be equal, and the two ``outer'' dimensions yield the size of the product.
\end{note}

\begin{note}
  As in the case with composition of functions, we have that matrix multi- plication is not commutative. Consider the following two products:
  \[
    \begin{pmatrix}
      1 & 1 \\
      0 & 0
    \end{pmatrix} \begin{pmatrix}
      0 & 1 \\
      1 & 0
    \end{pmatrix} = \begin{pmatrix}
      1 & 1 \\
      0 & 0
    \end{pmatrix} \quad \text{and} \quad \begin{pmatrix}
      0 & 1 \\
      1 & 0
    \end{pmatrix} \begin{pmatrix}
      1 & 1 \\
      0 & 0
    \end{pmatrix} = \begin{pmatrix}
      0 & 0 \\
      1 & 1
    \end{pmatrix}.
  \]
  Hence we see that even if both of the matrix products \(AB\) and \(BA\) are defined, it need not be true that \(AB = BA\).
\end{note}

\begin{eg}\label{2.3.2}
  If \(A\) is an \(m \times n\) matrix and \(B\) is an \(n \times p\) matrix, then \(\tp{(AB)} = \tp{B} \tp{A}\).
\end{eg}

\begin{proof}[\pf{2.3.2}]
  Since
  \[
    \tp{(AB)}_{i j} = (AB)_{j i} = \sum_{k = 1}^n A_{j k} B_{k i}
  \]
  and
  \[
    (\tp{B} \tp{A})_{i j} = \sum_{k = 1}^n \tp{B}_{i k} \tp{A}_{k j} = \sum_{k = 1}^n B_{k i} A_{j k},
  \]
  we are finished.
  Therefore the transpose of a product is the product of the transposes in the \emph{opposite order}.
\end{proof}

\begin{thm}\label{2.11}
  Let \(\V\), \(\W\), and \(\vs{Z}\) be finite-dimensional vector spaces over \(\F\) with ordered bases \(\alpha\), \(\beta\), and \(\gamma\) over \(\F\), respectively.
  Let \(\T : \V \to \W\) and \(\U : \W \to \vs{Z}\) be linear transformations.
  Then
  \[
    [\U \T]_{\alpha}^{\gamma} = [\U]_{\beta}^{\gamma} [\T]_{\alpha}^{\beta}.
  \]
\end{thm}

\begin{proof}[\pf{2.11}]
  Define
  \begin{align*}
    \alpha & = \set{\seq{v}{1,2,,n}}  \\
    \beta  & = \set{\seq{w}{1,2,,m}}  \\
    \gamma & = \set{\seq{z}{1,2,,p}}.
  \end{align*}
  For \(1 \leq j \leq n\), we have
  \begin{align*}
    (\U \T)(v_j) & = \sum_{i = 1}^p ([\U \T]_{\alpha}^{\gamma})_{i j} \cdot z_i                                                      &  & \text{(by \cref{2.2.4})}    \\
                 & = \U(\T(v_j))                                                                                                                                      \\
                 & = \U\pa{\sum_{k = 1}^m ([\T]_{\alpha}^{\beta})_{k j} \cdot w_k}                                                   &  & \text{(by \cref{2.2.4})}    \\
                 & = \sum_{k = 1}^m ([\T]_{\alpha}^{\beta})_{k j} \cdot \U(w_k)                                                      &  & \text{(by \cref{2.1.2}(d))} \\
                 & = \sum_{k = 1}^m ([\T]_{\alpha}^{\beta})_{k j} \cdot \pa{\sum_{i = 1}^p ([\U]_{\beta}^{\gamma})_{i k} \cdot z_i}  &  & \text{(by \cref{2.2.4})}    \\
                 & = \sum_{k = 1}^m \sum_{i = 1}^p \pa{([\T]_{\alpha}^{\beta})_{k j}  \cdot ([\U]_{\beta}^{\gamma})_{i k} \cdot z_i} &  & \text{(by \cref{1.2.1})}    \\
                 & = \sum_{k = 1}^m \sum_{i = 1}^p \pa{([\U]_{\beta}^{\gamma})_{i k} \cdot ([\T]_{\alpha}^{\beta})_{k j} \cdot z_i}  &  & \text{(by \cref{1.2.1})}    \\
                 & = \sum_{i = 1}^p \sum_{k = 1}^m \pa{([\U]_{\beta}^{\gamma})_{i k} \cdot ([\T]_{\alpha}^{\beta})_{k j} \cdot z_i}  &  & \text{(by \cref{1.2.1})}    \\
                 & = \sum_{i = 1}^p \pa{\sum_{k = 1}^m ([\U]_{\beta}^{\gamma})_{i k} \cdot ([\T]_{\alpha}^{\beta})_{k j}} \cdot z_i. &  & \text{(by \cref{1.2.1})}
  \end{align*}
  Thus by \cref{2.3.1} we see that \([\U \T]_{\alpha}^{\gamma} = [\U]_{\beta}^{\gamma} [\T]_{\alpha}^{\beta}\).
\end{proof}

\begin{cor}\label{2.3.3}
  Let \(\V\) be a finite-dimensional vector space over \(\F\) with an ordered basis \(\beta\).
  Let \(\T, \U \in \ls(\V)\).
  Then \([\U \T]_{\beta} = [\U]_{\beta} [\T]_{\beta}\).
\end{cor}

\begin{proof}[\pf{2.3.3}]
  We have
  \begin{align*}
    [\U \T]_{\beta} & = [\U \T]_{\beta}^{\beta}                   &  & \text{(by \cref{2.2.4})} \\
                    & = [\U]_{\beta}^{\beta} [\T]_{\beta}^{\beta} &  & \text{(by \cref{2.11})}  \\
                    & = [\U]_{\beta} [\T]_{\beta}.                &  & \text{(by \cref{2.2.4})}
  \end{align*}
\end{proof}
