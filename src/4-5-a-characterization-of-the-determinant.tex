\section{A Characterization of the Determinant}\label{sec:4.5}

\begin{defn}\label{4.5.1}
  A function \(\delta : \ms[n][n][\F] \to \F\) is called an \textbf{\(n\)-linear function} if it is a linear function of each row of an \(n \times n\) matrix when the remaining \(n - 1\) rows are held fixed, that is, \(\delta\) is \(n\)-linear if, for every \(r \in \set{1, \dots, n}\), we have
  \[
    \delta\begin{pmatrix}
      a_1       \\
      \vdots    \\
      a_{r - 1} \\
      u + kv    \\
      a_{r + 1} \\
      \vdots    \\
      a_n
    \end{pmatrix} = \delta\begin{pmatrix}
      a_1       \\
      \vdots    \\
      a_{r - 1} \\
      u         \\
      a_{r + 1} \\
      \vdots    \\
      a_n
    \end{pmatrix} + \delta\begin{pmatrix}
      a_1       \\
      \vdots    \\
      a_{r - 1} \\
      kv        \\
      a_{r + 1} \\
      \vdots    \\
      a_n
    \end{pmatrix}
  \]
  whenever \(k \in \F\) and \(u, v\), and each \(a_i\) are vectors in \(\vs{F}^n\).
\end{defn}

\begin{eg}\label{4.5.2}
  The function \(\delta : \ms[n][n][\F] \to \F\) defined by \(\delta(A) = 0\) for each \(A \in \ms[n][n][\F]\) is an \(n\)-linear function.
\end{eg}

\begin{proof}[\pf{4.5.2}]
  Let \(\seq{a}{1,,n}, u, v \in \vs{F}^n\) and let \(k \in \F\).
  Since
  \[
    \forall r \in \set{1, \dots, n}, \delta\begin{pmatrix}
      a_1       \\
      \vdots    \\
      a_{r - 1} \\
      u + kv    \\
      a_{r + 1} \\
      \vdots    \\
      a_n
    \end{pmatrix} = 0 = 0 + k \cdot 0 = \begin{pmatrix}
      a_1       \\
      \vdots    \\
      a_{r - 1} \\
      u         \\
      a_{r + 1} \\
      \vdots    \\
      a_n
    \end{pmatrix} + k \begin{pmatrix}
      a_1       \\
      \vdots    \\
      a_{r - 1} \\
      v         \\
      a_{r + 1} \\
      \vdots    \\
      a_n
    \end{pmatrix},
  \]
  by \cref{4.5.1} \(\delta\) is a \(n\)-linear function.
\end{proof}

\begin{eg}\label{4.5.3}
  For \(j \in \set{1, \dots, n}\), define \(\delta_j : \ms[n][n][\F] \to \F\) by \(\delta_j(A) = \prod_{i = 1}^n A_{i j}\) for each \(A \in \ms[n][n][\F]\);
  that is, \(\delta_j(A)\) equals the product of the entries of column \(j\) of \(A\).
  Then each \(\delta_j\) is an \(n\)-linear function.
\end{eg}

\begin{proof}[\pf{4.5.3}]
  Let \(A \in \ms[n][n][\F]\), \(a_i = (A_{i 1}, \dots, A_{i n})\), and \(v = \tuple{b}{1,,n} \in \vs{F}^n\).
  For any \(k \in \F\), we have
  \begin{align*}
    \delta_j\begin{pmatrix}
              a_1       \\
              \vdots    \\
              a_{r - 1} \\
              a_r + kv  \\
              a_{r + 1} \\
              \vdots    \\
              a_n
            \end{pmatrix} & = \pa{\prod_{i = 1}^{r - 1} A_{i j}} \cdot (A_{r j} + k b_j) \cdot \pa{\prod_{i = r + 1}^n A_{i j}}                        \\
                            & = \pa{\prod_{i = 1}^n A_{i j}} + \pa{\prod_{i = 1}^{r - 1} A_{i j}} \cdot (k b_j) \cdot \pa{\prod_{i = r + 1}^n A_{i j}} \\
                            & = \delta_j\begin{pmatrix}
                                          a_1       \\
                                          \vdots    \\
                                          a_{r - 1} \\
                                          a_r       \\
                                          a_{r + 1} \\
                                          \vdots    \\
                                          a_n
                                        \end{pmatrix} + k \delta_j\begin{pmatrix}
                                                                    a_1       \\
                                                                    \vdots    \\
                                                                    a_{r - 1} \\
                                                                    v         \\
                                                                    a_{r + 1} \\
                                                                    \vdots    \\
                                                                    a_n
                                                                  \end{pmatrix}.
  \end{align*}
  Thus by \cref{4.5.1} \(\delta_j\) is an \(n\)-linear function.
\end{proof}

\begin{eg}\label{4.5.4}
  The function \(\delta : \ms[n][n][\F] \to \F\) defined for each \(A \in \ms[n][n][\F]\) by \(\delta(A) = \prod_{i = 1}^n A_{i i}\) (i.e., \(\delta(A)\) equals the product of the diagonal entries of \(A\)) is an \(n\)-linear function.
\end{eg}

\begin{proof}[\pf{4.5.4}]
  Let \(A \in \ms[n][n][\F]\), \(a_i = (A_{i 1}, \dots, A_{i n})\), and \(v = \tuple{b}{1,,n} \in \vs{F}^n\).
  For any \(k \in \F\), we have
  \begin{align*}
    \delta\begin{pmatrix}
            a_1       \\
            \vdots    \\
            a_{r - 1} \\
            a_r + kv  \\
            a_{r + 1} \\
            \vdots    \\
            a_n
          \end{pmatrix} & = \pa{\prod_{i = 1}^{r - 1} A_{i i}} \cdot (A_{r r} + k b_r) \cdot \pa{\prod_{i = r + 1}^m A_{i i}}                   \\
                          & = \prod_{i = 1}^n A_{i i} + \pa{\prod_{i = 1}^{r - 1} A_{i i}} \cdot (k b_r) \cdot \pa{\prod_{i = r + 1}^m A_{i i}} \\
                          & = \delta\begin{pmatrix}
                                      a_1       \\
                                      \vdots    \\
                                      a_{r - 1} \\
                                      a_r       \\
                                      a_{r + 1} \\
                                      \vdots    \\
                                      a_n
                                    \end{pmatrix} + k \delta\begin{pmatrix}
                                                              a_1       \\
                                                              \vdots    \\
                                                              a_{r - 1} \\
                                                              v         \\
                                                              a_{r + 1} \\
                                                              \vdots    \\
                                                              a_n
                                                            \end{pmatrix}.
  \end{align*}
  Thus by \cref{4.5.1} \(\delta\) is an \(n\)-linear function.
\end{proof}

\begin{note}
  \cref{4.3} asserts that the determinant is an \(n\)-linear function.
\end{note}

\begin{defn}\label{4.5.5}
  An \(n\)-linear function \(\delta : \ms[n][n][\F] \to \F\) is called \textbf{alternating} if, for each \(A \in \ms[n][n][\F]\), we have \(\delta(A) = 0\) whenever two adjacent rows of \(A\) are identical.
\end{defn}

\begin{thm}\label{4.10}
  Let \(\delta : \ms[n][n][\F] \to \F\) be an alternating \(n\)-linear function.
  \begin{enumerate}
    \item If \(A \in \ms[n][n][\F]\) and \(B\) is a matrix obtained from \(A\) by interchanging any two rows of \(A\), then \(\delta(B) = -\delta(A)\).
    \item If \(A \in \ms[n][n][\F]\) has two identical rows, then \(\delta(A) = 0\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{4.10}(a)]
  Let \(A \in \ms[n][n][\F]\), and let \(B\) be the matrix obtained from \(A\) by interchanging rows \(r\) and \(s\), where \(r < s\).
  We first establish the result in the case that \(s = r + 1\).
  Because \(\delta : \ms[n][n][\F] \to \F\) is an \(n\)-linear function that is alternating, we have
  \begin{align*}
    0 & = \delta\begin{pmatrix}
                  a_1             \\
                  \vdots          \\
                  a_r + a_{r + 1} \\
                  a_r + a_{r + 1} \\
                  \vdots          \\
                  a_n
                \end{pmatrix}               &  & \by{4.5.5}                                                     \\
      & = \delta\begin{pmatrix}
                  a_1             \\
                  \vdots          \\
                  a_r             \\
                  a_r + a_{r + 1} \\
                  \vdots          \\
                  a_n
                \end{pmatrix} + \delta\begin{pmatrix}
                                        a_1             \\
                                        \vdots          \\
                                        a_{r + 1}       \\
                                        a_r + a_{r + 1} \\
                                        \vdots          \\
                                        a_n
                                      \end{pmatrix} &  & \by{4.5.1}                                             \\
      & = \delta\begin{pmatrix}
                  a_1    \\
                  \vdots \\
                  a_r    \\
                  a_r    \\
                  \vdots \\
                  a_n
                \end{pmatrix} + \delta\begin{pmatrix}
                                        a_1       \\
                                        \vdots    \\
                                        a_r       \\
                                        a_{r + 1} \\
                                        \vdots    \\
                                        a_n
                                      \end{pmatrix} + \delta\begin{pmatrix}
                                                              a_1       \\
                                                              \vdots    \\
                                                              a_{r + 1} \\
                                                              a_r       \\
                                                              \vdots    \\
                                                              a_n
                                                            \end{pmatrix} + \delta\begin{pmatrix}
                                                                                    a_1       \\
                                                                                    \vdots    \\
                                                                                    a_{r + 1} \\
                                                                                    a_{r + 1} \\
                                                                                    \vdots    \\
                                                                                    a_n
                                                                                  \end{pmatrix} &  & \by{4.5.1} \\
      & = 0 + \delta(A) + \delta(B) + 0.
  \end{align*}
  Thus \(\delta(B) = -\delta(A)\).

  Next suppose that \(s > r + 1\), and let the rows of \(A\) be \(\seq{a}{1,,n}\).
  Beginning with \(a_r\) and \(a_{r + 1}\), successively interchange \(a_r\) with the row that follows it until the rows are in the sequence
  \[
    \seq{a}{1,,r-1,r+1,,s,r,s+1,,n}.
  \]
  In all, \(s - r\) interchanges of adjacent rows are needed to produce this sequence.
  Then successively interchange \(a_s\) with the row that precedes it until the rows are in the order
  \[
    \seq{a}{1,,r-1,s,r+1,,s-1,r,s+1,,n}.
  \]
  This process requires an additional \(s - r - 1\) interchanges of adjacent rows and produces the matrix \(B\).
  It follows from the preceding paragraph that
  \[
    \delta(B) = (-1)^{(s - r) + (s - r - 1)} \delta(A) = -\delta(A).
  \]
\end{proof}

\begin{proof}[\pf{4.10}(b)]
  Suppose that rows \(r\) and \(s\) of \(A \in \ms[n][n][\F]\) are identical, where \(r < s\).
  If \(s = r + 1\), then \(\delta(A) = 0\) because \(\delta\) is alternating and two adjacent rows of \(A\) are identical.
  If \(s > r + 1\), let \(B\) be the matrix obtained from \(A\) by interchanging rows \(r + 1\) and \(s\).
  Then \(\delta(B) = 0\) because two adjacent rows of \(B\) are identical.
  But \(\delta(B) = -\delta(A)\) by \cref{4.10}(a).
  Hence \(\delta(A) = 0\).
\end{proof}

\begin{cor}\label{4.5.6}
  Let \(\delta : \ms[n][n][\F] \to \F\) be an alternating \(n\)-linear function.
  If \(B\) is a matrix obtained from \(A \in \ms[n][n][\F]\) by adding a multiple of some row of \(A\) to another row, then \(\delta(B) = \delta(A)\).
\end{cor}

\begin{proof}[\pf{4.5.6}]
  Let \(B\) be obtained from \(A \in \ms[n][n][\F]\) by adding \(k\) times row \(i\) of \(A\) to row \(j\), where \(j \neq i\), and let \(C\) be obtained from \(A\) by replacing row \(j\) of \(A\) by row \(i\) of \(A\).
  Then the rows of \(A\), \(B\), and \(C\) are identical except for row \(j\).
  Moreover, row \(j\) of \(B\) is the sum of row \(j\) of \(A\) and \(k\) times row \(j\) of \(C\).
  Since \(\delta\) is an \(n\)-linear function and \(C\) has two identical rows, it follows that
  \[
    \delta(B) = \delta(A) + k \delta(C) = \delta(A) + k \cdot 0 = \delta(A).
  \]
\end{proof}

\begin{cor}\label{4.5.7}
  Let \(\delta : \ms[n][n][\F] \to \F\) be an alternating \(n\)-linear function.
  If \(M \in \ms[n][n][\F]\) has rank less than \(n\), then \(\delta(M) = 0\).
\end{cor}

\begin{proof}[\pf{4.5.7}]
  If \(\rk{A} < n\), then the rows \(\seq{a}{1,,n}\) of \(A\) are linearly dependent.
  By \cref{ex:1.5.14}, some row of \(A\), say, row \(r\), is a linear combination of the other rows.
  So there exist scalars \(c_i \in \F\) such that
  \[
    a_r = \seq[+]{c,a}{1,,r-1,r+1,,n}.
  \]
  Let \(B\) be the matrix obtained from \(A\) by adding \(-c_i\) times row \(i\) to row \(r\) for each \(i \neq r\).
  Then row \(r\) of \(B\) consists entirely of zeros and by \cref{4.5.6} \(\delta(B) = \delta(A)\).
  Let \(C\) be the matrix obtained from \(B\) by adding row \(1\) to row \(r\).
  Then \(C\) has two identical rows and by \cref{4.5.6} and \cref{4.10}(b) \(\delta(B) = \delta(C) = 0\).
  Hence \(\delta(A) = 0\).
\end{proof}

\begin{cor}\label{4.5.8}
  Let \(\delta : \ms[n][n][\F] \to \F\) be an alternating \(n\)-linear function, and let \(\seq{E}{1,2,3} \in \ms[n][n][\F]\) be elementary matrices of types 1, 2, and 3, respectively.
  Suppose that \(E_2\) is obtained by multiplying some row of \(I_n\) by the nonzero scalar \(k\).
  Then \(\delta(E_1) = -\delta(I_n)\), \(\delta(E_2) = k \cdot \delta(I_n)\), and \(\delta(E_3) = \delta(I_n)\).
\end{cor}

\begin{proof}[\pf{4.5.8}]
  By \cref{4.10}(a) we have \(\delta(E_1) = -\delta(I_n)\).
  By \cref{4.5.1} we have \(\delta(E_2) = k \delta(I_n)\).
  By \cref{4.5.6}(a) we have \(\delta(E_3) = \delta(I_n)\).
\end{proof}

\begin{thm}\label{4.11}
  Let \(\delta : \ms[n][n][\F] \to \F\) be an alternating \(n\)-linear function such that \(\delta(I_n) = 1\).
  For any \(A, B \in \ms[n][n][\F]\), we have \(\delta(AB) = \delta(A) \cdot \delta(B)\).
\end{thm}

\begin{proof}[\pf{4.11}]
  We begin by establishing the result when \(A\) is an elementary matrix.
  \begin{itemize}
    \item If \(A\) is an elementary matrix obtained by interchanging two rows of \(I_n\), then by \cref{4.5.8} we have \(\delta(A) = -\delta(I_n) = -1\).
          But by \cref{3.1}, \(AB\) is a matrix obtained by interchanging two rows of \(B\).
          Hence by \cref{4.10}(a), \(\delta(AB) = -\delta(B) = \delta(A) \cdot \delta(B)\).
    \item If \(A\) is an elementary matrix obtained by multiplying one row of \(I_n\) with \(k \in \F\), then by \cref{4.5.8} we have \(\delta(A) = k \delta(I_n) = k\).
          But by \cref{3.1}, \(AB\) is a matrix obtained by multiplying the same row of \(B\) with \(k\).
          Hence by \cref{4.5.1}, \(\delta(AB) = k \delta(B) = \delta(A) \cdot \delta(B)\).
    \item If \(A\) is an elementary matrix obtained by adding a multiple of some row of \(I_n\) to another row, then by \cref{4.5.8} we have \(\delta(A) = \delta(I_n) = 1\).
          But by \cref{3.1}, \(AB\) is a matrix obtained by adding the same multiple of the same row of \(B\) to the same other row.
          Hence by \cref{4.10}(a), \(\delta(AB) = \delta(B) = \delta(A) \cdot \delta(B)\).
  \end{itemize}

  If \(\rk{A} < n\), then \(\delta(A) = 0\) by \cref{4.5.7}.
  Since \(\rk{AB} \leq \rk{A} < n\) by \cref{3.7}, we have \(\delta(AB) = 0\).
  Thus \(\delta(AB) = \delta(A) \cdot \delta(B)\) in this case.

  On the other hand, if \(\rk{A} = n\), then \(A\) is invertible and hence is the product of elementary matrices (\cref{3.2.6}), say, \(A = \seq[]{E}{m,,1}\).
  The first paragraph of this proof shows that
  \begin{align*}
    \delta(AB) & = \delta(\seq[]{E}{m,,1} B)                       \\
               & = \delta(E_m) \cdot \delta(\seq[]{E}{m - 1,,1} B) \\
               & = \delta(E_m) \cdots \delta(E_1) \cdot \delta(B)  \\
               & = \delta(\seq[]{E}{m,,1}) \cdot \delta(B)         \\
               & = \delta(A) \cdot \delta(B).
  \end{align*}
\end{proof}

\begin{thm}\label{4.12}
  If \(\delta : \ms[n][n][\F] \to \F\) is an alternating \(n\)-linear function such that \(\delta(I_n) = 1\), then \(\delta(A) = \det(A)\) for every \(A \in \ms[n][n][\F]\).
\end{thm}

\begin{proof}[\pf{4.12}]
  Let \(\delta : \ms[n][n][\F] \to \F\) be an alternating \(n\)-linear function such that \(\delta(I_n) = 1\), and let \(A \in \ms[n][n][\F]\).
  If \(A\) has rank less than \(n\), then by \cref{4.5.7}, \(\delta(A) = 0\).
  Since \cref{4.2.7} gives \(\det(A) = 0\), we have \(\delta(A) = \det(A)\) in this case.
  If, on the other hand, \(A\) has rank \(n\), then \(A\) is invertible and hence is the product of elementary matrices (\cref{3.2.6}), say \(A = \seq[]{E}{m,,1}\).
  Since \(\delta(I_n) = 1\), it follows from \cref{4.5.8} and the properties of determinant that \(\delta(E) = \det(E)\) for every elementary matrix \(E\).
  Hence by \cref{4.7,4.11}, we have
  \begin{align*}
    \delta(A) & = \delta(\seq[]{E}{m,,1})        \\
              & = \delta(E_m) \cdots \delta(E_1) \\
              & = \det(E_m) \cdots \det(E_1)     \\
              & = \det(\seq[]{E}{m,,1})          \\
              & = \det(A).
  \end{align*}
\end{proof}

\begin{note}
  \cref{4.12} provides the desired characterization of the determinant:
  It is the unique function \(\delta : \ms[n][n][\F] \to \F\) that is \(n\)-linear, is alternating, and has the property that \(\delta(I_n) = 1\).
\end{note}

\exercisesection

\setcounter{ex}{1}
\begin{ex}\label{ex:4.5.2}
  Determine all the \(1\)-linear functions \(\delta : \ms[1][1][\F] \to \F\).
\end{ex}

\begin{proof}[\pf{ex:4.5.2}]
  We claim that the set of all \(1\)-linear functions, denoted as \(S\), is \(\ls(\ms[1][1][\F], \F)\).
  Since
  \begin{align*}
         & f \in \ls(\ms[1][1][\F], \F)                                                     \\
    \iff & \forall x, y \in \ms[1][1][\F] \text{ and } \forall c \in \F,                    \\
         & f(cx + y) = cf(x) + f(y)                                      &  & \by{2.1.2}[b] \\
    \iff & f \in S,                                                      &  & \by{4.5.1}
  \end{align*}
  we see that \(S = \ls(\ms[1][1][\F], \F)\).
\end{proof}

\setcounter{ex}{12}
\begin{ex}\label{ex:4.5.13}
  Prove that \(\det : \ms[2][2][\F] \to \F\) is a \(2\)-linear function of the columns of a matrix.
\end{ex}

\begin{proof}[\pf{ex:4.5.13}]
  By \cref{4.1,4.8} we see that this is true.
\end{proof}

\begin{ex}\label{ex:4.5.14}
  Let \(a, b, c, d \in \F\).
  Prove that the function \(\delta : \ms[2][2][\F] \to \F\) defined by \(\delta(A) = A_{1 1} A_{2 2} a + A_{1 1} A_{2 1} b + A_{1 2} A_{2 2} c + A_{1 2} A_{2 1} d\) is a \(2\)-linear function.
\end{ex}

\begin{proof}[\pf{ex:4.5.14}]
  Let \(x, y, k \in \F\).
  Since
  \begin{align*}
     & \delta\begin{pmatrix}
               A_{1 1}      & A_{1 2}      \\
               A_{2 1} + kx & A_{2 2} + ky
             \end{pmatrix}                                                                           \\
     & = A_{1 1} (A_{2 2} + ky) a + A_{1 1} (A_{2 1} + kx) b + A_{1 2} (A_{2 2} + ky) c + A_{1 2} (A_{2 1} + kx) d \\
     & = A_{1 1} A_{2 2} a + A_{1 1} A_{2 1} b + A_{1 2} A_{2 2} c + A_{1 2} A_{2 1} d                             \\
     & \quad + A_{1 1} ky a + A_{1 1} kx b + A_{1 2} ky c + A_{1 2} kx d                                           \\
     & = \delta\begin{pmatrix}
                 A_{1 1} & A_{1 2} \\
                 A_{2 1} & A_{2 2}
               \end{pmatrix} + k \delta\begin{pmatrix}
                                         A_{1 1} & A_{1 2} \\
                                         x       & y
                                       \end{pmatrix}
  \end{align*}
  and
  \begin{align*}
     & \delta\begin{pmatrix}
               A_{1 1} + kx & A_{1 2} + ky \\
               A_{2 1}      & A_{2 2}
             \end{pmatrix}                                                                           \\
     & = (A_{1 1} + kx) A_{2 2} a + (A_{1 1} + kx) A_{2 1} b + (A_{1 2} + ky) A_{2 2} c + (A_{1 2} + ky) A_{2 1} d \\
     & = A_{1 1} A_{2 2} a + A_{1 1} A_{2 1} b + A_{1 2} A_{2 2} c + A_{1 2} A_{2 1} d                             \\
     & \quad + kx A_{2 2} a + kx A_{2 1} b + ky A_{2 2} c + ky A_{2 1} d                                           \\
     & = \delta\begin{pmatrix}
                 A_{1 1} & A_{1 2} \\
                 A_{2 1} & A_{2 2}
               \end{pmatrix} + k \delta\begin{pmatrix}
                                         x       & y       \\
                                         A_{2 1} & A_{2 2}
                                       \end{pmatrix},
  \end{align*}
  by \cref{4.5.1} we know that \(\delta\) is a \(2\)-linear function.
\end{proof}

\begin{ex}\label{ex:4.5.15}
  Prove that \(\delta : \ms[2][2][\F] \to \F\) is a \(2\)-linear function iff it has the form
  \[
    \delta(A) = A_{1 1} A_{2 2} a + A_{1 1} A_{2 1} b + A_{1 2} A_{2 2} c + A_{1 2} A_{2 1} d
  \]
  for some scalars \(a, b, c, d \in \F\).
\end{ex}

\begin{proof}[\pf{ex:4.5.15}]
  Thanks to \cref{ex:4.5.14}, we only need to prove that if \(\delta\) is a \(2\)-linear function, then
  \[
    \exists a, b, c, d \in \F : \forall A \in \ms[2][2][\F], \delta(A) = A_{1 1} A_{2 2} a + A_{1 1} A_{2 1} b + A_{1 2} A_{2 2} c + A_{1 2} A_{2 1} d.
  \]
  Define \(a, b, c, d \in \F\) such that
  \[
    a = \delta\begin{pmatrix}
      1 & 0 \\
      0 & 1
    \end{pmatrix} \quad b = \delta\begin{pmatrix}
      1 & 0 \\
      1 & 0
    \end{pmatrix} \quad c = \delta\begin{pmatrix}
      0 & 1 \\
      0 & 1
    \end{pmatrix} \quad d = \delta\begin{pmatrix}
      0 & 1 \\
      1 & 0
    \end{pmatrix}.
  \]
  Let \(A \in \ms[2][2][\F]\).
  Then we have
  \begin{align*}
    \delta\begin{pmatrix}
            A_{1 1} & A_{1 2} \\
            A_{2 1} & A_{2 2}
          \end{pmatrix} & = A_{1 1} \delta\begin{pmatrix}
                                            1       & 0       \\
                                            A_{2 1} & A_{2 2}
                                          \end{pmatrix} + A_{1 2} \delta\begin{pmatrix}
                                                                          0       & 1       \\
                                                                          A_{2 1} & A_{2 2}
                                                                        \end{pmatrix}                                    &  & \by{4.5.1}                 \\
                          & = A_{1 1} A_{2 1} \delta\begin{pmatrix}
                                                      1 & 0 \\
                                                      1 & 0
                                                    \end{pmatrix} + A_{1 1} A_{2 2} \delta\begin{pmatrix}
                                                                                            1 & 0 \\
                                                                                            0 & 1
                                                                                          \end{pmatrix}                            &  & \by{4.5.1}       \\
                          & \quad + A_{1 2} A_{2 1} \delta\begin{pmatrix}
                                                            0 & 1 \\
                                                            1 & 0
                                                          \end{pmatrix} + A_{1 2} A_{2 2} \delta\begin{pmatrix}
                                                                                                  0 & 1 \\
                                                                                                  0 & 1
                                                                                                \end{pmatrix}                            &  & \by{4.5.1} \\
                          & = A_{1 1} A_{2 2} a + A_{1 1} A_{2 1} b + A_{1 2} A_{2 2} c + A_{1 2} A_{2 1} d.
  \end{align*}
\end{proof}

\begin{ex}\label{ex:4.5.16}
  Prove that if \(\delta : \ms[n][n][\F] \to \F\) is an alternating \(n\)-linear function, then there exists a scalar \(k\) such that \(\delta(A) = k \det(A)\) for all \(A \in \ms[n][n][\F]\).
\end{ex}

\begin{proof}[\pf{ex:4.5.16}]
  First we claim that for any elementary matrix \(E \in \ms[n][n][\F]\), we have \(\delta(EA) = \det(E) \delta(A)\) for all \(A \in \ms[n][n][\F]\).
  \begin{itemize}
    \item If \(E\) is an elementary matrix of type 1, then by \cref{3.1,4.10}(a) we have \(\delta(EA) = -\delta(A) = \det(E) \cdot \delta(A)\).
    \item If \(E\) is an elementary matrix of type 2, then by \cref{3.1,4.5.1} we have \(\delta(EA) = \det(E) \delta(A)\).
    \item If \(E\) is an elementary matrix of type 3, then by \cref{3.1,4.5.6} we have \(\delta(EA) = \delta(A) = \det(E) \delta(A)\).
  \end{itemize}

  Now we claim that \(\delta(A) = \delta(I_n) \det(A)\) for all \(A \in \ms[n][n][\F]\).
  If \(\rk{A} < n\), then by \cref{4.5.7,4.3.1} we have \(\delta(A) = 0 = \delta(I_n) 0 = \delta(I_n) \det(A)\).
  If \(\rk{A} = n\), then \(A\) is invertible and hence is the product of elementary matrices (\cref{3.2.6}), say, \(A = \seq[]{E}{m,,1}\).
  Thus we have
  \begin{align*}
    \delta(A) & = \delta(\seq[]{E}{m,,1})                                                         \\
              & = \delta(\seq[]{E}{m,,1} I_n)                                                     \\
              & = \det(E_m) \cdot \delta(\seq[]{E}{m - 1,,1})  &  & \text{(from the proof above)} \\
              & = \det(E_m) \cdots \det(E_1) \cdot \delta(I_n) &  & \text{(from the proof above)} \\
              & = \det(\seq[]{E}{m,,1}) \cdot \delta(I_n)      &  & \by{4.7}                      \\
              & = \delta(I_n) \det(A).
  \end{align*}
\end{proof}

\begin{ex}\label{ex:4.5.17}
  Prove that a linear combination of two \(n\)-linear functions is an \(n\)-linear function.
\end{ex}

\begin{proof}[\pf{ex:4.5.17}]
  Let \(f_1, f_2\) be \(n\)-linear functions, let \(c_1, c_2, k \in \F\) and let \(\seq{a}{1,,n}, v \in \vs{F}^n\).
  Since
  \begin{align*}
     & (c_1 f_1 + c_2 f_2)\begin{pmatrix}
                            a_1       \\
                            \vdots    \\
                            a_{r - 1} \\
                            a_r + kv  \\
                            a_{r + 1} \\
                            \vdots    \\
                            a_n
                          \end{pmatrix} = c_1 f_1\begin{pmatrix}
                                                   a_1       \\
                                                   \vdots    \\
                                                   a_{r - 1} \\
                                                   a_r + kv  \\
                                                   a_{r + 1} \\
                                                   \vdots    \\
                                                   a_n
                                                 \end{pmatrix} + c_2 f_2\begin{pmatrix}
                                                                          a_1       \\
                                                                          \vdots    \\
                                                                          a_{r - 1} \\
                                                                          a_r + kv  \\
                                                                          a_{r + 1} \\
                                                                          \vdots    \\
                                                                          a_n
                                                                        \end{pmatrix}   \\
     & = c_1 f_1\begin{pmatrix}
                  a_1       \\
                  \vdots    \\
                  a_{r - 1} \\
                  a_r       \\
                  a_{r + 1} \\
                  \vdots    \\
                  a_n
                \end{pmatrix} + k c_1 f_1\begin{pmatrix}
                                           a_1       \\
                                           \vdots    \\
                                           a_{r - 1} \\
                                           v         \\
                                           a_{r + 1} \\
                                           \vdots    \\
                                           a_n
                                         \end{pmatrix}             &  & \by{4.5.1}       \\
     & \quad + c_2 f_2\begin{pmatrix}
                        a_1       \\
                        \vdots    \\
                        a_{r - 1} \\
                        a_r       \\
                        a_{r + 1} \\
                        \vdots    \\
                        a_n
                      \end{pmatrix} + k c_2 f_2\begin{pmatrix}
                                                 a_1       \\
                                                 \vdots    \\
                                                 a_{r - 1} \\
                                                 v         \\
                                                 a_{r + 1} \\
                                                 \vdots    \\
                                                 a_n
                                               \end{pmatrix}             &  & \by{4.5.1} \\
     & = (c_1 f_1 + c_2 f_2)\begin{pmatrix}
                              a_1       \\
                              \vdots    \\
                              a_{r - 1} \\
                              a_r       \\
                              a_{r + 1} \\
                              \vdots    \\
                              a_n
                            \end{pmatrix} + k (c_1 f_1 + c_2 f_2)\begin{pmatrix}
                                                                   a_1       \\
                                                                   \vdots    \\
                                                                   a_{r - 1} \\
                                                                   a_r       \\
                                                                   a_{r + 1} \\
                                                                   \vdots    \\
                                                                   a_n
                                                                 \end{pmatrix},
  \end{align*}
  by \cref{4.5.1} we see that \(c_1 f_1 + c_2 f_2\) is an \(n\)-linear function.
\end{proof}

\begin{ex}\label{ex:4.5.18}
  Prove that the set of all \(n\)-linear functions over a field \(\F\) is a vector space over \(\F\).
\end{ex}

\begin{proof}[\pf{ex:4.5.18}]
  By \cref{4.5.2,ex:4.5.17,1.3} we see that the set of all \(n\)-linear functions over a field \(\F\) is a subspace of \(\fs(\ms[n][n][\F], \F)\) over \(\F\) and thus a vector space over \(\F\).
\end{proof}

\begin{ex}\label{ex:4.5.19}
  Let \(\delta : \ms[n][n][\F] \to \F\) be an \(n\)-linear function and \(\F\) a field that does not have characteristic two.
  Prove that if \(\delta(B) = -\delta(A)\) whenever \(B\) is obtained from \(A \in \ms[n][n][\F]\) by interchanging any two rows of \(A\), then \(\delta(M) = 0\) whenever \(M \in \ms[n][n][\F]\) has two identical rows.
\end{ex}

\begin{proof}[\pf{ex:4.5.19}]
  Let \(M \in \ms[n][n][\F]\) such that row \(i\) and row \(j\) of \(M\) are identical and \(i < j\).
  To interchange row \(i\) and \(j\) of \(M\), we need to swap \(2(j - i) - 1\) times as calculated in \cref{4.10}.
  Since the swapping result is the same as \(M\), we have
  \begin{align*}
             & \delta(M) = (-1)^{2(j - 1) - 1} \delta(M)                     \\
    \implies & \delta(M) = -\delta(M)                                        \\
    \implies & 2\delta(M) = 0                                                \\
    \implies & \delta(M) = 0.                            &  & (\F \neq \Z_2)
  \end{align*}
\end{proof}

\begin{ex}\label{ex:4.5.20}
  Give an example to show that the implication in \cref{ex:4.5.19} need not hold if \(\F\) has characteristic two.
\end{ex}

\begin{proof}[\pf{ex:4.5.20}]
  Let \(\delta : \ms[2][2][\Z_2] \to \Z_2\) be as in \cref{ex:4.5.15} with \(a = b = c = d = 1\).
  Then we have
  \begin{align*}
    \delta\begin{pmatrix}
            A_{1 1} & A_{1 2} \\
            A_{2 1} & A_{2 2}
          \end{pmatrix} & = A_{1 1} A_{2 2} + A_{1 1} A_{2 1} + A_{1 2} A_{2 2} + A_{1 2} A_{2 1}                         \\
                          & = A_{2 1} A_{1 2} + A_{2 1} A_{1 1} + A_{2 2} A_{1 2} + A_{2 2} A_{1 1}                       \\
                          & = \delta\begin{pmatrix}
                                      A_{2 1} & A_{2 2} \\
                                      A_{1 1} & A_{1 2}
                                    \end{pmatrix}                                                                     \\
                          & = -\delta\begin{pmatrix}
                                       A_{2 1} & A_{2 2} \\
                                       A_{1 1} & A_{1 2}
                                     \end{pmatrix}.                                                &  & (-1 = 1 \in \Z_2)
  \end{align*}
  But
  \[
    \delta\begin{pmatrix}
      1 & 0 \\
      1 & 0
    \end{pmatrix} = 0 + 1 + 0 + 0 = 1 \neq 0.
  \]
\end{proof}
