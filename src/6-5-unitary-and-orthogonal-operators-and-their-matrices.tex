\section{Unitary and Orthogonal Operators and Their Matrices}\label{sec:6.5}

\begin{note}
	In \cref{sec:6.5}, we study those linear operators \(\T\) on an inner product space \(\V\) over \(\F\) such that \(\T \T^* = \T^* \T = \IT[\V]\).
	We will see that these are precisely the linear operators that ``preserve length'' in the sense that \(\norm{\T(x)} = \norm{x}\) for all \(x \in \V\).
	As another characterization, we prove that, on a finite-dimensional complex inner product space, these are the normal operators whose eigenvalues all have absolute value \(1\).

	In past chapters, we were interested in studying those functions that preserve the structure of the underlying space.
	In particular, linear operators preserve the operations of vector addition and scalar multiplication, and isomorphisms preserve all the vector space structure.
	It is now natural to consider those linear operators \(\T\) on an inner product space that preserve length.
	We will see that this condition guarantees, in fact, that \(\T\) preserves the inner product.
\end{note}

\begin{defn}\label{6.5.1}
	Let \(\T\) be a linear operator on a finite-dimensional inner product space \(\V\) over \(\F\).
	If \(\norm{\T(x)} = \norm{x}\) for all \(x \in \V\), we call \(\T\) a \textbf{unitary operator} if \(\F = \C\) and an \textbf{orthogonal operator} if \(\F = \R\).

	in the infinite-dimensional case, an operator satisfying the preceding norm requirement is generally called an \textbf{isometry}.
	If, in addition, the operator is onto (the condition guarantees one-to-one, see \cref{ex:6.1.17}), then the operator is called a \textbf{unitary} or \textbf{orthogonal operator}.
\end{defn}

\begin{note}
	Clearly, any rotation or reflection in \(\R^2\) preserves length and hence is an orthogonal operator.
	We study these operators in much more detail in \cref{sec:6.11}.
\end{note}

\begin{eg}\label{6.5.2}
	Let \(h \in \vs{H}\) (see \cref{6.1.8}) satisfy \(\abs{h(x)} = 1\) for all \(x \in [0, 2 \pi]\).
	Define \(\T \in \ls(\vs{H})\) by \(\T(f) = hf\).
	Then
	\begin{align*}
		\norm{\T(f)}^2 & = \norm{hf}^2                                                                     \\
		               & = \inn{hf, hf}                                                    &  & \by{6.1.9} \\
		               & = \frac{1}{2 \pi} \int_0^{2 \pi} h(t) f(t) \conj{h(t) f(t)} \; dt &  & \by{6.1.8} \\
		               & = \inn{f, f}                                                      &  & \by{6.1.8} \\
		               & = \norm{f}^2                                                      &  & \by{6.1.9}
	\end{align*}
	since \(\abs{h(t)}^2 = 1\) for all \(t \in [0, 2 \pi]\).
	So \(\T\) is a unitary operator.
\end{eg}

\begin{lem}\label{6.5.3}
	Let \(\U\) be a self-adjoint operator on a finite-dimensional inner product space \(\V\) over \(\F\).
	If \(\inn{x, \U(x)} = 0\) for all \(x \in \V\), then \(\U = \zT\).
\end{lem}

\begin{proof}[\pf{6.5.3}]
	By either \cref{6.16} or \cref{6.17}, we may choose an orthonormal basis \(\beta\) for \(\V\) over \(\F\) consisting of eigenvectors of \(\U\).
	If \(x \in \beta\), then \(\U(x) = \lambda x\) for some \(\lambda\).
	Thus
	\begin{align*}
		0 & = \inn{x, \U(x)}                              \\
		  & = \inn{x, \lambda x}         &  & \by{5.1.2}  \\
		  & = \conj{\lambda} \inn{x, x}; &  & \by{6.1}[b]
	\end{align*}
	so \(\lambda = 0\).
	Hence \(\U(x) = \zv\) for all \(x \in \beta\), and thus \(\U = \zT\).
\end{proof}

\begin{note}
	Compare \cref{6.5.3} to \cref{ex:6.4.11}(b).
\end{note}

\begin{thm}\label{6.18}
	Let \(\T\) be a linear operator on a finite-dimensional inner product space \(\V\) over \(\F\).
	Then the following statements are equivalent.
	\begin{enumerate}
		\item \(\T \T^* = \T^* \T = \IT[\V]\).
		\item \(\inn{\T(x), \T(y)} = \inn{x, y}\) for all \(x, y \in \V\).
		\item If \(\beta\) is an orthonormal basis for \(\V\) over \(\F\), then \(\T(\beta)\) is an orthonormal basis for \(\V\) over \(\F\).
		\item There exists an orthonormal basis \(\beta\) for \(\V\) over \(\F\) such that \(\T(\beta)\) is an orthonormal basis for \(\V\) over \(\F\).
		\item \(\norm{\T(x)} = \norm{x}\) for all \(x \in \V\).
	\end{enumerate}
	Thus all the conditions above are equivalent to the definition of a unitary or orthogonal operator.
	From (a), it follows that unitary or orthogonal operators are normal.
\end{thm}

\begin{proof}[\pf{6.18}]
	We prove first that (a) implies (b).
	Let \(x, y \in \V\).
	Then \(\inn{x, y} = \inn{\T^* \T(x), y} = \inn{\T(x), \T(y)}\).

	Second, we prove that (b) implies (c).
	Let \(\beta = \set{\seq{v}{1,,n}}\) be an orthonormal basis for \(\V\) over \(\F\);
	so \(\T(\beta) = \set{\T(v_1), \dots, \T(v_n)}\).
	It follows that \(\inn{\T(v_i), \T(v_j)} = \inn{v_i, v_j} = \delta_{i j}\).
	Therefore \(\T(\beta)\) is an orthonormal basis for \(\V\) over \(\F\).

	That (c) implies (d) is obvious.

	Next we prove that (d) implies (e).
	Let \(x \in \V\), and let \(\beta = \set{\seq{v}{1,,n}}\).
	Now
	\[
		x = \sum_{i = 1}^n a_i v_i
	\]
	for some scalars \(a_i \in \F\), and so
	\begin{align*}
		\norm{x}^2 & = \inn{\sum_{i = 1}^n a_i v_i, \sum_{j = 1}^n a_j v_j}         &  & \by{6.1.9}      \\
		           & = \sum_{i = 1}^n  a_i \inn{v_i, \sum_{j = 1}^n \conj{a_j} v_j} &  & \by{6.1.1}[a,b] \\
		           & = \sum_{i = 1}^n \sum_{j = 1}^n a_i \conj{a_j} \inn{v_i, v_j}  &  & \by{6.1}[a,b]   \\
		           & = \sum_{i = 1}^n \sum_{j = 1}^n a_i \conj{a_j} \delta_{i j}    &  & \by{6.1.12}     \\
		           & = \sum_{i = 1}^n a_i \conj{a_i}                                &  & \by{2.3.4}      \\
		           & = \sum_{i = 1}^n \abs{a_i}^2                                   &  & \by{d.0.5}
	\end{align*}
	since \(\beta\) is orthonormal.

	Applying the same manipulations to
	\[
		\T(x) = \sum_{i = 1}^n a_i \T(v_i)
	\]
	and using the fact that \(\T(\beta)\) is also orthonormal, we obtain
	\[
		\norm{\T(x)}^2 = \sum_{i = 1}^n \abs{a_i}^2.
	\]
	Hence \(\norm{\T(x)} = \norm{x}\).

	Finally, we prove that (e) implies (a).
	For any \(x \in \V\), we have
	\begin{align*}
		\inn{x, x} & = \norm{x}^2           &  & \by{6.1.9} \\
		           & = \norm{\T(x)}^2                       \\
		           & = \inn{\T(x), \T(x)}   &  & \by{6.1.9} \\
		           & = \inn{x, \T^* \T(x)}. &  & \by{6.9}
	\end{align*}
	So \(\inn{x, (\IT[\V] - \T^* \T)(x)} = 0\) for all \(x \in \V\).
	Let \(\U = \IT[\V] - \T^* \T\);
	then \(\U\) is self-adjoint (by \cref{6.11}) and \(\inn{x, \U(x)} = 0\) for all \(x \in \V\).
	Hence, by \cref{6.5.3}, we have \(\zT = \U = \IT[\V] - \T^* \T\), and therefore \(\T^* \T = \IT[\V]\).
	Since \(\V\) is finite-dimensional, we may use \cref{ex:2.4.10} to conclude that \(\T \T^* = \IT[\V]\).
\end{proof}

\begin{cor}\label{6.5.4}
	Let \(\T\) be a linear operator on a finite-dimensional real inner product space \(\V\).
	Then \(\V\) has an orthonormal basis of eigenvectors of \(\T\) with corresponding eigenvalues of absolute value \(1\) iff \(\T\) is both self-adjoint and orthogonal.
\end{cor}

\begin{proof}[\pf{6.5.4}]
	Suppose that \(\V\) has an orthonormal basis \(\set{\seq{v}{1,,n}}\) over \(\R\) such that \(\T(v_i) = \lambda_i v_i\) and \(\abs{\lambda_i} = 1\) for all \(i \in \set{1, \dots, n}\).
	By \cref{6.17}, \(\T\) is self-adjoint.
	Thus
	\begin{align*}
		(\T \T^*)(v_i) & = (\T \T)(v_i)            &  & \by{6.4.8}                                          \\
		               & = \T(\lambda_i v_i)       &  & \by{5.1.2}                                          \\
		               & = \lambda_i \lambda_i v_i &  & \by{5.1.2}                                          \\
		               & = \lambda_i^2 v_i                                                                  \\
		               & = v_i                     &  & (\abs{\lambda_i} = 1 \text{ and } \lambda_i \in \R)
	\end{align*}
	for each \(i \in \set{1, \dots, n}\).
	So \(\T \T^* = \IT[\V]\), and again by \cref{ex:2.4.10}, \(\T\) is orthogonal by \cref{6.18}(a).

	If \(\T\) is self-adjoint, then, by \cref{6.17}, we have that \(\V\) possesses an orthonormal basis \(\set{\seq{v}{1,,n}}\) over \(\R\) such that \(\T(v_i) = \lambda_i v_i\) for all \(i \in \set{1, \dots, n}\).
	If \(\T\) is also orthogonal, we have
	\begin{align*}
		\abs{\lambda_i} \cdot \norm{v_i} & = \norm{\lambda_i v_i} &  & \by{6.2}[a] \\
		                                 & = \norm{\T(v_i)}       &  & \by{5.1.2}  \\
		                                 & = \norm{v_i};          &  & \by{6.5.1}
	\end{align*}
	so \(\abs{\lambda_i} = 1\) for every \(i \in \set{1, \dots, n}\).
\end{proof}

\begin{cor}\label{6.5.5}
	Let \(\T\) be a linear operator on a finite-dimensional complex inner product space \(\V\).
	Then \(\V\) has an orthonormal basis of eigenvectors of \(\T\) with corresponding eigenvalues of absolute value \(1\) iff \(\T\) is unitary.
\end{cor}

\begin{proof}[\pf{6.5.5}]
	Suppose that \(\V\) has an orthonormal basis \(\set{\seq{v}{1,,n}}\) over \(\C\) such that \(\T(v_i) = \lambda_i v_i\) and \(\abs{\lambda_i} = 1\) for all \(i \in \set{1, \dots, n}\).
	By \cref{6.16}, \(\T\) is normal.
	Thus
	\begin{align*}
		(\T^* \T)(v_i) & = (\T \T^*)(v_i)                 &  & \by{6.4.3}            \\
		               & = \T(\conj{\lambda_i} v_i)       &  & \by{6.15}[c]          \\
		               & = \conj{\lambda_i} \lambda_i v_i &  & \by{5.1.2}            \\
		               & = \abs{\lambda_i}^2 v_i          &  & \by{d.0.5}            \\
		               & = v_i                            &  & (\abs{\lambda_i} = 1)
	\end{align*}
	for each \(i \in \set{1, \dots, n}\).
	So \(\T^* \T = \T \T^* = \IT[\V]\) by \cref{2.1.13}.
	Thus by \cref{6.18}(a) \(\T\) is unitary.

	If \(\T\) is unitary, then by \cref{6.18}(a)(e) we know that \(\T\) is normal.
	By \cref{6.16}, we have that \(\V\) possesses an orthonormal basis \(\set{\seq{v}{1,,n}}\) over \(\C\) such that \(\T(v_i) = \lambda_i v_i\) for all \(i \in \set{1, \dots, n}\).
	Then we have
	\begin{align*}
		\abs{\lambda_i} \cdot \norm{v_i} & = \norm{\lambda_i v_i} &  & \by{6.2}[a] \\
		                                 & = \norm{\T(v_i)}       &  & \by{5.1.2}  \\
		                                 & = \norm{v_i};          &  & \by{6.5.1}
	\end{align*}
	so \(\abs{\lambda_i} = 1\) for every \(i \in \set{1, \dots, n}\).
\end{proof}

\begin{eg}\label{6.5.6}
	Let \(\T : \R^2 \to \R^2\) be a rotation by \(\theta\), where \(0 < \theta < \pi\).
	It is clear geometrically that \(\T\) ``preserves length'', that is, that \(\norm{\T(x)} = \norm{x}\) for all \(x \in \R^2\).
	The fact that rotations by a fixed angle preserve perpendicularity not only can be seen geometrically but now follows from \cref{6.18}(b).
	Perhaps the fact that such a transformation preserves the inner product is not so obvious;
	however, we obtain this fact from \cref{6.18}(b) also.
	Finally, an inspection of the matrix representation of \(\T\) with respect to the standard ordered basis, which is
	\[
		\begin{pmatrix}
			\cos(\theta) & -\sin(\theta) \\
			\sin(\theta) & \cos(\theta)
		\end{pmatrix},
	\]
	reveals that \(\T\) is not self-adjoint for the given restriction on \(\theta\).
	As we mentioned earlier, this fact also follows from the geometric observation that \(\T\) has no eigenvectors (see \cref{5.1.3}) and from \cref{6.17}.
	It is seen easily from the preceding matrix that \(\T^*\) is the rotation by \(-\theta\) since
	\[
		\begin{pmatrix}
			\cos(\theta) & -\sin(\theta) \\
			\sin(\theta) & \cos(\theta)
		\end{pmatrix}^* = \begin{pmatrix}
			\cos(\theta)  & \sin(\theta) \\
			-\sin(\theta) & \cos(\theta)
		\end{pmatrix} = \begin{pmatrix}
			\cos(-\theta) & -\sin(-\theta) \\
			\sin(-\theta) & \cos(-\theta)
		\end{pmatrix}.
	\]
\end{eg}

\begin{defn}\label{6.5.7}
	Let \(\vs{L}\) be a one-dimensional subspace of \(\R^2\) over \(\R\).
	We may view \(\vs{L}\) as a line in the plane through the origin.
	A linear operator \(\T\) on \(\R^2\) is called a \textbf{reflection of \(\R^2\) about \(\vs{L}\)} if \(\T(x) = x\) for all \(x \in \vs{L}\) and \(\T(x) = -x\) for all \(x \in \vs{L}^{\perp}\).
\end{defn}

\begin{eg}\label{6.5.8}
	Let \(\T\) be a reflection of \(\R^2\) about a line \(\vs{L}\) through the origin.
	We show that \(\T\) is an orthogonal operator.
	Select vectors \(v_1 \in \vs{L}\) and \(v_2 \in \vs{L}^{\perp}\) such that \(\norm{v_1} = \norm{v_2} = 1\).
	Then \(\T(v_1) = v_1\) and \(\T(v_2) = -v_2\).
	Thus \(v_1\) and \(v_2\) are eigenvectors of \(\T\) with corresponding eigenvalues \(1\) and \(-1\), respectively.
	Furthermore, \(\set{v_1, v_2}\) is an orthonormal basis for \(\R^2\).
	It follows that \(\T\) is an orthogonal operator by \cref{6.5.4}.
\end{eg}

\begin{defn}\label{6.5.9}
	A square matrix \(A \in \ms[n][n][\F]\) is called an an \textbf{orthogonal matrix} if \(\tp{A} A = A \tp{A} = I_n\) and \textbf{unitary} if \(A^* A = A A^* = I_n\).

	Since for a real matrix \(A\) we have \(A^* = \tp{A}\), a real unitary matrix is also orthogonal.
	In this case, we call \(A\) \textbf{orthogonal} rather than unitary.

	The condition \(A A^* = I_n\) is equivalent to the statement that the rows of \(A\) form an orthonormal basis for \(\vs{F}^n\) because
	\begin{align*}
		\delta_{i j} & = (A A^*)_{i j}                          &  & \by{2.3.4} \\
		             & = \sum_{k = 1}^n A_{i k} (A^*)_{k j}     &  & \by{2.3.1} \\
		             & = \sum_{k = 1}^n A_{i k} \conj{A_{j k}}, &  & \by{6.1.5}
	\end{align*}
	and the last term represents the inner product of the \(i\)th and \(j\)th rows of \(A\).

	The condition \(A^* A = I_n\) is equivalent to the statement that the columns of \(A\) form an orthonormal basis for \(\vs{F}^n\) because
	\begin{align*}
		\delta_{i j} & = (A^* A)_{i j}                          &  & \by{2.3.4} \\
		             & = \sum_{k = 1}^n (A^*)_{i k} A_{k j}     &  & \by{2.3.1} \\
		             & = \sum_{k = 1}^n \conj{A_{k i}} A_{k j}, &  & \by{6.1.5}
	\end{align*}
	and the last term represents the inner product of the \(j\)th and \(i\)th columns of \(A\).
\end{defn}

\begin{cor}\label{6.5.10}
	A linear operator \(\T\) on a finite-dimensional inner product space \(\V\) over \(\F\) is unitary (orthogonal) iff \([\T]_{\beta}\) is unitary (orthogonal) for some orthonormal basis \(\beta\) for \(\V\) over \(\F\).
\end{cor}

\begin{proof}[\pf{6.5.10}]
	We have
	\begin{align*}
		     & \T \text{ is unitary}                                                               \\
		\iff & \forall x \in \V, \norm{\T(x)} = \norm{x}                       &  & \by{6.5.1}     \\
		\iff & \T \T^* = \T^* \T = \IT[\V]                                     &  & \by{6.18}[a,e] \\
		\iff & [\T \T^*]_{\beta} = [\T^* \T]_{\beta} = [\IT[\V]]_{\beta} = I_n &  & \by{2.12}[d]   \\
		\iff & [\T]_{\beta} [\T^*]_{\beta} = [\T^*]_{\beta} [\T]_{\beta} = I_n &  & \by{2.3.3}     \\
		\iff & [\T]_{\beta} [\T]_{\beta}^* = [\T]_{\beta}^* [\T]_{\beta} = I_n &  & \by{6.10}      \\
		\iff & [\T]_{\beta} \text{ is unitary}.                                &  & \by{6.5.9}
	\end{align*}
	Similar argument shows that \(\T\) is orthogonal iff \([\T]_{\beta}\) is orthogonal.
\end{proof}

\begin{eg}\label{6.5.11}
	From \cref{6.5.6}, the matrix
	\[
		\begin{pmatrix}
			\cos(\theta) & -\sin(\theta) \\
			\sin(\theta) & \cos(\theta)
		\end{pmatrix}
	\]
	is clearly orthogonal.
	One can easily see that the rows of the matrix form an orthonormal basis for \(\R^2\) since
	\begin{align*}
		\inn{(\cos(\theta), -\sin(\theta)), (\cos(\theta), -\sin(\theta))} & = (\cos(\theta))^2 + (\sin(\theta))^2                   &  & \by{6.1.2} \\
		                                                                   & = 1;                                                                    \\
		\inn{(\cos(\theta), -\sin(\theta)), (\sin(\theta), \cos(\theta))}  & = \sin(\theta) \cos(\theta) - \sin(\theta) \cos(\theta) &  & \by{6.1.2} \\
		                                                                   & = 0;                                                                    \\
		\inn{(\sin(\theta), \cos(\theta)), (\sin(\theta), \cos(\theta))}   & = (\sin(\theta))^2 + (\cos(\theta))^2                   &  & \by{6.1.2} \\
		                                                                   & = 1.
	\end{align*}
	Similarly, the columns of the matrix form an orthonormal basis for \(\R^2\).
\end{eg}

\begin{eg}\label{6.5.12}
	Let \(\T\) be a reflection of \(\R^2\) about a line \(\L\) through the origin, let \(\beta\) be the standard ordered basis for \(\R^2\) over \(\R\), and let \(A = [\T]_{\beta}\).
	Then \(\T = \L_A\).
	Since \(\T\) is an orthogonal operator and \(\beta\) is an orthonormal basis, \(A\) is an orthogonal matrix by \cref{6.5.10}.
	We describe \(A\).

	Suppose that \(\alpha\) is the angle from the positive \(x\)-axis to \(\L\).
	Let \(v_1 = (\cos(\alpha), \sin(\alpha))\) and \(v_2 = (-\sin(\alpha), \cos(\alpha))\).
	Then \(\norm{v_1} = \norm{v_2} = 1\), \(v_1 \in \L\), and \(v_2 \in \L^{\perp}\).
	Hence \(\gamma = \set{v_1, v_2}\) is an orthonormal basis for \(\R^2\) over \(\R\).
	Because \(\T(v_1) = v_1\) and \(\T(v_2) = -v_2\), we have
	\[
		[\T]_{\gamma} = [\L_A]_{\gamma} = \begin{pmatrix}
			1 & 0  \\
			0 & -1
		\end{pmatrix}.
	\]
	Let
	\[
		Q = \begin{pmatrix}
			\cos(\alpha) & -\sin(\alpha) \\
			\sin(\alpha) & \cos(\alpha)
		\end{pmatrix}.
	\]
	Then
	\begin{align*}
		A & = Q [\L_A]_{\gamma} Q^{-1}                                                      &  & \by{2.5.3} \\
		  & = \begin{pmatrix}
			      \cos(\alpha) & -\sin(\alpha) \\
			      \sin(\alpha) & \cos(\alpha)
		      \end{pmatrix} \begin{pmatrix}
			                    1 & 0  \\
			                    0 & -1
		                    \end{pmatrix} \begin{pmatrix}
			                                  \cos(\alpha)  & \sin(\alpha) \\
			                                  -\sin(\alpha) & \cos(\alpha)
		                                  \end{pmatrix}                                      \\
		  & = \begin{pmatrix}
			      (\cos(\alpha))^2 - (\sin(\alpha))^2 & 2 \sin(\alpha) \cos(\alpha)            \\
			      2 \sin(\alpha) \cos(\alpha)         & -((\cos(\alpha))^2 - (\sin(\alpha))^2)
		      \end{pmatrix}                  \\
		  & = \begin{pmatrix}
			      \cos(2 \alpha) & \sin(2 \alpha)  \\
			      \sin(2 \alpha) & -\cos(2 \alpha)
		      \end{pmatrix}.
	\end{align*}
\end{eg}

\begin{defn}\label{6.5.13}
	We know that, for a complex normal (real symmetric) matrix \(A\), there exists an orthonormal basis \(\beta\) for \(\vs{F}^n\) consisting of eigenvectors of \(A\).
	Hence \(A\) is similar to a diagonal matrix \(D\).
	By \cref{2.5.3}, the matrix \(Q\) whose columns are the vectors in \(\beta\) is such that \(D = Q^{-1} A Q\).
	But since the columns of \(Q\) are an orthonormal basis for \(\vs{F}^n\), it follows that \(Q\) is unitary (orthogonal) (see \cref{ex:6.1.23}(c)).
	In this case, we say that \(A\) is \textbf{unitarily equivalent} (\textbf{orthogonally equivalent}) to \(D\).

	It is easily seen (see \cref{ex:6.5.18}) that this relation is an equivalence relation on \(\ms[n][n][\C]\) (\(\ms[n][n][\R]\)).
	More generally, \(A\) and \(B\) are unitarily equivalent (orthogonally equivalent) iff there exists a unitary (orthogonal) matrix \(P\) such that \(A = P^* B P\).
\end{defn}

\exercisesection

\begin{ex}\label{ex:6.5.18}
	Show that ``is unitarily equivalent to'' is an equivalence relation on \(\ms[n][n][\C]\).
\end{ex}

\begin{proof}[\pf{ex:6.5.18}]
	Let \(A, B, C \in \ms[n][n][\C]\).
	For reflexivity, we have
	\begin{align*}
		         & A = I_n A I_n = I_n^* A I_n              &  & \by{6.3.2}(e) \\
		\implies & A \text{ is unitarily equivalent to } A. &  & \by{6.5.13}
	\end{align*}
	For symmetric, we have
	\begin{align*}
		         & A \text{ is unitarily equivalent to } B                             \\
		\implies & \exists P \in \ms[n][n][\C] : \begin{dcases}
			                                         P \text{ is unitary} \\
			                                         A = P^* B P
		                                         \end{dcases} &  & \by{6.5.13}         \\
		\implies & B = (P^*)^{-1} A P^{-1} = P A P^*            &  & \by{ex:6.1.23}[c] \\
		         & = (P^*)^* A P^*                              &  & \by{6.3.2}[d]     \\
		\implies & B \text{ is unitarily equivalent to } A.
	\end{align*}
	For transitivity, we have
	\begin{align*}
		         & \begin{dcases}
			           A \text{ is unitarily equivalent to } B \\
			           B \text{ is unitarily equivalent to } C
		           \end{dcases}                            \\
		\implies & \exists P, Q \in \ms[n][n][\C] : \begin{dcases}
			                                            P, Q \text{ are unitary} \\
			                                            A = P^* B P              \\
			                                            B = Q^* C Q
		                                            \end{dcases} &  & \by{6.5.13}     \\
		\implies & A = P^* Q^* C Q P = (QP)^* C (QP)               &  & \by{6.3.2}[c] \\
		\implies & A \text{ is unitarily equivalent to } C.        &  & \by{6.5.13}
	\end{align*}
	Thus ``is unitarily equivalent to'' is an equivalence relation on \(\ms[n][n][\C]\).
\end{proof}
