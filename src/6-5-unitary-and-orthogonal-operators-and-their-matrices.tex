\section{Unitary and Orthogonal Operators and Their Matrices}\label{sec:6.5}

\begin{note}
  In \cref{sec:6.5}, we study those linear operators \(\T\) on an inner product space \(\V\) over \(\F\) such that \(\T \T^* = \T^* \T = \IT[\V]\).
  We will see that these are precisely the linear operators that ``preserve length'' in the sense that \(\norm{\T(x)} = \norm{x}\) for all \(x \in \V\).
  As another characterization, we prove that, on a finite-dimensional complex inner product space, these are the normal operators whose eigenvalues all have absolute value \(1\).

  In past chapters, we were interested in studying those functions that preserve the structure of the underlying space.
  In particular, linear operators preserve the operations of vector addition and scalar multiplication, and isomorphisms preserve all the vector space structure.
  It is now natural to consider those linear operators \(\T\) on an inner product space that preserve length.
  We will see that this condition guarantees, in fact, that \(\T\) preserves the inner product.
\end{note}

\begin{defn}\label{6.5.1}
  Let \(\T\) be a linear operator on a finite-dimensional inner product space \(\V\) over \(\F\).
  If \(\norm{\T(x)} = \norm{x}\) for all \(x \in \V\), we call \(\T\) a \textbf{unitary operator} if \(\F = \C\) and an \textbf{orthogonal operator} if \(\F = \R\).

  in the infinite-dimensional case, an operator satisfying the preceding norm requirement is generally called an \textbf{isometry}.
  If, in addition, the operator is onto (the condition guarantees one-to-one, see \cref{ex:6.1.17}), then the operator is called a \textbf{unitary} or \textbf{orthogonal operator}.
\end{defn}

\begin{note}
  Clearly, any rotation or reflection in \(\R^2\) preserves length and hence is an orthogonal operator.
  We study these operators in much more detail in \cref{sec:6.11}.
\end{note}

\begin{eg}\label{6.5.2}
  Let \(h \in \vs{H}\) (see \cref{6.1.8}) satisfy \(\abs{h(x)} = 1\) for all \(x \in [0, 2 \pi]\).
  Define \(\T \in \ls(\vs{H})\) by \(\T(f) = hf\).
  Then
  \begin{align*}
    \norm{\T(f)}^2 & = \norm{hf}^2                                                                     \\
                   & = \inn{hf, hf}                                                    &  & \by{6.1.9} \\
                   & = \frac{1}{2 \pi} \int_0^{2 \pi} h(t) f(t) \conj{h(t) f(t)} \; dt &  & \by{6.1.8} \\
                   & = \inn{f, f}                                                      &  & \by{6.1.8} \\
                   & = \norm{f}^2                                                      &  & \by{6.1.9}
  \end{align*}
  since \(\abs{h(t)}^2 = 1\) for all \(t \in [0, 2 \pi]\).
  So \(\T\) is a unitary operator.
\end{eg}

\begin{lem}\label{6.5.3}
  Let \(\U\) be a self-adjoint operator on a finite-dimensional inner product space \(\V\) over \(\F\).
  If \(\inn{x, \U(x)} = 0\) for all \(x \in \V\), then \(\U = \zT\).
\end{lem}

\begin{proof}[\pf{6.5.3}]
  By either \cref{6.16} or \cref{6.17}, we may choose an orthonormal basis \(\beta\) for \(\V\) over \(\F\) consisting of eigenvectors of \(\U\).
  If \(x \in \beta\), then \(\U(x) = \lambda x\) for some \(\lambda\).
  Thus
  \begin{align*}
    0 & = \inn{x, \U(x)}                              \\
      & = \inn{x, \lambda x}         &  & \by{5.1.2}  \\
      & = \conj{\lambda} \inn{x, x}; &  & \by{6.1}[b]
  \end{align*}
  so \(\lambda = 0\).
  Hence \(\U(x) = \zv\) for all \(x \in \beta\), and thus \(\U = \zT\).
\end{proof}

\begin{note}
  Compare \cref{6.5.3} to \cref{ex:6.4.11}(b).
\end{note}

\begin{thm}\label{6.18}
  Let \(\T\) be a linear operator on a finite-dimensional inner product space \(\V\) over \(\F\).
  Then the following statements are equivalent.
  \begin{enumerate}
    \item \(\T \T^* = \T^* \T = \IT[\V]\).
    \item \(\inn{\T(x), \T(y)} = \inn{x, y}\) for all \(x, y \in \V\).
    \item If \(\beta\) is an orthonormal basis for \(\V\) over \(\F\), then \(\T(\beta)\) is an orthonormal basis for \(\V\) over \(\F\).
    \item There exists an orthonormal basis \(\beta\) for \(\V\) over \(\F\) such that \(\T(\beta)\) is an orthonormal basis for \(\V\) over \(\F\).
    \item \(\norm{\T(x)} = \norm{x}\) for all \(x \in \V\).
  \end{enumerate}
  Thus all the conditions above are equivalent to the definition of a unitary or orthogonal operator.
  From (a), it follows that unitary or orthogonal operators are normal.
\end{thm}

\begin{proof}[\pf{6.18}]
  We prove first that (a) implies (b).
  Let \(x, y \in \V\).
  Then \(\inn{x, y} = \inn{\T^* \T(x), y} = \inn{\T(x), \T(y)}\).

  Second, we prove that (b) implies (c).
  Let \(\beta = \set{\seq{v}{1,,n}}\) be an orthonormal basis for \(\V\) over \(\F\);
  so \(\T(\beta) = \set{\T(v_1), \dots, \T(v_n)}\).
  It follows that \(\inn{\T(v_i), \T(v_j)} = \inn{v_i, v_j} = \delta_{i j}\).
  Therefore \(\T(\beta)\) is an orthonormal basis for \(\V\) over \(\F\).

  That (c) implies (d) is obvious.

  Next we prove that (d) implies (e).
  Let \(x \in \V\), and let \(\beta = \set{\seq{v}{1,,n}}\).
  Now
  \[
    x = \sum_{i = 1}^n a_i v_i
  \]
  for some scalars \(a_i \in \F\), and so
  \begin{align*}
    \norm{x}^2 & = \inn{\sum_{i = 1}^n a_i v_i, \sum_{j = 1}^n a_j v_j}        &  & \by{6.1.9}      \\
               & = \sum_{i = 1}^n  a_i \inn{v_i, \sum_{j = 1}^n a_j v_j}       &  & \by{6.1.1}[a,b] \\
               & = \sum_{i = 1}^n \sum_{j = 1}^n a_i \conj{a_j} \inn{v_i, v_j} &  & \by{6.1}[a,b]   \\
               & = \sum_{i = 1}^n \sum_{j = 1}^n a_i \conj{a_j} \delta_{i j}   &  & \by{6.1.12}     \\
               & = \sum_{i = 1}^n a_i \conj{a_i}                               &  & \by{2.3.4}      \\
               & = \sum_{i = 1}^n \abs{a_i}^2                                  &  & \by{d.0.5}
  \end{align*}
  since \(\beta\) is orthonormal.

  Applying the same manipulations to
  \[
    \T(x) = \sum_{i = 1}^n a_i \T(v_i)
  \]
  and using the fact that \(\T(\beta)\) is also orthonormal, we obtain
  \[
    \norm{\T(x)}^2 = \sum_{i = 1}^n \abs{a_i}^2.
  \]
  Hence \(\norm{\T(x)} = \norm{x}\).

  Finally, we prove that (e) implies (a).
  For any \(x \in \V\), we have
  \begin{align*}
    \inn{x, x} & = \norm{x}^2           &  & \by{6.1.9} \\
               & = \norm{\T(x)}^2                       \\
               & = \inn{\T(x), \T(x)}   &  & \by{6.1.9} \\
               & = \inn{x, \T^* \T(x)}. &  & \by{6.9}
  \end{align*}
  So \(\inn{x, (\IT[\V] - \T^* \T)(x)} = 0\) for all \(x \in \V\).
  Let \(\U = \IT[\V] - \T^* \T\);
  then \(\U\) is self-adjoint (by \cref{6.11}) and \(\inn{x, \U(x)} = 0\) for all \(x \in \V\).
  Hence, by \cref{6.5.3}, we have \(\zT = \U = \IT[\V] - \T^* \T\), and therefore \(\T^* \T = \IT[\V]\).
  Since \(\V\) is finite-dimensional, we may use \cref{ex:2.4.10} to conclude that \(\T \T^* = \IT[\V]\).
\end{proof}

\begin{cor}\label{6.5.4}
  Let \(\T\) be a linear operator on a finite-dimensional real inner product space \(\V\).
  Then \(\V\) has an orthonormal basis of eigenvectors of \(\T\) with corresponding eigenvalues of absolute value \(1\) iff \(\T\) is both self-adjoint and orthogonal.
\end{cor}

\begin{proof}[\pf{6.5.4}]
  Suppose that \(\V\) has an orthonormal basis \(\set{\seq{v}{1,,n}}\) over \(\R\) such that \(\T(v_i) = \lambda_i v_i\) and \(\abs{\lambda_i} = 1\) for all \(i \in \set{1, \dots, n}\).
  By \cref{6.17}, \(\T\) is self-adjoint.
  Thus
  \begin{align*}
    (\T \T^*)(v_i) & = (\T \T)(v_i)            &  & \by{6.4.8}                                          \\
                   & = \T(\lambda_i v_i)       &  & \by{5.1.2}                                          \\
                   & = \lambda_i \lambda_i v_i &  & \by{5.1.2}                                          \\
                   & = \lambda_i^2 v_i                                                                  \\
                   & = v_i                     &  & (\abs{\lambda_i} = 1 \text{ and } \lambda_i \in \R)
  \end{align*}
  for each \(i \in \set{1, \dots, n}\).
  So \(\T \T^* = \IT[\V]\), and again by \cref{ex:2.4.10}, \(\T^* \T = \IT[\V]\).
  Thus \(\T\) is orthogonal by \cref{6.18}(a).

  If \(\T\) is self-adjoint, then, by \cref{6.17}, we have that \(\V\) possesses an orthonormal basis \(\set{\seq{v}{1,,n}}\) over \(\R\) such that \(\T(v_i) = \lambda_i v_i\) for all \(i \in \set{1, \dots, n}\).
  If \(\T\) is also orthogonal, we have
  \begin{align*}
    \abs{\lambda_i} \cdot \norm{v_i} & = \norm{\lambda_i v_i} &  & \by{6.2}[a] \\
                                     & = \norm{\T(v_i)}       &  & \by{5.1.2}  \\
                                     & = \norm{v_i};          &  & \by{6.5.1}
  \end{align*}
  so \(\abs{\lambda_i} = 1\) for every \(i \in \set{1, \dots, n}\).
\end{proof}

\begin{cor}\label{6.5.5}
  Let \(\T\) be a linear operator on a finite-dimensional complex inner product space \(\V\).
  Then \(\V\) has an orthonormal basis of eigenvectors of \(\T\) with corresponding eigenvalues of absolute value \(1\) iff \(\T\) is unitary.
\end{cor}

\begin{proof}[\pf{6.5.5}]
  Suppose that \(\V\) has an orthonormal basis \(\set{\seq{v}{1,,n}}\) over \(\C\) such that \(\T(v_i) = \lambda_i v_i\) and \(\abs{\lambda_i} = 1\) for all \(i \in \set{1, \dots, n}\).
  By \cref{6.16}, \(\T\) is normal.
  Thus
  \begin{align*}
    (\T^* \T)(v_i) & = (\T \T^*)(v_i)                 &  & \by{6.4.3}            \\
                   & = \T(\conj{\lambda_i} v_i)       &  & \by{6.15}[c]          \\
                   & = \conj{\lambda_i} \lambda_i v_i &  & \by{5.1.2}            \\
                   & = \abs{\lambda_i}^2 v_i          &  & \by{d.0.5}            \\
                   & = v_i                            &  & (\abs{\lambda_i} = 1)
  \end{align*}
  for each \(i \in \set{1, \dots, n}\).
  So \(\T^* \T = \T \T^* = \IT[\V]\) by \cref{2.1.13}.
  Thus by \cref{6.18}(a) \(\T\) is unitary.

  If \(\T\) is unitary, then by \cref{6.18}(a)(e) we know that \(\T\) is normal.
  By \cref{6.16}, we have that \(\V\) possesses an orthonormal basis \(\set{\seq{v}{1,,n}}\) over \(\C\) such that \(\T(v_i) = \lambda_i v_i\) for all \(i \in \set{1, \dots, n}\).
  Then we have
  \begin{align*}
    \abs{\lambda_i} \cdot \norm{v_i} & = \norm{\lambda_i v_i} &  & \by{6.2}[a] \\
                                     & = \norm{\T(v_i)}       &  & \by{5.1.2}  \\
                                     & = \norm{v_i};          &  & \by{6.5.1}
  \end{align*}
  so \(\abs{\lambda_i} = 1\) for every \(i \in \set{1, \dots, n}\).
\end{proof}

\begin{eg}\label{6.5.6}
  Let \(\T : \R^2 \to \R^2\) be a rotation by \(\theta\), where \(0 < \theta < \pi\).
  It is geometrically clear that \(\T\) ``preserves length'', that is, that \(\norm{\T(x)} = \norm{x}\) for all \(x \in \R^2\).
  The fact that rotations by a fixed angle preserve perpendicularity not only can be seen geometrically but now follows from \cref{6.18}(b).
  Perhaps the fact that such a transformation preserves the inner product is not so obvious;
  however, we obtain this fact from \cref{6.18}(b) also.
  Finally, an inspection of the matrix representation of \(\T\) with respect to the standard ordered basis, which is
  \[
    \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) \\
      \sin(\theta) & \cos(\theta)
    \end{pmatrix},
  \]
  reveals that \(\T\) is not self-adjoint for the given restriction on \(\theta\).
  As we mentioned earlier, this fact also follows from the geometric observation that \(\T\) has no eigenvectors (see \cref{5.1.3}) and from \cref{6.17}.
  It is seen easily from the preceding matrix that \(\T^*\) is the rotation by \(-\theta\) since
  \[
    \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) \\
      \sin(\theta) & \cos(\theta)
    \end{pmatrix}^* = \begin{pmatrix}
      \cos(\theta)  & \sin(\theta) \\
      -\sin(\theta) & \cos(\theta)
    \end{pmatrix} = \begin{pmatrix}
      \cos(-\theta) & -\sin(-\theta) \\
      \sin(-\theta) & \cos(-\theta)
    \end{pmatrix}.
  \]
\end{eg}

\begin{defn}\label{6.5.7}
  Let \(\vs{L}\) be a one-dimensional subspace of \(\R^2\) over \(\R\).
  We may view \(\vs{L}\) as a line in the plane through the origin.
  A linear operator \(\T\) on \(\R^2\) is called a \textbf{reflection of \(\R^2\) about \(\vs{L}\)} if \(\T(x) = x\) for all \(x \in \vs{L}\) and \(\T(x) = -x\) for all \(x \in \vs{L}^{\perp}\).
\end{defn}

\begin{eg}\label{6.5.8}
  Let \(\T\) be a reflection of \(\R^2\) about a line \(\vs{L}\) through the origin.
  We show that \(\T\) is an orthogonal operator.
  Select vectors \(v_1 \in \vs{L}\) and \(v_2 \in \vs{L}^{\perp}\) such that \(\norm{v_1} = \norm{v_2} = 1\).
  Then \(\T(v_1) = v_1\) and \(\T(v_2) = -v_2\).
  Thus \(v_1\) and \(v_2\) are eigenvectors of \(\T\) with corresponding eigenvalues \(1\) and \(-1\), respectively.
  Furthermore, \(\set{v_1, v_2}\) is an orthonormal basis for \(\R^2\).
  It follows that \(\T\) is an orthogonal operator by \cref{6.5.4}.
\end{eg}

\begin{defn}\label{6.5.9}
  A square matrix \(A \in \ms[n][n][\F]\) is called an an \textbf{orthogonal matrix} if \(\tp{A} A = A \tp{A} = I_n\) and \textbf{unitary} if \(A^* A = A A^* = I_n\).

  Since for a real matrix \(A\) we have \(A^* = \tp{A}\), a real unitary matrix is also orthogonal.
  In this case, we call \(A\) \textbf{orthogonal} rather than unitary.

  The condition \(A A^* = I_n\) is equivalent to the statement that the rows of \(A\) form an orthonormal basis for \(\vs{F}^n\) because
  \begin{align*}
    \delta_{i j} & = (A A^*)_{i j}                          &  & \by{2.3.4} \\
                 & = \sum_{k = 1}^n A_{i k} (A^*)_{k j}     &  & \by{2.3.1} \\
                 & = \sum_{k = 1}^n A_{i k} \conj{A_{j k}}, &  & \by{6.1.5}
  \end{align*}
  and the last term represents the inner product of the \(i\)th and \(j\)th rows of \(A\).

  The condition \(A^* A = I_n\) is equivalent to the statement that the columns of \(A\) form an orthonormal basis for \(\vs{F}^n\) because
  \begin{align*}
    \delta_{i j} & = (A^* A)_{i j}                          &  & \by{2.3.4} \\
                 & = \sum_{k = 1}^n (A^*)_{i k} A_{k j}     &  & \by{2.3.1} \\
                 & = \sum_{k = 1}^n \conj{A_{k i}} A_{k j}, &  & \by{6.1.5}
  \end{align*}
  and the last term represents the inner product of the \(j\)th and \(i\)th columns of \(A\).
\end{defn}

\begin{cor}\label{6.5.10}
  A linear operator \(\T\) on a finite-dimensional inner product space \(\V\) over \(\F\) is unitary (orthogonal) iff \([\T]_{\beta}\) is unitary (orthogonal) for some orthonormal basis \(\beta\) for \(\V\) over \(\F\).
\end{cor}

\begin{proof}[\pf{6.5.10}]
  We have
  \begin{align*}
         & \T \text{ is unitary}                                                               \\
    \iff & \forall x \in \V, \norm{\T(x)} = \norm{x}                       &  & \by{6.5.1}     \\
    \iff & \T \T^* = \T^* \T = \IT[\V]                                     &  & \by{6.18}[a,e] \\
    \iff & [\T \T^*]_{\beta} = [\T^* \T]_{\beta} = [\IT[\V]]_{\beta} = I_n &  & \by{2.12}[d]   \\
    \iff & [\T]_{\beta} [\T^*]_{\beta} = [\T^*]_{\beta} [\T]_{\beta} = I_n &  & \by{2.3.3}     \\
    \iff & [\T]_{\beta} [\T]_{\beta}^* = [\T]_{\beta}^* [\T]_{\beta} = I_n &  & \by{6.10}      \\
    \iff & [\T]_{\beta} \text{ is unitary}.                                &  & \by{6.5.9}
  \end{align*}
  Similar argument shows that \(\T\) is orthogonal iff \([\T]_{\beta}\) is orthogonal.
\end{proof}

\begin{eg}\label{6.5.11}
  From \cref{6.5.6}, the matrix
  \[
    \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) \\
      \sin(\theta) & \cos(\theta)
    \end{pmatrix}
  \]
  is clearly orthogonal.
  One can easily see that the rows of the matrix form an orthonormal basis for \(\R^2\) since
  \begin{align*}
    \inn{(\cos(\theta), -\sin(\theta)), (\cos(\theta), -\sin(\theta))} & = (\cos(\theta))^2 + (\sin(\theta))^2                   &  & \by{6.1.2} \\
                                                                       & = 1;                                                                    \\
    \inn{(\cos(\theta), -\sin(\theta)), (\sin(\theta), \cos(\theta))}  & = \sin(\theta) \cos(\theta) - \sin(\theta) \cos(\theta) &  & \by{6.1.2} \\
                                                                       & = 0;                                                                    \\
    \inn{(\sin(\theta), \cos(\theta)), (\sin(\theta), \cos(\theta))}   & = (\sin(\theta))^2 + (\cos(\theta))^2                   &  & \by{6.1.2} \\
                                                                       & = 1.
  \end{align*}
  Similarly, the columns of the matrix form an orthonormal basis for \(\R^2\).
\end{eg}

\begin{eg}\label{6.5.12}
  Let \(\T\) be a reflection of \(\R^2\) about a line \(\L\) through the origin, let \(\beta\) be the standard ordered basis for \(\R^2\) over \(\R\), and let \(A = [\T]_{\beta}\).
  Then \(\T = \L_A\).
  Since \(\T\) is an orthogonal operator and \(\beta\) is an orthonormal basis, \(A\) is an orthogonal matrix by \cref{6.5.10}.
  We describe \(A\).

  Suppose that \(\alpha\) is the angle from the positive \(x\)-axis to \(\L\).
  Let \(v_1 = (\cos(\alpha), \sin(\alpha))\) and \(v_2 = (-\sin(\alpha), \cos(\alpha))\).
  Then \(\norm{v_1} = \norm{v_2} = 1\), \(v_1 \in \L\), and \(v_2 \in \L^{\perp}\).
  Hence \(\gamma = \set{v_1, v_2}\) is an orthonormal basis for \(\R^2\) over \(\R\).
  Because \(\T(v_1) = v_1\) and \(\T(v_2) = -v_2\), we have
  \[
    [\T]_{\gamma} = [\L_A]_{\gamma} = \begin{pmatrix}
      1 & 0  \\
      0 & -1
    \end{pmatrix}.
  \]
  Let
  \[
    Q = \begin{pmatrix}
      \cos(\alpha) & -\sin(\alpha) \\
      \sin(\alpha) & \cos(\alpha)
    \end{pmatrix}.
  \]
  Then
  \begin{align*}
    A & = Q [\L_A]_{\gamma} Q^{-1}                                                      &  & \by{2.5.3} \\
      & = \begin{pmatrix}
            \cos(\alpha) & -\sin(\alpha) \\
            \sin(\alpha) & \cos(\alpha)
          \end{pmatrix} \begin{pmatrix}
                          1 & 0  \\
                          0 & -1
                        \end{pmatrix} \begin{pmatrix}
                                        \cos(\alpha)  & \sin(\alpha) \\
                                        -\sin(\alpha) & \cos(\alpha)
                                      \end{pmatrix}                                      \\
      & = \begin{pmatrix}
            (\cos(\alpha))^2 - (\sin(\alpha))^2 & 2 \sin(\alpha) \cos(\alpha)            \\
            2 \sin(\alpha) \cos(\alpha)         & -((\cos(\alpha))^2 - (\sin(\alpha))^2)
          \end{pmatrix}                  \\
      & = \begin{pmatrix}
            \cos(2 \alpha) & \sin(2 \alpha)  \\
            \sin(2 \alpha) & -\cos(2 \alpha)
          \end{pmatrix}.
  \end{align*}
\end{eg}

\begin{defn}\label{6.5.13}
  We know by \cref{6.16,6.17} that, for a complex normal (real symmetric) matrix \(A\), there exists an orthonormal basis \(\beta\) for \(\vs{F}^n\) consisting of eigenvectors of \(A\).
  Hence \(A\) is similar to a diagonal matrix \(D\).
  By \cref{2.5.3}, the matrix \(Q\) whose columns are the vectors in \(\beta\) is such that \(D = Q^{-1} A Q\).
  But since the columns of \(Q\) are an orthonormal basis for \(\vs{F}^n\), it follows that \(Q\) is unitary (orthogonal) (see \cref{ex:6.1.23}(c)).
  In this case, we say that \(A\) is \textbf{unitarily equivalent} (\textbf{orthogonally equivalent}) to \(D\).

  It is easily seen (see \cref{ex:6.5.18}) that this relation is an equivalence relation on \(\ms[n][n][\C]\) (\(\ms[n][n][\R]\)).
  More generally, \(A\) and \(B\) are unitarily equivalent (orthogonally equivalent) iff there exists a unitary (orthogonal) matrix \(P\) such that \(A = P^* B P\).
\end{defn}

\begin{thm}\label{6.19}
  Let \(A \in \ms[n][n][\C]\).
  Then \(A\) is normal iff \(A\) is unitarily equivalent to a diagonal matrix.
\end{thm}

\begin{proof}[\pf{6.19}]
  By \cref{6.5.13}, we need only prove that if \(A\) is unitarily equivalent to a diagonal matrix, then \(A\) is normal.

  Suppose that \(A = P^* D P\), where \(P\) is a unitary matrix and \(D\) is a diagonal matrix.
  Then
  \begin{align*}
    A A^* & = (P^* D P) (P^* D P)^*                        \\
          & = (P^* D P) (P^* D^* P) &  & \by{6.3.2}[c]     \\
          & = P^* D I_n D^* P       &  & \by{ex:6.1.23}[c] \\
          & = P^* D D^* P.
  \end{align*}
  Similarly, \(A^* A = P^* D^* D P\).
  Since \(D\) is a diagonal matrix, however, we have \(D D^* = D^* D\).
  Thus \(A A^* = A^* A\).
\end{proof}

\begin{thm}\label{6.20}
  Let \(A \in \ms[n][n][\R]\).
  Then \(A\) is symmetric iff \(A\) is orthogonally equivalent to a real diagonal matrix.
\end{thm}

\begin{proof}[\pf{6.20}]
  By \cref{6.5.13}, we need only prove that if \(A\) is orthogonally equivalent to a diagonal matrix, then \(A\) is symmetric.

  Suppose that \(A = \tp{P} D P\), where \(P\) is a orthogonal matrix and \(D\) is a diagonal matrix.
  Then
  \begin{align*}
    \tp{A} & = \tp{(\tp{P} D P)}                                                    \\
           & = \tp{P} \tp{D} \tp{(\tp{P})} &  & \by{2.3.2}                          \\
           & = \tp{P} \tp{D} P             &  & \by{ex:1.3.4}                       \\
           & = \tp{P} D P                  &  & \text{(\(D\) is a diagonal matrix)} \\
           & = A.
  \end{align*}
\end{proof}

\begin{thm}[Schur's Theorem]\label{6.21}
  Let \(A \in \ms[n][n][\F]\) be a matrix whose characteristic polynomial splits over \(\F\).
  \begin{itemize}
    \item If \(\F = \C\), then \(A\) is unitarily equivalent to a complex upper triangular matrix.
    \item If \(\F = \R\), then \(A\) is orthogonally equivalent to a real upper triangular matrix.
  \end{itemize}
\end{thm}

\begin{proof}[\pf{6.21}]
  Let \(\beta\) be the standard ordered basis for \(\vs{F}^n\) over \(\F\).
  By \cref{6.14}, there exists an orthonormal basis \(\gamma\) for \(\vs{F}^n\) over \(\F\) such that \([\L_A]_{\gamma}\) is upper triangular.
  Let \(Q = [\IT[\vs{F}^n]]_{\gamma}^{\beta}\).
  Then we have
  \begin{align*}
    [\L_A]_{\gamma} & = Q^{-1} [\L_A]_{\beta} Q &  & \by{2.23}         \\
                    & = Q^{-1} A Q              &  & \by{2.15}[a]      \\
                    & = Q^* A Q.                &  & \by{ex:6.1.23}[c]
  \end{align*}
  Thus by \cref{6.5.13} \(A\) is unitarily (orthogonally) equivalent to an complex (real) upper triangular matrix.
\end{proof}

\begin{defn}\label{6.5.14}
  Let \(\V\) be a real inner product space.
  A function \(f : \V \to \V\) is called a \textbf{rigid motion} if
  \[
    \norm{f(x) - f(y)} = \norm{x - y}
  \]
  for all \(x, y \in \V\).
\end{defn}

\begin{note}
  One may think intuitively of a rigid motion as a transformation that does not affect the shape of a figure under its action, hence the term \emph{rigid}.
  The key requirement for such a transformation is that it preserves distances.
\end{note}

\begin{eg}\label{6.5.15}
  Any orthogonal operator on a finite-dimensional real inner product space is a rigid motion.
\end{eg}

\begin{proof}[\pf{6.5.15}]
  Let \(\V\) be a finite-dimensional real inner product space and let \(\T \in \ls(\V)\) be orthogonal.
  Then we have
  \begin{align*}
    \forall x, y \in \V, \norm{x - y} & = \norm{\T(x - y)}      &  & \by{6.5.1}    \\
                                      & = \norm{\T(x) - \T(y)}. &  & \by{2.1.2}[c]
  \end{align*}
  Thus \(\T\) is a rigid motion.
\end{proof}

\begin{defn}\label{6.5.16}
  Another class of rigid motions are the \emph{translations}.
  A function \(g : \V \to \V\), where \(\V\) is a real inner product space, is called a \textbf{translation} if there exists a vector \(v_0 \in \V\) such that \(g(x) = x + v_0\) for all \(x \in \V\).
  We say that \(g\) is the \emph{translation by} \(v_0\).
\end{defn}

\begin{note}
  It is a simple exercise to show that translations, as well as composites of rigid motions on a real inner product space, are also rigid motions.
  (See \cref{ex:6.5.22}.)
  Thus an orthogonal operator on a finite-dimensional real inner product space \(\V\) followed by a translation on \(\V\) is a rigid motion on \(\V\).
  Remarkably, every rigid motion on \(\V\) may be characterized in this way (see \cref{6.22}).
\end{note}

\begin{thm}\label{6.22}
  Let \(f : \V \to \V\) be a rigid motion on a finite-dimensional real inner product space \(\V\).
  Then there exists a unique orthogonal operator \(\T\) on \(\V\) and a unique translation \(g\) on \(\V\) such that \(f = g \circ \T\).
\end{thm}

\begin{proof}[\pf{6.22}]
  Let \(\T : \V \to \V\) be defined by
  \[
    \T(x) = f(x) - f(\zv)
  \]
  for all \(x \in \V\).
  We show that \(\T\) is an orthogonal operator, from which it follows that \(f = g \circ \T\), where \(g\) is the translation by \(f(\zv)\).
  Observe that \(\T\) is the composite of \(f\) and the translation by \(-f(\zv)\);
  hence \(\T\) is a rigid motion (by \cref{ex:6.5.22}(b)).
  Furthermore, for any \(x \in \V\),
  \begin{align*}
    \norm{\T(x)}^2 & = \norm{f(x) - f(\zv)}^2                  \\
                   & = \norm{x - \zv}^2       &  & \by{6.5.14} \\
                   & = \norm{x}^2,
  \end{align*}
  and consequently \(\norm{\T(x)} = \norm{x}\) for any \(x \in \V\).
  Thus for any \(x, y \in \V\),
  \begin{align*}
     & \norm{\T(x) - \T(y)}^2                                                                                   \\
     & = \inn{\T(x) - \T(y), \T(x) - \T(y)}                                                &  & \by{6.1.9}      \\
     & = \inn{\T(x), \T(x) - \T(y)} - \inn{\T(y), \T(x) - \T(y)}                           &  & \by{6.1.1}[a,b] \\
     & = \inn{\T(x), \T(x)} - \inn{\T(x), \T(y)} - \inn{\T(y), \T(x)} + \inn{\T(y), \T(y)} &  & \by{6.1}[a,b]   \\
     & = \norm{\T(x)}^2 - \inn{\T(x), \T(y)} - \inn{\T(y), \T(x)} + \norm{\T(y)}^2         &  & \by{6.1.9}      \\
     & = \norm{\T(x)}^2 - 2 \inn{\T(x), \T(y)} + \norm{\T(y)}^2                            &  & (\F = \R)       \\
     & = \norm{x}^2 - 2 \inn{\T(x), \T(y)} + \norm{y}^2                                    &  & \by{6.5.1}
  \end{align*}
  and
  \[
    \norm{x - y}^2 = \norm{x}^2 - 2 \inn{x, y} + \norm{y}^2.
  \]
  But \(\norm{\T(x) - \T(y)}^2 = \norm{x - y}^2\);
  so \(\inn{\T(x), \T(y)} = \inn{x, y}\) for all \(x, y \in \V\).

  We are now in a position to show that \(\T\) is a linear transformation.
  Let \(x, y \in \V\), and let \(a \in \R\).
  Then
  \begin{align*}
     & \norm{\T(x + ay) - \T(x) - a \T(y)}^2                                                        \\
     & = \norm{(\T(x + ay) - \T(x)) - a \T(y)}^2                                                    \\
     & = \norm{\T(x + ay) - \T(x)}^2 + a^2 \norm{\T(y)}^2 - 2a \inn{\T(x + ay) - \T(x), \T(y)}      \\
     & = \norm{(x + ay) - x}^2 + a^2 \norm{y}^2 - 2a (\inn{\T(x + ay), \T(y)} - \inn{\T(x), \T(y)}) \\
     & = a^2 \norm{y}^2 + a^2 \norm{y}^2 - 2a (\inn{x + ay, y} - \inn{x, y})                        \\
     & = 2 a^2 \norm{y}^2 - 2a (\inn{x, y} + a \norm{y}^2 - \inn{x, y})                             \\
     & = 0.
  \end{align*}
  Thus by \cref{6.2}(b) \(\T(x + ay) = \T(x) + a \T(y)\), and hence \(\T\) is linear.
  Since \(\T\) also preserves inner products, \(\T\) is an orthogonal operator.

  To prove uniqueness, suppose that \(u_0\) and \(v_0\) are in \(\V\) and \(\T\) and \(\U\) are orthogonal operators on \(\V\) such that
  \[
    f(x) = \T(x) + u_0 = \U(x) + v_0
  \]
  for all \(x \in \V\).
  Substituting \(x = \zv\) in the preceding equation yields \(u_0 = v_0\), and hence the translation is unique.
  This equation, therefore, reduces to \(\T(x) = \U(x)\) for all \(x \in \V\), and hence \(\T = \U\) (by \cref{2.1.13}).
\end{proof}

\begin{note}
  By \cref{6.22}, any orthogonal operator is a special case of this composite, in which the translation is by \(\zv\).
  Any translation is also a special case, in which the orthogonal operator is the identity operator.
\end{note}

\begin{thm}\label{6.23}
  Let \(\T\) be an orthogonal operator on \(\R^2\), and let \(A = [\T]_{\beta}\), where \(\beta\) is the standard ordered basis for \(\R^2\) over \(\R\).
  Then exactly one of the following conditions is satisfied:
  \begin{enumerate}
    \item \(\T\) is a rotation, and \(\det(A) = 1\).
    \item \(\T\) is a reflection about a line through the origin, and \(\det(A) = -1\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{6.23}]
  Because \(\T\) is an orthogonal operator, \(\T(\beta) = \set{\T(e_1), \T(e_2)}\) is an orthonormal basis for \(\R^2\) over \(\R\) by \cref{6.18}(c).
  Since \(\T(e_1)\) is a unit vector, there is a unique angle \(\theta \in [0, 2 \pi)\) such that \(\T(e_1) = (\cos(\theta), \sin(\theta))\).
  Since \(\T(e_2)\) is a unit vector and is orthogonal to \(\T(e_1)\), there are only two possible choices for \(\T(e_2)\).
  Either
  \[
    \T(e_2) = (-\sin(\theta), \cos(\theta)) \quad \text{or} \quad \T(e_2) = (\sin(\theta), -\cos(\theta)).
  \]
  First, suppose that \(\T(e_2) = (-\sin(\theta), \cos(\theta))\).
  Then \(A = \begin{pmatrix}
    \cos(\theta) & -\sin(\theta) \\
    \sin(\theta) & \cos(\theta)
  \end{pmatrix}\).
  It follows from \cref{6.4.5} that \(\T\) is a rotation by the angle \(\theta\).
  Also
  \[
    \det(A) = (\cos(\theta))^2 + (\sin(\theta))^2 = 1.
  \]
  Now suppose that \(\T(e_2) = (\sin(\theta), -\cos(\theta))\).
  Then \(A = \begin{pmatrix}
    \cos(\theta) & \sin(\theta)  \\
    \sin(\theta) & -\cos(\theta)
  \end{pmatrix}\).
  Comparing this matrix to the matrix \(A\) of \cref{6.5.12}, we see that \(\T\) is the reflection of \(\R^2\) about a line \(\L\), so that \(\alpha = \theta / 2\) is the angle from the positive \(x\)-axis to \(\L\).
  Furthermore,
  \[
    \det(A) = - (\cos(\theta))^2 - (\sin(\theta))^2 = -1.
  \]
\end{proof}

\begin{cor}\label{6.5.17}
  Any rigid motion on \(\R^2\) is either a rotation followed by a translation or a reflection about a line through the origin followed by a translation.
\end{cor}

\begin{proof}[\pf{6.5.17}]
  Combining \cref{6.22,6.23} we are done.
\end{proof}

\begin{defn}\label{6.5.18}
  Consider the quadratic equation
  \begin{equation}\label{eq:6.5.1}
    ax^2 + 2bxy + cy^2 + dx + ey + f = 0.
  \end{equation}
  For special choices of the coefficients in the equation above, we obtain the various conic sections, namely, the circle, ellipse, parabola, and hyperbola.
  If \(b = 0\), then it is easy to graph the equation by the method of completing the square because the \(xy\)-term is absent.
  We now concentrate solely on the elimination of the \(xy\)-term.
  To accomplish this, we consider the expression
  \begin{equation}\label{eq:6.5.2}
    ax^2 + 2bxy + cy^2,
  \end{equation}
  which is called the \textbf{associated quadratic form} of \cref{eq:6.5.1}.
  Quadratic forms are studied in more generality in \cref{sec:6.8}.
\end{defn}

\begin{cor}[principal axis theorem]\label{6.5.19}
  the \(xy\)-term in \cref{eq:6.5.2} may be eliminated by a rotation of the \(x\)-axis and \(y\)-axis to new axes \(x'\) and \(y'\) given by \(X = P X'\), where \(P\) is an orthogonal matrix and \(\det(P) = 1\).
  Furthermore, the coefficients of \((x')^2\) and \((y')^2\) are the eigenvalues of
  \[
    A = \begin{pmatrix}
      a & b \\
      b & c
    \end{pmatrix}.
  \]
\end{cor}

\begin{proof}[\pf{6.5.19}]
  Let
  \[
    A = \begin{pmatrix}
      a & b \\
      b & c
    \end{pmatrix} \quad \text{and} \quad X = \begin{pmatrix}
      x \\
      y
    \end{pmatrix}.
  \]
  Then \cref{eq:6.5.2} may be written as \(\tp{X} A X = \inn{AX, X}\).

  The fact that \(A\) is symmetric is crucial in our discussion.
  For, by \cref{6.20}, we may choose an orthogonal matrix \(P\) and a diagonal matrix \(D\) with real diagonal entries \(\lambda_1\) and \(\lambda_2\) such that \(\tp{P} A P = D\).
  Now define
  \[
    X' = \begin{pmatrix}
      x' \\
      y'
    \end{pmatrix}
  \]
  by \(X' = \tp{P} X\) or, equivalently, by \(P X' = P \tp{P} X = X\).
  Then
  \begin{align*}
    \tp{X} A X & = \tp{(P X')} A (P X')                                 \\
               & = \tp{(X')} (\tp{P} A P) X'            &  & \by{2.3.2} \\
               & = \tp{(X')} D X'                                       \\
               & = \lambda_1 (x')^2 + \lambda_2 (y')^2.
  \end{align*}
  Thus the transformation \((x, y) \mapsto (x', y')\) allows us to eliminate the \(xy\)-term in \cref{eq:6.5.2}, and hence in \cref{eq:6.5.1}.

  Furthermore, since \(P\) is orthogonal, we have by \cref{6.23} (with \(\T = \L_P\)) that \(\det(P) = \pm 1\).
  If \(\det(P) = -1\), we may interchange the columns of \(P\) to obtain a matrix \(Q\).
  Because the columns of \(P\) form an orthonormal basis of eigenvectors of \(A\), the same is true of the columns of \(Q\).
  Therefore,
  \[
    \tp{Q} A Q = \begin{pmatrix}
      \lambda_2 & 0         \\
      0         & \lambda_1
    \end{pmatrix}.
  \]
  Notice that \(\det(Q) = -\det(P) = 1\) by \cref{4.5}.
  So, if \(\det(P) = -1\), we can take \(Q\) for our new \(P\);
  consequently, we may always choose \(P\) so that \(\det(P) = 1\).
  By \cref{6.23}(a) (with \(\T = \L_P\)), it follows that matrix \(P\) represents a rotation.
\end{proof}

\begin{note}
  The arguments above, of course, are easily extended to quadratic equations in \(n\) variables.
  For example, in the case \(n = 3\), by special choices of the coefficients, we obtain the quadratic surfaces
  --- the elliptic cone, the ellipsoid, the hyperbolic paraboloid, etc.
\end{note}

\exercisesection

\setcounter{ex}{2}
\begin{ex}\label{ex:6.5.3}
  Prove that the composite of unitary (orthogonal) operators is unitary (orthogonal).
\end{ex}

\begin{proof}[\pf{ex:6.5.3}]
  Let \(\V\) be a complex (real) vector space.
  Let \(\T, \U \in \ls(\V)\) be unitary (orthogonal).
  Then by \cref{2.9} we have \(\T \U \in \ls(\V)\) and
  \begin{align*}
    \forall x \in \V, \norm{(\T \U)(x)} & = \norm{\T(\U(x))}                 \\
                                        & = \norm{\U(x)}     &  & \by{6.5.1} \\
                                        & = \norm{x}.        &  & \by{6.5.1}
  \end{align*}
  Thus by \cref{6.5.1} \(\T \U\) is unitary (orthogonal).
\end{proof}

\begin{ex}\label{ex:6.5.4}
  For \(z \in \C\), define \(\T_z : \C \to \C\) by \(\T_z(u) = zu\).
  Characterize those \(z\) for which \(\T_z\) is normal, self-adjoint, or unitary.
\end{ex}

\begin{proof}[\pf{ex:6.5.4}]
  Since
  \begin{align*}
    \forall x, y \in \C, \inn{x, \T_z^*(y)} & = \inn{\T_z(x), y}           &  & \by{6.9}      \\
                                            & = \inn{zx, y}                                   \\
                                            & = z \inn{x, y}               &  & \by{6.1.1}[b] \\
                                            & = \inn{x, \conj{z} y}        &  & \by{6.1}[b]   \\
                                            & = \inn{x, \T_{\conj{z}}(y)},
  \end{align*}
  by \cref{6.1}(e) we know that \(\T_z^* = \T_{\conj{z}}\) for all \(z \in \C\).

  First we claim that \(\T_z\) is normal for all \(z \in \C\).
  Since
  \begin{align*}
    \forall u, z \in \C, \T_z(\T_z^*(u)) & = \T_z(\T_{\conj{z}}(u)) &  & \text{(from the proof above)} \\
                                         & = z \conj{z} u                                              \\
                                         & = \conj{z} z u                                              \\
                                         & = \T_{\conj{z}}(\T_z(u))                                    \\
                                         & = \T_z^*(\T_z(u)),       &  & \text{(from the proof above)}
  \end{align*}
  by \cref{6.4.3} we know that \(\T_z\) is normal for all \(z \in \C\).

  Next we claim that \(\T_z\) is self adjoint iff \(z \in \R\).
  This is true since
  \begin{align*}
         & \T_z = \T_z^*                                                  &  & \by{6.4.8}                    \\
    \iff & \T_z = \T_{\conj{z}}                                           &  & \text{(from the proof above)} \\
    \iff & \forall u \in \C, zu = \T_z(u) = \T_{\conj{z}}(u) = \conj{z} u                                    \\
    \iff & z = \conj{z}                                                                                      \\
    \iff & z \in \R.
  \end{align*}

  Finally we claim that \(\T_z\) is unitary iff \(\abs{z} = 1\).
  This is true since
  \begin{align*}
         & \T_z \text{ is unitary}                                         \\
    \iff & \forall u \in \C, \abs{z} \norm{u} = \norm{zu} &  & \by{6.2}[a] \\
         & = \norm{\T_z(u)} = \norm{u}                    &  & \by{6.5.1}  \\
    \iff & \abs{z} = 1.
  \end{align*}
\end{proof}

\setcounter{ex}{5}
\begin{ex}\label{ex:6.5.6}
  Let \(\V\) be the inner product space of complex-valued continuous functions on \([0, 1]\) with the inner product
  \[
    \inn{f, g} = \int_0^1 f(t) \conj{g(t)} \; dt.
  \]
  Let \(h \in \V\), and define \(\T : \V \to \V\) by \(\T(f) = hf\).
  Prove that \(\T\) is a unitary operator iff \(\abs{h(t)} = 1\) for \(t \in [0, 1]\).
\end{ex}

\begin{proof}[\pf{ex:6.5.6}]
  First suppose that \(\T\) is unitary.
  Then we have
  \begin{align*}
    \forall f \in \V, 0 & = \norm{\T(f)}^2 - \norm{f}^2                                                 &  & \by{6.5.1} \\
                        & = \inn{\T(f), \T(f)} - \inn{f, f}                                             &  & \by{6.1.9} \\
                        & = \inn{hf, hf} - \inn{f, f}                                                                   \\
                        & = \int_0^1 h(t) f(t) \conj{h(t) f(t)} \; dt - \int_0^1 f(t) \conj{f(t)} \; dt                 \\
                        & = \int_0^1 (h(t) \conj{h(t)} - 1) f(t) \conj{f(t)} \; dt                                      \\
                        & = \int_0^1 (\abs{h(t)}^2 - 1) \abs{f(t)}^2 \; dt.                             &  & \by{d.0.5}
  \end{align*}
  Since \(h \in \V\), we can choose a \(f \in \V\) such that \(f(t) = (\abs{h(t)}^2 - 1)^{1 / 2}\), and we have
  \begin{align*}
             & \int_0^1 (\abs{h(t)}^2 - 1) \abs{\abs{h(t)}^2 - 1} \; dt = 0                                                 \\
    \implies & \int_0^1 (\abs{h(t)}^2 - 1)^2 \; dt = 0                                                                      \\
    \implies & \forall t \in [0, 1], \abs{h(t)}^2 - 1 = 0                   &  & \text{(\(h\) is continuous on \([0, 1]\))} \\
    \implies & \forall t \in [0, 1], \abs{h(t)} = 1.
  \end{align*}

  Now suppose that \(\abs{h(t)} = 1\) for all \(t \in [0, 1]\).
  Then we have
  \begin{align*}
    \forall f \in \V, \norm{\T(f)}^2 & = \inn{\T(f), \T(f)}                        &  & \by{6.1.9}       \\
                                     & = \inn{hf, hf}                                                    \\
                                     & = \int_0^1 h(t) f(t) \conj{h(t) f(t)} \; dt                       \\
                                     & = \int_0^1 f(t) \conj{f(t)} \; dt           &  & (\abs{h(t)} = 1) \\
                                     & = \inn{f, f}                                                      \\
                                     & = \norm{f}^2.                               &  & \by{6.1.9}
  \end{align*}
  Thus by \cref{6.5.1} \(\T\) is unitary.
\end{proof}

\begin{ex}\label{ex:6.5.7}
  Prove that if \(\T\) is a unitary operator on a finite-dimensional inner product space \(\V\) over \(\C\), then \(\T\) has a unitary \emph{square root};
  that is, there exists a unitary operator \(\U\) such that \(\T = \U^2\).
\end{ex}

\begin{proof}[\pf{ex:6.5.7}]
  Suppose that \(\dim(\V) = n\).
  By \cref{6.5.5} there exists an orthonormal basis \(\beta\) for \(\V\) over \(\C\) such that \([\T]_{\beta}\) is a diagonal matrix.
  Since \([\T]_{\beta} \in \ms[n][n][\C]\), we know that there exists some \(c_i \in \C\) such that \(c_i^2 = ([\T]_{\beta})_{i i}\) for all \(i \in \set{1, \dots, n}\).
  Thus by \cref{2.6} there exists an \(\U \in \ls(\V)\) such that
  \[
    [\U]_{\beta} = \begin{pmatrix}
      c_1    & 0      & \cdots & 0      \\
      0      & c_2    & \cdots & 0      \\
      \vdots & \vdots & \ddots & \vdots \\
      0      & 0      & \cdots & c_n
    \end{pmatrix}.
  \]
  By \cref{2.11} we have
  \[
    [\T]_{\beta} = [\U]_{\beta} [\U]_{\beta} = [\U^2]_{\beta}
  \]
  and thus by \cref{2.1.13} \(\T = \U^2\).
\end{proof}

\begin{ex}\label{ex:6.5.18}
  Show that ``is unitarily equivalent to'' is an equivalence relation on \(\ms[n][n][\C]\).
\end{ex}

\begin{proof}[\pf{ex:6.5.18}]
  Let \(A, B, C \in \ms[n][n][\C]\).
  For reflexivity, we have
  \begin{align*}
             & A = I_n A I_n = I_n^* A I_n              &  & \by{6.3.2}[e] \\
    \implies & A \text{ is unitarily equivalent to } A. &  & \by{6.5.13}
  \end{align*}
  For symmetric, we have
  \begin{align*}
             & A \text{ is unitarily equivalent to } B                             \\
    \implies & \exists P \in \ms[n][n][\C] : \begin{dcases}
                                               P \text{ is unitary} \\
                                               A = P^* B P
                                             \end{dcases} &  & \by{6.5.13}         \\
    \implies & B = (P^*)^{-1} A P^{-1} = P A P^*            &  & \by{ex:6.1.23}[c] \\
             & = (P^*)^* A P^*                              &  & \by{6.3.2}[d]     \\
    \implies & B \text{ is unitarily equivalent to } A.
  \end{align*}
  For transitivity, we have
  \begin{align*}
             & \begin{dcases}
                 A \text{ is unitarily equivalent to } B \\
                 B \text{ is unitarily equivalent to } C
               \end{dcases}                            \\
    \implies & \exists P, Q \in \ms[n][n][\C] : \begin{dcases}
                                                  P, Q \text{ are unitary} \\
                                                  A = P^* B P              \\
                                                  B = Q^* C Q
                                                \end{dcases} &  & \by{6.5.13}     \\
    \implies & A = P^* Q^* C Q P = (QP)^* C (QP)               &  & \by{6.3.2}[c] \\
    \implies & A \text{ is unitarily equivalent to } C.        &  & \by{6.5.13}
  \end{align*}
  Thus ``is unitarily equivalent to'' is an equivalence relation on \(\ms[n][n][\C]\).
\end{proof}

\begin{ex}\label{ex:6.5.22}
  Let \(\V\) be a real inner product space.
  \begin{enumerate}
    \item Prove that any translation on \(\V\) is a rigid motion.
    \item Prove that the composite of any two rigid motions on \(\V\) is a rigid motion on \(\V\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.5.22}(a)]
  Let \(g : \V \to \V\) be a translation.
  By \cref{6.5.16} there exists a \(v_0 \in \V\) such that \(g(x) = x + v_0\) for all \(x \in \V\).
  Then we have
  \begin{align*}
    \forall x, y \in \V, \norm{g(x) - g(y)} & = \norm{x + v_0 - (y + v_0)} &  & \by{6.5.16} \\
                                            & = \norm{x - y}               &  & \by{1.2.1}
  \end{align*}
  and thus by \cref{6.5.14} \(g\) is a rigid motion.
\end{proof}

\begin{proof}[\pf{ex:6.5.22}(b)]
  Let \(g, h: \V \to \V\) be two rigid motions.
  Then we have
  \begin{align*}
    \forall x, y \in \V, \norm{(g \circ h)(x) - (g \circ h)(y)} & = \norm{g(h(x)) - g(h(y))}                  \\
                                                                & = \norm{h(x) - h(y)}       &  & \by{6.5.14} \\
                                                                & = \norm{x - y}.            &  & \by{6.5.14}
  \end{align*}
  Thus \(g \circ h\) is a rigid motion.
\end{proof}
