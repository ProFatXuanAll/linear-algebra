\section{Unitary and Orthogonal Operators and Their Matrices}\label{sec:6.5}

\begin{note}
  In \cref{sec:6.5}, we study those linear operators \(\T\) on an inner product space \(\V\) over \(\F\) such that \(\T \T^* = \T^* \T = \IT[\V]\).
  We will see that these are precisely the linear operators that ``preserve length'' in the sense that \(\norm{\T(x)} = \norm{x}\) for all \(x \in \V\).
  As another characterization, we prove that, on a finite-dimensional complex inner product space, these are the normal operators whose eigenvalues all have absolute value \(1\).

  In past chapters, we were interested in studying those functions that preserve the structure of the underlying space.
  In particular, linear operators preserve the operations of vector addition and scalar multiplication, and isomorphisms preserve all the vector space structure.
  It is now natural to consider those linear operators \(\T\) on an inner product space that preserve length.
  We will see that this condition guarantees, in fact, that \(\T\) preserves the inner product.
\end{note}

\begin{defn}\label{6.5.1}
  Let \(\T\) be a linear operator on a finite-dimensional inner product space \(\V\) over \(\F\).
  If \(\norm{\T(x)} = \norm{x}\) for all \(x \in \V\), we call \(\T\) a \textbf{unitary operator} if \(\F = \C\) and an \textbf{orthogonal operator} if \(\F = \R\).

  in the infinite-dimensional case, an operator satisfying the preceding norm requirement is generally called an \textbf{isometry}.
  If, in addition, the operator is onto (the condition guarantees one-to-one, see \cref{ex:6.1.17}), then the operator is called a \textbf{unitary} or \textbf{orthogonal operator}.
\end{defn}

\begin{note}
  Clearly, any rotation or reflection in \(\R^2\) preserves length and hence is an orthogonal operator.
  We study these operators in much more detail in \cref{sec:6.11}.
\end{note}

\begin{eg}\label{6.5.2}
  Let \(h \in \vs{H}\) (see \cref{6.1.8}) satisfy \(\abs{h(x)} = 1\) for all \(x \in [0, 2 \pi]\).
  Define \(\T \in \ls(\vs{H})\) by \(\T(f) = hf\).
  Then
  \begin{align*}
    \norm{\T(f)}^2 & = \norm{hf}^2                                                                      \\
                   & = \inn{hf, hf}                                                     &  & \by{6.1.9} \\
                   & = \dfrac{1}{2 \pi} \int_0^{2 \pi} h(t) f(t) \conj{h(t) f(t)} \; dt &  & \by{6.1.8} \\
                   & = \inn{f, f}                                                       &  & \by{6.1.8} \\
                   & = \norm{f}^2                                                       &  & \by{6.1.9}
  \end{align*}
  since \(\abs{h(t)}^2 = 1\) for all \(t \in [0, 2 \pi]\).
  So \(\T\) is a unitary operator.
\end{eg}

\begin{lem}\label{6.5.3}
  Let \(\U\) be a self-adjoint operator on a finite-dimensional inner product space \(\V\) over \(\F\).
  If \(\inn{x, \U(x)} = 0\) for all \(x \in \V\), then \(\U = \zT\).
\end{lem}

\begin{proof}[\pf{6.5.3}]
  By either \cref{6.16} or \cref{6.17}, we may choose an orthonormal basis \(\beta\) for \(\V\) over \(\F\) consisting of eigenvectors of \(\U\).
  If \(x \in \beta\), then \(\U(x) = \lambda x\) for some \(\lambda\).
  Thus
  \begin{align*}
    0 & = \inn{x, \U(x)}                              \\
      & = \inn{x, \lambda x}         &  & \by{5.1.2}  \\
      & = \conj{\lambda} \inn{x, x}; &  & \by{6.1}[b]
  \end{align*}
  so \(\lambda = 0\).
  Hence \(\U(x) = \zv\) for all \(x \in \beta\), and thus \(\U = \zT\).
\end{proof}

\begin{note}
  Compare \cref{6.5.3} to \cref{ex:6.4.11}(b).
\end{note}

\begin{thm}\label{6.18}
  Let \(\T\) be a linear operator on a finite-dimensional inner product space \(\V\) over \(\F\).
  Then the following statements are equivalent.
  \begin{enumerate}
    \item \(\T \T^* = \T^* \T = \IT[\V]\).
    \item \(\inn{\T(x), \T(y)} = \inn{x, y}\) for all \(x, y \in \V\).
    \item If \(\beta\) is an orthonormal basis for \(\V\) over \(\F\), then \(\T(\beta)\) is an orthonormal basis for \(\V\) over \(\F\).
    \item There exists an orthonormal basis \(\beta\) for \(\V\) over \(\F\) such that \(\T(\beta)\) is an orthonormal basis for \(\V\) over \(\F\).
    \item \(\norm{\T(x)} = \norm{x}\) for all \(x \in \V\).
  \end{enumerate}
  Thus all the conditions above are equivalent to the definition of a unitary or orthogonal operator.
  From (a), it follows that unitary or orthogonal operators are normal.
\end{thm}

\begin{proof}[\pf{6.18}]
  We prove first that (a) implies (b).
  Let \(x, y \in \V\).
  Then \(\inn{x, y} = \inn{\T^* \T(x), y} = \inn{\T(x), \T(y)}\).

  Second, we prove that (b) implies (c).
  Let \(\beta = \set{\seq{v}{1,,n}}\) be an orthonormal basis for \(\V\) over \(\F\);
  so \(\T(\beta) = \set{\T(v_1), \dots, \T(v_n)}\).
  It follows that \(\inn{\T(v_i), \T(v_j)} = \inn{v_i, v_j} = \delta_{i j}\).
  Therefore \(\T(\beta)\) is an orthonormal basis for \(\V\) over \(\F\).

  That (c) implies (d) is obvious.

  Next we prove that (d) implies (e).
  Let \(x \in \V\), and let \(\beta = \set{\seq{v}{1,,n}}\).
  Now
  \[
    x = \sum_{i = 1}^n a_i v_i
  \]
  for some scalars \(a_i \in \F\), and so
  \begin{align*}
    \norm{x}^2 & = \inn{\sum_{i = 1}^n a_i v_i, \sum_{j = 1}^n a_j v_j}        &  & \by{6.1.9}      \\
               & = \sum_{i = 1}^n  a_i \inn{v_i, \sum_{j = 1}^n a_j v_j}       &  & \by{6.1.1}[a,b] \\
               & = \sum_{i = 1}^n \sum_{j = 1}^n a_i \conj{a_j} \inn{v_i, v_j} &  & \by{6.1}[a,b]   \\
               & = \sum_{i = 1}^n \sum_{j = 1}^n a_i \conj{a_j} \delta_{i j}   &  & \by{6.1.12}     \\
               & = \sum_{i = 1}^n a_i \conj{a_i}                               &  & \by{2.3.4}      \\
               & = \sum_{i = 1}^n \abs{a_i}^2                                  &  & \by{d.0.5}
  \end{align*}
  since \(\beta\) is orthonormal.

  Applying the same manipulations to
  \[
    \T(x) = \sum_{i = 1}^n a_i \T(v_i)
  \]
  and using the fact that \(\T(\beta)\) is also orthonormal, we obtain
  \[
    \norm{\T(x)}^2 = \sum_{i = 1}^n \abs{a_i}^2.
  \]
  Hence \(\norm{\T(x)} = \norm{x}\).

  Finally, we prove that (e) implies (a).
  For any \(x \in \V\), we have
  \begin{align*}
    \inn{x, x} & = \norm{x}^2           &  & \by{6.1.9} \\
               & = \norm{\T(x)}^2                       \\
               & = \inn{\T(x), \T(x)}   &  & \by{6.1.9} \\
               & = \inn{x, \T^* \T(x)}. &  & \by{6.9}
  \end{align*}
  So \(\inn{x, (\IT[\V] - \T^* \T)(x)} = 0\) for all \(x \in \V\).
  Let \(\U = \IT[\V] - \T^* \T\);
  then \(\U\) is self-adjoint (by \cref{6.11}) and \(\inn{x, \U(x)} = 0\) for all \(x \in \V\).
  Hence, by \cref{6.5.3}, we have \(\zT = \U = \IT[\V] - \T^* \T\), and therefore \(\T^* \T = \IT[\V]\).
  Since \(\V\) is finite-dimensional, we may use \cref{ex:2.4.10} to conclude that \(\T \T^* = \IT[\V]\).
\end{proof}

\begin{cor}\label{6.5.4}
  Let \(\T\) be a linear operator on a finite-dimensional real inner product space \(\V\).
  Then \(\V\) has an orthonormal basis of eigenvectors of \(\T\) with corresponding eigenvalues of absolute value \(1\) iff \(\T\) is both self-adjoint and orthogonal.
\end{cor}

\begin{proof}[\pf{6.5.4}]
  Suppose that \(\V\) has an orthonormal basis \(\set{\seq{v}{1,,n}}\) over \(\R\) such that \(\T(v_i) = \lambda_i v_i\) and \(\abs{\lambda_i} = 1\) for all \(i \in \set{1, \dots, n}\).
  By \cref{6.17}, \(\T\) is self-adjoint.
  Thus
  \begin{align*}
    (\T \T^*)(v_i) & = (\T \T)(v_i)            &  & \by{6.4.8}                                          \\
                   & = \T(\lambda_i v_i)       &  & \by{5.1.2}                                          \\
                   & = \lambda_i \lambda_i v_i &  & \by{5.1.2}                                          \\
                   & = \lambda_i^2 v_i                                                                  \\
                   & = v_i                     &  & (\abs{\lambda_i} = 1 \text{ and } \lambda_i \in \R)
  \end{align*}
  for each \(i \in \set{1, \dots, n}\).
  So \(\T \T^* = \IT[\V]\), and again by \cref{ex:2.4.10}, \(\T^* \T = \IT[\V]\).
  Thus \(\T\) is orthogonal by \cref{6.18}(a).

  If \(\T\) is self-adjoint, then, by \cref{6.17}, we have that \(\V\) possesses an orthonormal basis \(\set{\seq{v}{1,,n}}\) over \(\R\) such that \(\T(v_i) = \lambda_i v_i\) for all \(i \in \set{1, \dots, n}\).
  If \(\T\) is also orthogonal, we have
  \begin{align*}
    \abs{\lambda_i} \cdot \norm{v_i} & = \norm{\lambda_i v_i} &  & \by{6.2}[a] \\
                                     & = \norm{\T(v_i)}       &  & \by{5.1.2}  \\
                                     & = \norm{v_i};          &  & \by{6.5.1}
  \end{align*}
  so \(\abs{\lambda_i} = 1\) for every \(i \in \set{1, \dots, n}\).
\end{proof}

\begin{cor}\label{6.5.5}
  Let \(\T\) be a linear operator on a finite-dimensional complex inner product space \(\V\).
  Then \(\V\) has an orthonormal basis of eigenvectors of \(\T\) with corresponding eigenvalues of absolute value \(1\) iff \(\T\) is unitary.
\end{cor}

\begin{proof}[\pf{6.5.5}]
  Suppose that \(\V\) has an orthonormal basis \(\set{\seq{v}{1,,n}}\) over \(\C\) such that \(\T(v_i) = \lambda_i v_i\) and \(\abs{\lambda_i} = 1\) for all \(i \in \set{1, \dots, n}\).
  By \cref{6.16}, \(\T\) is normal.
  Thus
  \begin{align*}
    (\T^* \T)(v_i) & = (\T \T^*)(v_i)                 &  & \by{6.4.3}            \\
                   & = \T(\conj{\lambda_i} v_i)       &  & \by{6.15}[c]          \\
                   & = \conj{\lambda_i} \lambda_i v_i &  & \by{5.1.2}            \\
                   & = \abs{\lambda_i}^2 v_i          &  & \by{d.0.5}            \\
                   & = v_i                            &  & (\abs{\lambda_i} = 1)
  \end{align*}
  for each \(i \in \set{1, \dots, n}\).
  So \(\T^* \T = \T \T^* = \IT[\V]\) by \cref{2.1.13}.
  Thus by \cref{6.18}(a) \(\T\) is unitary.

  If \(\T\) is unitary, then by \cref{6.18}(a)(e) we know that \(\T\) is normal.
  By \cref{6.16}, we have that \(\V\) possesses an orthonormal basis \(\set{\seq{v}{1,,n}}\) over \(\C\) such that \(\T(v_i) = \lambda_i v_i\) for all \(i \in \set{1, \dots, n}\).
  Then we have
  \begin{align*}
    \abs{\lambda_i} \cdot \norm{v_i} & = \norm{\lambda_i v_i} &  & \by{6.2}[a] \\
                                     & = \norm{\T(v_i)}       &  & \by{5.1.2}  \\
                                     & = \norm{v_i};          &  & \by{6.5.1}
  \end{align*}
  so \(\abs{\lambda_i} = 1\) for every \(i \in \set{1, \dots, n}\).
\end{proof}

\begin{eg}\label{6.5.6}
  Let \(\T : \R^2 \to \R^2\) be a rotation by \(\theta\), where \(0 < \theta < \pi\).
  It is geometrically clear that \(\T\) ``preserves length'', that is, that \(\norm{\T(x)} = \norm{x}\) for all \(x \in \R^2\).
  The fact that rotations by a fixed angle preserve perpendicularity not only can be seen geometrically but now follows from \cref{6.18}(b).
  Perhaps the fact that such a transformation preserves the inner product is not so obvious;
  however, we obtain this fact from \cref{6.18}(b) also.
  Finally, an inspection of the matrix representation of \(\T\) with respect to the standard ordered basis, which is
  \[
    \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) \\
      \sin(\theta) & \cos(\theta)
    \end{pmatrix},
  \]
  reveals that \(\T\) is not self-adjoint for the given restriction on \(\theta\).
  As we mentioned earlier, this fact also follows from the geometric observation that \(\T\) has no eigenvectors (see \cref{5.1.3}) and from \cref{6.17}.
  It is seen easily from the preceding matrix that \(\T^*\) is the rotation by \(-\theta\) since
  \[
    \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) \\
      \sin(\theta) & \cos(\theta)
    \end{pmatrix}^* = \begin{pmatrix}
      \cos(\theta)  & \sin(\theta) \\
      -\sin(\theta) & \cos(\theta)
    \end{pmatrix} = \begin{pmatrix}
      \cos(-\theta) & -\sin(-\theta) \\
      \sin(-\theta) & \cos(-\theta)
    \end{pmatrix}.
  \]
\end{eg}

\begin{defn}\label{6.5.7}
  Let \(\vs{L}\) be a one-dimensional subspace of \(\R^2\) over \(\R\).
  We may view \(\vs{L}\) as a line in the plane through the origin.
  A linear operator \(\T\) on \(\R^2\) is called a \textbf{reflection of \(\R^2\) about \(\vs{L}\)} if \(\T(x) = x\) for all \(x \in \vs{L}\) and \(\T(x) = -x\) for all \(x \in \vs{L}^{\perp}\).
\end{defn}

\begin{eg}\label{6.5.8}
  Let \(\T\) be a reflection of \(\R^2\) about a line \(\vs{L}\) through the origin.
  We show that \(\T\) is an orthogonal operator.
  Select vectors \(v_1 \in \vs{L}\) and \(v_2 \in \vs{L}^{\perp}\) such that \(\norm{v_1} = \norm{v_2} = 1\).
  Then \(\T(v_1) = v_1\) and \(\T(v_2) = -v_2\).
  Thus \(v_1\) and \(v_2\) are eigenvectors of \(\T\) with corresponding eigenvalues \(1\) and \(-1\), respectively.
  Furthermore, \(\set{v_1, v_2}\) is an orthonormal basis for \(\R^2\).
  It follows that \(\T\) is an orthogonal operator by \cref{6.5.4}.
\end{eg}

\begin{defn}\label{6.5.9}
  A square matrix \(A \in \ms[n][n][\F]\) is called an an \textbf{orthogonal matrix} if \(\tp{A} A = A \tp{A} = I_n\) and \textbf{unitary} if \(A^* A = A A^* = I_n\).

  Since for a real matrix \(A\) we have \(A^* = \tp{A}\), a real unitary matrix is also orthogonal.
  In this case, we call \(A\) \textbf{orthogonal} rather than unitary.

  The condition \(A A^* = I_n\) is equivalent to the statement that the rows of \(A\) form an orthonormal basis for \(\vs{F}^n\) because
  \begin{align*}
    \delta_{i j} & = (A A^*)_{i j}                          &  & \by{2.3.4} \\
                 & = \sum_{k = 1}^n A_{i k} (A^*)_{k j}     &  & \by{2.3.1} \\
                 & = \sum_{k = 1}^n A_{i k} \conj{A_{j k}}, &  & \by{6.1.5}
  \end{align*}
  and the last term represents the inner product of the \(i\)th and \(j\)th rows of \(A\).

  The condition \(A^* A = I_n\) is equivalent to the statement that the columns of \(A\) form an orthonormal basis for \(\vs{F}^n\) because
  \begin{align*}
    \delta_{i j} & = (A^* A)_{i j}                          &  & \by{2.3.4} \\
                 & = \sum_{k = 1}^n (A^*)_{i k} A_{k j}     &  & \by{2.3.1} \\
                 & = \sum_{k = 1}^n \conj{A_{k i}} A_{k j}, &  & \by{6.1.5}
  \end{align*}
  and the last term represents the inner product of the \(j\)th and \(i\)th columns of \(A\).
\end{defn}

\begin{cor}\label{6.5.10}
  A linear operator \(\T\) on a finite-dimensional inner product space \(\V\) over \(\F\) is unitary (orthogonal) iff \([\T]_{\beta}\) is unitary (orthogonal) for some orthonormal basis \(\beta\) for \(\V\) over \(\F\).
\end{cor}

\begin{proof}[\pf{6.5.10}]
  We have
  \begin{align*}
         & \T \text{ is unitary}                                                               \\
    \iff & \forall x \in \V, \norm{\T(x)} = \norm{x}                       &  & \by{6.5.1}     \\
    \iff & \T \T^* = \T^* \T = \IT[\V]                                     &  & \by{6.18}[a,e] \\
    \iff & [\T \T^*]_{\beta} = [\T^* \T]_{\beta} = [\IT[\V]]_{\beta} = I_n &  & \by{2.12}[d]   \\
    \iff & [\T]_{\beta} [\T^*]_{\beta} = [\T^*]_{\beta} [\T]_{\beta} = I_n &  & \by{2.3.3}     \\
    \iff & [\T]_{\beta} [\T]_{\beta}^* = [\T]_{\beta}^* [\T]_{\beta} = I_n &  & \by{6.10}      \\
    \iff & [\T]_{\beta} \text{ is unitary}.                                &  & \by{6.5.9}
  \end{align*}
  Similar argument shows that \(\T\) is orthogonal iff \([\T]_{\beta}\) is orthogonal.
\end{proof}

\begin{eg}\label{6.5.11}
  From \cref{6.5.6}, the matrix
  \[
    \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) \\
      \sin(\theta) & \cos(\theta)
    \end{pmatrix}
  \]
  is clearly orthogonal.
  One can easily see that the rows of the matrix form an orthonormal basis for \(\R^2\) since
  \begin{align*}
    \inn{(\cos(\theta), -\sin(\theta)), (\cos(\theta), -\sin(\theta))} & = (\cos(\theta))^2 + (\sin(\theta))^2                   &  & \by{6.1.2} \\
                                                                       & = 1;                                                                    \\
    \inn{(\cos(\theta), -\sin(\theta)), (\sin(\theta), \cos(\theta))}  & = \sin(\theta) \cos(\theta) - \sin(\theta) \cos(\theta) &  & \by{6.1.2} \\
                                                                       & = 0;                                                                    \\
    \inn{(\sin(\theta), \cos(\theta)), (\sin(\theta), \cos(\theta))}   & = (\sin(\theta))^2 + (\cos(\theta))^2                   &  & \by{6.1.2} \\
                                                                       & = 1.
  \end{align*}
  Similarly, the columns of the matrix form an orthonormal basis for \(\R^2\).
\end{eg}

\begin{eg}\label{6.5.12}
  Let \(\T\) be a reflection of \(\R^2\) about a line \(\L\) through the origin, let \(\beta\) be the standard ordered basis for \(\R^2\) over \(\R\), and let \(A = [\T]_{\beta}\).
  Then \(\T = \L_A\).
  Since \(\T\) is an orthogonal operator and \(\beta\) is an orthonormal basis, \(A\) is an orthogonal matrix by \cref{6.5.10}.
  We describe \(A\).

  Suppose that \(\alpha\) is the angle from the positive \(x\)-axis to \(\L\).
  Let \(v_1 = (\cos(\alpha), \sin(\alpha))\) and \(v_2 = (-\sin(\alpha), \cos(\alpha))\).
  Then \(\norm{v_1} = \norm{v_2} = 1\), \(v_1 \in \L\), and \(v_2 \in \L^{\perp}\).
  Hence \(\gamma = \set{v_1, v_2}\) is an orthonormal basis for \(\R^2\) over \(\R\).
  Because \(\T(v_1) = v_1\) and \(\T(v_2) = -v_2\), we have
  \[
    [\T]_{\gamma} = [\L_A]_{\gamma} = \begin{pmatrix}
      1 & 0  \\
      0 & -1
    \end{pmatrix}.
  \]
  Let
  \[
    Q = \begin{pmatrix}
      \cos(\alpha) & -\sin(\alpha) \\
      \sin(\alpha) & \cos(\alpha)
    \end{pmatrix}.
  \]
  Then
  \begin{align*}
    A & = Q [\L_A]_{\gamma} Q^{-1}                                                      &  & \by{2.5.3} \\
      & = \begin{pmatrix}
            \cos(\alpha) & -\sin(\alpha) \\
            \sin(\alpha) & \cos(\alpha)
          \end{pmatrix} \begin{pmatrix}
                          1 & 0  \\
                          0 & -1
                        \end{pmatrix} \begin{pmatrix}
                                        \cos(\alpha)  & \sin(\alpha) \\
                                        -\sin(\alpha) & \cos(\alpha)
                                      \end{pmatrix}                                      \\
      & = \begin{pmatrix}
            (\cos(\alpha))^2 - (\sin(\alpha))^2 & 2 \sin(\alpha) \cos(\alpha)            \\
            2 \sin(\alpha) \cos(\alpha)         & -((\cos(\alpha))^2 - (\sin(\alpha))^2)
          \end{pmatrix}                  \\
      & = \begin{pmatrix}
            \cos(2 \alpha) & \sin(2 \alpha)  \\
            \sin(2 \alpha) & -\cos(2 \alpha)
          \end{pmatrix}.
  \end{align*}
\end{eg}

\begin{defn}\label{6.5.13}
  We know by \cref{6.16,6.17} that, for a complex normal (real symmetric) matrix \(A\), there exists an orthonormal basis \(\beta\) for \(\vs{F}^n\) consisting of eigenvectors of \(A\).
  Hence \(A\) is similar to a diagonal matrix \(D\).
  By \cref{2.5.3}, the matrix \(Q\) whose columns are the vectors in \(\beta\) is such that \(D = Q^{-1} A Q\).
  But since the columns of \(Q\) are an orthonormal basis for \(\vs{F}^n\), it follows that \(Q\) is unitary (orthogonal) (see \cref{ex:6.1.23}(c)).
  In this case, we say that \(A\) is \textbf{unitarily equivalent} (\textbf{orthogonally equivalent}) to \(D\).

  It is easily seen (see \cref{ex:6.5.18}) that this relation is an equivalence relation on \(\ms[n][n][\C]\) (\(\ms[n][n][\R]\)).
  More generally, \(A\) and \(B\) are unitarily equivalent (orthogonally equivalent) iff there exists a unitary (orthogonal) matrix \(P\) such that \(A = P^* B P\).
\end{defn}

\begin{thm}\label{6.19}
  Let \(A \in \ms[n][n][\C]\).
  Then \(A\) is normal iff \(A\) is unitarily equivalent to a diagonal matrix.
\end{thm}

\begin{proof}[\pf{6.19}]
  By \cref{6.5.13}, we need only prove that if \(A\) is unitarily equivalent to a diagonal matrix, then \(A\) is normal.

  Suppose that \(A = P^* D P\), where \(P\) is a unitary matrix and \(D\) is a diagonal matrix.
  Then
  \begin{align*}
    A A^* & = (P^* D P) (P^* D P)^*                        \\
          & = (P^* D P) (P^* D^* P) &  & \by{6.3.2}[c]     \\
          & = P^* D I_n D^* P       &  & \by{ex:6.1.23}[c] \\
          & = P^* D D^* P.
  \end{align*}
  Similarly, \(A^* A = P^* D^* D P\).
  Since \(D\) is a diagonal matrix, however, we have \(D D^* = D^* D\).
  Thus \(A A^* = A^* A\).
\end{proof}

\begin{thm}\label{6.20}
  Let \(A \in \ms[n][n][\R]\).
  Then \(A\) is symmetric iff \(A\) is orthogonally equivalent to a real diagonal matrix.
\end{thm}

\begin{proof}[\pf{6.20}]
  By \cref{6.5.13}, we need only prove that if \(A\) is orthogonally equivalent to a diagonal matrix, then \(A\) is symmetric.

  Suppose that \(A = \tp{P} D P\), where \(P\) is a orthogonal matrix and \(D\) is a diagonal matrix.
  Then
  \begin{align*}
    \tp{A} & = \tp{(\tp{P} D P)}                                                    \\
           & = \tp{P} \tp{D} \tp{(\tp{P})} &  & \by{2.3.2}                          \\
           & = \tp{P} \tp{D} P             &  & \by{ex:1.3.4}                       \\
           & = \tp{P} D P                  &  & \text{(\(D\) is a diagonal matrix)} \\
           & = A.
  \end{align*}
\end{proof}

\begin{thm}[Schur's Theorem]\label{6.21}
  Let \(A \in \ms[n][n][\F]\) be a matrix whose characteristic polynomial splits over \(\F\).
  \begin{itemize}
    \item If \(\F = \C\), then \(A\) is unitarily equivalent to a complex upper triangular matrix.
    \item If \(\F = \R\), then \(A\) is orthogonally equivalent to a real upper triangular matrix.
  \end{itemize}
\end{thm}

\begin{proof}[\pf{6.21}]
  Let \(\beta\) be the standard ordered basis for \(\vs{F}^n\) over \(\F\).
  By \cref{6.14}, there exists an orthonormal basis \(\gamma\) for \(\vs{F}^n\) over \(\F\) such that \([\L_A]_{\gamma}\) is upper triangular.
  Let \(Q = [\IT[\vs{F}^n]]_{\gamma}^{\beta}\).
  Then we have
  \begin{align*}
    [\L_A]_{\gamma} & = Q^{-1} [\L_A]_{\beta} Q &  & \by{2.23}         \\
                    & = Q^{-1} A Q              &  & \by{2.15}[a]      \\
                    & = Q^* A Q.                &  & \by{ex:6.1.23}[c]
  \end{align*}
  Thus by \cref{6.5.13} \(A\) is unitarily (orthogonally) equivalent to an complex (real) upper triangular matrix.
\end{proof}

\begin{defn}\label{6.5.14}
  Let \(\V\) be a real inner product space.
  A function \(f : \V \to \V\) is called a \textbf{rigid motion} if
  \[
    \norm{f(x) - f(y)} = \norm{x - y}
  \]
  for all \(x, y \in \V\).
\end{defn}

\begin{note}
  One may think intuitively of a rigid motion as a transformation that does not affect the shape of a figure under its action, hence the term \emph{rigid}.
  The key requirement for such a transformation is that it preserves distances.
\end{note}

\begin{eg}\label{6.5.15}
  Any orthogonal operator on a finite-dimensional real inner product space is a rigid motion.
\end{eg}

\begin{proof}[\pf{6.5.15}]
  Let \(\V\) be a finite-dimensional real inner product space and let \(\T \in \ls(\V)\) be orthogonal.
  Then we have
  \begin{align*}
    \forall x, y \in \V, \norm{x - y} & = \norm{\T(x - y)}      &  & \by{6.5.1}    \\
                                      & = \norm{\T(x) - \T(y)}. &  & \by{2.1.2}[c]
  \end{align*}
  Thus \(\T\) is a rigid motion.
\end{proof}

\begin{defn}\label{6.5.16}
  Another class of rigid motions are the \emph{translations}.
  A function \(g : \V \to \V\), where \(\V\) is a real inner product space, is called a \textbf{translation} if there exists a vector \(v_0 \in \V\) such that \(g(x) = x + v_0\) for all \(x \in \V\).
  We say that \(g\) is the \emph{translation by} \(v_0\).
\end{defn}

\begin{note}
  It is a simple exercise to show that translations, as well as composites of rigid motions on a real inner product space, are also rigid motions.
  (See \cref{ex:6.5.22}.)
  Thus an orthogonal operator on a finite-dimensional real inner product space \(\V\) followed by a translation on \(\V\) is a rigid motion on \(\V\).
  Remarkably, every rigid motion on \(\V\) may be characterized in this way (see \cref{6.22}).
\end{note}

\begin{thm}\label{6.22}
  Let \(f : \V \to \V\) be a rigid motion on a finite-dimensional real inner product space \(\V\).
  Then there exists an unique orthogonal operator \(\T\) on \(\V\) and an unique translation \(g\) on \(\V\) such that \(f = g \circ \T\).
\end{thm}

\begin{proof}[\pf{6.22}]
  Let \(\T : \V \to \V\) be defined by
  \[
    \T(x) = f(x) - f(\zv)
  \]
  for all \(x \in \V\).
  We show that \(\T\) is an orthogonal operator, from which it follows that \(f = g \circ \T\), where \(g\) is the translation by \(f(\zv)\).
  Observe that \(\T\) is the composite of \(f\) and the translation by \(-f(\zv)\);
  hence \(\T\) is a rigid motion (by \cref{ex:6.5.22}(b)).
  Furthermore, for any \(x \in \V\),
  \begin{align*}
    \norm{\T(x)}^2 & = \norm{f(x) - f(\zv)}^2                  \\
                   & = \norm{x - \zv}^2       &  & \by{6.5.14} \\
                   & = \norm{x}^2,
  \end{align*}
  and consequently \(\norm{\T(x)} = \norm{x}\) for any \(x \in \V\).
  Thus for any \(x, y \in \V\),
  \begin{align*}
     & \norm{\T(x) - \T(y)}^2                                                                                   \\
     & = \inn{\T(x) - \T(y), \T(x) - \T(y)}                                                &  & \by{6.1.9}      \\
     & = \inn{\T(x), \T(x) - \T(y)} - \inn{\T(y), \T(x) - \T(y)}                           &  & \by{6.1.1}[a,b] \\
     & = \inn{\T(x), \T(x)} - \inn{\T(x), \T(y)} - \inn{\T(y), \T(x)} + \inn{\T(y), \T(y)} &  & \by{6.1}[a,b]   \\
     & = \norm{\T(x)}^2 - \inn{\T(x), \T(y)} - \inn{\T(y), \T(x)} + \norm{\T(y)}^2         &  & \by{6.1.9}      \\
     & = \norm{\T(x)}^2 - 2 \inn{\T(x), \T(y)} + \norm{\T(y)}^2                            &  & (\F = \R)       \\
     & = \norm{x}^2 - 2 \inn{\T(x), \T(y)} + \norm{y}^2                                    &  & \by{6.5.1}
  \end{align*}
  and
  \[
    \norm{x - y}^2 = \norm{x}^2 - 2 \inn{x, y} + \norm{y}^2.
  \]
  But \(\norm{\T(x) - \T(y)}^2 = \norm{x - y}^2\);
  so \(\inn{\T(x), \T(y)} = \inn{x, y}\) for all \(x, y \in \V\).

  We are now in a position to show that \(\T\) is a linear transformation.
  Let \(x, y \in \V\), and let \(a \in \R\).
  Then
  \begin{align*}
     & \norm{\T(x + ay) - \T(x) - a \T(y)}^2                                                        \\
     & = \norm{(\T(x + ay) - \T(x)) - a \T(y)}^2                                                    \\
     & = \norm{\T(x + ay) - \T(x)}^2 + a^2 \norm{\T(y)}^2 - 2a \inn{\T(x + ay) - \T(x), \T(y)}      \\
     & = \norm{(x + ay) - x}^2 + a^2 \norm{y}^2 - 2a (\inn{\T(x + ay), \T(y)} - \inn{\T(x), \T(y)}) \\
     & = a^2 \norm{y}^2 + a^2 \norm{y}^2 - 2a (\inn{x + ay, y} - \inn{x, y})                        \\
     & = 2 a^2 \norm{y}^2 - 2a (\inn{x, y} + a \norm{y}^2 - \inn{x, y})                             \\
     & = 0.
  \end{align*}
  Thus by \cref{6.2}(b) \(\T(x + ay) = \T(x) + a \T(y)\), and hence \(\T\) is linear.
  Since \(\T\) also preserves inner products, \(\T\) is an orthogonal operator.

  To prove uniqueness, suppose that \(u_0\) and \(v_0\) are in \(\V\) and \(\T\) and \(\U\) are orthogonal operators on \(\V\) such that
  \[
    f(x) = \T(x) + u_0 = \U(x) + v_0
  \]
  for all \(x \in \V\).
  Substituting \(x = \zv\) in the preceding equation yields \(u_0 = v_0\), and hence the translation is unique.
  This equation, therefore, reduces to \(\T(x) = \U(x)\) for all \(x \in \V\), and hence \(\T = \U\) (by \cref{2.1.13}).
\end{proof}

\begin{note}
  By \cref{6.22}, any orthogonal operator is a special case of this composite, in which the translation is by \(\zv\).
  Any translation is also a special case, in which the orthogonal operator is the identity operator.
\end{note}

\begin{thm}\label{6.23}
  Let \(\T\) be an orthogonal operator on \(\R^2\), and let \(A = [\T]_{\beta}\), where \(\beta\) is the standard ordered basis for \(\R^2\) over \(\R\).
  Then exactly one of the following conditions is satisfied:
  \begin{enumerate}
    \item \(\T\) is a rotation, and \(\det(A) = 1\).
    \item \(\T\) is a reflection about a line through the origin, and \(\det(A) = -1\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{6.23}]
  Because \(\T\) is an orthogonal operator, \(\T(\beta) = \set{\T(e_1), \T(e_2)}\) is an orthonormal basis for \(\R^2\) over \(\R\) by \cref{6.18}(c).
  Since \(\T(e_1)\) is a unit vector, there is an unique angle \(\theta \in [0, 2 \pi)\) such that \(\T(e_1) = (\cos(\theta), \sin(\theta))\).
  Since \(\T(e_2)\) is a unit vector and is orthogonal to \(\T(e_1)\), there are only two possible choices for \(\T(e_2)\).
  Either
  \[
    \T(e_2) = (-\sin(\theta), \cos(\theta)) \quad \text{or} \quad \T(e_2) = (\sin(\theta), -\cos(\theta)).
  \]
  First, suppose that \(\T(e_2) = (-\sin(\theta), \cos(\theta))\).
  Then \(A = \begin{pmatrix}
    \cos(\theta) & -\sin(\theta) \\
    \sin(\theta) & \cos(\theta)
  \end{pmatrix}\).
  It follows from \cref{6.4.5} that \(\T\) is a rotation by the angle \(\theta\).
  Also
  \[
    \det(A) = (\cos(\theta))^2 + (\sin(\theta))^2 = 1.
  \]
  Now suppose that \(\T(e_2) = (\sin(\theta), -\cos(\theta))\).
  Then \(A = \begin{pmatrix}
    \cos(\theta) & \sin(\theta)  \\
    \sin(\theta) & -\cos(\theta)
  \end{pmatrix}\).
  Comparing this matrix to the matrix \(A\) of \cref{6.5.12}, we see that \(\T\) is the reflection of \(\R^2\) about a line \(\L\), so that \(\alpha = \theta / 2\) is the angle from the positive \(x\)-axis to \(\L\).
  Furthermore,
  \[
    \det(A) = - (\cos(\theta))^2 - (\sin(\theta))^2 = -1.
  \]
\end{proof}

\begin{cor}\label{6.5.17}
  Any rigid motion on \(\R^2\) is either a rotation followed by a translation or a reflection about a line through the origin followed by a translation.
\end{cor}

\begin{proof}[\pf{6.5.17}]
  Combining \cref{6.22,6.23} we are done.
\end{proof}

\begin{defn}\label{6.5.18}
  Consider the quadratic equation
  \begin{equation}\label{eq:6.5.1}
    ax^2 + 2bxy + cy^2 + dx + ey + f = 0.
  \end{equation}
  For special choices of the coefficients in the equation above, we obtain the various conic sections, namely, the circle, ellipse, parabola, and hyperbola.
  If \(b = 0\), then it is easy to graph the equation by the method of completing the square because the \(xy\)-term is absent.
  We now concentrate solely on the elimination of the \(xy\)-term.
  To accomplish this, we consider the expression
  \begin{equation}\label{eq:6.5.2}
    ax^2 + 2bxy + cy^2,
  \end{equation}
  which is called the \textbf{associated quadratic form} of \cref{eq:6.5.1}.
  Quadratic forms are studied in more generality in \cref{sec:6.8}.
\end{defn}

\begin{cor}[principal axis theorem]\label{6.5.19}
  the \(xy\)-term in \cref{eq:6.5.2} may be eliminated by a rotation of the \(x\)-axis and \(y\)-axis to new axes \(x'\) and \(y'\) given by \(X = P X'\), where \(P\) is an orthogonal matrix and \(\det(P) = 1\).
  Furthermore, the coefficients of \((x')^2\) and \((y')^2\) are the eigenvalues of
  \[
    A = \begin{pmatrix}
      a & b \\
      b & c
    \end{pmatrix}.
  \]
\end{cor}

\begin{proof}[\pf{6.5.19}]
  Let
  \[
    A = \begin{pmatrix}
      a & b \\
      b & c
    \end{pmatrix} \quad \text{and} \quad X = \begin{pmatrix}
      x \\
      y
    \end{pmatrix}.
  \]
  Then \cref{eq:6.5.2} may be written as \(\tp{X} A X = \inn{AX, X}\).

  The fact that \(A\) is symmetric is crucial in our discussion.
  For, by \cref{6.20}, we may choose an orthogonal matrix \(P\) and a diagonal matrix \(D\) with real diagonal entries \(\lambda_1\) and \(\lambda_2\) such that \(\tp{P} A P = D\).
  Now define
  \[
    X' = \begin{pmatrix}
      x' \\
      y'
    \end{pmatrix}
  \]
  by \(X' = \tp{P} X\) or, equivalently, by \(P X' = P \tp{P} X = X\).
  Then
  \begin{align*}
    \tp{X} A X & = \tp{(P X')} A (P X')                                 \\
               & = \tp{(X')} (\tp{P} A P) X'            &  & \by{2.3.2} \\
               & = \tp{(X')} D X'                                       \\
               & = \lambda_1 (x')^2 + \lambda_2 (y')^2.
  \end{align*}
  Thus the transformation \((x, y) \mapsto (x', y')\) allows us to eliminate the \(xy\)-term in \cref{eq:6.5.2}, and hence in \cref{eq:6.5.1}.

  Furthermore, since \(P\) is orthogonal, we have by \cref{6.23} (with \(\T = \L_P\)) that \(\det(P) = \pm 1\).
  If \(\det(P) = -1\), we may interchange the columns of \(P\) to obtain a matrix \(Q\).
  Because the columns of \(P\) form an orthonormal basis of eigenvectors of \(A\), the same is true of the columns of \(Q\).
  Therefore,
  \[
    \tp{Q} A Q = \begin{pmatrix}
      \lambda_2 & 0         \\
      0         & \lambda_1
    \end{pmatrix}.
  \]
  Notice that \(\det(Q) = -\det(P) = 1\) by \cref{4.5}.
  So, if \(\det(P) = -1\), we can take \(Q\) for our new \(P\);
  consequently, we may always choose \(P\) so that \(\det(P) = 1\).
  By \cref{6.23}(a) (with \(\T = \L_P\)), it follows that matrix \(P\) represents a rotation.
\end{proof}

\begin{note}
  The arguments above, of course, are easily extended to quadratic equations in \(n\) variables.
  For example, in the case \(n = 3\), by special choices of the coefficients, we obtain the quadratic surfaces
  --- the elliptic cone, the ellipsoid, the hyperbolic paraboloid, etc.
\end{note}

\exercisesection

\setcounter{ex}{2}
\begin{ex}\label{ex:6.5.3}
  Prove that the composite of unitary (orthogonal) operators is unitary (orthogonal).
\end{ex}

\begin{proof}[\pf{ex:6.5.3}]
  Let \(\V\) be a complex (real) vector space.
  Let \(\T, \U \in \ls(\V)\) be unitary (orthogonal).
  Then by \cref{2.9} we have \(\T \U \in \ls(\V)\) and
  \begin{align*}
    \forall x \in \V, \norm{(\T \U)(x)} & = \norm{\T(\U(x))}                 \\
                                        & = \norm{\U(x)}     &  & \by{6.5.1} \\
                                        & = \norm{x}.        &  & \by{6.5.1}
  \end{align*}
  Thus by \cref{6.5.1} \(\T \U\) is unitary (orthogonal).
\end{proof}

\begin{ex}\label{ex:6.5.4}
  For \(z \in \C\), define \(\T_z : \C \to \C\) by \(\T_z(u) = zu\).
  Characterize those \(z\) for which \(\T_z\) is normal, self-adjoint, or unitary.
\end{ex}

\begin{proof}[\pf{ex:6.5.4}]
  Since
  \begin{align*}
    \forall x, y \in \C, \inn{x, \T_z^*(y)} & = \inn{\T_z(x), y}           &  & \by{6.9}      \\
                                            & = \inn{zx, y}                                   \\
                                            & = z \inn{x, y}               &  & \by{6.1.1}[b] \\
                                            & = \inn{x, \conj{z} y}        &  & \by{6.1}[b]   \\
                                            & = \inn{x, \T_{\conj{z}}(y)},
  \end{align*}
  by \cref{6.1}(e) we know that \(\T_z^* = \T_{\conj{z}}\) for all \(z \in \C\).

  First we claim that \(\T_z\) is normal for all \(z \in \C\).
  Since
  \begin{align*}
    \forall u, z \in \C, \T_z(\T_z^*(u)) & = \T_z(\T_{\conj{z}}(u)) &  & \text{(from the proof above)} \\
                                         & = z \conj{z} u                                              \\
                                         & = \conj{z} z u                                              \\
                                         & = \T_{\conj{z}}(\T_z(u))                                    \\
                                         & = \T_z^*(\T_z(u)),       &  & \text{(from the proof above)}
  \end{align*}
  by \cref{6.4.3} we know that \(\T_z\) is normal for all \(z \in \C\).

  Next we claim that \(\T_z\) is self adjoint iff \(z \in \R\).
  This is true since
  \begin{align*}
         & \T_z = \T_z^*                                                  &  & \by{6.4.8}                    \\
    \iff & \T_z = \T_{\conj{z}}                                           &  & \text{(from the proof above)} \\
    \iff & \forall u \in \C, zu = \T_z(u) = \T_{\conj{z}}(u) = \conj{z} u                                    \\
    \iff & z = \conj{z}                                                                                      \\
    \iff & z \in \R.
  \end{align*}

  Finally we claim that \(\T_z\) is unitary iff \(\abs{z} = 1\).
  This is true since
  \begin{align*}
         & \T_z \text{ is unitary}                                         \\
    \iff & \forall u \in \C, \abs{z} \norm{u} = \norm{zu} &  & \by{6.2}[a] \\
         & = \norm{\T_z(u)} = \norm{u}                    &  & \by{6.5.1}  \\
    \iff & \abs{z} = 1.
  \end{align*}
\end{proof}

\setcounter{ex}{5}
\begin{ex}\label{ex:6.5.6}
  Let \(\V\) be the inner product space of complex-valued continuous functions on \([0, 1]\) with the inner product
  \[
    \inn{f, g} = \int_0^1 f(t) \conj{g(t)} \; dt.
  \]
  Let \(h \in \V\), and define \(\T : \V \to \V\) by \(\T(f) = hf\).
  Prove that \(\T\) is a unitary operator iff \(\abs{h(t)} = 1\) for \(t \in [0, 1]\).
\end{ex}

\begin{proof}[\pf{ex:6.5.6}]
  First suppose that \(\T\) is unitary.
  Then we have
  \begin{align*}
    \forall f \in \V, 0 & = \norm{\T(f)}^2 - \norm{f}^2                                                 &  & \by{6.5.1} \\
                        & = \inn{\T(f), \T(f)} - \inn{f, f}                                             &  & \by{6.1.9} \\
                        & = \inn{hf, hf} - \inn{f, f}                                                                   \\
                        & = \int_0^1 h(t) f(t) \conj{h(t) f(t)} \; dt - \int_0^1 f(t) \conj{f(t)} \; dt                 \\
                        & = \int_0^1 (h(t) \conj{h(t)} - 1) f(t) \conj{f(t)} \; dt                                      \\
                        & = \int_0^1 (\abs{h(t)}^2 - 1) \abs{f(t)}^2 \; dt.                             &  & \by{d.0.5}
  \end{align*}
  Since \(h \in \V\), we can choose a \(f \in \V\) such that \(f(t) = (\abs{h(t)}^2 - 1)^{1 / 2}\), and we have
  \begin{align*}
             & \int_0^1 (\abs{h(t)}^2 - 1) \abs{\abs{h(t)}^2 - 1} \; dt = 0                                                 \\
    \implies & \int_0^1 (\abs{h(t)}^2 - 1)^2 \; dt = 0                                                                      \\
    \implies & \forall t \in [0, 1], \abs{h(t)}^2 - 1 = 0                   &  & \text{(\(h\) is continuous on \([0, 1]\))} \\
    \implies & \forall t \in [0, 1], \abs{h(t)} = 1.
  \end{align*}

  Now suppose that \(\abs{h(t)} = 1\) for all \(t \in [0, 1]\).
  Then we have
  \begin{align*}
    \forall f \in \V, \norm{\T(f)}^2 & = \inn{\T(f), \T(f)}                        &  & \by{6.1.9}       \\
                                     & = \inn{hf, hf}                                                    \\
                                     & = \int_0^1 h(t) f(t) \conj{h(t) f(t)} \; dt                       \\
                                     & = \int_0^1 f(t) \conj{f(t)} \; dt           &  & (\abs{h(t)} = 1) \\
                                     & = \inn{f, f}                                                      \\
                                     & = \norm{f}^2.                               &  & \by{6.1.9}
  \end{align*}
  Thus by \cref{6.5.1} \(\T\) is unitary.
\end{proof}

\begin{ex}\label{ex:6.5.7}
  Prove that if \(\T\) is a unitary operator on a finite-dimensional inner product space \(\V\) over \(\C\), then \(\T\) has a unitary \emph{square root};
  that is, there exists a unitary operator \(\U\) such that \(\T = \U^2\).
\end{ex}

\begin{proof}[\pf{ex:6.5.7}]
  Suppose that \(\dim(\V) = n\).
  By \cref{6.5.5} there exists an orthonormal basis \(\beta\) for \(\V\) over \(\C\) such that \([\T]_{\beta}\) is a diagonal matrix.
  Since \([\T]_{\beta} \in \ms[n][n][\C]\), we know that there exists some \(c_i \in \C\) such that \(c_i^2 = ([\T]_{\beta})_{i i}\) for all \(i \in \set{1, \dots, n}\).
  Thus by \cref{2.6} there exists an \(\U \in \ls(\V)\) such that
  \[
    [\U]_{\beta} = \begin{pmatrix}
      c_1    & 0      & \cdots & 0      \\
      0      & c_2    & \cdots & 0      \\
      \vdots & \vdots & \ddots & \vdots \\
      0      & 0      & \cdots & c_n
    \end{pmatrix}.
  \]
  By \cref{2.11} we have
  \[
    [\T]_{\beta} = [\U]_{\beta} [\U]_{\beta} = [\U^2]_{\beta}
  \]
  and thus by \cref{2.1.13} \(\T = \U^2\).
\end{proof}

\begin{ex}\label{ex:6.5.8}
  Let \(\T\) be a self-adjoint linear operator on a finite-dimensional inner product space \(\V\) over \(\C\).
  Prove that \((\T + i \IT[\V])(\T - i \IT[\V])^{-1}\) is unitary using \cref{ex:6.4.10}.
\end{ex}

\begin{proof}[\pf{ex:6.5.8}]
  We have
  \begin{align*}
     & \pa{(\T + i \IT[\V]) (\T - i \IT[\V])^{-1}}^* (\T + i \IT[\V]) (\T - i \IT[\V])^{-1}                           \\
     & = \pa{(\T - i \IT[\V])^{-1}}^* (\T + i \IT[\V])^* (\T + i \IT[\V]) (\T - i \IT[\V])^{-1} &  & \by{6.11}[c]     \\
     & = \pa{(\T + i \IT[\V])^{-1}} (\T + i \IT[\V])^* (\T + i \IT[\V]) (\T - i \IT[\V])^{-1}   &  & \by{ex:6.4.10}   \\
     & = (\T + i \IT[\V])^{-1} (\T + i \IT[\V]) (\T + i \IT[\V])^* (\T - i \IT[\V])^{-1}        &  & \by{6.15}[b]     \\
     & = (\T + i \IT[\V])^{-1} (\T + i \IT[\V]) (\T^* - i \IT[\V]) (\T - i \IT[\V])^{-1}        &  & \by{6.11}[a,b,e] \\
     & = (\T + i \IT[\V])^{-1} (\T + i \IT[\V]) (\T - i \IT[\V]) (\T - i \IT[\V])^{-1}          &  & \by{6.4.8}       \\
     & = \IT[\V].
  \end{align*}
  Since \(\V\) is finite-dimensional, by \cref{ex:2.4.10}(b) we know that
  \[
    (\T + i \IT[\V]) (\T - i \IT[\V])^{-1} \pa{(\T + i \IT[\V]) (\T - i \IT[\V])^{-1}}^* = \IT[\V].
  \]
  Thus by \cref{6.18}(a)(e) we know that \(\T\) is unitary.
\end{proof}

\begin{ex}\label{ex:6.5.9}
  Let \(\U\) be a linear operator on a finite-dimensional inner product space \(\V\) over \(\F\).
  If \(\norm{\U(x)} = \norm{x}\) for all \(x\) in some orthonormal basis for \(\V\) over \(\F\), must \(\U\) be unitary?
  Justify your answer with a proof or a counterexample.
\end{ex}

\begin{proof}[\pf{ex:6.5.9}]
  We claim that \(\U\) may not be unitary.
  Let \(A = (e_1, e_1)\) and let \(\U = \L_A\).
  We know that \(\set{e_1, e_2}\) is an orthonormal basis for \(\vs{F}^2\) over \(\F\), and
  \[
    \norm{\U(e_1)} = \norm{e_1} = \norm{\U(e_2)} = 1.
  \]
  But \(\norm{\U(e_1 + e_2)} = \norm{2e_1} = 2 \neq \sqrt{2} = \norm{e_1 + e_2}\).
\end{proof}

\begin{ex}\label{ex:6.5.10}
  Let \(A\) be an \(n \times n\) real symmetric or complex normal matrix.
  Prove that
  \[
    \tr[A] = \sum_{i = 1}^n \lambda_i \quad \text{and} \quad \tr[A^* A] = \sum_{i = 1}^n \abs{\lambda_i}^2,
  \]
  where the \(\lambda_i\)'s are the (not necessarily distinct) eigenvalues of \(A\).
\end{ex}

\begin{proof}[\pf{ex:6.5.10}]
  Let \(\beta\) be the standard ordered basis for \(\vs{F}^n\) over \(\F\).
  By \cref{6.16,6.17} there exists an orthonormal basis \(\gamma\) for \(\vs{F}^n\) over \(\F\) such that \([\L_A]_{\gamma}\) is a diagonal matrix.
  Then we have
  \begin{align*}
    \tr[A] & = \tr[[\L_A]_{\beta}]                                                                    &  & \by{2.15}[a]   \\
           & = \tr[[\IT[\vs{F}^n]]_{\gamma}^{\beta} [\L_A]_{\gamma} [\IT[\vs{F}^n]]_{\beta}^{\gamma}] &  & \by{2.23}      \\
           & = \tr[[\L_A]_{\gamma} [\IT[\vs{F}^n]]_{\gamma}^{\beta} [\IT[\vs{F}^n]]_{\beta}^{\gamma}] &  & \by{ex:2.3.13} \\
           & = \tr[[\L_A]_{\gamma}]                                                                   &  & \by{2.23}      \\
           & = \sum_{i = 1}^n \lambda_i                                                               &  & \by{5.1.2}
  \end{align*}
  and
  \begin{align*}
    \tr[A^* A] & = \tr[[\L_{A^*}]_{\beta} [\L_A]_{\beta}]                                                                                                                                                        &  & \by{2.15}[a]   \\
               & = \tr[[\L_A^*]_{\beta} [\L_A]_{\beta}]                                                                                                                                                          &  & \by{6.3.1}     \\
               & = \tr[[\L_A]_{\beta}^* [\L_A]_{\beta}]                                                                                                                                                          &  & \by{6.10}      \\
               & = \tr[\pa{[\IT[\vs{F}^n]]_{\gamma}^{\beta} [\L_A]_{\gamma} [\IT[\vs{F}^n]]_{\beta}^{\gamma}}^* [\IT[\vs{F}^n]]_{\gamma}^{\beta} [\L_A]_{\gamma} [\IT[\vs{F}^n]]_{\beta}^{\gamma}]               &  & \by{2.23}      \\
               & = \tr[\pa{[\IT[\vs{F}^n]]_{\beta}^{\gamma}}^* \pa{[\L_A]_{\gamma}}^* \pa{[\IT[\vs{F}^n]]_{\gamma}^{\beta}}^* [\IT[\vs{F}^n]]_{\gamma}^{\beta} [\L_A]_{\gamma} [\IT[\vs{F}^n]]_{\beta}^{\gamma}] &  & \by{6.3.2}[c]  \\
               & = \tr[\pa{[\L_A]_{\gamma}}^* [\L_A]_{\gamma} \pa{[\IT[\vs{F}^n]]_{\beta}^{\gamma}}^* \pa{[\IT[\vs{F}^n]]_{\gamma}^{\beta}}^* [\IT[\vs{F}^n]]_{\gamma}^{\beta} [\IT[\vs{F}^n]]_{\beta}^{\gamma}] &  & \by{ex:2.3.13} \\
               & = \tr[\pa{[\L_A]_{\gamma}}^* [\L_A]_{\gamma} \pa{[\IT[\vs{F}^n]]_{\gamma}^{\beta} [\IT[\vs{F}^n]]_{\beta}^{\gamma}}^* [\IT[\vs{F}^n]]_{\gamma}^{\beta} [\IT[\vs{F}^n]]_{\beta}^{\gamma}]        &  & \by{6.3.2}[c]  \\
               & = \tr[\pa{[\L_A]_{\gamma}}^* [\L_A]_{\gamma} \pa{I_n}^* I_n]                                                                                                                                    &  & \by{2.23}      \\
               & = \tr[\pa{[\L_A]_{\gamma}}^* [\L_A]_{\gamma}]                                                                                                                                                   &  & \by{6.3.2}[e]  \\
               & = \tr[\conj{[\L_A]_{\gamma}} [\L_A]_{\gamma}]                                                                                                                                                   &  & \by{6.1.5}     \\
               & = \sum_{i = 1}^n \conj{\lambda_i} \lambda_i                                                                                                                                                     &  & \by{5.1.2}     \\
               & = \sum_{i = 1}^n \abs{\lambda_i}^2.                                                                                                                                                             &  & \by{d.0.5}
  \end{align*}
\end{proof}

\setcounter{ex}{11}
\begin{ex}\label{ex:6.5.12}
  Let \(A\) be an \(n \times n\) real symmetric or complex normal matrix.
  Prove that
  \[
    \det(A) = \prod_{i = 1}^n \lambda_i,
  \]
  where the \(\lambda_i\)'s are the (not necessarily distinct) eigenvalues of \(A\).
\end{ex}

\begin{proof}[\pf{ex:6.5.12}]
  Let \(\beta\) be the standard ordered basis for \(\vs{F}^n\) over \(\F\).
  By \cref{6.16,6.17} there exists an orthonormal basis \(\gamma\) for \(\vs{F}^n\) over \(\F\) such that \([\L_A]_{\gamma}\) is a diagonal matrix.
  Then we have
  \begin{align*}
    \det(A) & = \det([\L_A]_{\beta})                                                                                &  & \by{2.15}[a] \\
            & = \det([\IT[\vs{F}^n]]_{\gamma}^{\beta} [\L_A]_{\gamma} [\IT[\vs{F}^n]]_{\beta}^{\gamma})             &  & \by{2.23}    \\
            & = \det([\IT[\vs{F}^n]]_{\gamma}^{\beta}) \det([\L_A]_{\gamma}) \det([\IT[\vs{F}^n]]_{\beta}^{\gamma}) &  & \by{4.7}     \\
            & = \det([\L_A]_{\gamma}) \det([\IT[\vs{F}^n]]_{\gamma}^{\beta} [\IT[\vs{F}^n]]_{\beta}^{\gamma})       &  & \by{4.7}     \\
            & = \det([\L_A]_{\gamma}) \det(I_n)                                                                     &  & \by{2.23}    \\
            & = \det([\L_A]_{\gamma})                                                                               &  & \by{4.2.3}   \\
            & = \prod_{i = 1}^n \lambda_i.                                                                          &  & \by{5.1.2}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:6.5.13}
  Suppose that \(A, B \in \ms[n][n][\F]\) are diagonalizable matrices.
  Prove or disprove that \(A, B\) are similar iff \(A, B\) are unitarily equivalent.
\end{ex}

\begin{proof}[\pf{ex:6.5.13}]
  We claim that \(A, B\) are similar may not imply \(A, B\) are unitary equivalent.
  Let
  \[
    A = \begin{pmatrix}
      1 & 0 \\
      0 & 0
    \end{pmatrix} \quad \text{and} \quad B = \begin{pmatrix}
      1 & 1 \\
      0 & 0
    \end{pmatrix}.
  \]
  Then
  \[
    \begin{pmatrix}
      1 & 0 \\
      0 & 0
    \end{pmatrix} = \begin{pmatrix}
      1 & 1  \\
      0 & -1
    \end{pmatrix}^{-1} \begin{pmatrix}
      1 & 1 \\
      0 & 0
    \end{pmatrix} \begin{pmatrix}
      1 & 1  \\
      0 & -1
    \end{pmatrix}
  \]
  and thus by \cref{2.5.4} \(A, B\) are similar and diagonalizable.
  But \(\begin{pmatrix}
    1 & 1  \\
    0 & -1
  \end{pmatrix}\) is not unitary.
\end{proof}

\begin{ex}\label{ex:6.5.14}
  Prove that if \(A, B \in \ms[n][n][\F]\) are unitarily equivalent matrices, then \(A\) is positive definite (semidefinite) iff \(B\) is positive definite (semidefinite)
  (See \cref{6.4.11}).
\end{ex}

\begin{proof}[\pf{ex:6.5.14}]
  Since \(A, B\) are unitarily equivalent, by \cref{6.5.13} we know that there exists a \(Q \in \ms[n][n][\F]\) such that \(Q\) is unitary and \(B = Q^* A Q\).
  First suppose that \(A\) is positive definite.
  Then we have
  \begin{align*}
    B^* & = (Q^* A Q)^*                         \\
        & = Q^* A^* Q   &  & \by{ex:6.3.5}[c,d] \\
        & = Q^* A Q     &  & \by{6.4.11}        \\
        & = B
  \end{align*}
  and
  \begin{align*}
    \forall x \in \vs{F}^n, \inn{Bx, x} & = \inn{Q^* A Q x, x} &  & \by{6.5.13} \\
                                        & = \inn{AQx, Qx}      &  & \by{6.3.4}  \\
                                        & > 0.                 &  & \by{6.4.11}
  \end{align*}
  Thus by \cref{6.4.8} \(B\) is self-adjoint and by \cref{6.4.11} \(B\) is positive definite.
  By reversing the role of \(B\) and \(A\) we see that \(A = Q B Q^*\) and \(B\) is positive definite implies \(A\) is positive definite.
  Thus we conclude that \(A\) is positive definite iff \(B\) is positive definite.
  Using similar arguments we can conclude that \(A\) is positive semidefinite iff \(B\) is positive semidefinite.
\end{proof}

\begin{ex}\label{ex:6.5.15}
  Let \(\U\) be a unitary operator on an inner product space \(\V\) over \(\F\), and let \(\W\) be a finite-dimensional \(\U\)-invariant subspace of \(\V\) over \(\F\).
  Prove that
  \begin{enumerate}
    \item \(\U(\W) = \W\);
    \item \(\W^{\perp}\) is \(\U\)-invariant.
  \end{enumerate}
  Contrast (b) with \cref{ex:6.5.16}.
\end{ex}

\begin{proof}[\pf{ex:6.5.15}(a)]
  Since
  \begin{align*}
             & \U \text{ is unitary}                                           \\
    \implies & \forall x \in \V, \norm{\U(x)} = \norm{x}       &  & \by{6.5.1} \\
    \implies & \forall x \in \W, \norm{\U_{\W}(x)} = \norm{x}, &  & \by{5.4.1}
  \end{align*}
  by \cref{6.18}(a)(e) we know that \(\U_{\W}\) is invertible.
  Thus by \cref{5.4.1} we have \(\U(\W) = \U_{\W}(\W) = \W\).
\end{proof}

\begin{proof}[\pf{ex:6.5.15}(b)]
  We have
  \begin{align*}
             & x \in \W^{\perp} \setminus \set{\zv}                            \\
    \implies & x \notin \W                              &  & \by{ex:6.2.13}[d] \\
    \implies & \U(x) \notin \W                          &  & \by{ex:6.5.15}[a] \\
    \implies & \U(x) \in \W^{\perp} \setminus \set{\zv} &  & \by{ex:6.2.13}[d]
  \end{align*}
  and thus by \cref{5.4.1} \(\W^{\perp}\) is \(\U\)-invariant.
\end{proof}

\begin{ex}\label{ex:6.5.16}
  Find an example of a unitary operator \(\U\) on an inner product space \(\V\) over \(\F\) and a \(\U\)-invariant subspace \(\W\) over \(\F\) such that \(\W^{\perp}\) is not \(\U\)-invariant.
\end{ex}

\begin{proof}[\pf{ex:6.5.16}]
  The point is that \(\W\) is infinite-dimensional may not guarantee that \(\W^{\perp}\) is \(\U\)-invariant.
  Let \(\V = \spn{\set{e_n : n \in \Z^+}}\) where \(e_n\) is defined as in \cref{ex:6.2.23}(b).
  Now we use \cref{ex:2.1.34} to define \(\U \in \ls(\V)\) as follow:
  \[
    \forall n \in \Z^+, \U(e_n) = \begin{dcases}
      e_2       & \text{if } n = 1 \\
      e_{n + 2} & \text{if } n > 1
    \end{dcases}.
  \]
  Since
  \begin{align*}
             & \forall x \in \V, \exists m \in \Z^+ : x \in \spn{\set{\seq{e}{1,,m}}}         &  & \by{1.7.10}       \\
    \implies & \forall x \in \V, \exists \seq{a}{1,,m} \in \F : x = \sum_{i = 1}^m a_i e_i    &  & \by{1.6.1}        \\
    \implies & \forall x \in \V, \norm{\U(x)}^2 = \inn{\U(x), \U(x)}                          &  & \by{6.1.9}        \\
             & = \sum_{k = 1}^\infty \abs{(\U(x))(k)}^2                                       &  & \by{ex:6.2.23}    \\
             & = \sum_{k = 1}^\infty \abs{\U\pa{\sum_{i = 1}^m a_i e_i}(k)}^2                                        \\
             & = \sum_{k = 1}^\infty \abs{\pa{\sum_{i = 1}^m a_i \U(e_i)}(k)}^2               &  & \by{2.1.2}[d]     \\
             & = \sum_{k = 1}^\infty \abs{\pa{a_1 \U(e_1) + \sum_{i = 2}^m a_i \U(e_i)}(k)}^2                        \\
             & = \sum_{k = 1}^\infty \abs{\pa{a_1 e_2 + \sum_{i = 2}^m a_i e_{i + 2}}(k)}^2                          \\
             & = \sum_{k = 1}^\infty \abs{a_1 e_2(k) + \sum_{i = 2}^m a_i e_{i + 2}(k)}^2                            \\
             & = \abs{a_1}^2 + \sum_{i = 2}^m \abs{a_i}^2                                     &  & \by{ex:6.2.23}[b] \\
             & = \sum_{i = 1}^m \abs{a_i}^2                                                                          \\
             & = \sum_{k = 1}^\infty \abs{\pa{\sum_{i = 1}^m a_i e_i}(k)}^2                   &  & \by{ex:6.2.23}[b] \\
             & = \sum_{k = 1}^\infty \abs{x(k)}^2                                                                    \\
             & = \inn{x, x}                                                                   &  & \by{ex:6.2.23}    \\
             & = \norm{x}^2                                                                   &  & \by{6.1.9}        \\
    \implies & \forall x \in \V, \norm{\U(x)} = \norm{x},                                     &  & \by{6.2}[b]
  \end{align*}
  by \cref{6.5.1} we know that \(\U\) is unitary.
  Let \(\W = \spn{\set{\seq{e}{2,4,}}}\).
  Clearly \(\W\) is \(\U\)-invariant since \(\U(e_i) = e_{i + 2}\).
  By \cref{ex:6.2.23}(b) we know that \(\W^{\perp} = \spn{\set{\seq{e}{1,3,}}}\).
  But \(\W^{\perp}\) is not \(\U\)-invariant since \(\U(e_1) = e_2 \notin \W^{\perp}\).
\end{proof}

\begin{ex}\label{ex:6.5.17}
  Prove that a matrix that is both unitary and upper triangular must be a diagonal matrix.
\end{ex}

\begin{proof}[\pf{ex:6.5.17}]
  Let \(A \in \ms[n][n][\F]\) and suppose that \(A\) is unitary and upper triangular.
  For each \(i \in \set{1, \dots, n}\), let \(a_i\) be the \(i\)th column of \(A\).
  We use induction on \(k\) to show that for all \(k \in \set{1, \dots, n}\), \(a_i = A_{i i} e_i\) for all \(i \in \set{1, \dots, k}\).
  The case for \(n = 1\) is true since \(A\) is upper triangular.
  So suppose that for some \(k \geq 1\), we have \(a_i = A_{i i} e_i\) for all \(i \in \set{1, \dots, k}\).
  Then for \(k + 1\), we have
  \begin{align*}
             & A^* A = A A^* = I_n                                                                               &  & \by{6.5.9}                       \\
    \implies & \forall i \in \set{1, \dots, k}, 0 = (A^* A)_{i, k + 1} = \sum_{j = 1}^n (A^*)_{i j} A_{j, k + 1} &  & \by{2.3.1}                       \\
             & = \sum_{j = 1}^n \conj{A_{j i}} A_{j, k + 1}                                                      &  & \by{6.1.5}                       \\
             & = \sum_{j = 1}^i \conj{A_{j i}} A_{j, k + 1}                                                      &  & \by{ex:1.3.12}                   \\
             & = \conj{A_{i i}} A_{i, k + 1}                                                                     &  & \text{(by induction hypothesis)} \\
    \implies & \forall i \in \set{1, \dots, k}, A_{i, k + 1} = 0                                                 &  & (A_{i i} \neq 0)                 \\
    \implies & a_{k + 1} = A_{k + 1, k + 1} e_{k + 1}.
  \end{align*}
  And this closes the induction.
  Therefore \(A\) is a diagonal matrix.
\end{proof}

\begin{ex}\label{ex:6.5.18}
  Show that ``is unitarily equivalent to'' is an equivalence relation on \(\ms[n][n][\C]\).
\end{ex}

\begin{proof}[\pf{ex:6.5.18}]
  Let \(A, B, C \in \ms[n][n][\C]\).
  For reflexivity, we have
  \begin{align*}
             & A = I_n A I_n = I_n^* A I_n              &  & \by{6.3.2}[e] \\
    \implies & A \text{ is unitarily equivalent to } A. &  & \by{6.5.13}
  \end{align*}
  For symmetric, we have
  \begin{align*}
             & A \text{ is unitarily equivalent to } B                             \\
    \implies & \exists P \in \ms[n][n][\C] : \begin{dcases}
                                               P \text{ is unitary} \\
                                               A = P^* B P
                                             \end{dcases} &  & \by{6.5.13}         \\
    \implies & B = (P^*)^{-1} A P^{-1} = P A P^*            &  & \by{ex:6.1.23}[c] \\
             & = (P^*)^* A P^*                              &  & \by{6.3.2}[d]     \\
    \implies & B \text{ is unitarily equivalent to } A.
  \end{align*}
  For transitivity, we have
  \begin{align*}
             & \begin{dcases}
                 A \text{ is unitarily equivalent to } B \\
                 B \text{ is unitarily equivalent to } C
               \end{dcases}                            \\
    \implies & \exists P, Q \in \ms[n][n][\C] : \begin{dcases}
                                                  P, Q \text{ are unitary} \\
                                                  A = P^* B P              \\
                                                  B = Q^* C Q
                                                \end{dcases} &  & \by{6.5.13}     \\
    \implies & A = P^* Q^* C Q P = (QP)^* C (QP)               &  & \by{6.3.2}[c] \\
    \implies & A \text{ is unitarily equivalent to } C.        &  & \by{6.5.13}
  \end{align*}
  Thus ``is unitarily equivalent to'' is an equivalence relation on \(\ms[n][n][\C]\).
\end{proof}

\begin{ex}\label{ex:6.5.19}
  Let \(\W\) be a finite-dimensional subspace of an inner product space \(\V\) over \(\F\).
  By \cref{6.7}(c) and \cref{1.3.11}, \(\V = \W \oplus \W^{\perp}\).
  Define \(\U : \V \to \V\) by \(\U(v_1 + v_2) = v_1 - v_2\), where \(v_1 \in \W\) and \(v_2 \in \W^{\perp}\).
  Prove that \(\U\) is a self-adjoint unitary operator.
\end{ex}

\begin{proof}[\pf{ex:6.5.19}]
  First we show that \(\U \in \ls(\V)\).
  Let \(x, y \in \V\) and let \(c \in \F\).
  Let \((x_1, x_2), (y_1, y_2) \in \W \times \W^{\perp}\) such that \(x = x_1 + x_2\) and \(y = y_1 + y_2\).
  Since
  \begin{align*}
    \U(cx + y) & = \U(c(x_1 + x_2) + y_1 + y_2)    &  & \by{6.7}[c] \\
               & = \U((cx_1 + y_1) + (cx_2 + y_2)) &  & \by{1.2.1}  \\
               & = (cx_1 + y_1) - (cx_2 + y_2)     &  & \by{1.3}    \\
               & = c (x_1 - x_2) + (y_1 - y_2)     &  & \by{1.2.1}  \\
               & = c \U(x_1 + x_2) + \U(y_1 + y_2)                  \\
               & = c \U(x) + \U(y),                &  & \by{6.7}[c]
  \end{align*}
  by \cref{2.1.2}(b) we know that \(\U \in \ls(\V)\).

  Next we show that \(\U\) is self-adjoint.
  Since
  \begin{align*}
    \forall x, y \in \V, \inn{x, \U^*(y)} & = \inn{\U(x), y}                                                    &  & \by{6.9}        \\
                                          & = \inn{\U(x_1 + x_2), y_1 + y_2}                                    &  & \by{6.7}[c]     \\
                                          & = \inn{x_1 - x_2, y_1 + y_2}                                                             \\
                                          & = \inn{x_1, y_1 + y_2} - \inn{x_2, y_1 + y_2}                       &  & \by{6.1.1}[a,b] \\
                                          & = \inn{x_1, y_1} + \inn{x_1, y_2} - \inn{x_2, y_1} - \inn{x_2, y_2} &  & \by{6.1}[a]     \\
                                          & = \inn{x_1, y_1} - \inn{x_2, y_2}                                   &  & \by{6.7}[c]     \\
                                          & = \inn{x_1, y_1} - \inn{x_1, y_2} + \inn{x_2, y_1} - \inn{x_2, y_2} &  & \by{6.7}[c]     \\
                                          & = \inn{x_1, y_1 - y_2} + \inn{x_2, y_1 - y_2}                       &  & \by{6.1}[a,b]   \\
                                          & = \inn{x_1 + x_2, y_1 - y_2}                                        &  & \by{6.1.1}[a]   \\
                                          & = \inn{x_1 + x_2, \U(y_1 + y_2)}                                                         \\
                                          & = \inn{x, \U(y)},                                                   &  & \by{6.7}[c]
  \end{align*}
  by \cref{6.1}(e) we know that \(\U^* = \U\).
  Thus by \cref{6.4.8} \(\U\) is self-adjoint.

  Finally we show that \(\U\) is unitary.
  Since
  \begin{align*}
    \forall x \in \V, \norm{\U(x)}^2 & = \norm{\U(x_1 + x_2)}^2      &  & \by{6.7}[c]    \\
                                     & = \norm{x_1 - x_2}^2                              \\
                                     & = \norm{x_1}^2 + \norm{x_2}^2 &  & \by{ex:6.1.10} \\
                                     & = \norm{x_1 + x_2}^2          &  & \by{ex:6.1.10} \\
                                     & = \norm{x}^2,                 &  & \by{6.7}[c]
  \end{align*}
  by \cref{6.5.1} we know that \(\U\) is unitary.
\end{proof}

\begin{ex}\label{ex:6.5.20}
  Let \(\V\) be a finite-dimensional inner product space over \(\F\).
  A linear operator \(\U\) on \(\V\) is called a \textbf{partial isometry} if there exists a subspace \(\W\) of \(\V\) over \(\F\) such that \(\norm{\U(x)} = \norm{x}\) for all \(x \in \W\) and \(\U(x) = \zv\) for all \(x \in \W^{\perp}\).
  Observe that \(\W\) need \emph{not} to be \(\U\)-invariant.
  Suppose that \(\U\) is such an operator and \(\set{\seq{v}{1,,k}}\) is an orthonormal basis for \(\W\) over \(\F\).
  Prove the following results.
  \begin{enumerate}
    \item \(\inn{\U(x), \U(y)} = \inn{x, y}\) for all \(x, y \in \W\).
    \item \(\set{\U(v_1), \dots, \U(v_k)}\) is an orthonormal basis for \(\rg{\U}\) over \(\F\).
    \item There exists an orthonormal basis \(\gamma\) for \(\V\) over \(\F\) such that the first \(k\) columns of \([\U]_{\gamma}\) form an orthonormal set and the remaining columns are zero.
    \item Let \(\set{\seq{w}{1,,j}}\) be an orthonormal basis for \(\rg{\U}^{\perp}\) and
          \[
            \beta = \set{\U(v_1), \dots, \U(v_k), \seq{w}{1,,j}}.
          \]
          Then \(\beta\) is an orthonormal basis for \(\V\) over \(\F\).
    \item Let \(\T\) be the linear operator on \(\V\) that satisfies \(\T(\U(v_i)) = v_i\) where \(i \in \set{1, \dots, k}\) and \(\T(w_i) = \zv\) where \(i \in \set{1, \dots, j}\).
          Then \(\T\) is well defined, and \(\T = \U^*\).
    \item \(\U^*\) is a partial isometry.
  \end{enumerate}
  This exercise is continued in \cref{ex:6.6.9}.
\end{ex}

\begin{proof}[\pf{ex:6.5.20}(a)]
  Let \(x, y \in \W\).
  If \(\F = \R\), then we have
  \begin{align*}
    \inn{\U(x), \U(y)} & = \dfrac{1}{4} \norm{\U(x) + \U(y)}^2 - \dfrac{1}{4} \norm{\U(x) - \U(y)}^2 &  & \by{ex:6.1.20}  \\
                       & = \dfrac{1}{4} \norm{\U(x + y)}^2 - \dfrac{1}{4} \norm{\U(x - y)}^2         &  & \by{2.1.2}[b,c] \\
                       & = \dfrac{1}{4} \norm{x + y}^2 - \dfrac{1}{4} \norm{x - y}^2                 &  & \by{ex:6.5.20}  \\
                       & = \inn{x, y}.                                                               &  & \by{ex:6.1.20}
  \end{align*}
  If \(\F = \C\), then we have
  \begin{align*}
    \inn{\U(x), \U(y)} & = \dfrac{1}{4} \sum_{k = 1}^4 i^k \norm{\U(x) + i^k \U(y)}^2 &  & \by{ex:6.1.20} \\
                       & = \dfrac{1}{4} \sum_{k = 1}^4 i^k \norm{\U(x + i^k y)}^2     &  & \by{2.1.2}[b]  \\
                       & = \dfrac{1}{4} \sum_{k = 1}^4 i^k \norm{x + i^k y}^2         &  & \by{ex:6.5.20} \\
                       & = \inn{x, y}.                                                &  & \by{ex:6.1.20}
  \end{align*}
  Thus we conclude that \(\inn{\U(x), \U(y)} = \inn{x, y}\) for all \(x, y \in \W\).
\end{proof}

\begin{proof}[\pf{ex:6.5.20}(b)]
  Since \(\alpha' = \set{\seq{v}{1,,k}}\) is a basis for \(\W\) over \(\F\), by \cref{1.6.15}(c) we can extend \(\alpha'\) to another basis \(\alpha\) for \(\V\) over \(\F\).
  By \cref{6.7}(c) we know that \(\alpha \setminus \alpha'\) is a basis for \(\W^{\perp}\) over \(\F\).
  By \cref{ex:6.5.20} we have \(\U(\alpha \setminus \alpha') = \set{\zv}\).
  Since
  \begin{align*}
    \forall i, j \in \set{1, \dots, k}, \delta_{i j} & = \inn{v_i, v_j}          &  & \by{6.1.12}       \\
                                                     & = \inn{\U(v_i), \U(v_j)}, &  & \by{ex:6.5.20}[a]
  \end{align*}
  by \cref{6.1.12} we know that \(\set{\U(v_1), \dots, \U(v_k)}\) is orthonormal with respect to \(\inn{\cdot, \cdot}\).
  By \cref{2.2,6.2.4} we conclude that \(\set{\U(v_1), \dots, \U(v_k)}\) is an orthonormal basis for \(\rg{\U}\) over \(\F\).
\end{proof}

\begin{proof}[\pf{ex:6.5.20}(c)]
  If \(\dim(\V) = \dim(\W)\), then by \cref{ex:6.5.20}(b) we are done.
  So suppose that \(\dim(\V) > \dim(\W)\) (\cref{1.11}).
  Since \(\V\) is finite-dimensional and \(\W^{\perp}\) is a subspace of \(\V\) over \(\F\) (\cref{6.2.10}), we know that there exists an orthonormal basis \(\alpha\) for \(\W^{\perp}\) over \(\F\) with respect to \(\inn{\cdot, \cdot}\) (\cref{6.5}).
  Let \(\gamma = \set{\seq{v}{1,,k}} \cup \alpha\).
  By \cref{6.2.4} and \cref{1.6.15}(b) we know that \(\gamma\) is an orthonormal basis for \(\V\) over \(\F\) with respect to \(\inn{\cdot, \cdot}\).
  By definition we know that \(\U(\alpha) = \set{\zv}\).
  Thus by \cref{ex:6.5.20}(b) we know that the first \(k\) column of \([\U]_{\gamma}\) form an orthonormal set and the remaining columns are zeros.
\end{proof}

\begin{proof}[\pf{ex:6.5.20}(d)]
  By \cref{2.2}, \cref{6.7}(d) and \cref{ex:6.5.20}(b) we know that this is true.
\end{proof}

\begin{proof}[\pf{ex:6.5.20}(e)]
  By \cref{2.6} we know that \(\T\) is well-defined.
  Now we show that \(\T = \U^*\) where \(\U^*\) is the adjoint operator of \(\U\) with respect to \(\inn{\cdot, \cdot}\).
  By \cref{ex:6.1.9}(b) we only need to show that \(\inn{x, \T(y)} = \inn{x, \U^*(y)}\) for all \(x, y \in \beta\).
  We now split into four cases:
  \begin{itemize}
    \item If \(x, y \in \set{\U(v_1), \dots, \U(v_k)}\), then \(x = \U(v_p)\) and \(y = \U(v_q)\) for some \(p, q \in \set{1, \dots, k}\).
          By \cref{6.7}(c) we can split into two further cases:
          \begin{itemize}
            \item If \(x = \U(v_p) \in \W\), then we have
                  \begin{align*}
                    \inn{x, \T(y)} & = \inn{\U(v_p), \T(\U(v_q))}                          \\
                                   & = \inn{\U(v_p), v_q}           &  & \by{ex:6.5.20}[e] \\
                                   & = \inn{\U^2(v_p), \U(v_q)}     &  & \by{ex:6.5.20}[a] \\
                                   & = \inn{\U(v_p), \U^*(\U(v_q))} &  & \by{6.9}          \\
                                   & = \inn{x, \U^*(y)}.
                  \end{align*}
            \item If \(x = \U(v_p) \in \W^{\perp}\), then we have
                  \begin{align*}
                    \inn{x, \U^*(y)} & = \inn{\U(x), y}       &  & \by{6.9}          \\
                                     & = \inn{\zv, y}         &  & \by{ex:6.5.20}    \\
                                     & = 0                    &  & \by{6.1}[c]       \\
                                     & = \inn{x, v_q}         &  & \by{ex:6.5.20}    \\
                                     & = \inn{x, \T(\U(v_q))} &  & \by{ex:6.5.20}[e] \\
                                     & = \inn{x, \T(y)}.
                  \end{align*}
          \end{itemize}
          Thus we have \(\inn{x, \U^*(y)} = \inn{x, \T(y)}\) for all \(x, y \in \set{\U(v_1), \dots, \U(v_k)}\).
    \item If \(x \in \set{\U(v_1), \dots, \U(v_k)}\) and \(y \in \set{\seq{w}{1,,j}}\), then \(x = \U(v_p)\) and \(y = w_q\) for some \(p \in \set{1, \dots, k}\) and \(q \in \set{1, \dots, j}\).
          Thus
          \begin{align*}
            \inn{x, \U^*(y)} & = \inn{\U(v_p), \U^*(w_q)}                        \\
                             & = \inn{\U^2(v_p), w_q}     &  & \by{6.9}          \\
                             & = 0                        &  & \by{ex:6.5.20}[d] \\
                             & = \inn{\U(v_p), \zv}       &  & \by{6.1}[c]       \\
                             & = \inn{\U(v_p), \T(w_q)}   &  & \by{ex:6.5.20}[e] \\
                             & = \inn{x, \T(y)}.
          \end{align*}
    \item If \(x \in \set{\seq{w}{1,,j}}\) and \(y \in \set{\U(v_1), \dots, \U(v_k)}\), then \(x = w_p\) and \(y = \U(v_q)\) for some \(p \in \set{1, \dots, j}\) and \(q \in \set{1, \dots, k}\).
          By \cref{6.7}(c) we can split into two further cases:
          \begin{itemize}
            \item If \(x = w_p \in \W\), then we have
                  \begin{align*}
                    \inn{x, \U^*(y)} & = \inn{w_p, \U^*(\U(v_q))}                        \\
                                     & = \inn{\U(w_p), \U(v_q)}   &  & \by{6.9}          \\
                                     & = \inn{w_p, v_q}           &  & \by{ex:6.5.20}[a] \\
                                     & = \inn{w_p, \T(\U(v_q))}   &  & \by{ex:6.5.20}[e] \\
                                     & = \inn{x, \T(y)}.
                  \end{align*}
            \item If \(x = w_p \in \W^{\perp}\), then we have
                  \begin{align*}
                    \inn{x, \U^*(y)} & = \inn{\U(x), y}       &  & \by{6.9}          \\
                                     & = \inn{\zv, y}         &  & \by{ex:6.5.20}    \\
                                     & = 0                    &  & \by{6.1}[c]       \\
                                     & = \inn{x, v_q}         &  & \by{ex:6.5.20}    \\
                                     & = \inn{x, \T(\U(v_q))} &  & \by{ex:6.5.20}[e] \\
                                     & = \inn{x, \T(y)}.
                  \end{align*}
          \end{itemize}
          Thus we have \(\inn{x, \U^*(y)} = \inn{x, \T(y)}\) for all \(x \in \set{\U(v_1), \dots, \U(v_k)}\) and \(y \in \set{\seq{w}{1,,j}}\).
    \item If \(x, y \in \set{\seq{w}{1,,j}}\), then \(x = w_p\) and \(y = w_q\) for some \(p, q \in \set{1, \dots, j}\).
          Thus
          \begin{align*}
            \inn{x, \U^*(y)} & = \inn{\U(x), y}                         \\
                             & = 0               &  & \by{ex:6.5.20}[d] \\
                             & = \inn{x, \zv}    &  & \by{6.1}[c]       \\
                             & = \inn{x, \T(y)}. &  & \by{ex:6.5.20}[e]
          \end{align*}
  \end{itemize}
  From all cases above we conclude that \(\inn{x, \U^*(y)} = \inn{x, \T(y)}\) for all \(x, y \in \beta\).
  And thus \(\T = \U^*\).
\end{proof}

\begin{proof}[\pf{ex:6.5.20}(f)]
  By \cref{ex:6.5.20} we need to find a subspace \(\vs{X}\) of \(\V\) such that \(\norm{U^*(x)} = \norm{x}\) for all \(x \in \vs{X}\) and \(\U^*(\vs{X}^{\perp}) = \set{\zv}\).
  We claim that \(\rg{\U}\) is a subspace that makes \(\U^*\) partially isometry.
  Let \(x \in \V\).
  By \cref{ex:6.5.20}(d) we know that there exist some \(\seq{a}{1,,k}, \seq{b}{1,,j} \in \F\) such that \(x = \sum_{i = 1}^k a_i \U(v_i) + \sum_{i = 1}^j b_i w_j\).
  Then we have
  \begin{align*}
    \norm{\U^*(x)} & = \norm{\U^*\pa{\sum_{i = 1}^k a_i \U(v_i) + \sum_{i = 1}^j b_i w_j}}                           \\
                   & = \norm{\sum_{i = 1}^k a_i \U^*(\U(v_i)) + \sum_{i = 1}^j b_i \U^*(w_j)} &  & \by{2.1.2}[d]     \\
                   & = \norm{\sum_{i = 1}^k a_i v_i}.                                         &  & \by{ex:6.5.20}[d]
  \end{align*}
  Now we split into two cases:
  \begin{itemize}
    \item If \(x \in \rg{\U}\), then \(\seq[=]{b}{1,,j} = 0\) and
          \begin{align*}
            \norm{\U^*(x)} & = \norm{\U^*\pa{\sum_{i = 1}^k a_i \U(v_i)}}                        \\
                           & = \norm{\sum_{i = 1}^k a_i \U^*(\U(v_i))}    &  & \by{2.1.2}[d]     \\
                           & = \norm{\sum_{i = 1}^k a_i v_i}              &  & \by{ex:6.5.20}[e] \\
                           & = \norm{\U\pa{\sum_{i = 1}^k a_i v_i}}       &  & \by{ex:6.5.20}    \\
                           & = \norm{\sum_{i = 1}^k a_i \U(v_i)}          &  & \by{2.1.2}[d]     \\
                           & = \norm{x}.
          \end{align*}
    \item If \(x \in \rg{\U}^{\perp}\), then \(\seq[=]{a}{1,,k} = 0\) and
          \begin{align*}
            \norm{\U^*(x)} & = \norm{\U^*\pa{\sum_{i = 1}^j b_i w_i}}                        \\
                           & = \norm{\sum_{i = 1}^k b_i \U^*(w_i)}    &  & \by{2.1.2}[d]     \\
                           & = \norm{\zv}                             &  & \by{ex:6.5.20}[e] \\
                           & = 0.                                     &  & \by{6.2}[b]
          \end{align*}
  \end{itemize}
  From all cases above we see that \(\U^*\) is partially isometry.
\end{proof}

\begin{ex}\label{ex:6.5.21}
  Let \(A, B \in \ms[n][n][\C]\) that are unitarily equivalent.
  \begin{enumerate}
    \item Prove that \(\tr[A^* A] = \tr[B^* B]\).
    \item Use (a) to prove that
          \[
            \sum_{i = 1}^n \sum_{j = 1}^n \abs{A_{i j}}^2 = \sum_{i = 1}^n \sum_{j = 1}^n \abs{B_{i j}}^2.
          \]
    \item Use (b) to show that the matrices
          \[
            C = \begin{pmatrix}
              1 & 2 \\
              2 & i
            \end{pmatrix} \quad \text{and} \quad D = \begin{pmatrix}
              i & 4 \\
              1 & 1
            \end{pmatrix}
          \]
          are \emph{not} unitarily equivalent.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.5.21}(a)]
  Since \(A, B\) are unitary equivalent, by \cref{6.5.13} there exists a \(Q \in \ms[n][n][\C]\) such that \(Q\) is unitary and \(B = Q^* A Q\).
  Then we have
  \begin{align*}
    \tr[A^* A] & = \tr[(Q B Q^*)^* (Q B Q^*)] &  & \by{ex:6.1.23}[c] \\
               & = \tr[Q B^* Q^* Q B Q^*]     &  & \by{6.3.2}[c]     \\
               & = \tr[Q B^* B Q^*]           &  & \by{ex:6.1.23}[c] \\
               & = \tr[B^* B Q Q^*]           &  & \by{ex:2.3.13}    \\
               & = \tr[B^* B].                &  & \by{ex:6.1.23}[c]
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.5.21}(b)]
  We have
  \begin{align*}
    \sum_{i = 1}^n \sum_{j = 1}^n \abs{A_{i j}}^2 & = \sum_{i = 1}^n \sum_{j = 1}^n \conj{A_{i j}} A_{i j} &  & \by{d.0.5}        \\
                                                  & = \sum_{i = 1}^n \sum_{j = 1}^n (A^*)_{j i} A_{i j}    &  & \by{6.1.5}        \\
                                                  & = \sum_{j = 1}^n \sum_{i = 1}^n (A^*)_{j i} A_{i j}    &  & \by{1.2.1}        \\
                                                  & = \sum_{j = 1}^n (A^* A)_{j j}                         &  & \by{2.3.1}        \\
                                                  & = \tr[A^* A]                                           &  & \by{1.3.9}        \\
                                                  & = \tr[B^* B]                                           &  & \by{ex:6.5.21}[a] \\
                                                  & = \sum_{j = 1}^n (B^* B)_{j j}                         &  & \by{1.3.9}        \\
                                                  & = \sum_{j = 1}^n \sum_{i = 1}^n (B^*)_{j i} B_{i j}    &  & \by{2.3.1}        \\
                                                  & = \sum_{j = 1}^n \sum_{i = 1}^n \conj{B_{i j}} B_{i j} &  & \by{6.1.5}        \\
                                                  & = \sum_{j = 1}^n \sum_{i = 1}^n \abs{B_{i j}}^2        &  & \by{d.0.5}        \\
                                                  & = \sum_{i = 1}^n \sum_{j = 1}^n \abs{B_{i j}}^2.       &  & \by{1.2.1}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.5.21}(c)]
  Since
  \[
    \sum_{i = 1}^n \sum_{j = 1}^n \abs{C_{i j}}^2 = 10 \neq 19 = \sum_{i = 1}^n \sum_{j = 1}^n \abs{D_{i j}}^2,
  \]
  by \cref{ex:6.5.21}(b) we know that \(A, B\) are not unitary equivalent.
\end{proof}

\begin{ex}\label{ex:6.5.22}
  Let \(\V\) be a real inner product space.
  \begin{enumerate}
    \item Prove that any translation on \(\V\) is a rigid motion.
    \item Prove that the composite of any two rigid motions on \(\V\) is a rigid motion on \(\V\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.5.22}(a)]
  Let \(g : \V \to \V\) be a translation.
  By \cref{6.5.16} there exists a \(v_0 \in \V\) such that \(g(x) = x + v_0\) for all \(x \in \V\).
  Then we have
  \begin{align*}
    \forall x, y \in \V, \norm{g(x) - g(y)} & = \norm{x + v_0 - (y + v_0)} &  & \by{6.5.16} \\
                                            & = \norm{x - y}               &  & \by{1.2.1}
  \end{align*}
  and thus by \cref{6.5.14} \(g\) is a rigid motion.
\end{proof}

\begin{proof}[\pf{ex:6.5.22}(b)]
  Let \(g, h: \V \to \V\) be two rigid motions.
  Then we have
  \begin{align*}
    \forall x, y \in \V, \norm{(g \circ h)(x) - (g \circ h)(y)} & = \norm{g(h(x)) - g(h(y))}                  \\
                                                                & = \norm{h(x) - h(y)}       &  & \by{6.5.14} \\
                                                                & = \norm{x - y}.            &  & \by{6.5.14}
  \end{align*}
  Thus \(g \circ h\) is a rigid motion.
\end{proof}

\begin{ex}\label{ex:6.5.23}
  Prove the following variation of \cref{6.22}:
  If \(f : \V \to \V\) is a rigid motion on a finite-dimensional real inner product space \(\V\) over \(\F\), then there exists an unique orthogonal operator \(\T\) on \(\V\) and an unique translation \(g\) on \(\V\) such that \(f = \T \circ g\).
\end{ex}

\begin{proof}[\pf{ex:6.5.23}]
  Since \(f\) is a rigid motion, by \cref{6.22} we know that there exists an unique orthogonal operator \(\U\) on \(\V\) and an unique translation \(h\) on \(\V\) such that \(f = h \circ \U\).
  Then we have
  \begin{align*}
    f & = h \circ \U                                          \\
      & = \IT[\V] \circ h \circ \U        &  & \by{2.1.9}     \\
      & = \U \circ \U^* \circ h \circ \U. &  & \by{6.18}[a,e]
  \end{align*}
  Now we show that \(\U^* \circ h \circ \U\) is a translation.
  Since \(h\) is a translation, by \cref{6.5.16} there exists a \(v_0 \in \V\) such that \(h(x) = x + v_0\) for all \(x \in \V\).
  Then we have
  \begin{align*}
    \forall x \in \V, (\U^* \circ h \circ \U)(x) & = \U^*(h(\U(x)))                              \\
                                                 & = \U^*(\U(x) + v_0)       &  & \by{6.5.16}    \\
                                                 & = \U^*(\U(x)) + \U^*(v_0) &  & \by{2.1.1}[a]  \\
                                                 & = x + \U^*(v_0)           &  & \by{6.18}[a,e]
  \end{align*}
  and thus by \cref{6.5.16} \(\U^* \circ h \circ \U\) is a translation.
  By setting \(\T = \U\) and \(g = \U^* \circ h \circ \U\) we see that \(f = \T \circ g\).

  Now we show the uniqueness of \(\T\) and \(g\).
  Suppose that \(u_0\) and \(v_0\) are in \(\V\) and \(\T\) and \(\U\) are orthogonal operators on \(\V\) such that
  \[
    \forall x \in \V, f(x) = \T(x + u_0) = \T(x) + \T(u_0) = \U(x + v_0) = \U(x) + \U(v_0).
  \]
  Substituting \(x = \zv\) in the preceding equation yields \(\T(u_0) = \U(v_0)\), and hence
  \begin{align*}
             & \forall x \in \V, \T(x) = \U(x)                     \\
    \implies & \T = \U                                             \\
    \implies & u_0 = v_0.                      &  & \by{6.18}[a,e]
  \end{align*}
\end{proof}

\begin{ex}\label{ex:6.5.24}
  Let \(\T\) and \(\U\) be orthogonal operators on \(\R^2\).
  Use \cref{6.23} to prove the following results.
  \begin{enumerate}
    \item If \(\T\) and \(\U\) are both reflections about lines through the origin, then \(\U \T\) is a rotation.
    \item If \(\T\) is a rotation and \(\U\) is a reflection about a line through the origin, then both \(\U \T\) and \(\T \U\) are reflections about lines through the origin.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.5.24}(a)]
  Let \(\beta\) be the standard ordered basis for \(\R^2\) over \(\R\).
  Then we have
  \begin{align*}
    \forall x \in \V, \norm{\T(\U(x))} & = \norm{\U(x)} &  & \by{6.5.1} \\
                                       & = \norm{x}     &  & \by{6.5.1}
  \end{align*}
  and
  \begin{align*}
    \det([\U \T]_{\beta}) & = \det([\U]_{\beta} [\T]_{\beta})       &  & \by{2.11}    \\
                          & = \det([\U]_{\beta}) \det([\T]_{\beta}) &  & \by{4.7}     \\
                          & = (-1) (-1)                             &  & \by{6.23}[b] \\
                          & = 1.
  \end{align*}
  Thus by \cref{6.23}(a) we know that \(\U \T\) is a rotation on \(\R^2\) over \(\R\).
\end{proof}

\begin{proof}[\pf{ex:6.5.24}(b)]
  Let \(\beta\) be the standard ordered basis for \(\R^2\) over \(\R\).
  Since
  \begin{align*}
    \forall x \in \V, \norm{\T(\U(x))} & = \norm{\U(x)}     &  & \by{6.5.1} \\
                                       & = \norm{x}         &  & \by{6.5.1} \\
                                       & = \norm{\T(x)}     &  & \by{6.5.1} \\
                                       & = \norm{\U(\T(x))} &  & \by{6.5.1}
  \end{align*}
  and
  \begin{align*}
    \det([\U \T]_{\beta}) & = \det([\U]_{\beta} [\T]_{\beta})       &  & \by{2.11}      \\
                          & = \det([\U]_{\beta}) \det([\T]_{\beta}) &  & \by{4.7}       \\
                          & = (-1) 1                                &  & \by{6.23}[a,b] \\
                          & = -1                                                        \\
                          & = 1 (-1)                                                    \\
                          & = \det([\T]_{\beta}) \det([\U]_{\beta}) &  & \by{6.23}[a,b] \\
                          & = \det([\T]_{\beta} [\U]_{\beta})       &  & \by{4.7}       \\
                          & = \det([\T \U]_{\beta}),                &  & \by{2.11}
  \end{align*}
  by \cref{6.23}(b) we know that \(\U \T\) and \(\T \U\) are reflections about lines through origin on \(\R^2\) over \(\R\).
\end{proof}

\begin{ex}\label{ex:6.5.25}
  Suppose that \(\T\) and \(\U\) are reflections of \(\R^2\) about the respective lines \(\L\) and \(\L'\) through the origin and that \(\phi\) and \(\psi\) are the angles from the positive \(x\)-axis to \(\L\) and \(\L'\), respectively.
  By \cref{ex:6.5.24}(a), \(\U \T\) is a rotation.
  Find its angle of rotation.
\end{ex}

\begin{proof}[\pf{ex:6.5.25}]
  Let \(\beta\) be the standard ordered basis for \(\R^2\) over \(\R\).
  Then we have
  \begin{align*}
    [\U \T]_{\beta} & = [\U]_{\beta} [\T]_{\beta}                       &  & \by{2.11} \\
                    & = \begin{pmatrix}
                          \cos(2 \psi) & \sin(2 \psi)  \\
                          \sin(2 \psi) & -\cos(2 \psi)
                        \end{pmatrix} \begin{pmatrix}
                                        \cos(2 \phi) & \sin(2 \phi)  \\
                                        \sin(2 \phi) & -\cos(2 \phi)
                                      \end{pmatrix}                   &  & \by{6.5.12} \\
                    & = \begin{pmatrix}
                          \cos(2 \psi - 2 \phi) & -\sin(2 \psi - 2 \phi) \\
                          \sin(2 \psi - 2 \phi) & \cos(2 \psi - 2 \phi)
                        \end{pmatrix}. &  & \by{2.3.1}
  \end{align*}
  Thus the rotation angle is \(2 \psi - 2 \phi\).
\end{proof}

\begin{ex}\label{ex:6.5.26}
  Suppose that \(\T\) and \(\U\) are orthogonal operators on \(\R^2\) such that \(\T\) is the rotation by the angle \(\phi\) and \(\U\) is the reflection about the line \(\L\) through the origin.
  Let \(\psi\) be the angle from the positive \(x\)-axis to \(\L\).
  By \cref{ex:6.5.24}(b), both \(\U \T\) and \(\T \U\) are reflections about lines \(\L_1\) and \(\L_2\), respectively, through the origin.
  \begin{enumerate}
    \item Find the angle \(\theta\) from the positive \(x\)-axis to \(\L_1\).
    \item Find the angle \(\theta\) from the positive \(x\)-axis to \(\L_2\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.5.26}(a)]
  Let \(\beta\) be the standard ordered basis for \(\R^2\) over \(\R\).
  Then we have
  \begin{align*}
    [\U \T]_{\beta} & = [\U]_{\beta} [\T]_{\beta}                   &  & \by{2.11} \\
                    & = \begin{pmatrix}
                          \cos(2 \psi) & \sin(2 \psi)  \\
                          \sin(2 \psi) & -\cos(2 \psi)
                        \end{pmatrix} [\T]_{\beta}               &  & \by{6.5.12}  \\
                    & = \begin{pmatrix}
                          \cos(2 \psi) & \sin(2 \psi)  \\
                          \sin(2 \psi) & -\cos(2 \psi)
                        \end{pmatrix} \begin{pmatrix}
                                        \cos(\phi) & -\sin(\phi) \\
                                        \sin(\phi) & \cos(\phi)
                                      \end{pmatrix}               &  & \by{6.4.5}  \\
                    & = \begin{pmatrix}
                          \cos(2 \psi - \phi) & \sin(2 \psi - \phi)  \\
                          \sin(2 \psi - \phi) & -\cos(2 \psi - \phi)
                        \end{pmatrix}. &  & \by{2.3.1}
  \end{align*}
  Thus the angle from the positive \(x\)-axis to \(\L_1\) is \(\psi - \dfrac{1}{2} \phi\).
\end{proof}

\begin{proof}[\pf{ex:6.5.26}(b)]
  Let \(\beta\) be the standard ordered basis for \(\R^2\) over \(\R\).
  Then we have
  \begin{align*}
    [\T \U]_{\beta} & = [\T]_{\beta} [\U]_{\beta}                   &  & \by{2.11} \\
                    & = [\T]_{\beta} \begin{pmatrix}
                                       \cos(2 \psi) & \sin(2 \psi)  \\
                                       \sin(2 \psi) & -\cos(2 \psi)
                                     \end{pmatrix}               &  & \by{6.5.12}  \\
                    & = \begin{pmatrix}
                          \cos(\phi) & -\sin(\phi) \\
                          \sin(\phi) & \cos(\phi)
                        \end{pmatrix} \begin{pmatrix}
                                        \cos(2 \psi) & \sin(2 \psi)  \\
                                        \sin(2 \psi) & -\cos(2 \psi)
                                      \end{pmatrix}               &  & \by{6.4.5}  \\
                    & = \begin{pmatrix}
                          \cos(\phi + 2 \psi) & \sin(\phi + 2 \psi)  \\
                          \sin(\phi + 2 \psi) & -\cos(\phi + 2 \psi)
                        \end{pmatrix}. &  & \by{2.3.1}
  \end{align*}
  Thus the angle from the positive \(x\)-axis to \(\L_2\) is \(\dfrac{1}{2} \phi + \psi\).
\end{proof}

\setcounter{ex}{28}
\begin{ex}[\(QR\)-Factorization]\label{ex:6.5.29}
  Let \(\seq{w}{1,,n}\) be linearly independent vectors in \(\vs{F}^n\), and let \(\seq{v}{1,,n}\) be the orthogonal vectors obtained from \(\seq{w}{1,,n}\) by the Gram--Schmidt process.
  Let \(\seq{u}{1,,n}\) be the orthonormal basis obtained by normalizing the \(v_i\)'s.
  \begin{enumerate}
    \item Solving \cref{eq:6.2.1} for \(w_k\) in terms of \(u_k\), show that
          \[
            w_k = \norm{v_k} u_k + \sum_{j = 1}^{k - 1} \inn{w_k, u_j} u_j \quad \text{for all } k \in \set{1, \dots, n}.
          \]
    \item Let \(A\) and \(Q\) denote the \(n \times n\) matrices in which the \(k\)th columns are \(w_k\) and \(u_k\), respectively.
          Define \(R \in \ms[n][n][\F]\) by
          \[
            R_{j k} = \begin{dcases}
              \norm{v_j}     & \text{if } j = k \\
              \inn{w_k, u_j} & \text{if } j < k \\
              0              & \text{if } j > k
            \end{dcases}.
          \]
          Prove \(A = QR\).
    \item Since \(Q\) is unitary (orthogonal) and \(R\) is upper triangular in (b), we have shown that every invertible matrix is the product of a unitary (orthogonal) matrix and an upper triangular matrix.
          Suppose that \(A \in \ms[n][n][\F]\) is invertible and \(A = Q_1 R_1 = Q_2 R_2\), where \(Q_1, Q_2 \in \ms[n][n][\F]\) are unitary and \(R_1, R_2 \in \ms[n][n][\F]\) are upper triangular.
          Prove that \(D = R_2 R_1^{-1}\) is an unitary diagonal matrix.
    \item The \(QR\) factorization described in (b) provides an orthogonalization method for solving a linear system \(Ax = b\) when \(A\) is invertible.
          Decompose \(A\) to \(QR\), by the Gram--Schmidt process or other means, where \(Q\) is unitary and \(R\) is upper triangular.
          Then \(QRx = b\), and hence \(Rx = Q^* b\).
          This last system can be easily solved since \(R\) is upper triangular.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.5.29}(a)]
  For all \(k \in \set{1, \dots, n}\), we have
  \begin{align*}
    w_k & = v_k + \sum_{j = 1}^{k - 1} \dfrac{\inn{w_k, v_j}}{\norm{v_j}^2} v_j                            &  & \by{6.4}    \\
        & = v_k + \sum_{j = 1}^{k - 1} \inn{w_k, \dfrac{1}{\norm{v_j}} v_j} \pa{\dfrac{1}{\norm{v_j}} v_j} &  & \by{6.1}[b] \\
        & = \norm{v_k} u_k + \sum_{j = 1}^{k - 1} \inn{w_k, u_j} u_j.                                      &  & \by{6.1.12}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.5.29}(b)]
  Let \(i, k \in \set{1, \dots, n}\).
  Then we have
  \begin{align*}
    (QR)_{i k} & = \sum_{j = 1}^n Q_{i j} R_{j k}                                                                            &  & \by{2.3.1}        \\
               & = \sum_{j = 1}^n (u_j)_i R_{j k}                                                                                                   \\
               & = \pa{\sum_{j = 1}^{k - 1} (u_j)_i R_{j k}} + (u_k)_i R_{k k} + \pa{\sum_{j = k + 1}^n (u_j)_i R_{j k}}                            \\
               & = \pa{\sum_{j = 1}^{k - 1} (u_j)_i \inn{w_k, u_j}} + (u_k)_i \norm{v_k} + \pa{\sum_{j = k + 1}^n (u_j)_i 0}                        \\
               & = \pa{\norm{v_k} u_k + \sum_{j = 1}^{k - 1} \inn{w_k, u_j} u_j}_i                                           &  & \by{1.2.4}        \\
               & = (w_k)_i                                                                                                   &  & \by{ex:6.5.29}[a] \\
               & = A_{i k}.
  \end{align*}
  Thus by \cref{1.2.8} we know that \(A = QR\).
\end{proof}

\begin{proof}[\pf{ex:6.5.29}(c)]
  Since \(A\) is invertible, by \cref{3.2.2,3.4} we know that \(R_1\) is invertible.
  Since
  \begin{align*}
    (Q_2^* Q_1)^* (Q_2^* Q_1) & = Q_1^* Q_2 Q_2^* Q_1       &  & \by{6.3.2}[c]   \\
                              & = Q_1^* Q_1                 &  & \by{6.5.9}      \\
                              & = I_n                       &  & \by{6.5.9}      \\
                              & = (Q_2^* Q_1) (Q_2^* Q_1)^* &  & \by{6.3.2}[c,e]
  \end{align*}
  and
  \begin{align*}
             & Q_1 R_1 = Q_2 R_2                                  \\
    \implies & Q_2^{-1} Q_1 = R_2 R_1^{-1} &  & \by{2.4.3}        \\
    \implies & Q_2^* Q_1 = R_2 R_1^{-1},   &  & \by{ex:6.1.23}[c]
  \end{align*}
  by \cref{6.5.9} we know that \(Q_2^* Q_1 = R_2 R_1^{-1}\) is unitary.
  Since \(R_1\) is upper triangular, by \cref{ex:4.3.27}(c) we know that \(R_1^{-1}\) is upper triangular.
  If we can show that \(R_2 R_1^{-1}\) is upper triangular, then by \cref{ex:6.5.17} we know that \(D = R_2 R_1^{-1}\) is an unitary diagonal matrix.
  So let \(i, j \in \set{1, \dots, n}\) such that \(i > j\).
  Then we have
  \begin{align*}
    (R_2 R_1^{-1})_{i j} & = \sum_{k = 1}^n (R_2)_{i k} (R_1^{-1})_{k j} &  & \by{2.3.1}     \\
                         & = \sum_{k = i}^j (R_2)_{i k} (R_1^{-1})_{k j} &  & \by{ex:1.3.12} \\
                         & = 0.                                          &  & (i > j)
  \end{align*}
  Thus by \cref{ex:1.3.12} \(R_2 R_1^{-1}\) is upper triangular.
\end{proof}

\begin{note}
  At one time, because of the great stability of QR factorization (\cref{ex:6.5.29}), this method for solving large systems of linear equations with a computer was being advocated as a better method than Gaussian elimination even though it requires about three times as much work.
  (Later, however, J. H. Wilkinson showed that if Gaussian elimination is done ``properly,'' then it is nearly as stable as the orthogonalization method.)
\end{note}

\begin{ex}\label{ex:6.5.30}
  Suppose that \(\beta\) and \(\gamma\) are ordered bases for an \(n\)-dimensional real (complex) inner product space \(\V\).
  Prove that if \(Q\) is an orthogonal (unitary) \(n \times n\) matrix that changes \(\gamma\)-coordinates into \(\beta\)-coordinates, then \(\beta\) is orthonormal iff \(\gamma\) is orthonormal.
\end{ex}

\begin{proof}[\pf{ex:6.5.30}]
  Let \(\beta = \set{\seq{v}{1,,n}}\) and let \(\gamma = \set{\seq{u}{1,,n}}\).
  By \cref{2.23} we have \(Q = [\IT[\V]]_{\gamma}^{\beta}\).
  Since
  \begin{align*}
    \forall i, j \in \set{1, \dots, n}, \inn{u_i, u_j} & = \inn{\IT[\V](u_i), \IT[\V](u_j)}                                    &  & \by{2.1.9}      \\
                                                       & = \inn{\sum_{k = 1}^n Q_{k i} v_k, \sum_{p = 1}^n Q_{p j} v_p}        &  & \by{2.2.4}      \\
                                                       & = \sum_{k = 1}^n Q_{k i} \inn{v_k, \sum_{p = 1}^n Q_{p j} v_p}        &  & \by{6.1.1}[a,b] \\
                                                       & = \sum_{k = 1}^n \sum_{p = 1}^n \conj{Q_{p j}} Q_{k i} \inn{v_k, v_p} &  & \by{6.1}[a,b]   \\
                                                       & = \sum_{k = 1}^n \sum_{p = 1}^n (Q^*)_{j p} Q_{k i} \inn{v_k, v_p},   &  & \by{6.1.5}
  \end{align*}
  we know that
  \begin{align*}
             & \beta \text{ is orthonormal with respect to } \inn{\cdot, \cdot}                                                                       \\
    \implies & \forall i, j \in \set{1, \dots, n}, \inn{u_i, u_j} = \sum_{k = 1}^n \sum_{p = 1}^n (Q^*)_{j p} Q_{k i} \inn{v_k, v_p}                  \\
             & = \sum_{k = 1}^n \sum_{p = 1}^n (Q^*)_{j p} Q_{k i} \delta_{k p}                                                      &  & \by{6.1.12} \\
             & = \sum_{k = 1}^n (Q^*)_{j k} Q_{k i}                                                                                  &  & \by{2.3.4}  \\
             & = (Q^* Q)_{j i}                                                                                                       &  & \by{2.3.1}  \\
             & = \delta_{j i}                                                                                                        &  & \by{6.5.9}  \\
    \implies & \gamma \text{ is orthonormal with respect to } \inn{\cdot, \cdot}.                                                    &  & \by{6.1.12}
  \end{align*}
  Using the same argument and the fact that \(Q^* = Q^{-1} = [\IT[\V]]_{\beta}^{\gamma}\), we see that \(\gamma\) is orthonormal with respect to \(\inn{\dot, \cdot}\) implies \(\beta\) is orthonormal with respect to \(\inn{\dot, \cdot}\).
\end{proof}

\begin{defn}\label{6.5.20}
  Let \(\V\) be a finite-dimensional complex (real) inner product space, and let \(u\) be a unit vector in \(\V\).
  Define the \textbf{Householder} operator \(\lt{H}_u : \V \to \V\) by \(\lt{H}_u(x) = x - 2 \inn{x, u} u\) for all \(x \in \V\).
\end{defn}

\begin{ex}\label{ex:6.5.31}
  Let \(\lt{H}_u\) be a Householder operator on a finite-dimensional inner product space \(\V\) over \(\F\).
  Prove the following results.
  \begin{enumerate}
    \item \(\lt{H}_u\) is linear.
    \item \(\lt{H}_u(x) = x\) iff \(x\) is orthogonal to \(u\).
    \item \(\lt{H}_u(u) = -u\).
    \item \(\lt{H}_u^* = \lt{H}_u\) and \(\lt{H}_u^2 = \IT[\V]\), and hence \(\lt{H}_u\) is a unitary (orthogonal) operator on \(\V\).
  \end{enumerate}
  (Note:
  If \(\V\) is a real inner product space, then in the language of \cref{sec:6.11}, \(\lt{H}_u\) is a reflection.)
\end{ex}

\begin{proof}[\pf{ex:6.5.31}(a)]
  Let \(x, y \in \V\) and let \(c \in \F\).
  Then we have
  \begin{align*}
    \lt{H}_u(cx + y) & = cx + y - 2 \inn{cx + y, u} u                &  & \by{6.5.20}     \\
                     & = cx + y - 2 (c \inn{x, u} + \inn{y, u}) u    &  & \by{6.1.1}[a,b] \\
                     & = c (x - 2 \inn{x, u} u) + y - 2 \inn{y, u} u &  & \by{1.2.1}      \\
                     & = c \lt{H}_u(x) + \lt{H}_u(y)                 &  & \by{6.5.20}
  \end{align*}
  and thus by \cref{2.1.2}(b) we know that \(\lt{H}_u \in \ls(\V)\).
\end{proof}

\begin{proof}[\pf{ex:6.5.31}(b)]
  We have
  \begin{align*}
         & \lt{H}_u(x) = x                                \\
    \iff & x - 2 \inn{x, u} u = x       &  & \by{6.5.20}  \\
    \iff & \inn{x, u} u = \zv                             \\
    \iff & \inn{x, u} = 0               &  & (u \neq \zv) \\
    \iff & x, u \text{ are orthogonal}. &  & \by{6.1.12}
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.5.31}(c)]
  We have
  \begin{align*}
    \lt{H}_u(u) & = u - 2 \inn{u, u} u &  & \by{6.5.20} \\
                & = u - 2u             &  & \by{6.1.12} \\
                & = -u.
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.5.31}(d)]
  Let \(\lt{H}_u^*\) be the adjoint operator of \(\lt{H}_u\) with respect to \(\inn{\cdot, \cdot}\).
  Then we have
  \begin{align*}
    \forall x, y \in \V, \inn{x, \lt{H}_u^*(y)} & = \inn{\lt{H}_u(x), y}                 &  & \by{6.9}        \\
                                                & = \inn{x - 2 \inn{x, u} u, y}          &  & \by{6.5.20}     \\
                                                & = \inn{x, y} - 2 \inn{x, u} \inn{u, y} &  & \by{6.1.1}[a,b] \\
                                                & = \inn{x, y - 2 \conj{\inn{u, y}} u}   &  & \by{6.1}[a,b]   \\
                                                & = \inn{x, y - 2 \inn{y, u} u}          &  & \by{6.1.1}[c]   \\
                                                & = \inn{x, \lt{H}_u(y)}                 &  & \by{6.5.20}
  \end{align*}
  and thus by \cref{6.1}(e) we have \(\lt{H}_u^* = \lt{H}_u\).

  Since
  \begin{align*}
    \forall x \in \V, \lt{H}_u^2(x) & = \lt{H}_u(\lt{H}_u(x))                                         \\
                                    & = \lt{H}_u(x - 2 \inn{x, u} u)           &  & \by{6.5.20}       \\
                                    & = \lt{H}_u(x) - 2 \inn{x, u} \lt{H}_u(u) &  & \by{ex:6.5.31}[a] \\
                                    & = \lt{H}_u(x) + 2 \inn{x, u} u           &  & \by{ex:6.5.31}[c] \\
                                    & = x - 2 \inn{x, u} u + 2 \inn{x, u} u    &  & \by{6.5.20}       \\
                                    & = x,
  \end{align*}
  we have \(\lt{H}_u^2 = \lt{H}_u^* \lt{H}_u = \lt{H}_u \lt{H}_u^* = \IT[\V]\).
  By \cref{6.5.1} \(\lt{H}_u\) is unitary.
\end{proof}

\begin{ex}\label{ex:6.5.32}
  Let \(\V\) be a finite-dimensional inner product space over \(\F\).
  Let \(x\) and \(y\) be linearly independent vectors in \(\V\) such that \(\norm{x} = \norm{y}\).
  \begin{enumerate}
    \item If \(\F = \C\), prove that there exists a unit vector \(u\) in \(\V\) and a complex number \(\theta\) with \(\abs{\theta} = 1\) such that \(\lt{H}_u(x) = \theta y\).
    \item If \(\F = \R\), prove that there exists a unit vector \(u\) in \(\V\) such that \(\lt{H}_u(x) = y\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.5.32}(a)]
  Let \(\inn{\cdot, \cdot}\) be an inner product on \(\V\) over \(\C\) such that \(\norm{x} = \inn{x, x} = \inn{y, y} = \norm{y}\).
  Since \(x, y\) are linearly independent, we know that \(x - \theta y \neq \zv\) for all \(\theta \in \C\).

  First suppose that \(\inn{x, y} = 0\).
  Let \(\theta = 1\) and let \(u = \dfrac{x - y}{\norm{x- y}}\).
  Then we have
  \begin{align*}
    \lt{H}_u(x) & = x - 2 \inn{x, u} u                                                       &  & \by{6.5.20}            \\
                & = x - \dfrac{2 \inn{x, \dfrac{x - y}{\norm{x - y}}}}{\norm{x - y}} (x - y)                             \\
                & = x - \dfrac{2 \inn{x, x - y}}{\norm{x - y}^2} (x - y)                     &  & \by{6.1}[b]            \\
                & = x - \dfrac{2 \inn{x, x} - 2 \inn{x, y}}{\norm{x - y}^2} (x - y)          &  & \by{6.1}[a,b]          \\
                & = x - \dfrac{2 \inn{x, x}}{\norm{x - y}^2} (x - y)                         &  & \by{6.1.12}            \\
                & = x - \dfrac{2 \inn{x, x}}{\norm{x}^2 + \norm{y}^2} (x - y)                &  & \by{ex:6.1.10}         \\
                & = x - \dfrac{2 \norm{x}^2}{\norm{x}^2 + \norm{y}^2} (x - y)                &  & \by{6.1.9}             \\
                & = x - \dfrac{\norm{x}^2 + \norm{y}^2}{\norm{x}^2 + \norm{y}^2} (x - y)     &  & \text{(by hypothesis)} \\
                & = y.
  \end{align*}

  Now suppose that \(\inn{x, y} \neq 0\).
  Let \(\theta = \dfrac{\inn{x, y}}{\abs{\inn{x, y}}}\) and let \(u = \dfrac{x - \theta y}{\norm{x - \theta y}}\).
  Then we have
  \begin{align*}
    \abs{\theta} & = \abs{\dfrac{\inn{x, y}}{\abs{\inn{x, y}}}} \\
                 & = \dfrac{\abs{\inn{x, y}}}{\abs{\inn{x, y}}} \\
                 & = 1
  \end{align*}
  and
  \begin{align*}
    \inn{x, \theta y} & = \conj{\theta} \inn{x, y}                               &  & \by{6.1}[b]   \\
                      & = \conj{\dfrac{\inn{x, y}}{\abs{\inn{x, y}}}} \inn{x, y}                    \\
                      & = \dfrac{\inn{y, x}}{\abs{x, y}} \inn{x, y}              &  & \by{6.1.1}[c] \\
                      & = \abs{x, y}                                             &  & \by{d.0.5}    \\
                      & \in \R.
  \end{align*}
  Thus
  \begin{align*}
    \lt{H}_u(x) & = x - 2 \inn{x, u} u                                                                                                &  & \by{6.5.20}                \\
                & = x - \dfrac{2 \inn{x, \dfrac{x - \theta y}{\norm{x - \theta y}}}}{\norm{x - \theta y}} (x - \theta y)                                              \\
                & = x - \dfrac{2 \inn{x, x - \theta y}}{\norm{x - \theta y}^2} (x - \theta y)                                         &  & \by{6.1}[b]                \\
                & = x - \dfrac{2 \inn{x, x} - 2 \conj{\theta} \inn{x, y}}{\norm{x - \theta y}^2} (x - \theta y)                       &  & \by{6.1}[a,b]              \\
                & = x - \dfrac{2 \norm{x}^2 - 2 \conj{\theta} \inn{x, y}}{\norm{x - \theta y}^2} (x - \theta y)                       &  & \by{6.1.9}                 \\
                & = x - \dfrac{\norm{x}^2 - 2 \conj{\theta} \inn{x, y} + \norm{y}^2}{\norm{x - \theta y}^2} (x - \theta y)            &  & \text{(by hypothesis)}     \\
                & = x - \dfrac{\norm{x}^2 - 2 \inn{x, \theta y} + \norm{y}^2}{\norm{x - \theta y}^2} (x - \theta y)                   &  & \by{6.1}[b]                \\
                & = x - \dfrac{\norm{x}^2 - 2 \Re(\inn{x, \theta y}) + \norm{y}^2}{\norm{x - \theta y}^2} (x - \theta y)              &  & (\inn{x, \theta y} \in \R) \\
                & = x - \dfrac{\norm{x}^2 - 2 \Re(\inn{x, \theta y}) + \abs{\theta} \norm{y}^2}{\norm{x - \theta y}^2} (x - \theta y) &  & (\abs{\theta} = 1)         \\
                & = x - \dfrac{\norm{x}^2 - 2 \Re(\inn{x, \theta y}) + \norm{\theta y}^2}{\norm{x - \theta y}^2} (x - \theta y)       &  & \by{6.2}[a]                \\
                & = x - \dfrac{\norm{x - \theta y}^2}{\norm{x - \theta y}^2} (x - \theta y)                                           &  & \by{ex:6.1.19}[a]          \\
                & = \theta y.
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:6.5.32}(b)]
  Let \(\inn{\cdot, \cdot}\) be an inner product on \(\V\) over \(\R\) such that \(\norm{x} = \inn{x, x} = \inn{y, y} = \norm{y}\).
  Since \(x, y\) are linearly independent, we know that \(x - y \neq \zv\).
  Let \(u = \dfrac{x - y}{\norm{x- y}}\).
  Then we have
  \begin{align*}
    \lt{H}_u(x) & = x - 2 \inn{x, u} u                                                              &  & \by{6.5.20}            \\
                & = x - \dfrac{2 \inn{x, \dfrac{x - y}{\norm{x - y}}}}{\norm{x - y}} (x - y)                                    \\
                & = x - \dfrac{2 \inn{x, x - y}}{\norm{x - y}^2} (x - y)                            &  & \by{6.1}[b]            \\
                & = x - \dfrac{2 \inn{x, x} - 2 \inn{x, y}}{\norm{x - y}^2} (x - y)                 &  & \by{6.1}[a,b]          \\
                & = x - \dfrac{2 \norm{x}^2 - 2 \inn{x, y}}{\norm{x - y}^2} (x - y)                 &  & \by{6.1.9}             \\
                & = x - \dfrac{\norm{x}^2 - 2 \inn{x, y} + \norm{y}^2}{\norm{x - y}^2} (x - y)      &  & \text{(by hypothesis)} \\
                & = x - \dfrac{\norm{x}^2 - 2 \Re(\inn{x, y}) + \norm{y}^2}{\norm{x - y}^2} (x - y) &  & (\F = \R)              \\
                & = x - \dfrac{\norm{x - y}^2}{\norm{x - y}^2} (x - y)                              &  & \by{ex:6.1.19}[a]      \\
                & = y.
  \end{align*}
\end{proof}
