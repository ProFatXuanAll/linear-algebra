\section{The Geometry of Orthogonal Operators}\label{sec:6.11}

\begin{note}
  By \cref{6.22}, any rigid motion on a finite-dimensional real inner product space is the composite of an orthogonal operator and a translation.
  Thus, to understand the geometry of rigid motions thoroughly, we must analyze the structure of orthogonal operators.
  Such is the aim of this section.
  We show that any orthogonal operator on a finite-dimensional real inner product space is the composite of rotations and reflections.
\end{note}

\begin{defn}\label{6.11.1}
  Let \(\T\) be a linear operator on a finite-dimensional real inner product space \(\V\).
  The operator \(\T\) is called a \textbf{rotation} if \(\T\) is the identity on \(\V\) or if there exists a two-dimensional subspace \(\W\) of \(\V\) over \(\R\), an orthonormal basis \(\beta = \set{\seq{x}{1,2}}\) for \(\W\) over \(\R\), and a real number \(\theta\) such that
  \[
    \T(x_1) = \cos(\theta) x_1 + \sin(\theta) x_2, \quad \T(x_2) = -\sin(\theta) x_1 + \cos(\theta) x_2,
  \]
  and \(\T(y) = y\) for all \(y \in \W^{\perp}\).
  In this context, \(\T\) is called a \textbf{rotation of \(\W\) about \(\W^{\perp}\)}.
  The subspace \(\W^{\perp}\) is called the \textbf{axis of rotation}.
\end{defn}

\begin{defn}\label{6.11.2}
  Let \(\T\) be a linear operator on a finite-dimensional real inner product space \(\V\).
  The operator \(\T\) is called a \textbf{reflection} if there exists a one-dimensional subspace \(\W\) of \(\V\) over \(\R\) such that \(\T(x) = -x\) for all \(x \in \W\) and \(\T(y) = y\) for all \(y \in \W^{\perp}\).
  In this context, \(\T\) is called a \textbf{reflection of \(\V\) about \(\W^{\perp}\)}.
\end{defn}

\begin{note}
  It should be noted that rotations and reflections (or composites of these) are orthogonal operators (see \cref{ex:6.11.2}).
  The principal aim of this section is to establish that the converse is also true, that is, any orthogonal operator on a finite-dimensional real inner product space is the composite of rotations and reflections.
\end{note}

\begin{eg}\label{6.11.3}
  A Characterization of Orthogonal Operators on a One-Dimensional Real Inner Product Space.

  Let \(\T\) be an orthogonal operator on a one-dimensional real inner product space \(\V\).
  Choose any nonzero vector \(x\) in \(\V\).
  Then \(\V = \spn{\set{x}}\), and so \(\T(x) = \lambda x\) for some \(\lambda \in \R\).
  Since \(\T\) is orthogonal and \(\lambda\) is an eigenvalue of \(\T\), \(\lambda = \pm 1\) (see \cref{6.2}(a) and \cref{6.5.1}).
  If \(\lambda = 1\), then \(\T\) is the identity on \(\V\), and hence \(\T\) is a rotation (\cref{6.11.1}).
  If \(\lambda = -1\), then \(\T(x) = -x\) for all \(x \in \V\);
  so \(\T\) is a reflection of \(\V\) about \(\V^{\perp} = \set{\zv}\) (\cref{6.2.11,6.11.2}).
  Thus \(\T\) is either a rotation or a reflection.
  Note that in the first case, \(\det(\T) = 1\), and in the second case, \(\det(\T) = -1\) (\cref{ex:5.1.7}).
\end{eg}

\begin{cor}\label{6.11.4}
  Let \(\V\) be an \(n\)-dimensional vector space over \(\R\).
  Let \(\T \in \ls(\V)\) be a rotation.
  Then there exists an orthonormal basis \(\gamma\) for \(\V\) over \(\R\) and a \(\theta \in \R\) such that
  \[
    [\T]_{\gamma} = \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) & \zm       \\
      \sin(\theta) & \cos(\theta)  & \zm       \\
      \zm          & \zm           & I_{n - 2}
    \end{pmatrix}.
  \]
\end{cor}

\begin{proof}[\pf{6.11.4}]
  Following the notation of \cref{6.11.1}, let \(\gamma = \set{\seq{x}{1,2}, \seq{v}{1,,n-2}}\) be an orthonormal basis for \(\V\) over \(\R\), where \(\set{\seq{v}{1,,n-2}}\) is an orthonormal basis for \(\W^{\perp}\).
  By \cref{6.5} such \(\gamma\) must exist.
  The proof follows from \cref{2.2.4,6.11.1}.
\end{proof}

\begin{thm}\label{6.45}
  Let \(\T\) be an orthogonal operator on a two-dimensional real inner product space \(\V\).
  Then \(\T\) is either a rotation or a reflection.
  Furthermore, \(\T\) is a rotation iff \(\det(\T) = 1\), and \(\T\) is a reflection iff \(\det(\T) = -1\).
\end{thm}

\begin{proof}[\pf{6.45}]
  Let \(\inn{\cdot, \cdot}\) be an inner product on \(\V\) over \(\R\) and let \(\inn{\cdot, \cdot}'\) be the standard inner product on \(\R^2\) over \(\R\).
  Let \(\norm{\cdot}\) be a norm on \(\V\) over \(\R\) such that \(\norm{\cdot}^2 = \inn{\cdot, \cdot}\).
  Let \(\norm{\cdot}'\) be a norm on \(\R^2\) over \(\R\) such that \((\norm{\cdot}')^2 = \inn{\cdot, \cdot}'\).
  Let \(\T \in \ls(\V)\) be orthogonal with respect to \(\norm{\cdot}\) and let \(\beta = \set{\seq{v}{1,2}}\) be an orthonormal basis with respect to \(\inn{\cdot, \cdot}\) for \(\V\) over \(\R\).
  Let \(\gamma\) be the standard ordered basis for \(\R^2\) over \(\R\).
  Define \(\phi_{\beta} \in \ls(\V, \R^2)\) as in \cref{2.4.11}.

  Because \(\T\) is an orthogonal operator, \(\T(\beta)\) is an orthonormal basis for \(\V\) over \(\R\) by \cref{6.18}(c).
  Since
  \begin{align*}
    1 & = \norm{v_1}^2                                      &  & \by{6.1.12}    \\
      & = \norm{\T(v_1)}^2                                  &  & \by{6.5.1}     \\
      & = \inn{\T(v_1), \T(v_1)}                            &  & \by{6.1.9}     \\
      & = \inn{\phi_{\beta} \T(v_1), \phi_{\beta} \T(v_1)}' &  & \by{ex:6.2.15} \\
      & = (\norm{\phi_{\beta} \T(v_1)}')^2,                 &  & \by{6.1.9}
  \end{align*}
  we know that \(\phi_{\beta} \T(v_1)\) is a unit vector and there is an unique angle \(\theta \in [0, 2 \pi)\) such that \(\phi_{\beta} \T(v_1) = (\cos(\theta), \sin(\theta))\).
  Similarly \(\phi_{\beta} \T(v_2)\) is a unit vector and is orthogonal to \(\phi_{\beta} \T(v_1)\) (\cref{ex:6.2.15}), there are only two possible choices for \(\phi_{\beta} \T(v_2)\).
  Either
  \[
    \phi_{\beta} \T(v_2) = (-\sin(\theta), \cos(\theta)) \quad \text{or} \quad \phi_{\beta} \T(v_2) = (\sin(\theta), -\cos(\theta)).
  \]
  First, suppose that \(\phi_{\beta} \T(v_2) = (-\sin(\theta), \cos(\theta))\).
  Then
  \begin{align*}
    \begin{pmatrix}
      \cos(\theta) & -\sin(\theta) \\
      \sin(\theta) & \cos(\theta)
    \end{pmatrix} & = [\phi_{\beta} \T]_{\beta}^{\gamma}           &  & \by{2.2.4}                   \\
                                    & = [\phi_{\beta}]_{\beta}^{\gamma} [\T]_{\beta} &  & \by{2.11}  \\
                                    & = I_2 [\T]_{\beta}                             &  & \by{2.2.4} \\
                                    & = [\T]_{\beta}.                                &  & \by{2.3.8}
  \end{align*}
  It follows from \cref{6.11.1} that \(\T\) is a rotation of \(\V\) about \(\V^{\perp} = \set{\zv}\) (\cref{6.2.11}).
  Also
  \begin{align*}
    \det(\T) & = \det([\T]_{\beta})                       &  & \by{ex:5.1.7} \\
             & = (\cos(\theta))^2 + (\sin(\theta))^2 = 1. &  & \by{4.1.1}
  \end{align*}
  Now suppose that \(\phi_{\beta} \T(v_2) = (\sin(\theta), -\cos(\theta))\).
  Then
  \[
    \begin{pmatrix}
      \cos(\theta) & \sin(\theta)  \\
      \sin(\theta) & -\cos(\theta)
    \end{pmatrix} = [\phi_{\beta} \T]_{\beta}^{\gamma} = [\T]_{\beta}.
  \]
  It follows from \cref{6.11.2} that \(\T\) is a reflection on \(\V\) about
  \[
    \W^{\perp} = \spn{\set{\sin(\theta) v_1 + (1 - \cos(\theta)) v_2}}
  \]
  where
  \[
    \W = \spn{\set{(\cos(\theta) - 1) v_1 + \sin(\theta) v_2}}.
  \]
  Furthermore,
  \begin{align*}
    \det(\T) & = \det([\T]_{\beta})                          &  & \by{ex:5.1.7} \\
             & = - (\cos(\theta))^2 - (\sin(\theta))^2 = -1. &  & \by{4.1.1}
  \end{align*}
\end{proof}

\begin{cor}\label{6.11.5}
  Let \(\V\) be a two-dimensional real inner product space.
  The composite of a reflection and a rotation on \(\V\) is a reflection on \(\V\).
\end{cor}

\begin{proof}[\pf{6.11.5}]
  If \(\T_1\) is a reflection on \(\V\) and \(\T_2\) is a rotation on \(\V\), then by \cref{6.45}, \(\det(\T_1) = 1\) and \(\det(\T_2) = -1\).
  Let \(\T = \T_2 \T_1\) be the composite.
  Since \(\T_2\) and \(\T_1\) are orthogonal, so is \(\T\) (\cref{ex:6.5.3}).
  Moreover,
  \begin{align*}
    \det(\T) & = \det(\T_2) \cdot \det(\T_1) &  & \by{ex:5.1.7}[d] \\
             & = -1.                         &  & \by{6.45}
  \end{align*}
  Thus, by \cref{6.45}, \(\T\) is a reflection.
  The proof for \(\T_1 \T_2\) is similar, i.e.,
  \begin{align*}
    \det(\T_1 \T_2) & = \det(\T_1) \cdot \det(\T_2) &  & \by{ex:5.1.7}[d] \\
                    & = -1.
  \end{align*}
\end{proof}

\begin{lem}\label{6.11.6}
  If \(\T\) is a linear operator on a nonzero finite-dimensional real vector space \(\V\), then there exists a \(\T\)-invariant subspace \(\W\) of \(\V\) over \(\R\) such that \(1 \leq \dim(\W) \leq 2\).
\end{lem}

\begin{proof}[\pf{6.11.6}]
  Fix an ordered basis \(\beta = \set{\seq{y}{1,,n}}\) for \(\V\) over \(\R\), and let \(A = [\T]_{\beta}\).
  Let \(\phi_{\beta} \in \ls(\V, \R^n)\) be the isomorphism defined by \cref{2.4.11}.
  By \cref{2.4.12} \(\L_A \phi_{\beta} = \phi_{\beta} \T\).
  As a consequence, it suffices to show that there exists an \(\L_A\)-invariant subspace \(\vs{Z}\) of \(\R^n\) such that \(1 \leq \dim(\vs{Z}) \leq 2\).
  If we then define \(\W = \phi_{\beta}^{-1}(\vs{Z})\), it follows that \(\W\) satisfies the conclusions of the lemma (see \cref{ex:6.11.13}).

  The matrix \(A\) can be considered as an \(n \times n\) matrix over \(\C\) and, as such, can be used to define a linear operator \(\U\) on \(\C^n\) by \(\U(v) = Av\).
  Since \(\U\) is a linear operator on a finite-dimensional vector space over \(\C\), it has an eigenvalue \(\lambda \in \C\) (\cref{5.2,d.4}).
  Let \(x \in \C^n\) be an eigenvector corresponding to \(\lambda\).
  We may write \(\lambda = \lambda_1 + i \lambda_2\), where \(\lambda_1\) and \(\lambda_2\) are real, and
  \[
    x = \begin{pmatrix}
      a_1 + i b_1 \\
      \vdots      \\
      a_n + i b_n
    \end{pmatrix},
  \]
  where the \(a_i\)'s and \(b_i\)'s are real.
  Thus, setting
  \[
    x_1 = \begin{pmatrix}
      a_1    \\
      \vdots \\
      a_n
    \end{pmatrix} \quad \text{and} \quad x_2 = \begin{pmatrix}
      b_1    \\
      \vdots \\
      b_n
    \end{pmatrix},
  \]
  we have \(x = x_1 + i x_2\), where \(x_1\) and \(x_2\) have real entries.
  Note that at least one of \(x_1\) or \(x_2\) is nonzero since \(x \neq \zv\).
  Hence
  \begin{align*}
    \U(x) & = \lambda x                                                            \\
          & = (\lambda_1 + i \lambda_2) (x_1 + i x_2)                              \\
          & = (\lambda_1 x_1 - \lambda_2 x_2) + i (\lambda_1 x_2 + \lambda_2 x_1).
  \end{align*}
  Similarly,
  \[
    \U(x) = A (x_1 + i x_2) = A x_1 + i A x_2.
  \]
  Comparing the real and imaginary parts of these two expressions for \(\U(x)\), we conclude that
  \[
    A x_1 = \lambda_1 x_1 - \lambda_2 x_2 \quad \text{and} \quad A x_2 = \lambda_1 x_2 + \lambda_2 x_1.
  \]
  Finally, let \(\vs{Z} = \spn{\set{\seq{x}{1,2}}}\), the span being taken as a subspace of \(\R^n\) over \(\R\).
  Since \(x_1 \neq \zv\) or \(x_2 \neq \zv\), \(\vs{Z}\) is a nonzero subspace.
  Thus \(1 \leq \dim(\vs{Z}) \leq 2\), and the preceding pair of equations shows that \(\vs{Z}\) is \(\L_A\)-invariant.
\end{proof}

\begin{thm}\label{6.46}
  Let \(\T\) be an orthogonal operator on a nonzero finite-dimensional real inner product space \(\V\).
  Then there exists a collection of pairwise orthogonal \(\T\)-invariant subspaces \(\set{\seq{\W}{1,,m}}\) of \(\V\) such that
  \begin{enumerate}
    \item \(1 \leq \dim(\W_i) \leq 2\) for \(i \in \set{1, \dots, m}\).
    \item \(\V = \seq[\oplus]{\W}{1,,m}\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{6.46}]
  The proof is by mathematical induction on \(\dim(\V)\).
  If \(\dim(\V) = 1\), the result is obvious.
  So assume that the result is true whenever \(\dim(\V) = n\) for some fixed integer \(n \geq 1\).

  Suppose \(\dim(\V) = n + 1\).
  By \cref{6.11.6}, there exists a \(\T\)-invariant subspace \(\W_1\) of \(\V\) such that \(1 \leq \dim(\W_1) \leq 2\).
  If \(\W_1 = \V\), the result is established.
  Otherwise, \(\W_1^{\perp} \neq \set{\zv}\) (\cref{6.2.11}).
  By \cref{ex:6.11.14}, \(\W_1^{\perp}\) is \(\T\)-invariant and the restriction of \(\T\) to \(\W_1^{\perp}\) is orthogonal.
  Since \(\dim(\W_1^{\perp}) \leq n\), we may apply the induction hypothesis to \(\T_{\W_1^{\perp}}\) and conclude that there exists a collection of pairwise orthogonal \(\T\)-invariant subspaces \(\set{\seq{\W}{2,,m}}\) of \(\W_1^{\perp}\) such that \(1 \leq \dim(\W_i) \leq 2\) for \(i \in \set{2, \dots, m}\) and \(\W_1^{\perp} = \seq[\oplus]{\W}{2,,m}\).
  Thus \(\set{\seq{\W}{1,,m}}\) is pairwise orthogonal, and by \cref{ex:6.2.13}(d),
  \[
    \V = \W_1 \oplus \W_1^{\perp} = \seq[\oplus]{\W}{1,,m}.
  \]
\end{proof}

\begin{note}
  Applying \cref{6.11.3,6.45} in the context of \cref{6.46}, we conclude that the restriction of \(\T\) to \(\W_i\) is either a rotation or a reflection for each \(i \in \set{1, \dots, m}\).
  Thus, in some sense, \(\T\) is composed of rotations and reflections.
  Unfortunately, very little can be said about the uniqueness of the decomposition of \(\V\) in \cref{6.46}.
  For example, the \(\W_i\)'s, the number \(m\) of \(\W_i\)'s, and the number of \(\W_i\)'s for which \(\T_{\W_i}\) is a reflection are not unique.
  Although the number of \(\W_i\)'s for which \(\T_{\W_i}\) is a reflection is not unique, whether this number is even or odd is an intrinsic property of \(\T\).
  Moreover, we can always decompose \(\V\) so that \(\T_{\W_i}\) is a reflection for at most one \(\W_i\).
  These facts are established in \cref{6.47}.
\end{note}

\begin{thm}\label{6.47}
  Let \(\T, \V, \seq{\W}{1,,m}\) be as in \cref{6.46}.
  \begin{enumerate}
    \item The number of \(\W_i\)'s for which \(\T_{\W_i}\) is a reflection is even or odd according to whether \(\det(\T) = 1\) or \(\det(\T) = -1\).
    \item It is always possible to decompose \(\V\) as in \cref{6.46} so that the number of \(\W_i\)'s for which \(\T_{\W_i}\) is a reflection is zero or one according to whether \(\det(\T) = 1\) or \(\det(\T) = -1\).
          Furthermore, if \(\T_{\W_i}\) is a reflection, then \(\dim(\W_i) = 1\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{6.47}(a)]
  Let \(r\) denote the number of \(\W_i\)'s in the decomposition for which \(\T_{\W_i}\) is a reflection.
  Then we have
  \begin{align*}
    \det(\T) & = \det(\T_{\W_1}) \cdots \det(\T_{\W_m}) &  & \by{ex:6.11.15}  \\
             & = 1^{m - r} \cdot (-1)^r                 &  & \by{6.11.3,6.45} \\
             & = (-1)^r.
  \end{align*}
\end{proof}

\begin{proof}[\pf{6.47}(b)]
  Let \(E = \set{x \in \V : \T(x) = -x}\);
  then \(E\) is a \(\T\)-invariant subspace of \(\V\) over \(\R\).
  If \(\W = E^{\perp}\), then \(\W\) is \(\T\)-invariant (\cref{ex:6.11.14}(b)).
  So by applying \cref{6.46} to \(\T_{\W}\), we obtain a collection of pairwise orthogonal \(\T\)-invariant subspaces \(\set{\seq{\W}{1,,k}}\) of \(\W\) such that \(\W = \seq[\oplus]{\W}{1,,k}\) and for \(i \in \set{1, \dots, k}\), the dimension of each \(\W_i\) is either \(1\) or \(2\).
  Observe that, for each \(i \in \set{1, \dots, k}\), \(\T_{\W_i}\) is a rotation.
  For otherwise, if \(\T_{\W_i}\) is a reflection, there exists a nonzero \(x \in \W_i\) for which \(\T(x) = -x\).
  But then, \(x \in \W_i \cap E \subseteq E^{\perp} \cap E = \set{\zv}\), a contradiction.
  If \(E = \set{\zv}\), the result follows.
  Otherwise, choose an orthonormal basis \(\beta\) for \(E\) over \(\R\) containing \(p\) vectors (\(p > 0\)).
  It is possible to decompose \(\beta\) into a pairwise disjoint union \(\beta = \seq[\cup]{\beta}{1,,r}\) such that each \(\beta_i\) contains exactly two vectors for \(i \in \set{1, \dots, r - 1}\), and \(\beta_r\) contains two vectors if \(p\) is even and one vector if \(p\) is odd.
  For each \(i \in \set{1, \dots, r}\), let \(\W_{k + i} = \spn{\beta_i}\).
  Then, clearly, \(\set{\seq{\W}{1,,k,,k+r}}\) is pairwise orthogonal, and
  \[
    \V = \seq[\oplus]{\W}{1,,k,,k+r}.
  \]
  Moreover, if any \(\beta_i\) contains two vectors, then
  \begin{align*}
    \det(\T_{\W_{k + i}}) & = \det([\T_{\W_{k + i}}]_{\beta_i}) &  & \by{ex:5.1.7} \\
                          & = \det\begin{pmatrix}
                                    -1 & 0  \\
                                    0  & -1
                                  \end{pmatrix}               &  & (\T(x) = -x)    \\
                          & = 1.                                &  & \by{4.1.1}
  \end{align*}
  So \(\T_{\W_{k + i}}\) is a rotation, and hence \(\T_{\W_j}\) is a rotation for \(j \in \set{1, \dots, k + r - 1}\).
  If \(\beta_r\) consists of one vector, then \(\dim(\W_{k + r}) = 1\) and
  \begin{align*}
    \det(\T_{\W_{k + r}}) & = \det([\T_{\W_{k + r}}]_{\beta_r}) &  & \by{ex:5.1.7} \\
                          & = \det(-1)                          &  & (\T(x) = -x)  \\
                          & = -1.                               &  & \by{4.2.2}
  \end{align*}
  Thus \(\T_{\W_{k + r}}\) is a reflection by \cref{6.46}, and we conclude that the decomposition
  \[
    \V = \seq[\oplus]{\W}{1,,k,,k+r}
  \]
  satisfies the condition of (b).
\end{proof}

\begin{cor}\label{6.11.7}
  Let \(\T\) be an orthogonal operator on a finite-dimensional real inner product space \(\V\).
  Then there exists a collection \(\set{\seq{\T}{1,,m}}\) of orthogonal operators on \(\V\) such that the following statements are true.
  \begin{enumerate}
    \item For each \(i \in \set{1, \dots, m}\), \(\T_i\) is either a reflection or a rotation.
    \item For at most one \(i \in \set{1, \dots, m}\), \(\T_i\) is a reflection.
    \item \(\T_i \T_j = \T_j \T_i\) for all \(i, j \in \set{1, \dots, m}\).
    \item \(\T = \seq[]{\T}{1,,m}\).
    \item \[
            \det(\T) = \begin{dcases}
              1  & \text{if } \T_i \text{ is a rotation for each } i \in \set{1, \dots, m} \\
              -1 & \text{otherwise}.
            \end{dcases}
          \]
  \end{enumerate}
\end{cor}

\begin{proof}[\pf{6.11.7}]
  As in the proof of \cref{6.47}(b), we can write
  \[
    \V = \seq[\oplus]{\W}{1,,m},
  \]
  where \(\T_{\W_i}\) is a rotation for \(i \in \set{1, \dots, m - 1}\).
  For each \(i \in \set{1, \dots, m}\), define \(\T_i : \V \to \V\) by
  \[
    \T_i(\seq[+]{x}{1,,m}) = \seq[+]{x}{1,,i-1} + \T(x_i) + \seq[+]{x}{i+1,,m},
  \]
  where \(x_j \in \W_j\) for all \(j \in \set{1, \dots, m}\).

  First we show that \(\T_i\) is an orthogonal operator on \(\V\) for all \(i \in \set{1, \dots, m}\).
  Clearly \(\T_i \in \ls(\V)\).
  Since
  \begin{align*}
             & \forall x \in \V, \exists \tuple{x}{1,,m} \in \seq[\times]{\W}{1,,m} : x = \seq[+]{x}{1,,m}                  &  & \by{5.2.7}     \\
    \implies & \forall x \in \V, \norm{\T_i(x)}^2                                                                                               \\
             & = \norm{\seq[+]{x}{1,,i-1} + \T(x_i) + \seq[+]{x}{i+1,,m}}^2                                                                     \\
             & = \norm{x_1}^2 + \cdots + \norm{x_{i - 1}}^2 + \norm{\T(x_i)}^2 + \norm{x_{i + 1}}^2 + \cdots + \norm{x_m}^2 &  & \by{ex:6.1.10} \\
             & = \norm{x_1}^2 + \cdots + \norm{x_{i - 1}}^2 + \norm{x_i}^2 + \norm{x_{i + 1}}^2 + \cdots + \norm{x_m}^2     &  & \by{6.5.1}     \\
             & = \norm{\seq[+]{x}{1,,m}}^2                                                                                  &  & \by{ex:6.1.10} \\
             & = \norm{x}^2                                                                                                 &  & \by{5.2.7}     \\
    \implies & \forall x \in \V, \norm{\T(x)} = \norm{x},                                                                   &  & \by{6.2}[b]
  \end{align*}
  by \cref{6.5.1} we know that \(\T_i\) is orthogonal for all \(i \in \set{1, \dots, m}\).

  Next we claim that \(\T_i\) is a rotation or a reflection according to whether \(\T_{\W_i}\) is a rotation or a reflection.
  We split into two cases:
  \begin{itemize}
    \item If \(\T_{\W_i}\) is a rotation, then by \cref{6.11.1} there exists an orthonormal basis \(\beta = \set{\seq{v}{1,2}}\) for \(\W_i\) over \(\R\), and a \(\theta \in \R\) such that
          \begin{align*}
            \T_{\W_i}(v_1) & = \cos(\theta) v_1 + \sin(\theta) v_2;  \\
            \T_{\W_i}(v_2) & = -\sin(\theta) v_1 + \cos(\theta) v_2.
          \end{align*}
          Note that \(\dim(\W_i) = 2\) and thus \(\W_i^{\perp} = \set{\zv}\) when we only consider vectors in \(\W_i\) itself.
          Since \(\seq{\W}{1,,k}\) are pairwise orthogonal, we see that
          \[
            \W_i^{\perp} = \pa{\sum_{j = 1}^m \W_j} \setminus \W_i \quad \text{and} \quad \forall x \in \pa{\sum_{j = 1}^m \W_j} \setminus \W_i, \T_i(x) = x.
          \]
          Thus by \cref{6.11.1} \(\T_i\) is rotation when \(\T_{\W_i}\) is a rotation.
    \item If \(\T_{\W_i}\) is a reflection, then by \cref{6.47}(b) we have \(\dim(\W_i) = 1\).
          By \cref{6.11.2} we see that \(\T_{\W_i}(x) = -x\) for all \(x \in \W_i\).
          Note that \(\dim(\W_i) = 1\) and thus \(\W_i^{\perp} = \set{\zv}\) when we only consider vectors in \(\W_i\) itself.
          Since \(\seq{\W}{1,,k}\) are pairwise orthogonal, we see that
          \[
            \W_i^{\perp} = \pa{\sum_{j = 1}^m \W_j} \setminus \W_i \quad \text{and} \quad \forall x \in \pa{\sum_{j = 1}^m \W_j} \setminus \W_i, \T_i(x) = x.
          \]
          Thus by \cref{6.11.2} \(\T_i\) is reflection when \(\T_{\W_i}\) is a reflection.
  \end{itemize}
  This establishes (a).

  Next we proof (b).
  Since there is at most one \(\T_{\W_i}\) is a reflection by \cref{6.47}(b), we know that at most one \(\T_i\) is a reflection.
  This establish (b).

  Next we proof (c).
  Thus case for \(i = j\) is trivial.
  So we proof the case \(i \neq j\).
  Without the loss of generality, suppose that \(i < j\).
  Since
  \begin{align*}
    \forall x \in \V, (\T_i \T_j)(x) & = \T_i(\T_j(x))                                                                   \\
                                     & = \T_i(\T_j(\seq[+]{x}{1,,m}))                                 &  & \by{5.2.7}    \\
                                     & = \T_i(\T_j(x_1)) + \cdots + \T_i(\T_j(x_i))                                      \\
                                     & \quad + \cdots \T_i(\T_j(x_j)) + \cdots + \T_i(\T_j(x_m))      &  & \by{2.1.1}[a] \\
                                     & = x_1 + \cdots + \T_i(x_i) + \cdots + \T_j(x_j) + \cdots + x_m                    \\
                                     & = \T_j(\T_i(x_1)) + \cdots + \T_j(\T_i(x_i))                                      \\
                                     & \quad + \cdots \T_j(\T_i(x_j)) + \cdots + \T_j(\T_i(x_m))      &  & \by{2.1.1}[a] \\
                                     & = \T_j(\T_i(\seq[+]{x}{1,,m}))                                 &  & \by{2.1.1}[a] \\
                                     & = \T_j(\T_i(x))                                                &  & \by{5.2.7}    \\
                                     & = (\T_j \T_i)(x),
  \end{align*}
  we see that \(\T_i \T_j = \T_j \T_i\).
  This establish (c).

  Next we proof (d).
  Since
  \begin{align*}
    \forall x \in \V, (\seq[]{\T}{1,,m})(x) & = (\seq[]{\T}{1,,m-1})(\T_m(x))                                                  \\
                                            & = (\seq[]{\T}{1,,m-1})\pa{\sum_{i = 1}^{m - 1} x_i + \T(x_m)} &  & \by{5.2.7}    \\
                                            & = (\seq[]{\T}{1,,m-1})\pa{\sum_{i = 1}^{m - 1} x_i} + \T(x_m) &  & \by{2.1.1}[a] \\
                                            & = \sum_{i = 1}^m \T(x_i)                                                         \\
                                            & = \T\pa{\sum_{i = 1}^m x_i}                                   &  & \by{2.1.1}[a] \\
                                            & = \T(x),                                                      &  & \by{5.2.7}
  \end{align*}
  we see that \(\T = \seq[]{\T}{1,,m}\).
  This establish (d).

  Finally we proof (e).
  By \cref{ex:5.1.7}(d) and \cref{6.11.7}(d) we have \(\det(\T) = \det(\T_1) \cdots \det(\T_m)\).
  The rest follows from \cref{6.47}(b).
\end{proof}

\begin{eg}\label{6.11.8}
  Orthogonal Operators on a Three-Dimensional Real Inner Product Space.

  Let \(\T\) be an orthogonal operator on a three-dimensional real inner product space \(\V\).
  We show that \(\T\) can be decomposed into the composite of a rotation and at most one reflection.
  Let
  \[
    \V = \seq[\oplus]{\W}{1,,m}
  \]
  be a decomposition as in \cref{6.47}(b).
  Clearly, \(m = 2\) or \(m = 3\).

  If \(m = 2\), then \(\V = \W_1 \oplus \W_2\).
  Without loss of generality, suppose that \(\dim(\W_1) = 1\) and \(\dim(\W_2) = 2\).
  Thus \(\T_{\W_1}\) is a reflection or the identity on \(\W_1\), and \(\T_{\W_2}\) is a rotation.
  Defining \(\T_1\) and \(\T_2\) as in the proof of \cref{6.11.7}, we have that \(\T = \T_1 \T_2\) is the composite of a rotation and at most one reflection.
  (Note that if \(\T_{\W_1}\) is not a reflection, then \(\T_1\) is the identity on \(\V\) and \(\T = \T_2\).)

  If \(m = 3\), then \(\V = \W_1 \oplus \W_2 \oplus \W_3\) and \(\dim(\W_i) = 1\) for all \(i \in \set{1, 2, 3}\).
  For each \(i \in \set{1, 2, 3}\), let \(\T_i\) be as in the proof of \cref{6.11.7}.
  If \(\T_{\W_i}\) is not a reflection, then \(\T_i\) is the identity on \(\W_i\).
  Otherwise, \(\T_i\) is a reflection.
  Since \(\T_{\W_i}\) is a reflection for at most one \(i \in \set{1, 2, 3}\), we conclude that \(\T\) is either a single reflection or the identity (a rotation).
\end{eg}

\exercisesection

\setcounter{ex}{1}
\begin{ex}\label{ex:6.11.2}
  Prove that rotations, reflections, and composites of rotations and reflections are orthogonal operators.
\end{ex}

\begin{proof}[\pf{ex:6.11.2}]
  Let \(\T\) be a linear operator on a \(n\)-dimensional real inner product space \(\V\).

  First we show that rotations are orthogonal operators.
  Suppose that \(\T\) is a rotation.
  By \cref{6.11.1} there exists a two-dimensional subspace \(\W\) of \(\V\) over \(\R\), an orthonormal basis \(\beta = \set{\seq{x}{1,2}}\) for \(\W\) over \(\R\), and a \(\theta \in \R\) such that
  \begin{align*}
    \T(x_1)                         & = \cos(\theta) x_1 + \sin(\theta) x_2;  \\
    \T(x_2)                         & = -\sin(\theta) x_1 + \cos(\theta) x_2; \\
    \forall y \in \W^{\perp}, \T(y) & = y.
  \end{align*}
  Let \(\gamma = \set{\seq{v}{1,,n-2}}\) be an orthonormal basis for \(\W^{\perp}\) over \(\R\).
  By \cref{6.2.4} we know that \(\beta \cup \gamma\) is an orthonormal basis for \(\V\) over \(\R\).
  Since
  \begin{align*}
    \inn{\T(x_1), \T(x_1)} & = \inn{\cos(\theta) x_1 + \sin(\theta) x_2, \cos(\theta) x_1 + \sin(\theta) x_2}        &  & \by{6.11.1}     \\
                           & = \cos(\theta) \inn{x_1, \cos(\theta) x_1 + \sin(\theta) x_2}                           &  & \by{6.1.1}[a,b] \\
                           & \quad + \sin(\theta) \inn{x_2, \cos(\theta) x_1 + \sin(\theta) x_2}                     &  & \by{6.1.1}[a,b] \\
                           & = \cos(\theta) \inn{x_1, \cos(\theta) x_1} + \sin(\theta) \inn{x_2, \sin(\theta) x_2}   &  & \by{6.1.12}     \\
                           & = (\cos(\theta))^2 \inn{x_1, x_1} + (\sin(\theta))^2 \inn{x_2, x_2}                     &  & \by{6.1.1}[b]   \\
                           & = (\cos(\theta))^2 + (\sin(\theta))^2                                                   &  & \by{6.1.12}     \\
                           & = 1;                                                                                                         \\
    \inn{\T(x_1), \T(x_2)} & = \inn{\cos(\theta) x_1 + \sin(\theta) x_2, -\sin(\theta) x_1 + \cos(\theta) x_2}       &  & \by{6.11.1}     \\
                           & = \cos(\theta) \inn{x_1, -\sin(\theta) x_1 + \cos(\theta) x_2}                          &  & \by{6.1.1}[a,b] \\
                           & \quad + \sin(\theta) \inn{x_2, -\sin(\theta) x_1 + \cos(\theta) x_2}                    &  & \by{6.1.1}[a,b] \\
                           & = \cos(\theta) \inn{x_1, -\sin(\theta) x_1} + \sin(\theta) \inn{x_2, \cos(\theta) x_2}  &  & \by{6.1.12}     \\
                           & = -\sin(\theta) \cos(\theta) \inn{x_1, x_1} + \sin(\theta) \cos(\theta) \inn{x_2, x_2}  &  & \by{6.1}[b]     \\
                           & = -\sin(\theta) \cos(\theta) + \sin(\theta) \cos(\theta)                                &  & \by{6.1.12}     \\
                           & = 0;                                                                                                         \\
    \inn{\T(x_2), \T(x_2)} & = \inn{-\sin(\theta) x_1 + \cos(\theta) x_2, -\sin(\theta) x_1 + \cos(\theta) x_2}      &  & \by{6.11.1}     \\
                           & = -\sin(\theta) \inn{x_1, -\sin(\theta) x_1 + \cos(\theta) x_2}                         &  & \by{6.1.1}[a,b] \\
                           & \quad + \cos(\theta) \inn{x_2, -\sin(\theta) x_1 + \cos(\theta) x_2}                    &  & \by{6.1.1}[a,b] \\
                           & = -\sin(\theta) \inn{x_1, -\sin(\theta) x_1} + \cos(\theta) \inn{x_2, \cos(\theta) x_2} &  & \by{6.1.12}     \\
                           & = (\sin(\theta))^2 \inn{x_1, x_1} + (\cos(\theta))^2 \inn{x_2, x_2}                     &  & \by{6.1.1}[b]   \\
                           & = (\sin(\theta))^2 + (\cos(\theta))^2                                                   &  & \by{6.1.12}     \\
                           & = 1,
  \end{align*}
  we know that \(\T(\beta) \subseteq \W\) is orthonormal.
  Since \(\T(\gamma) = \gamma\), we know that \(\T(\beta) \cup \T(\gamma)\) is an orthonormal basis for \(\V\) over \(\R\).
  Thus by \cref{6.18}(c)(e) we know that \(\T\) is an orthogonal operator.

  Next we show that reflections are orthogonal operators.
  Suppose that \(\T\) is a reflection.
  By \cref{6.11.2} there exists a one-dimensional subspace \(\W\) of \(\V\) over \(\R\) such that
  \begin{align*}
     & \forall x \in \W, \T(x) = -x;        \\
     & \forall x \in \W^{\perp}, \T(x) = x.
  \end{align*}
  Let \(y \in \V\).
  By \cref{6.6} there exists an unique tuple \(\tuple{y}{1,2} \in \W \times \W^{\perp}\) such that \(y = y_1 + y_2\).
  Since
  \begin{align*}
    \norm{\T(y)}^2 & = \norm{\T(y_1 + y_2)}^2       &  & \by{6.6}       \\
                   & = \norm{\T(y_1) + \T(y_2)}^2   &  & \by{2.1.1}[a]  \\
                   & = \norm{-y_1 + y_2}^2          &  & \by{6.11.2}    \\
                   & = \norm{-y_1}^2 + \norm{y_2}^2 &  & \by{ex:6.1.10} \\
                   & = \norm{y_1}^2 + \norm{y_2}^2  &  & \by{6.2}[a]    \\
                   & = \norm{y_1 + y_2}^2           &  & \by{ex:6.1.10} \\
                   & = \norm{y}^2,                  &  & \by{6.6}
  \end{align*}
  by \cref{6.2}(b) we have \(\norm{\T(y)} = \norm{y}\).
  Thus by \cref{6.5.1} we know that \(\T\) is an orthogonal operator.

  Finally we show that composites of rotations and reflections are orthogonal operators.
  From the proof above we see that rotations and reflections are orthogonal operators.
  Thus we only need to prove that the composites of orthogonal operators are orthogonal operators.
  This is done by \cref{ex:6.5.3}.
\end{proof}

\setcounter{ex}{3}
\begin{ex}\label{ex:6.11.4}
  For any real number \(\phi\), let
  \[
    A = \begin{pmatrix}
      \cos(\phi) & \sin(\phi)  \\
      \sin(\phi) & -\cos(\phi)
    \end{pmatrix}.
  \]
  \begin{enumerate}
    \item Prove that \(\L_A\) is a reflection.
    \item Find the axis in \(\R^2\) about which \(\L_A\) reflects.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.11.4}(a)]
  Since
  \begin{align*}
    \det(\L_A) & = \det(A)                          &  & \by{ex:5.1.7} \\
               & = -(\cos(\phi))^2 - (\sin(\phi))^2 &  & \by{4.1.1}    \\
               & = -1,
  \end{align*}
  by \cref{6.45} we know that \(\L_A\) is a reflection.
\end{proof}

\begin{proof}[\pf{ex:6.11.4}(b)]
  Since
  \begin{align*}
             & A \begin{pmatrix}
                   x \\
                   y
                 \end{pmatrix} = \begin{pmatrix}
                                   x \\
                                   y
                                 \end{pmatrix}                                 &  & \by{6.11.2} \\
    \implies & \begin{pmatrix}
                 \cos(\phi) x + \sin(\phi) y \\
                 \sin(\phi) x - \cos(\phi) y
               \end{pmatrix} = \begin{pmatrix}
                                 x \\
                                 y
                               \end{pmatrix}                                 &  & \by{2.3.1}    \\
    \implies & \begin{dcases}
                 \sin(\phi) y = (1 - \cos(\phi)) x \\
                 \sin(\phi) x = (1 + \cos(\phi)) y
               \end{dcases}                                                \\
    \implies & (\sin(\phi), 1 - \cos(\phi)) \in \set{x \in \R^2 : Ax = x},                      \\
    \implies & \set{x \in \R^2 : Ax = x} = \spn{(\sin(\phi), 1 - \cos(\phi))}, &  & \by{6.11.2}
  \end{align*}
  by \cref{6.11.2} we know that \(\L_A\) is a reflection of \(\R^2\) about \(\spn{(\sin(\phi), 1 - \cos(\phi))}\).
\end{proof}

\begin{ex}\label{ex:6.11.5}
  For any real number \(\phi\), define \(\T_{\phi} = \L_A\), where
  \[
    A = \begin{pmatrix}
      \cos(\phi) & -\sin(\phi) \\
      \sin(\phi) & \cos(\phi)
    \end{pmatrix}.
  \]
  \begin{enumerate}
    \item Prove that any rotation on \(\R^2\) is of the form \(\T_{\phi}\) for some \(\phi\).
    \item Prove that \(\T_{\phi} \T_{\psi} = \T_{(\phi + \psi)}\) for any \(\phi, \psi \in \R\).
    \item Deduce that any two rotations on \(\R^2\) commute.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.11.5}(a)]
  See \cref{6.11.4}.
\end{proof}

\begin{proof}[\pf{ex:6.11.5}(b)]
  Since
  \begin{align*}
     & \begin{pmatrix}
         \cos(\phi) & -\sin(\phi) \\
         \sin(\phi) & \cos(\phi)
       \end{pmatrix} \begin{pmatrix}
                       \cos(\psi) & -\sin(\psi) \\
                       \sin(\psi) & \cos(\psi)
                     \end{pmatrix}                                                            \\
     & = \begin{pmatrix}
           \cos(\phi) \cos(\psi) - \sin(\phi) \sin(\psi) & - \cos(\phi) \sin(\psi) - \sin(\phi) \cos(\psi) \\
           \sin(\phi) \cos(\psi) + \cos(\phi) \sin(\psi) & - \sin(\phi) \sin(\psi) + \cos(\phi) \cos(\psi)
         \end{pmatrix} &  & \by{2.3.1} \\
     & = \begin{pmatrix}
           \cos(\phi + \psi) & -\sin(\phi + \psi) \\
           \sin(\phi + \psi) & \cos(\phi + \psi)
         \end{pmatrix},
  \end{align*}
  by \cref{2.15}(e) we know that \(\T_{\phi} \T_{\psi} = \T_{\phi + \psi}\).
\end{proof}

\begin{proof}[\pf{ex:6.11.5}(c)]
  We have
  \begin{align*}
    \T_{\phi} \T_{\psi} & = \T_{\phi + \psi}    &  & \by{ex:6.11.5}[b] \\
                        & = \T_{\psi + \phi}                           \\
                        & = \T_{\psi} \T_{\phi} &  & \by{ex:6.11.5}[b]
  \end{align*}
  and thus any two rotations on \(\R^2\) commute.
\end{proof}

\begin{ex}\label{ex:6.11.6}
  Prove that the composite of any two rotations on \(\R^3\) is a rotation on \(\R^3\).
\end{ex}

\begin{proof}[\pf{ex:6.11.6}]

\end{proof}

\begin{ex}\label{ex:6.11.7}
  Given real numbers \(\phi\) and \(\psi\), define matrices
  \[
    A = \begin{pmatrix}
      1 & 0          & 0           \\
      0 & \cos(\phi) & -\sin(\phi) \\
      0 & \sin(\phi) & \cos(\phi)
    \end{pmatrix} \quad \text{and} \quad B = \begin{pmatrix}
      \cos(\psi) & -\sin(\psi) & 0 \\
      \sin(\psi) & \cos(\psi)  & 0 \\
      0          & 0           & 1
    \end{pmatrix}.
  \]
  \begin{enumerate}
    \item Prove that \(\L_{A}\) and \(\L_{B}\) are rotations.
    \item Prove that \(\L_{AB}\) is a rotation.
    \item Find the axis of rotation for \(\L_{AB}\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.11.7}(a)]

\end{proof}

\begin{proof}[\pf{ex:6.11.7}(b)]

\end{proof}

\begin{proof}[\pf{ex:6.11.7}(c)]

\end{proof}

\begin{ex}\label{ex:6.11.8}
  Prove \cref{6.45} using the hints preceding the statement of the theorem.
\end{ex}

\begin{proof}[\pf{ex:6.11.8}]
  See \cref{6.45}.
\end{proof}

\begin{ex}\label{ex:6.11.9}
  Prove that no orthogonal operator can be both a rotation and a reflection.
\end{ex}

\begin{proof}[\pf{ex:6.11.9}]

\end{proof}

\begin{ex}\label{ex:6.11.10}
  Prove that if \(\V\) is a two- or three-dimensional real inner product space, then the composite of two reflections on \(\V\) is a rotation of \(\V\).
\end{ex}

\begin{proof}[\pf{ex:6.11.10}]

\end{proof}

\begin{ex}\label{ex:6.11.11}
  Give an example of an orthogonal operator that is neither a reflection nor a rotation.
\end{ex}

\begin{proof}[\pf{ex:6.11.11}]

\end{proof}

\begin{ex}\label{ex:6.11.12}
  Let \(\V\) be a finite-dimensional real inner product space.
  Define \(\T : \V \to \V\) by \(\T(x) = -x\).
  Prove that \(\T\) is a product of rotations iff \(\dim(\V)\) is even.
\end{ex}

\begin{proof}[\pf{ex:6.11.12}]

\end{proof}

\begin{ex}\label{ex:6.11.13}
  Complete the proof of \cref{6.11.6} by showing that \(\W = \phi_{\beta}^{-1}(\vs{Z})\) satisfies the required conditions.
\end{ex}

\begin{proof}[\pf{ex:6.11.13}]
  By \cref{6.11.6} we need to show that \(\W\) is a subspace of \(\V\) over \(\R\), \(\W\) is \(\T\)-invariant and \(1 \leq \dim(\W) \leq 2\).

  First we show that \(\W\) is a subspace of \(\V\) over \(\R\).
  Since \(\vs{Z}\) is a subspace of \(\R^n\) over \(\R\) and \(\phi_{\beta}^{-1} \in \ls(\R^n, \V)\), by \cref{2.1} we know that \(\W = \phi_{\beta}^{-1}(\vs{Z})\) is a subspace of \(\V\) over \(\R\).

  Next we show that \(\W\) is \(\T\)-invariant.
  Let \(x \in \W\).
  Then there exists a \(y \in \vs{Z}\) such that \(\phi_{\beta}^{-1}(y) = x\) or \(\phi_{\beta}(x) = y\).
  By \cref{6.11.6} we know that \(\vs{Z}\) is \(\L_A\)-invariant.
  Thus by \cref{5.4.1} \(\L_A(y) \in \vs{Z}\).
  By \cref{2.21} this means \((\phi_{\beta}^{-1} \L_A)(y) \in \W\).
  Thus we have
  \begin{align*}
    \T(x) & = (\phi_{\beta}^{-1} \phi_{\beta} \T)(x)   &  & \by{2.21}   \\
          & = (\phi_{\beta}^{-1} \L_A \phi_{\beta})(x) &  & \by{2.4.12} \\
          & = (\phi_{\beta}^{-1} \L_A)(y)                               \\
          & \in \W.
  \end{align*}
  By \cref{5.4.1} \(\W\) is \(\T\)-invariant.

  Finally we show that \(1 \leq \dim(\W) \leq 2\).
  Since \(1 \leq \dim(\vs{Z}) \leq 2\) and \(\W = \phi_{\beta}^{-1}(\vs{Z})\), by \cref{2.19} we have \(1 \leq \dim(\W) \leq 2\).
\end{proof}

\begin{ex}\label{ex:6.11.14}
  Let \(\T\) be an orthogonal (unitary) operator on a finite-dimensional real (complex) inner product space \(\V\).
  If \(\W\) is a \(\T\)-invariant subspace of \(\V\) over \(\R\), prove the following results.
  \begin{enumerate}
    \item \(\T_{\W}\) is an orthogonal (unitary) operator on \(\W\).
    \item \(\W^{\perp}\) is a \(\T\)-invariant subspace of \(\V\).
    \item \(\T_{\W^{\perp}}\) is an orthogonal (unitary) operator on \(\W\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:6.11.14}(a)]
  We have
  \begin{align*}
    \forall x \in \W, \norm{\T_{\W}(x)} & = \norm{\T(x)} &  & \by{b.0.4} \\
                                        & = \norm{x}     &  & \by{6.5.1}
  \end{align*}
  and thus by \cref{6.5.1} \(\T_{\W}\) is orthogonal (unitary).
\end{proof}

\begin{proof}[\pf{ex:6.11.14}(b)]
  See \cref{ex:6.5.15}.
\end{proof}

\begin{proof}[\pf{ex:6.11.14}(c)]
  This follows from \cref{ex:6.11.14}(a)(b).
\end{proof}

\begin{ex}\label{ex:6.11.15}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), where \(\V\) is a direct sum of \(\T\)-invariant subspaces, say, \(\V = \seq[\oplus]{\W}{1,,k}\).
  Prove that \(\det(\T) = \det(\T_{\W_1}) \cdots \det(\T_{\W_k})\).
\end{ex}

\begin{proof}[\pf{ex:6.11.15}]
  By \cref{5.25} we have
  \[
    [\T]_{\beta} = \begin{pmatrix}
      [\T_{\W_1}]_{\beta_1} & \zm                   & \cdots & \zm                   \\
      \zm                   & [\T_{\W_2}]_{\beta_2} & \cdots & \zm                   \\
      \zm                   & \zm                   & \cdots & [\T_{\W_k}]_{\beta_k}
    \end{pmatrix},
  \]
  where \(\beta\) is an ordered basis for \(\V\) over \(\F\) and \(\beta_i = \beta \cap \W_i\) for all \(i \in \set{1, \dots, k}\).
  Thus we have
  \begin{align*}
    \det(\T) & = \det([\T]_{\beta})                                             &  & \by{ex:5.1.7}  \\
             & = \det([\T_{\W_1}]_{\beta_1}) \cdots \det([\T_{\W_k}]_{\beta_k}) &  & \by{ex:4.3.21} \\
             & = \det(\T_{\W_1}) \cdots \det(\T_{\W_k}).                        &  & \by{ex:5.1.7}
  \end{align*}
\end{proof}
