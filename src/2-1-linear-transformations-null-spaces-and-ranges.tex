\section{Linear Transformations, Null Spaces and Ranges}\label{sec:2.1}

\begin{defn}\label{2.1.1}
  Let \(\V\) and \(\W\) be vector spaces over \(\F\).
  We call a function \(\T : \V \to \W\) a \textbf{linear transformation from \(\V\) to \(\W\)} if, for all \(x, y \in \V\) and \(c \in \F\), we have
  \begin{enumerate}
    \item \(\T(x + y) = \T(x) + \T(y)\) and
    \item \(\T(cx) = c\T(x)\).
  \end{enumerate}
  We often simply call \(\T\) \textbf{linear}.
\end{defn}

\begin{note}
  If the underlying field \(\F\) is the field of rational numbers, then \cref{2.1.1}(a) implies \cref{2.1.1}(b) (see \cref{ex:2.1.37}), but, in general \cref{2.1.1}(a)(b) are logically independent.
\end{note}

\begin{prop}\label{2.1.2}
  Let \(\V\) and \(\W\) be vector spaces over \(\F\) and let \(\T : \V \to \W\) be a function.
  \begin{enumerate}
    \item If \(\T\) is linear, then \(\T(\zv_{\V}) = \zv_{\W}\).
    \item \(\T\) is linear if and only if \(\T(cx + y) = c\T(x) + \T(y)\) for all \(x, y \in \V\) and \(c \in \F\).
    \item If \(\T\) is linear, then \(\T(x - y) = \T(x) - \T(y)\) for all \(x, y \in \V\).
    \item \(\T\) is linear if and only if, for \(\seq{x}{1,2,,n} \in \V\) and \(\seq{a}{1,2,,n} \in F\), we have
          \[
            \T\pa{\sum_{i = 1}^n a_i x_i} = \sum_{i = 1}^n a_i \T(x_i).
          \]
  \end{enumerate}
\end{prop}

\begin{proof}[\pf{2.1.2}(a)]
  We have
  \begin{align*}
             & \T \text{ is linear}                                                                   \\
    \implies & \T(\zv_{\V}) + \T(\zv_{\V}) = \T(\zv_{\V} + \zv_{\V}) &  & \text{(by \cref{2.1.1}(a))} \\
             & = \T(\zv_{\V}) = \T(\zv_{\V}) + \zv_{\W}              &  & \text{(by \ref{vs3})}       \\
    \implies & \T(\zv_{\V}) = \zv_{\W}.                              &  & \text{(by \cref{1.1})}
  \end{align*}
\end{proof}

\begin{proof}[\pf{2.1.2}(b)]
  We have
  \begin{align*}
             & \T \text{ is linear}                                                                                  \\
    \implies & \begin{dcases}
      \forall x, y \in \V \\
      \forall c \in \F
    \end{dcases}, \begin{dcases}
      \T(x + y) = \T(x) + \T(y) \\
      \T(cx) = c\T(x)
    \end{dcases}                    &  & \text{(by \cref{2.1.1})} \\
    \implies & \begin{dcases}
      \forall x, y \in \V \\
      \forall c \in \F
    \end{dcases}, \T(cx + y) = \T(cx) + \T(y) = c\T(x) + \T(y) &  & \text{(by \cref{2.1.1})}
  \end{align*}
  and
  \begin{align*}
             & \begin{dcases}
      \forall x, y \in \V \\
      \forall c \in \F
    \end{dcases}, \T(cx + y) = c\T(x) + \T(y)                                  \\
    \implies & \begin{dcases}
      \forall x, y \in \V \\
      \forall c \in \F
    \end{dcases}, \begin{dcases}
      \T(x + y) = \T(x) + \T(y)             & \text{if } c = 0        \\
      \T(cx + \zv_{\V}) = c\T(x) + \zv_{\W} & \text{if } y = \zv_{\V}
    \end{dcases}  &  & \text{(by \cref{2.1.2}(a))} \\
    \implies & \begin{dcases}
      \forall x, y \in \V \\
      \forall c \in \F
    \end{dcases}, \begin{dcases}
      \T(x + y) = \T(x) + \T(y) & \text{if } c = 0        \\
      \T(cx) = c\T(x)           & \text{if } y = \zv_{\V}
    \end{dcases} &  & \text{(by \ref{vs3})}       \\
    \implies & \T \text{ is linear}.                                  &  & \text{(by \cref{2.1.1})}
  \end{align*}
  Thus
  \[
    \T \text{ is linear} \iff \begin{dcases}
      \forall x, y \in \V \\
      \forall c \in \F
    \end{dcases}, \T(cx + y) = c\T(x) + \T(y).
  \]
\end{proof}

\begin{proof}[\pf{2.1.2}(c)]
  For all \(x, y \in \V\), we have
  \begin{align*}
    \T(x - y) & = \T(x + (-1)y)      &  & \text{(by \cref{1.2}(b))}   \\
              & = \T(x) + \T((-1)y)  &  & \text{(by \cref{2.1.1}(a))} \\
              & = \T(x) + (-1) \T(y) &  & \text{(by \cref{2.1.1}(b))} \\
              & = \T(x) - \T(y).     &  & \text{(by \cref{1.2}(b))}
  \end{align*}
\end{proof}

\begin{proof}[\pf{2.1.2}(d)]
  We have
  \begin{align*}
             & \T \text{ is linear}                                                                                                  \\
    \implies & \begin{dcases}
      \forall \seq{x}{1,2,,n} \in \V \\
      \forall \seq{a}{1,2,,n} \in \F
    \end{dcases},                                                                                           \\
             & \T\pa{\sum_{i = 1}^n a_i x_i} = \sum_{i = 1}^n \T(a_i x_i) = \sum_{i = 1}^n a_i \T(x_i) &  & \text{(by \cref{2.1.1})}
  \end{align*}
  and
  \begin{align*}
             & \begin{dcases}
      \forall \seq{x}{1,2,,n} \in \V \\
      \forall \seq{a}{1,2,,n} \in \F
    \end{dcases},                                                                 \\
             & \T\pa{\sum_{i = 1}^n a_i x_i} = \sum_{i = 1}^n a_i \T(x_i) &  & \text{(by \cref{2.1.1})}    \\
    \implies & \begin{dcases}
      \forall x, y \in \V \\
      \forall c \in \F
    \end{dcases},                                                                 \\
             & \T(cx + 1y) = c\T(x) + 1\T(y) = c\T(x) + \T(y)             &  & \text{(by \ref{vs5})}       \\
    \implies & \T \text{ is linear}.                                      &  & \text{(by \cref{2.1.2}(b))}
  \end{align*}
  Thus
  \[
    \T \text{ is linear} \iff \begin{dcases}
      \forall \seq{x}{1,2,,n} \in \V \\
      \forall \seq{a}{1,2,,n} \in \F
    \end{dcases}, \T\pa{\sum_{i = 1}^n a_i x_i} = \sum_{i = 1}^n a_i \T(x_i).
  \]
\end{proof}

\begin{note}
  We generally use \cref{2.1.2}(b) to prove that a given transformation is linear.
\end{note}

\begin{eg}\label{2.1.3}
  For any angle \(\theta\), define \(\T_{\theta} : \R^2 \to \R^2\) by the rule: \(\T_{\theta}(a_1, a_2)\) is the vector obtained by rotating \((a_1, a_2)\) counterclockwise by \(\theta\) if \((a_1, a_2) \neq (0, 0)\), and \(\T_{\theta}(0, 0) = (0, 0)\).
  Then \(\T_{\theta} : \R^2 \to \R^2\) is a linear transformation that is called the \textbf{rotation by \(\theta\)}.

  We determine an explicit formula for \(\T_{\theta}\).
  Fix a nonzero vector \((a_1, a_2) \in \R^2\).
  Let \(\alpha\) be the angle that \((a_1, a_2)\) makes with the positive \(x\)-axis, and let \(r = \sqrt{a_1^2 +a_2^2}\).
  Then \(a_1 = r \cos(\alpha)\) and \(a_2 = r \sin(\alpha)\).
  Also, \(\T_{\theta}(a_1, a_2)\) has length \(r\) and makes an angle \(\alpha + \theta\) with the positive \(x\)-axis.
  It follows that
  \begin{align*}
    \T_{\theta}(a_1, a_2) & = (r \cos(\alpha + \theta), r \sin(\alpha + \theta))                                                                     \\
                          & = (r \cos(\alpha) \cos(\theta) - r \sin(\alpha) \sin(\theta), r \cos(\alpha) \sin(\theta) + r \sin(\alpha) \cos(\theta)) \\
                          & = (a_1 \cos(\theta) - a_2 \sin(\theta), a_1 \sin(\theta) + a_2 \cos(\theta)).
  \end{align*}
  Finally, observe that this same formula is valid for \((a_1 ,a_2) = (0, 0)\).
  It is now easy to show that \(\T_{\theta}\) is linear.
\end{eg}

\begin{proof}[\pf{2.1.3}]
  For all \(x, y \in \R^2\) and \(c \in \R\), we have
  \begin{align*}
    \T_{\theta}(cx + y) & = \T_{\theta}(cx_1 + y_1, cx_2 + y_2)                                              &  & \text{(by \cref{1.2.4})} \\
                        & = ((cx_1 + y_1) \cos(\theta) - (cx_2 + y_2)\sin(\theta),                                                         \\
                        & \quad (cx_1 + y_1) \sin(\theta) + (cx_2 + y_2) \cos(\theta))                       &  & \text{(by \cref{2.1.3})} \\
                        & = c (x_1 \cos(\theta) - x_2 \sin(\theta), x_1 \sin(\theta) + x_2 \cos(\theta))     &  & \text{(by \cref{1.2.1})} \\
                        & \quad + (y_1 \cos(\theta) - y_2 \sin(\theta), y_1 \sin(\theta) + y_2 \cos(\theta))                               \\
                        & = c\T(x_1, x_2) + \T(y_1, y_2)                                                     &  & \text{(by \cref{2.1.3})} \\
                        & = c\T_{\theta}(x) + \T_{\theta}(y).                                                &  & \text{(by \cref{1.2.4})}
  \end{align*}
  Thus by \cref{2.1.2}(b) \(\T_{\theta}\) is linear.
\end{proof}

\begin{eg}\label{2.1.4}
  Define \(\T : \R^2 \to \R^2\) by \(\T(a_1, a_2) = (a_1, -a_2)\).
  \(\T\) is called the \textbf{reflection about the \(x\)-axis} and \(\T\) is linear.
\end{eg}

\begin{proof}[\pf{2.1.4}]
  For all \(x, y \in \R^2\) and \(c \in \R\), we have
  \begin{align*}
    \T(cx + y) & = \T(cx_1 + y_1, cx_2 + y_2)   &  & \text{(by \cref{1.2.4})} \\
               & = (cx_1 + y_1, -cx_2 - y_2)    &  & \text{(by \cref{2.1.4})} \\
               & = c(x_1, -x_2) + (y_1, -y_2)   &  & \text{(by \cref{1.2.1})} \\
               & = c\T(x_1, x_2) + \T(y_1, y_2) &  & \text{(by \cref{2.1.4})} \\
               & = c\T(x) + \T(y).              &  & \text{(by \cref{1.2.4})}
  \end{align*}
  Thus by \cref{2.1.2}(b) \(\T\) is linear.
\end{proof}

\begin{eg}\label{2.1.5}
  Define \(\T : \R^2 \to \R^2\) by \(\T(a_1, a_2) = (a_1, 0)\).
  \(\T\) is called the \textbf{projection on the \(x\)-axis} and \(\T\) is linear.
\end{eg}

\begin{proof}[\pf{2.1.5}]
  For all \(x, y \in \R^2\) and \(c \in \R\), we have
  \begin{align*}
    \T(cx + y) & = \T(cx_1 + y_1, cx_2 + y_2)   &  & \text{(by \cref{1.2.4})} \\
               & = (cx_1 + y_1, 0)              &  & \text{(by \cref{2.1.5})} \\
               & = c(x_1, 0) + (y_1, 0)         &  & \text{(by \cref{1.2.1})} \\
               & = c\T(x_1, x_2) + \T(y_1, y_2) &  & \text{(by \cref{2.1.5})} \\
               & = c\T(x) + \T(y).              &  & \text{(by \cref{1.2.4})}
  \end{align*}
  Thus by \cref{2.1.2}(b) \(\T\) is linear.
\end{proof}

\begin{eg}\label{2.1.6}
  Define \(\T : \ms{m}{n}{\F} \to \ms{n}{m}{\F}\) by \(\T(A) = \tp{A}\), where \(\tp{A}\) is the transpose of \(A\), defined in \cref{1.3.3}.
  Then \(\T\) is linear.
\end{eg}

\begin{proof}[\pf{2.1.6}]
  By \cref{ex:1.3.3} and \cref{2.1.2}(b) we see that \(\T\) is linear.
\end{proof}

\begin{eg}\label{2.1.7}
  Define \(\T : \ps[n]{\R} \to \ps[n - 1]{\R}\) by \(\T(f(x)) = f'(x)\), where \(f'(x)\) denotes the derivative of \(f(x)\).
  Then \(\T\) is linear.
\end{eg}

\begin{proof}[\pf{2.1.7}]
  Let \(g(x), h(x) \in \ps[n]{\R}\) and \(a \in \R\).
  Now
  \[
    \T(ag(x) + h(x)) = (ag(x) + h(x))' = ag'(x) + h'(x) = a\T(g(x)) + \T(h(x)).
  \]
  So by \cref{2.1.2}(b) \(\T\) is linear.
\end{proof}

\begin{eg}\label{2.1.8}
  Let \(\V = \cfs{\R}\), the vector space of continuous real-valued functions on \(\R\).
  Let \(a, b \in \R\), \(a < b\).
  Define \(\T : \V \to \R\) by
  \[
    \T(f) = \int_a^b f(t) \; dt
  \]
  for all \(f \in \V\).
  Then \(\T\) is linear.
\end{eg}

\begin{proof}[\pf{2.1.8}]
  Let \(g, h \in \cfs{\R}\) and \(a \in \R\).
  Now
  \begin{align*}
    \T(cg + h) & = \int_a^b (cg + h)(t) \; dt                  &  & \text{(by \cref{2.1.8})}  \\
               & = \int_a^b cg(t) + h(t) \; dt                 &  & \text{(by \cref{1.2.10})} \\
               & = c \int_a^b g(t) \; dt + \int_a^b h(t) \; dt                                \\
               & = c \T(g) + \T(h).                            &  & \text{(by \cref{2.1.8})}
  \end{align*}
  So by \cref{2.1.2}(b) \(\T\) is linear.
\end{proof}

\begin{eg}\label{2.1.9}
  For vector spaces \(\V\) and \(\W\) over \(\F\), we define the \textbf{identity transformation} \(\IT[\V] : \V \to \V\) by \(\IT[\V](x) = x\) for all \(x \in \V\) and the \textbf{zero transformation} \(\zT : \V \to \W\) by \(\zT(x) = \zv_{\W}\) for all \(x \in \V\).
  It is clear that both of these transformations are linear.
  We often write \(\IT\) instead of \(\IT[\V]\).
\end{eg}

\begin{proof}[\pf{2.1.9}]
  For all \(x, y \in \V\) and \(c \in \F\), we have
  \begin{align*}
    \IT[\V](cx + y) & = cx + y                    &  & \text{(by \cref{2.1.9})} \\
                    & = c \IT[\V](x) + \IT[\V](y) &  & \text{(by \cref{2.1.9})}
  \end{align*}
  and
  \begin{align*}
    \zT(cx + y) & = \zv_{\W}              &  & \text{(by \cref{2.1.9})}  \\
                & = c \zv_{\W}            &  & \text{(by \cref{1.2}(c))} \\
                & = c \zv_{\W} + \zv_{\W} &  & \text{(by \ref{vs3})}     \\
                & = c\T(x) + \T(y).       &  & \text{(by \cref{2.1.9})}
  \end{align*}
  Thus by \cref{2.1.2}(b) \(\IT[\V], \zT\) are linear.
\end{proof}

\begin{defn}\label{2.1.10}
  Let \(\V\) and \(\W\) be vector spaces over \(\F\), and let \(\T : \V \to \W\) be linear.
  We define the \textbf{null space} (or \textbf{kernel}) \(\ns{\T}\) of \(\T\) to be the set of all vectors \(x\) in \(\V\) such that \(\T(x) = \zv_{\W}\);
  that is, \(\ns{\T} = \set{x \in \V : \T(x) = \zv_{\W}}\).

  We define the \textbf{range} (or \textbf{image}) \(\rg{\T}\) of \(\T\) to be the subset of \(\W\) consisting of all images (under \(\T\)) of vectors in \(\V\);
  that is, \(\rg{\T} = \set{\T(x) : x \in V}\).
\end{defn}

\begin{eg}\label{2.1.11}
  Let \(\V\) and \(\W\) be vector spaces over \(\F\), and let \(\IT : \V \to \V\) and \(\zT : \V \to \W\) be the identity and zero transformations, respectively.
  Then \(\ns{\IT} = \set{\zv_{\V}}\), \(\rg{\IT} = \V\), \(\ns{\zT} = \V\), and \(\rg{\zT} = \set{\zv_{\W}}\).
\end{eg}

\begin{thm}\label{2.1}
  Let \(\V\) and \(\W\) be vector spaces over \(\F\) and \(\T : \V \to \W\) be linear.
  Then \(\ns{\T}\) and \(\rg{\T}\) are subspaces of \(\V\) and \(\W\) over \(\F\), respectively.
\end{thm}

\begin{proof}[\pf{2.1}]
  To clarify the notation, we use the symbols \(\zv_{\V}\) and \(\zv_{\W}\) to denote the zero vectors of \(\V\) and \(\W\), respectively.

  Since \(\T(\zv_{\V}) = \zv_{\W}\), we have that \(\zv_{\V} \in \ns{\T}\).
  Let \(x, y \in \ns{\T}\) and \(c \in \F\).
  Then \(\T(x + y) = \T(x) + \T(y) = \zv_{\W} +\zv_{\W} = \zv_{\W}\), and \(\T(cx) = c \T(x) = c \zv_{\W} = \zv_{\W}\).
  Hence \(x + y \in \ns{\T}\) and \(cx \in \ns{\T}\), so that \(\ns{\T}\) is a subspace of \(\V\) over \(\F\) (see \cref{1.3}).

  Because \(\T(\zv_{\V}) = \zv_{\W}\), we have that \(\zv_{\W} \in \rg{\T}\).
  Now let \(x, y \in \rg{\T}\) and \(c \in \F\).
  Then there exist \(v\) and \(w\) in \(\V\) such that \(\T(v) = x\) and \(\T(w) = y\).
  So \(\T(v + w) = \T(v) + \T(w) = x + y\), and \(\T(cv) = c \T(v) = cx\).
  Thus \(x + y \in \rg{\T}\) and \(cx \in \rg{\T}\), so \(\rg{\T}\) is a subspace of \(\W\) over \(\F\) (see \cref{1.3}).
\end{proof}

\begin{thm}\label{2.2}
  Let \(\V\) and \(\W\) be vector spaces over \(\F\), and let \(\T : \V \to \W\) be linear.
  If \(\beta = \set{\seq{v}{1,2,,n}}\) is a basis for \(\V\) over \(\F\), then
  \[
    \rg{\T} = \spn{\T(\beta)} = \spn{\set{\T(v_1), \T(v_2), \dots, \T(v_n)}}.
  \]
\end{thm}

\begin{proof}[\pf{2.2}]
  Clearly \(\T(v_i) \in \rg{\T}\) for each \(i\).
  Because \(\rg{\T}\) is a subspace, \(\rg{\T}\) contains \(\spn{\set{\T(v_1), \T(v_2), \dots, \T(v_n)}} = \spn{\T(\beta)}\) by \cref{1.5}.

  Now suppose that \(w \in \rg{\T}\).
  Then \(w = \T(v)\) for some \(v \in \V\).
  Because \(\beta\) is a basis for \(\V\) over \(\F\), we have
  \[
    v = \sum_{i = 1}^n a_i v_i \quad \text{for some } \seq{a}{1,2,,n} \in \F.
  \]
  Since \(\T\) is linear, it follows that
  \[
    w = \T(v) = \sum_{i = 1}^n a_i \T(v_i) \in \spn{T(\beta)}.
  \]
  So \(\rg{\T}\) is contained in \(\spn{\T(\beta)}\).
\end{proof}

\begin{note}
  It should be noted that \cref{2.2} is true if \(\beta\) is infinite.
  (See \cref{ex:2.1.33}.)
\end{note}

\begin{defn}\label{2.1.12}
  Let \(\V\) and \(\W\) be vector spaces over \(\F\), and let \(\T : \V \to \W\) be linear.
  If \(\ns{\T}\) and \(\rg{\T}\) are finite-dimensional, then we define the \textbf{nullity} of \(\T\), denoted \(\nt{\T}\), and the \textbf{rank} of \(\T\), denoted \(\rk{\T}\), to be the dimensions of \(\ns{\T}\) and \(\rg{\T}\), respectively.
\end{defn}

\begin{note}
  Reflecting on the action of a linear transformation, we see intuitively that the larger the nullity, the smaller the rank.
  In other words, the more vectors that are carried into \(\zv\), the smaller the range.
  The same heuristic reasoning tells us that the larger the rank, the smaller the nullity.
\end{note}

\begin{thm}[Dimension Theorem]\label{2.3}
  Let \(\V\) and \(\W\) be vector spaces over \(\F\), and let \(\T : \V \to \W\) be linear.
  If \(\V\) is finite-dimensional, then
  \[
    \nt{\T} + \rk{\T} = \dim(\V).
  \]
\end{thm}

\begin{proof}[\pf{2.3}]
  Suppose that \(\dim(\V) = n\), \(\dim(\ns{\T}) = k\), and \(\set{\seq{v}{1,2,,k}}\) is a basis for \(\ns{\T}\) over \(\F\).
  By the \cref{1.6.19} we may extend \(\set{\seq{v}{1,2,,k}}\) to a basis \(\beta = \set{\seq{v}{1,2,,n}}\) for \(\V\) over \(\F\).
  We claim that \(S = \set{\T(v_{k + 1}), \T(v_{k + 2}), \dots, \T(v_n)}\) is a basis for \(\rg{\T}\) over \(\F\).

  First we prove that \(S\) generates \(\rg{\T}\).
  Using \cref{2.2} and the fact that \(\T(v_i) = \zv\) for \(1 \leq i \leq k\), we have
  \begin{align*}
    \rg{\T} & = \spn{\set{\T(v_1), \T(v_2), \dots, \T(v_n)}}             \\
            & = \spn{\set{\T(v_{k + 1}), \T(v_{k + 2}), \dots, \T(v_n)}} \\
            & = \spn{S}.
  \end{align*}
  Now we prove that \(S\) is linearly independent.
  Suppose that
  \[
    \sum_{i = k + 1}^n b_i \T(v_i) = \zv \quad \text{for } \seq{b}{k + 1,k + 2,,n} \in \F.
  \]
  Using the fact that \(\T\) is linear, we have
  \[
    \T\pa{\sum_{i = k + 1}^n b_i v_i} = \zv.
  \]
  So
  \[
    \sum_{i = k + 1}^n b_i v_i \in \ns{\T}.
  \]
  Hence there exist \(\seq{c}{1,2,,k} \in \F\) such that
  \[
    \sum_{i = k + 1}^n b_i v_i = \sum_{i = 1}^k c_i v_i \quad \text{or} \quad \sum_{i = 1}^k (-c_i) v_i + \sum_{i = k + 1}^n b_i v_i = \zv.
  \]
  Since \(\beta\) is a basis for \(\V\) over \(\F\), we have \(b_i = 0\) for all \(i\).
  Hence \(S\) is linearly independent.
  Notice that this argument also shows that \(\T(v_{k + 1}), \T(v_{k + 2}), \dots, \T(v_n)\) are distinct;
  therefore \(\rk{\T} = n - k\).
\end{proof}

\exercisesection

\begin{ex}\label{ex:2.1.33}
  Prove \cref{2.2} for the case that \(\beta\) is infinite, that is, \(\rg{\T} = \spn{\set{\T(v) : v \in \beta}}\).
\end{ex}

\begin{ex}\label{ex:2.1.37}
  A function \(\T : \V \to \W\) between vector spaces \(\V\) and \(\W\) over \(\F\) is called \textbf{additive} if \(\T(x + y) = \T(x) + \T(y)\) for all \(x, y \in \V\).
  Prove that if \(\V\) and \(\W\) are vector spaces over the field of rational numbers \(\Q\), then any additive function from \(\V\) into \(\W\) is a linear transformation.
\end{ex}
