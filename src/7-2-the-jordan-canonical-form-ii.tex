\section{The Jordan Canonical Form II}\label{sec:7.2}

\begin{defn}\label{7.2.1}
  For the purposes of this section, we fix a linear operator \(\T\) on an \(n\)-dimensional vector space \(\V\) over \(\F\) such that the characteristic polynomial of \(\T\) splits.
  Let \(\seq{\lambda}{1,,k}\) be the distinct eigenvalues of \(\T\).

  By \cref{7.7}, each generalized eigenspace \(\vs{K}_{\lambda_i}\) contains an ordered basis \(\beta_i\) consisting of a union of disjoint cycles of generalized eigenvectors corresponding to \(\lambda_i\).
  So by \cref{7.4}(b) and \cref{7.5}, the union \(\beta = \bigcup_{i = 1}^k \beta_i\) is a Jordan canonical basis for \(\T\).
  For each \(i \in \set{1, \dots, k}\), let \(\T_i\) be the restriction of \(\T\) to \(\vs{K}_{\lambda_i}\), and let \(A_i = [\T_i]_{\beta_i}\).
  Then \(A_i\) is a Jordan canonical form of \(\T_i\), and
  \[
    J = [\T]_{\beta} = \begin{pmatrix}
      A_1    & \zm    & \cdots & \zm    \\
      \zm    & A_2    & \cdots & \zm    \\
      \vdots & \vdots &        & \vdots \\
      \zm    & \zm    & \cdots & A_k
    \end{pmatrix}
  \]
  is a Jordan canonical form of \(\T\).
  In this matrix, each \(\zm\) is a zero matrix of appropriate size.

  In this section, we compute the matrices \(A_i\) and the bases \(\beta_i\), thereby computing \(J\) and \(\beta\) as well.
  While developing a method for finding \(J\), it becomes evident that in some sense the matrices \(A_i\) are unique.

  To aid in formulating the uniqueness theorem for \(J\), we adopt the following convention:
  The basis \(\beta_i\) for \(\vs{K}_{\lambda_i}\) will henceforth be ordered in such a way that the cycles appear in order of decreasing length.
  That is, if \(\beta_i\) is a disjoint union of cycles \(\seq{\gamma}{1,,n_i}\) and if the length of the cycle \(\gamma_j\) is \(p_j\), we index the cycles so that \(\seq[\geq]{p}{1,,n_i}\).
  This ordering of the cycles limits the possible orderings of vectors in \(\beta_i\), which in turn determines the matrix \(A_i\).
  It is in this sense that \(A_i\) is unique.
  It then follows that the Jordan canonical form for \(\T\) is unique up to an ordering of the eigenvalues of \(\T\).
  As we will see, there is no uniqueness theorem for the bases \(\beta_i\) or for \(\beta\)
  (See \cref{ex:7.2.8}, it is for this reason that we associate the dot diagram with \(\T_i\) rather than with \(\beta_i\)).
  Specifically, we show that for each \(i \in \set{1, \dots, k}\), the number \(n_i\) of cycles that form \(\beta_i\), and the length \(p_j\) (\(j \in \set{1, \dots, n_i}\)) of each cycle, is completely determined by \(\T\).

  To help us visualize each of the matrices \(A_i\) and ordered bases \(\beta_i\), we use an array of dots called a \textbf{dot diagram} of \(\T_i\), where \(\T_i\) is the restriction of \(\T\) to \(\vs{K}_{\lambda_i}\).
  Suppose that \(\beta_i\) is a disjoint union of cycles of generalized eigenvectors \(\seq{\gamma}{1,,n_i}\) with lengths \(\seq[\geq]{p}{1,,n_i}\), respectively.
  The dot diagram of \(\T_i\) contains one dot for each vector in \(\beta_i\), and the dots are configured according to the following rules.
  \begin{itemize}
    \item The array consists of \(n_i\) columns (one column for each cycle).
    \item Counting from left to right, the \(j\)th column consists of the \(p_j\) dots that correspond to the vectors of \(\gamma_j\) starting with the initial vector at the top and continuing down to the end vector.
  \end{itemize}

  Denote the end vectors of the cycles by \(\seq{v}{1,,n_i}\).
  In the following dot diagram of \(\T_i\), each dot is labeled with the name of the vector in \(\beta_i\) to which it corresponds.
  \begin{align*}
     & \bullet (\T - \lambda_i \IT[\V])^{p_1 - 1}(v_1) &  & \bullet (\T - \lambda_i \IT[\V])^{p_2 - 1}(v_2) &  & \cdots &  & \bullet (\T - \lambda_i \IT[\V])^{p_{n_i} - 1}(v_{n_i}) \\
     & \bullet (\T - \lambda_i \IT[\V])^{p_1 - 2}(v_1) &  & \bullet (\T - \lambda_i \IT[\V])^{p_2 - 2}(v_2) &  & \cdots &  & \bullet (\T - \lambda_i \IT[\V])^{p_{n_i} - 2}(v_{n_i}) \\
     & \vdots                                          &  & \vdots                                          &  &        &  & \vdots                                                  \\
     &                                                 &  &                                                 &  &        &  & \bullet (\T - \lambda_i \IT[\V])(v_{n_i})               \\
     &                                                 &  &                                                 &  &        &  & \bullet v_{n_i}                                         \\
     &                                                 &  & \bullet (\T - \lambda_i \IT[\V])(v_2)           &  &        &  &                                                         \\
     &                                                 &  & \bullet v_2                                     &  &        &  &                                                         \\
     & \bullet (\T - \lambda_i \IT[\V])(v_1)           &  &                                                 &  &        &  &                                                         \\
     & \bullet v_1                                     &  &                                                 &  &        &  &
  \end{align*}
  Notice that the dot diagram of \(\T_i\) has \(n_i\) columns (one for each cycle) and \(p_1\) rows.
  Since \(\seq[\geq]{p}{1,,n_i}\), the columns of the dot diagram become shorter (or at least not longer) as we move from left to right.

  Now let \(r_j\) denote the number of dots in the \(j\)th row of the dot diagram.
  Observe that \(\seq[\geq]{r}{1,,p_1}\).
  Furthermore, the diagram can be reconstructed from the values of the \(r_i\)'s.
  The proofs of these facts, which are combinatorial in nature, are treated in \cref{ex:7.2.9}.
\end{defn}

\begin{thm}\label{7.9}
  Using the notations in \cref{7.2.1}.
  For any positive integer \(r\), the vectors in \(\beta_i\) that are associated with the dots in the first \(r\) rows of the dot diagram of \(\T_i\) constitute a basis for \(\ns{(\T - \lambda_i \IT[\V])^r}\) over \(\F\).
  Hence the number of dots in the first \(r\) rows of the dot diagram equals \(\nt{(\T - \lambda_i \IT[\V])^r}\).
\end{thm}

\begin{proof}[\pf{7.9}]
  By \cref{7.1.4} \(\ns{(\T - \lambda_i \IT[\V])^r} \subseteq \vs{K}_{\lambda_i}\), and \(\vs{K}_{\lambda_i}\) is invariant under \((\T - \lambda_i \IT[\V])^r\).
  Let \(\U\) denote the restriction of \((\T - \lambda_i \IT[\V])^r\) to \(\vs{K}_{\lambda_i}\).
  By \cref{7.1}(b), \(\ns{(\T - \lambda_i \IT[\V])^r} = \ns{\U}\), and hence it suffices to establish the theorem for \(\U\).
  Now define
  \[
    S_1 = \set{x \in \beta_i : \U(x) = \zv} \quad \text{and} \quad S_2 = \set{x \in \beta_i : \U(x) \neq \zv}.
  \]
  Let \(a\) and \(b\) denote the number of vectors in \(S_1\) and \(S_2\), respectively, and let \(m_i = \dim(\vs{K}_{\lambda_i})\).
  Then \(a + b = m_i\).
  For any \(x \in \beta_i\), \(x \in S_1\) iff \(x\) is one of the first \(r\) vectors of a cycle (\cref{7.1.6}), and this is true iff \(x\) corresponds to a dot in the first \(r\) rows of the dot diagram (\cref{7.2.1}).
  Hence \(a\) is the number of dots in the first \(r\) rows of the dot diagram.
  For any \(x \in S_2\), the effect of applying \(\U\) to \(x\) is to move the dot corresponding to \(x\) exactly \(r\) places up its column to another dot.
  It follows that \(\U\) maps \(S_2\) in a one-to-one fashion into \(\beta_i\) (since \(\beta_i\) is linearly independent).
  Thus \(\set{\U(x) : x \in S_2}\) is a basis for \(\rg{\U}\) over \(\F\) consisting of \(b\) vectors (\cref{2.2}).
  Hence \(\rk{\U} = b\), and so \(\nt{\U} = m_i - b = a\) (\cref{2.3}).
  But \(S_1\) is a linearly independent subset of \(\ns{\U}\) consisting of \(a\) vectors;
  therefore \(S_1\) is a basis for \(\ns{\U}\).
\end{proof}

\begin{note}
  In the case that \(r = 1\), \cref{7.9} yields \cref{7.2.2}.
\end{note}

\begin{cor}\label{7.2.2}
  Using the notations in \cref{7.2.1}.
  The dimension of \(\vs{E}_{\lambda_i}\) is \(n_i\).
  Hence in a Jordan canonical form of \(\T\), the number of Jordan blocks corresponding to \(\lambda_i\) equals the dimension of \(\vs{E}_{\lambda_i}\).
\end{cor}

\begin{proof}[\pf{7.2.2}]
  By \cref{5.4} we have \(\vs{E}_{\lambda_i} = \ns{\T - \lambda_i \IT[\V]}\).
  Thus by \cref{7.9} the first row of the dot diagram of \(\T_i\) constitute a basis for \(\vs{E}_{\lambda_i}\) over \(\F\).
\end{proof}

\begin{thm}\label{7.10}
  Using the notations in \cref{7.2.1}.
  Let \(r_j\) denote the number of dots in the \(j\)th row of the dot diagram of \(\T_i\), the restriction of \(\T\) to \(\vs{K}_{\lambda_i}\).
  Then the following statements are true.
  \begin{enumerate}
    \item \(r_1 = \dim(\V) - \rk{\T - \lambda_i \IT[\V]}\).
    \item \(r_j = \rk{(\T - \lambda_i \IT[\V])^{j - 1}} - \rk{(\T - \lambda_i \IT[\V])^j}\) if \(j \in \set{2, \dots, p_1}\).
  \end{enumerate}
\end{thm}

\begin{proof}[\pf{7.10}]
  By \cref{7.9}, for \(j \in \set{1, \dots, p_1}\), we have
  \begin{align*}
    \seq[+]{r}{1,,j} & = \nt{(\T - \lambda_i \IT[\V])^j}             &  & \by{7.9} \\
                     & = \dim(\V) - \rk{(\T - \lambda_i \IT[\V])^j}. &  & \by{2.3}
  \end{align*}
  Hence \(r_1 = \dim(\V) - \rk{\T - \lambda_i \IT[\V]}\), and for \(j \in \set{2, \dots, p_1}\),
  \begin{align*}
    r_j & = (\seq[+]{r}{1,,j}) - (\seq[+]{r}{1,,j-1})                                                               \\
        & = \pa{\dim(\V) - \rk{(\T - \lambda_i \IT[\V])^j}} - \pa{\dim(\V) - \rk{(\T - \lambda_i \IT[\V])^{j - 1}}} \\
        & = \rk{(\T - \lambda_i \IT[\V])^{j - 1}} - \rk{(\T - \lambda_i \IT[\V])^j}.
  \end{align*}
\end{proof}

\begin{cor}\label{7.2.3}
  Using the notations in \cref{7.2.1}.
  For any eigenvalue \(\lambda_i\) of \(\T\), the dot diagram of \(\T_i\) is unique.
  Thus, subject to the convention that the cycles of generalized eigenvectors for the bases of each generalized eigenspace are listed in order of decreasing length, the Jordan canonical form of a linear operator or a matrix is unique up to the ordering of the eigenvalues.
\end{cor}

\begin{proof}[\pf{7.2.3}]
  \cref{7.10} shows that the dot diagram of \(\T_i\) is completely determined by \(\T\) and \(\lambda_i\).
\end{proof}

\begin{thm}\label{7.11}
  Let \(A\) and \(B\) be \(n \times n\) matrices, each having Jordan canonical forms computed according to the conventions of this section.
  Then \(A\) and \(B\) are similar iff they have (up to an ordering of their eigenvalues) the same Jordan canonical form.
\end{thm}

\begin{proof}[\pf{7.11}]
  If \(A\) and \(B\) have the same Jordan canonical form \(J\), then \(A\) and \(B\) are each similar to \(J\) and hence are similar to each other.

  Conversely, suppose that \(A\) and \(B\) are similar.
  Then \(A\) and \(B\) have the same eigenvalues (\cref{ex:5.1.12}).
  Let \(J_A\) and \(J_B\) denote the Jordan canonical forms of \(A\) and \(B\), respectively, with the same ordering of their eigenvalues.
  Then \(A\) is similar to both \(J_A\) and \(J_B\), and therefore, by the \cref{2.5.3}, \(J_A\) and \(J_B\) are matrix representations of \(\L_A\).
  Hence \(J_A\) and \(J_B\) are Jordan canonical forms of \(\L_A\). Thus \(J_A = J_B\) by the \cref{7.2.3}.
\end{proof}

\begin{cor}\label{7.2.4}
  A linear operator \(\T\) on a finite-dimensional vector space \(\V\) over \(\F\) is diagonalizable iff its Jordan canonical form is a diagonal matrix.
  Hence \(\T\) is diagonalizable iff the Jordan canonical basis for \(\T\) consists of eigenvectors of \(\T\).
\end{cor}

\begin{proof}[\pf{7.2.4}]
  By \cref{7.1.5,7.2.3} we see that this is true.
\end{proof}

\exercisesection

\setcounter{ex}{5}
\begin{ex}\label{ex:7.2.6}
  Let \(A \in \ms[n][n][\F]\) whose characteristic polynomial splits.
  Prove that \(A\) and \(\tp{A}\) have the same Jordan canonical form, and conclude that \(A\) and \(\tp{A}\) are similar.
\end{ex}

\begin{proof}[\pf{ex:7.2.6}]
  Let \(\lambda \in \F\) be an eigenvalue of \(A\).
  Since
  \begin{align*}
    \forall r \in \Z^+, \rk{\pa{A - \lambda I_n}^r} & = \rk{\tp{\pa{(A - \lambda I_n)^r}}} &  & \by{3.2.5}[a] \\
                                                    & = \rk{\pa{\tp{(A - \lambda I_n)}}^r} &  & \by{2.3.2}    \\
                                                    & = \rk{\pa{\tp{A} - \lambda I_n}^r},  &  & \by{ex:1.3.3}
  \end{align*}
  by \cref{7.10} we see that the dot diagrams of \(A\) and \(\tp{A}\) correspond to \(\lambda\) are the same.
  Since \(\lambda\) is arbitrary, by \cref{7.2.3} we conclude that \(A\) and \(\tp{A}\) have the same Jordan canonical form.
  By \cref{7.11} this means \(A\) and \(\tp{A}\) are similar.
\end{proof}

\begin{ex}\label{ex:7.2.7}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\) such that the characteristic polynomial of \(\T\) splits.
  Let \(\gamma\) be a cycle of generalized eigenvectors corresponding to an eigenvalue \(\lambda\), and \(\W\) be the subspace spanned by \(\gamma\).
  Define \(\gamma'\) to be the ordered set obtained from \(\gamma\) by reversing the order of the vectors in \(\gamma\).
  \begin{enumerate}
    \item Prove that \([\T_{\W}]_{\gamma'} = \tp{([\T_{\W}]_{\gamma})}\).
    \item Let \(J\) be the Jordan canonical form of \(\T\).
          Use (a) to prove that \(J\) and \(\tp{J}\) are similar.
    \item Let \(A \in \ms[n][n][\F]\) whose characteristic polynomial splits.
          Use (b) to prove that \(A\) and \(\tp{A}\) are similar.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:7.2.7}(a)]
  Let \(\gamma = \set{\seq{v}{1,,m}}\).
  By \cref{7.1.6} we have
  \[
    \forall i \in \set{1, \dots, m}, \T(v_i) = \begin{dcases}
      \lambda v_i             & \text{if } i = 1                   \\
      \lambda v_i + v_{i - 1} & \text{if } i \in \set{2, \dots, m}
    \end{dcases}.
  \]
  Now define \(\gamma' = \set{v_1', \dots, v_m'}\).
  By definition we have \(v_i' = v_{m + 1 - i}\) for all \(i \in \set{1, \dots, m}\) and
  \begin{align*}
    \forall i \in \set{1, \dots, m}, \T(v_i') & = \T(v_{m + 1 - i})                                                              \\
                                              & = \begin{dcases}
                                                    \lambda v_{m + 1 - i}             & \text{if } m + 1 - i = 1                   \\
                                                    \lambda v_{m + 1 - i} + v_{m - i} & \text{if } m + 1 - i \in \set{2, \dots, m}
                                                  \end{dcases} \\
                                              & = \begin{dcases}
                                                    \lambda v_i'              & \text{if } i = m                       \\
                                                    \lambda v_i' + v_{i + 1}' & \text{if } i \in \set{1, \dots, m - 1}
                                                  \end{dcases}.
  \end{align*}
  Thus we have
  \begin{align*}
    \forall i, j \in \set{1, \dots, m}, \pa{\tp{([\T_{\W}]_{\gamma})}}_{i j} & = ([\T_{\W}]_{\gamma})_{j i}      &  & \by{1.3.3} \\
                                                                             & = \begin{dcases}
                                                                                   \lambda & \text{if } i = j     \\
                                                                                   1       & \text{if } i + 1 = j \\
                                                                                   0       & \text{otherwise}
                                                                                 \end{dcases} &  & \by{7.1.1}                  \\
                                                                             & = ([\T_{\W}]_{\gamma'})_{i j}.    &  & \by{2.2.4}
  \end{align*}
  By \cref{1.2.8} this means \(\tp{([\T_{\W}]_{\beta})} = [\T_{\W}]_{\gamma'}\).
\end{proof}

\begin{proof}[\pf{ex:7.2.7}(b)]
  Let \(\beta\) be an Jordan canonical basis following the convention in \cref{7.2.1}.
  Let \(J = [\T]_{\beta}\).
  Let \(\beta'\) be the ordered set obtained from \(\beta\) by reversing the ordered of each disjoint cycles in \(\beta\).
  By \cref{5.25} and \cref{ex:7.2.7}(a) we see that \([\T]_{\beta'} = \tp{([\T]_{\beta})} = \tp{J}\).
  By \cref{2.23} we hve \([\T]_{\beta'} = \pa{[\IT[\V]]_{\beta'}^{\beta}}^{-1} [\T]_{\beta} [\IT[\V]]_{\beta'}^{\beta}\).
  Thus by \cref{2.5.4} \(J\) and \(\tp{J}\) are similar.
\end{proof}

\begin{proof}[\pf{ex:7.2.7}(c)]
  Let \(\beta\) be the standard ordered basis for \(\vs{F}^n\) over \(\F\) and let \(\alpha\) be a Jordan canonical basis for \(A\).
  By \cref{ex:7.2.7}(b) we see that \([\L_A]_{\alpha}\) and \(\tp{([\L_A]_{\alpha})}\) are similar.
  By \cref{2.23} we know that \(A = [\L_A]_{\beta}\) and \([\L_A]_{\alpha}\) are similar.
  Thus by \cref{ex:2.5.9} we know that \(A\) and \(\tp{([\L_A]_{\alpha})}\) are similar.
  If we can show that \(\tp{A}\) and \(\tp{([\L_A]_{\alpha})}\) are similar, then by \cref{ex:2.5.9} again we see that \(A\) and \(\tp{A}\) are similar.
  This is true since
  \begin{align*}
    \tp{A} & = \tp{([\L_A]_{\beta})}                                                                                       &  & \by{2.15}[a]  \\
           & = \tp{\pa{\pa{[\IT[\V]]_{\beta}^{\alpha}}^{-1} [\L_A]_{\alpha} [\IT[\V]]_{\beta}^{\alpha}}}                   &  & \by{2.23}     \\
           & = \tp{([\IT[\V]]_{\beta}^{\alpha})} \tp{([\L_A]_{\alpha})} \tp{\pa{\pa{[\IT[\V]]_{\beta}^{\alpha}}^{-1}}}     &  & \by{2.3.2}    \\
           & = \tp{\pa{\pa{[\IT[\V]]_{\alpha}^{\beta}}^{-1}}} \tp{([\L_A]_{\alpha})} \tp{\pa{[\IT[\V]]_{\alpha}^{\beta}}}  &  & \by{2.23}     \\
           & = \pa{\tp{\pa{[\IT[\V]]_{\alpha}^{\beta}}}}^{-1} \tp{([\L_A]_{\alpha})} \tp{\pa{[\IT[\V]]_{\alpha}^{\beta}}}. &  & \by{ex:2.4.5}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:7.2.8}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and suppose that the characteristic polynomial of \(\T\) splits.
  Let \(\beta\) be a Jordan canonical basis for \(\T\).
  \begin{enumerate}
    \item Prove that for any nonzero scalar \(c\), \(\set{cx : x \in \beta}\) is a Jordan canonical basis for \(\T\).
    \item Suppose that \(\gamma\) is one of the cycles of generalized eigenvectors that forms \(\beta\), and suppose that \(\gamma\) corresponds to the eigenvalue \(\lambda\) and has length greater than \(1\).
          Let \(x\) be the end vector of \(\gamma\), and let \(y\) be a nonzero vector in \(\vs{E}_{\lambda}\).
          Let \(\gamma'\) be the ordered set obtained from \(\gamma\) by replacing \(x\) by \(x + y\).
          Prove that \(\gamma'\) is a cycle of generalized eigenvectors corresponding to \(\lambda\), and that if \(\gamma'\) replaces \(\gamma\) in the union that defines \(\beta\), then the new union is also a Jordan canonical basis for \(\T\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:7.2.8}(a)]
  Let \(\lambda\) be an eigenvalue of \(\T\) and let \(x \in \beta\) be an generalized eigenvector of \(\T\) corresponding to \(\lambda\).
  If \(x \in \vs{E}_{\lambda}\), then by \cref{5.1.2} we have \(\T(x) = \lambda x\).
  If \(x \in \vs{K}_{\lambda} \setminus \vs{E}_{\lambda}\), then by \cref{7.1.1} there exists a \(v \in \beta\) such that \(\T(x) = \lambda x + v\).
  In either cases we have
  \[
    \T(cx) = c \T(x) = \begin{dcases}
      c \lambda x \\
      c \lambda x + cv
    \end{dcases} = \begin{dcases}
      \lambda (cx) \\
      \lambda (cx) + (cv)
    \end{dcases}.
  \]
  Thus by \cref{7.1.1} \(\set{cx : x \in \beta}\) is a Jordan canonical basis for \(\T\).
\end{proof}

\begin{proof}[\pf{ex:7.2.8}(b)]
  Since
  \begin{align*}
    (\T - \lambda \IT[\V])(x + y) & = (\T - \lambda \IT[\V])(x) + (\T - \lambda \IT[\V])(y) &  & \by{2.1.1}[a] \\
                                  & = (\T - \lambda \IT[\V])(x) + \zv                       &  & \by{5.4}      \\
                                  & = (\T - \lambda \IT[\V])(x),                            &  & \by{1.2.1}
  \end{align*}
  we see that \((\T - \lambda \IT[\V])^p(x + y) = (\T - \lambda \IT[\V])^p(x)\) for any \(p \in \Z^+\).
  Thus \(\gamma'\) is a cycle of generalized eigenvectors corresponding to \(\lambda\), and the rest claim follows.
\end{proof}

\begin{ex}\label{ex:7.2.9}
  Suppose that a dot diagram has \(k\) columns and \(m\) rows with \(p_j\) dots in column \(j\) and \(r_i\) dots in row \(i\).
  Prove the following results.
  \begin{enumerate}
    \item \(m = p_1\) and \(k = r_1\).
    \item We have
          \begin{align*}
             & \forall j \in \set{1, \dots, k}, p_j = \max \set{i \in \set{1, \dots, m} : r_i \geq j}; \\
             & \forall i \in \set{1, \dots, m}, r_i = \max \set{j \in \set{1, \dots, k} : p_j \geq i}.
          \end{align*}
    \item \(\seq[\geq]{r}{1,,m}\).
    \item Deduce that the number of dots in each column of a dot diagram is completely determined by the number of dots in the rows.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:7.2.9}(a)]
  By \cref{7.2.1} we know that \(p_1\) is the column with the most number of dots.
  Thus \(m = p_1\).
  Since the number of columns equals the number of disjoint cycles, we have \(k = r_1\).
\end{proof}


\begin{proof}[\pf{ex:7.2.9}(b)]
  We first fix \(k\) and use induction on \(m\) to prove that
  \[
    \forall i \in \set{1, \dots, m}, r_i = \max \set{j \in \set{1, \dots, k} : p_j \geq i}.
  \]
  For \(m = 1\), our dot diagram have \(1\) row and \(k\) columns.
  This means the first row has \(k\) dots and each column has one dot.
  Thus
  \begin{align*}
    \forall i \in \set{1}, r_i & = k                                                    \\
                               & = \max \set{1, \dots, k}                               \\
                               & = \max \set{j \in \set{1, \dots, k} : 1 = p_j \geq i}.
  \end{align*}
  So the base case holds.
  Suppose inductively that for some \(m \geq 1\) the statement is true.
  We need to show that for \(m + 1\) the statement is also true.
  So suppose that there are \(k\) columns and \(m + 1\) rows in a dot diagram.
  By \cref{7.2.1} we see that columns in dot diagram are ordered by decreasing length.
  Thus the first \(r_{m + 1}\) columns are the longest columns of all.
  Since there are \(m + 1\) rows, we know that the first \(r_{m + 1}\) columns have \(m + 1\) dots and the rest \((k - r_{m + 1})\) columns have less than \(m + 1\) dots.
  Thus we have
  \begin{align*}
             & \begin{dcases}
                 \forall j \in \set{1, \dots, r_{m + 1}}, p_j = m + 1 \\
                 \forall j \in \set{r_{m + 1} + 1, \dots, k}, p_j < m + 1
               \end{dcases} \\
    \implies & r_{m + 1} = \max \set{1, \dots, r_{m + 1}}                                   \\
             & = \max \set{j \in \set{1, \dots, r_{m + 1}} : p_j \geq m + 1}                \\
             & = \max \set{j \in \set{1, \dots, k} : p_j \geq m + 1}.
  \end{align*}
  By induction hypothesis we have
  \[
    \forall i \in \set{1, \dots, m + 1}, r_i = \max \set{j \in \set{1, \dots, k} : p_j \geq i}.
  \]
  This closes the induction.

  Now we fix \(m\) and use induction on \(k\) to prove that
  \[
    \forall j \in \set{1, \dots, k}, p_j = \max \set{i \in \set{1, \dots, m} : r_i \geq j}.
  \]
  For \(k = 1\), our dot diagram have \(m\) rows and \(1\) column.
  This means the first column has \(m\) dots and each row has one dot.
  Thus
  \begin{align*}
    \forall j \in \set{1}, p_j & = m                                                    \\
                               & = \max \set{1, \dots, m}                               \\
                               & = \max \set{i \in \set{1, \dots, m} : 1 = r_i \geq j}.
  \end{align*}
  So the base case holds.
  Suppose inductively that for some \(k \geq 1\) the statement is true.
  We need to show that for \(k + 1\) the statement is also true.
  So suppose that there are \(k + 1\) columns and \(m\) rows in a dot diagram.
  By \cref{7.2.1} we see that columns in dot diagram are ordered by decreasing length.
  Thus the first \(p_{k + 1}\) rows are the longest rows of all.
  Since there are \(k + 1\) columns, we know that the first \(p_{k + 1}\) rows have \(k + 1\) dots and the rest \((m - p_{k + 1})\) rows have less than \(k + 1\) dots.
  Thus we have
  \begin{align*}
             & \begin{dcases}
                 \forall i \in \set{1, \dots, p_{k + 1}}, r_i = k + 1 \\
                 \forall i \in \set{p_{k + 1} + 1, \dots, m}, r_i < k + 1
               \end{dcases} \\
    \implies & p_{k + 1} = \max \set{1, \dots, p_{k + 1}}                                   \\
             & = \max \set{i \in \set{1, \dots, p_{k + 1}} : r_i \geq k + 1}                \\
             & = \max \set{i \in \set{1, \dots, m} : r_i \geq k + 1}.
  \end{align*}
  By induction hypothesis we have
  \[
    \forall j \in \set{1, \dots, k + 1}, p_j = \max \set{i \in \set{1, \dots, m} : r_i \geq j}.
  \]
  This closes the induction.
\end{proof}

\begin{proof}[\pf{ex:7.2.9}(c)]
  Suppose that \(i_1, i_2 \in \set{1, \dots, m}\) and \(i_1 < i_2\).
  Since
  \begin{align*}
             & i_1 < i_2                                                                                                                      \\
    \implies & \set{j \in \set{1, \dots, k} : p_j \geq i_2} \subseteq \set{j \in \set{1, \dots, k} : p_j \geq i_1}                            \\
    \implies & \max \set{j \in \set{1, \dots, k} : p_j \geq i_2} \leq \max \set{j \in \set{1, \dots, k} : p_j \geq i_1} &  & \by{7.2.1}       \\
    \implies & r_{i_2} \leq r_{i_1}                                                                                     &  & \by{ex:7.2.9}[b]
  \end{align*}
  and \(i_1, i_2\) are arbitrary, we have \(\seq[\geq]{r}{1,,m}\).
\end{proof}

\begin{proof}[\pf{ex:7.2.9}(d)]
  By \cref{ex:7.2.9}(b) we have
  \[
    \forall j \in \set{1, \dots, k}, p_j = \max \set{i \in \set{1, \dots, m} : r_i \geq j}.
  \]
  This means when number of dots in the rows of a dot diagram is defined, the number of dots in the columns of the same dot diagram is also defined.
\end{proof}

\begin{ex}\label{ex:7.2.10}
  Let \(\T\) be a linear operator whose characteristic polynomial splits, and let \(\lambda\) be an eigenvalue of \(\T\).
  \begin{enumerate}
    \item Prove that \(\dim(\vs{K}_{\lambda})\) is the sum of the lengths of all the cycles corresponding to \(\lambda\) in the Jordan canonical form of \(\T\).
    \item Deduce that \(\vs{E}_{\lambda} = \vs{K}_{\lambda}\) iff all the Jordan blocks corresponding to \(\lambda\) are \(1 \times 1\) matrices.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:7.2.10}(a)]
  By \cref{7.6} we know that the union of disjoint cycles are linearly independent.
  By \cref{7.5} each cycle forms a Jordan block corresponding to \(\lambda\), and the union of cycles is a basis for \(\vs{K}_{\lambda}\).
  Thus \(\dim(\vs{K}_{\lambda})\) is the sum of the lengths of all cycles corresponding to \(\lambda\).
\end{proof}

\begin{proof}[\pf{ex:7.2.10}(b)]
  We have
  \begin{align*}
         & \vs{E}_{\lambda} = \vs{K}_{\lambda}                                                         \\
    \iff & \text{each cycle has length } 1                                             &  & \by{5.4}   \\
    \iff & \text{all Jordan blocks corresponding to } \lambda \text{ are } 1 \times 1. &  & \by{7.1.1}
  \end{align*}
\end{proof}

\begin{defn}\label{7.2.5}
  A linear operator \(\T\) on a vector space \(\V\) is called \textbf{nilpotent} if \(\T^p = \zT\) for some positive integer \(p\).
  An \(n \times n\) matrix \(A\) is called \textbf{nilpotent} if \(A^p = \zm\) for some positive integer \(p\).
\end{defn}

\begin{ex}\label{ex:7.2.11}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\), and let \(\beta\) be an ordered basis for \(\V\) over \(\F\).
  Prove that \(\T\) is nilpotent iff \([\T]_{\beta}\) is nilpotent.
\end{ex}

\begin{proof}[\pf{ex:7.2.11}]
  We have
  \begin{align*}
         & \T \text{ is nilpotent}                                                                            \\
    \iff & \exists p \in \Z^+ : \T^p = \zT                                              &  & \by{7.2.5}       \\
    \iff & \exists p \in \Z^+ : ([\T]_{\beta})^p = [\T^p]_{\beta} = [\zT]_{\beta} = \zm &  & \by{2.11,2.1.13} \\
    \iff & [\T]_{\beta} \text{ is nilpotent}.                                           &  & \by{7.2.5}
  \end{align*}
\end{proof}

\begin{ex}\label{ex:7.2.12}
  Prove that any square upper triangular matrix with each diagonal entry equal to zero is nilpotent.
\end{ex}

\begin{proof}[\pf{ex:7.2.12}]
  Let \(A \in \ms[n][n][\F]\) be an upper triangular matrix with each diagonal entry equal to zero.
  Let \(\beta = \set{\seq{e}{1,,n}}\) be the standard ordered basis for \(\vs{F}^n\) over \(\F\).
  By \cref{2.2.4} we have \(\L_A(e_1) = \zv\) and
  \[
    \forall i \in \set{2, \dots, n}, \L_A(e_i) = \sum_{k = 1}^{i - 1} A_{k i} e_k.
  \]
  This means \(\L_A^n(e_i) = \zv\) for all \(i \in \set{1, \dots, n}\).
  Thus by \cref{2.1.13} \(\L_A^p = \zT\).
  By \cref{7.2.5} this means \(\L_A\) is nilpotent.
  By \cref{ex:7.2.11} we conclude that \(A\) is nilpotent.
\end{proof}

\begin{ex}\label{ex:7.2.13}
  Let \(\T\) be a nilpotent operator on an \(n\)-dimensional vector space \(\V\) over \(\F\), and suppose that \(p\) is the smallest positive integer for which \(\T^p = \zT\).
  Prove the following results.
  \begin{enumerate}
    \item \(\ns{\T^i} \subseteq \ns{\T^{i + 1}}\) for every positive integer \(i\).
    \item There is a sequence of ordered bases \(\seq{\beta}{1,,p}\) such that \(\beta_i\) is a basis for \(\ns{\T^i}\) over \(\F\) and \(\beta_{i + 1}\) contains \(\beta_i\) for \(i \in \set{1, \dots, p - 1}\).
    \item Let \(\beta = \beta_p\) be the ordered basis for \(\ns{\T^p} = \V\) over \(\F\) in (b).
          Then \([\T]_{\beta}\) is an upper triangular matrix with each diagonal entry equal to zero.
    \item The characteristic polynomial of \(\T\) is \((-1)^n t^n\).
          Hence the characteristic polynomial of \(\T\) splits, and \(0\) is the only eigenvalue of \(\T\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:7.2.13}(a)]
  See \cref{ex:7.1.7}(a).
\end{proof}

\begin{proof}[\pf{ex:7.2.13}(b)]
  Let \(\beta_1\) be a basis for \(\ns{\T}\) over \(\F\).
  Since \(\beta_1 \subseteq \ns{\T} \subseteq \ns{\T^2}\), by \cref{1.6.15}(c) we can extend \(\beta_1\) to an ordered basis \(\beta_2\) for \(\ns{\T^2}\) over \(\F\).
  Similarly we can extend \(\beta_2\) to an ordered basis \(\beta_3\) for \(\ns{\T^3}\) over \(\F\).
  Continue this process we can construct the sequence \(\seq{\beta}{1,,p}\) which satisfy the requirement of \cref{ex:7.2.13}(b).
\end{proof}

\begin{proof}[\pf{ex:7.2.13}(c)]
  Let \(i \in \set{1, \dots, p - 1}\).
  By \cref{ex:7.1.7}(c) we have \(\ns{\T^i} \neq \ns{\T^{i + 1}}\).
  Thus
  \begin{align*}
             & \beta_{i + 1} \setminus \beta_i \neq \varnothing                   &  & \by{ex:7.2.13}[b]                 \\
    \implies & \forall v \in \beta_{i + 1} \setminus \beta_i, \begin{dcases}
                                                                \T^i(v) \neq \zv \\
                                                                \T^{i + 1}(v) = \zv
                                                              \end{dcases}      &  & \by{2.1.10}                         \\
    \implies & \forall v \in \beta_{i + 1} \setminus \beta_i, \T(v) \in \ns{\T^i} &  & \by{2.1.10}                       \\
    \implies & \forall v \in \beta_{i + 1}, \T(v) \in \ns{\T^i}                   &  & (\beta_i \subseteq \beta_{i + 1}) \\
    \implies & \T(\beta_{i + 1}) \subseteq \ns{\T^i} = \spn{\beta_i}.             &  & \by{ex:7.2.13}[b]
  \end{align*}
  By \cref{2.2.4} this means \([\T]_{\beta}\) is an upper triangular matrix with each diagonal entry equal to \(0\).
\end{proof}

\begin{proof}[\pf{ex:7.2.13}(d)]
  By \cref{ex:7.2.13}(c) we know that \([\T]_{\beta}\) is an upper triangular matrix with each diagonal entry equal to \(0\).
  Thus by \cref{ex:4.2.23} we see that the characteristic polynomial of \(\T\) is \((-1)^n t^n\).
  By \cref{5.2} we know that \(0\) is the only eigenvalue of \(\T\).
\end{proof}

\begin{ex}\label{ex:7.2.14}
  Prove the converse of \cref{ex:7.2.13}(d):
  If \(\T\) is a linear operator on an \(n\)-dimensional vector space \(\V\) over \(\F\) and \((-1)^n t^n\) is the characteristic polynomial of \(\T\), then \(\T\) is nilpotent.
\end{ex}

\begin{proof}[\pf{ex:7.2.14}]
  Since the characteristic polynomial of \(\T\) splits, by \cref{7.1.8} we know that \(\T\) has a Jordan form and a Jordan canonical basis \(\beta\).
  By \cref{5.2} we know that \(0\) is the only eigenvalue of \(\T\), thus by \cref{7.1.1} \([\T]_{\beta}\) is an upper triangular matrix with each diagonal entry equal to \(0\).
  By \cref{ex:7.2.12} we know that \([\T]_{\beta}\) is nilpotent.
  Thus by \cref{ex:7.2.11} we conclude that \(\T\) is nilpotent.
\end{proof}

\begin{ex}\label{ex:7.2.15}
  Give an example of a linear operator \(\T\) on a finite-dimensional vector space \(\V\) over \(\F\) such that \(\T\) is not nilpotent, but zero is the only eigenvalue of \(\T\).
  Characterize all such operators.
\end{ex}

\begin{proof}[\pf{ex:7.2.15}]
  First we give an example as required.
  Let \(\V = \R^3\) and let \(\F = \R\).
  Define
  \[
    A = \begin{pmatrix}
      0 & 0  & 0 \\
      0 & 0  & 1 \\
      0 & -1 & 0
    \end{pmatrix}.
  \]
  Then \(\det(A - t I_3) = (-t)(t^2 + 1)\).
  Thus the characteristic polynomial of \(A\) does not split and \(0\) is the only eigenvalue of \(A\).
  Since \(A^3 = -A\), we know that \(A^p \neq \zm\) for all \(p \in \Z^+\).
  Thus by \cref{7.2.5} \(A\) is not nilpotent.

  Now we characterize all such operator.
  By \cref{ex:7.2.13}(d) and \cref{ex:7.2.14} we know that \(\T\) is nilpotent iff the characteristic polynomial of \(\T\) is \((-1)^n t^n\).
  Thus \(\T\) is not nilpotent iff the characteristic polynomial of \(\T\) is not \((-1)^n t^n\).
\end{proof}

\begin{ex}\label{ex:7.2.16}
  Let \(\T\) be a nilpotent linear operator on a finite-dimensional vector space \(\V\) over \(\F\).
  Recall from \cref{ex:7.2.13} that \(\lambda = 0\) is the only eigenvalue of \(\T\), and hence \(\V = \vs{K}_0\).
  Let \(\beta\) be a Jordan canonical basis for \(\T\).
  Prove that for any positive integer \(i\), if we delete from \(\beta\) the vectors corresponding to the last \(i\) dots in each column of a dot diagram of \(\beta\), the resulting set is a basis for \(\rg{\T^i}\) over \(\F\).
  (If a column of the dot diagram contains fewer than \(i\) dots, all the vectors associated with that column are removed from \(\beta\).)
\end{ex}

\begin{proof}[\pf{ex:7.2.16}]
  Let \(\gamma\) be a cycle in \(\beta\) with length \(p\) and end vector \(x\).
  By \cref{7.1.6} we have
  \[
    \gamma = \set{\T^{p - 1}(x), \dots, \T^i(x), \T^{i - 1}(x), \dots, \T(x), x}.
  \]
  Let \(\alpha\) be the set obtained from removing the last \(i\) vector in \(\gamma\), i.e.,
  \[
    \alpha = \set{\T^{p - 1}(x), \dots, \T^i(x)}.
  \]
  Note that \(\alpha = \varnothing\) when \(p \leq i\).
  If \(\alpha \neq \varnothing\), then each vector in \(\alpha\) is mapped by \(\T^i\) from exactly one vector in \(\gamma\).
  Thus by \cref{2.2} \(\alpha\) is linearly independent and the union of all \(\alpha\) is a basis for \(\rg{\T^i}\) over \(\F\).
\end{proof}

\begin{ex}\label{ex:7.2.17}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\F\) such that the characteristic polynomial of \(\T\) splits, and let \(\seq{\lambda}{1,,k}\) be the distinct eigenvalues of \(\T\).
  Let \(\lt{S} : \V \to \V\) be the mapping defined by
  \[
    \lt{S}(x) = \seq[+]{\lambda,v}{1,,k},
  \]
  where, for each \(i \in \set{1, \dots, k}\), \(v_i\) is the unique vector in \(\vs{K}_{\lambda_i}\) such that \(x = \seq[+]{v}{1,,k}\).
  (This unique representation is guaranteed by \cref{7.3} and \cref{ex:7.1.8}.)
  \begin{enumerate}
    \item Prove that \(\lt{S}\) is a diagonalizable linear operator on \(\V\).
    \item Let \(\U = \T - \lt{S}\).
          Prove that \(\U\) is nilpotent and commutes with \(\lt{S}\), that is, \(\lt{S} \U = \U \lt{S}\).
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:7.2.17}(a)]
  First we show that \(\lt{S} \in \ls(\V)\).
  Let \(x, y \in \V\) and let \(c \in \F\).
  By \cref{7.3} there exist \(\tuple{v}{1,,k}, \tuple{u}{1,,k} \in \prod_{i = 1}^k \vs{K}_{\lambda_i}\) such that \(x = \sum_{i = 1}^k v_i\) and \(y = \sum_{i = 1}^k u_i\).
  Then we have
  \begin{align*}
    \lt{S}(cx + y) & = \lt{S}\pa{\sum_{i = 1}^k c v_i + u_i}                                   &  & \by{7.3}   \\
                   & = \sum_{i = 1}^k \lambda_i (c v_i + u_i)                                                  \\
                   & = c \pa{\sum_{i = 1}^k \lambda_i v_i} + \pa{\sum_{i = 1}^k \lambda_i u_i} &  & \by{1.2.1} \\
                   & = c \lt{S}(x) + \lt{S}(y).
  \end{align*}
  Thus by \cref{2.1.2}(b) \(\lt{S} \in \ls(\V)\).

  Now we show that \(\lt{S}\) is diagonalizable.
  Since
  \[
    \forall i \in \set{1, \dots, k}, \forall v \in \vs{K}_{\lambda_i}, \lt{S}(v) = \lambda_i v
  \]
  and \(\V = \vs{K}_{\lambda_1} \oplus \cdots \oplus \vs{K}_{\lambda_k}\) (\cref{7.8}), we see that every vectors of \(\V\) is an eigenvector of \(\lt{S}\) (\cref{5.1.2}).
  Thus by \cref{5.1.1} \(\lt{S}\) is diagonalizable.
\end{proof}

\begin{proof}[\pf{ex:7.2.17}(b)]
  Fix one \(i \in \set{1, \dots, k}\) and let \(v \in \vs{K}_{\lambda_i}\).
  Let \(p\) be the longest cycle of all \(\vs{K}_{\lambda_1}, \dots, \vs{K}_{\lambda_k}\)
  (\(p\) is well-defined since \(\V\) is finite-dimensional).
  Since
  \begin{align*}
    \U(v) & = \T(v) - \lt{S}(v)                           \\
          & = \T(v) - \lambda_i v         &  & \by{7.3}   \\
          & = (\T - \lambda_i \IT[\V])(v) &  & \by{2.2.5} \\
          & \in \vs{K}_{\lambda_i},       &  & \by{7.1.6}
  \end{align*}
  we see that
  \begin{align*}
    \U^p(v) & = (\T - \lambda_i \IT[\V])^p(v)                 \\
            & = \zv.                          &  & \by{7.1.6}
  \end{align*}
  Since \(v\) is arbitrary, we see that \(\U^p(\vs{K}_{\lambda_i}) = \set{\zv}\).
  Since \(i\) is arbitrary, by \cref{2.1.13,7.8} we see that \(\U^p = \zT\).
  Thus by \cref{7.2.5} \(\U\) is nilpotent.

  Now we show that \(\U \lt{S} = \lt{S} \U\).
  Since
  \begin{align*}
    \U \lt{S}(v) & = \U(\lambda_i v)                       &  & \by{7.3}                                             \\
                 & = \lambda_i \U(v)                       &  & \by{2.1.1}[b]                                        \\
                 & = \lambda_i (\T - \lambda_i \IT[\V])(v)                                                           \\
                 & = \lt{S}((\T - \lambda_i \IT[\V])(v))   &  & ((\T - \lambda_i \IT[\V])(v) \in \vs{K}_{\lambda_i}) \\
                 & = \lt{S} \U(v)
  \end{align*}
  and \(v\) is arbitrary, we see that \(\U \lt{S} = \lt{S} \U\) on \(\vs{K_{\lambda_i}}\).
  Since \(i\) is arbitrary, by \cref{2.1.13,7.8} we see that \(\U \lt{S} = \lt{S} \U\) on \(\V\).
\end{proof}

\begin{ex}\label{ex:7.2.18}
  Let \(\T\) be a linear operator on a finite-dimensional vector space \(\V\) over \(\C\), and let \(J\) be the Jordan canonical form of \(\T\).
  Let \(D\) be the diagonal matrix whose diagonal entries are the diagonal entries of \(J\), and let \(M = J - D\).
  Prove the following results.
  \begin{enumerate}
    \item \(M\) is nilpotent.
    \item \(MD = DM\).
    \item If \(p\) is the smallest positive integer for which \(M^p = \zm\), then, for any positive integer \(r < p\),
          \[
            J^r = D^r + r D^{r - 1} M + \dfrac{r (r - 1)}{2!} D^{r - 2} M^2 + \cdots + r D M^{r - 1} + M^r,
          \]
          and, for any positive integer \(r \geq p\),
          \begin{align*}
            J^r & = D^r + r D^{r - 1} M + \dfrac{r (r - 1)}{2!} D^{r - 2} M^2 + \cdots \\
                & \quad + \dfrac{r!}{(r - p + 1)! (p - 1)!} D^{r - p + 1} M^{p - 1}.
          \end{align*}
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:7.2.18}(a)]
  By \cref{7.1.1} \(M\) is an upper triangular matrix with each diagonal entry equal to zero.
  Thus by \cref{ex:7.2.12} \(M\) is nilpotent.
\end{proof}

\begin{proof}[\pf{ex:7.2.18}(b)]
  Suppose that \(\dim(\V) = n\).
  Then we have
  \begin{align*}
    \forall i, j \in \set{1, \dots, n}, (MD)_{i j} & = \sum_{k = 1}^n M_{i k} D_{k j}    &  & \by{2.3.1} \\
                                                   & = M_{i j} D_{j j}                   &  & \by{1.3.8} \\
                                                   & = \begin{dcases}
                                                         D_{j j} & \text{if } M_{i j} = 1 \\
                                                         0       & \text{if } M_{i j} = 0
                                                       \end{dcases} &  & \by{7.1.1}                  \\
                                                   & = \begin{dcases}
                                                         D_{i i} & \text{if } M_{i j} = 1 \\
                                                         0       & \text{if } M_{i j} = 0
                                                       \end{dcases}; &  & \by{7.1.1}                  \\
                                                   & = D_{i i} M_{i j}                                   \\
                                                   & = \sum_{k = 1}^n D_{i k} M_{k j}    &  & \by{1.3.8} \\
                                                   & = (DM)_{i j}.                       &  & \by{2.3.1}
  \end{align*}
  Thus by \cref{1.2.8} \(MD = DM\).
\end{proof}

\begin{proof}[\pf{ex:7.2.18}(c)]
  We have
  \[
    J^r = (M + D)^r = \sum_{i = 0}^r \binom{r}{i} M^i D^{r - i}
  \]
  for \(r < p\) and
  \[
    J^r = (M + D)^r = \sum_{i = 0}^{p - 1} \binom{r}{i} M^i D^{r - i}
  \]
  for \(r \geq p\) since \(M^p = \zm\).
\end{proof}

\begin{ex}\label{ex:7.2.19}
  Let
  \[
    J = \begin{pmatrix}
      \lambda & 1       & 0       & \cdots & 0       \\
      0       & \lambda & 1       & \cdots & 0       \\
      0       & 0       & \lambda & \cdots & 0       \\
      \vdots  & \vdots  & \vdots  &        & \vdots  \\
      0       & 0       & 0       & \cdots & 1       \\
      0       & 0       & 0       & \cdots & \lambda
    \end{pmatrix}
  \]
  be the \(m \times m\) Jordan block corresponding to \(\lambda\), and let \(N = J - \lambda I_m\).
  Prove the following results:
  \begin{enumerate}
    \item \(N^m = \zm\), and for \(r \in \set{1, \dots, m - 1}\),
          \[
            N_{i j}^r = \begin{dcases}
              1 & \text{if } j = i + r \\
              0 & \text{otherwise}
            \end{dcases}.
          \]
    \item For any integer \(r \geq m\),
          \[
            J^r = \begin{pmatrix}
              \lambda^r & r \lambda^{r - 1} & \dfrac{r (r - 1)}{2!} \lambda^{r - 2} & \cdots & \dfrac{r(r - 1) \cdots (r - m + 2)}{(m - 1)!} \lambda^{r - m + 1} \\
              0         & \lambda^r         & r \lambda^{r - 1}                     & \cdots & \dfrac{r(r - 1) \cdots (r - m + 3)}{(m - 2)!} \lambda^{r - m + 2} \\
              \vdots    & \vdots            & \vdots                                &        & \vdots                                                            \\
              0         & 0                 & 0                                     & \cdots & \lambda^r
            \end{pmatrix}.
          \]
    \item \(\Lim_{r \to \infty} J^r\) exists iff one of the following holds:
          \begin{enumerate}[label=(\roman*)]
            \item \(\abs{\lambda} < 1\).
            \item \(\lambda = 1\) and \(m = 1\).
                  (Note that \(\Lim_{r \to \infty} \lambda^r\) exists under these conditions.
                  See the discussion preceding \cref{5.13}.)
          \end{enumerate}
          Furthermore, \(\Lim_{r \to \infty} J^r\) is the zero matrix if condition (i) holds and is the \(1 \times 1\) matrix \((1)\) if condition (ii) holds.
    \item Prove \cref{5.13}.
  \end{enumerate}
\end{ex}

\begin{proof}[\pf{ex:7.2.19}(a)]
  We have \(N e_1 = \zv\) and
  \[
    \forall i \in \set{2, \dots, m}, N e_i = e_{i - 1}.
  \]
  Thus \(N^m e_i = \zv\) for all \(i \in \set{1, \dots, m}\).
  By \cref{2.1.13} this means \(N^m = \zm\).
  Since \(N^r e_j = e_{j - r}\) for all \(j \in \set{1, \dots, m}\), by \cref{2.2.4} we see that
  \[
    \forall i, j \in \set{1, \dots, m}, N_{i j}^r = \begin{dcases}
      1 & \text{if } j = i + r \\
      0 & \text{otherwise}
    \end{dcases}.
  \]
\end{proof}

\begin{proof}[\pf{ex:7.2.19}(b)]
  By \cref{ex:7.2.18}(c) we have
  \[
    J^r = (N + \lambda I_m)^r = \sum_{k = 0}^{m - 1} \binom{r}{k} N^k (\lambda I_m)^{r - k}.
  \]
  Let \(i, j \in \set{1, \dots, m}\).
  Since
  \begin{align*}
     & (J^r)_{i j} = \pa{\sum_{k = 0}^{m - 1} \binom{r}{k} N^k (\lambda I_m)^{r - k}}_{i j}                 &  & \by{ex:7.2.18}[c] \\
     & = \sum_{k = 0}^{m - 1} \binom{r}{k} \pa{N^k (\lambda I_m)^{r - k}}_{i j}                             &  & \by{1.2.9}        \\
     & = \sum_{k = 0}^{m - 1} \binom{r}{k} \pa{\sum_{p = 1}^m (N^k)_{i p} \pa{(\lambda I_m)^{r - k}}_{p j}} &  & \by{2.3.1}        \\
     & = \sum_{k = 0}^{m - 1} \binom{r}{k} \pa{(N^k)_{i j} \pa{(\lambda I_m)^{r - k}}_{j j}}                &  & \by{1.3.8}        \\
     & = \sum_{k = 0}^{m - 1} \binom{r}{k} \pa{(N^k)_{i j} \lambda^{r - k}},                                &  & \by{1.3.8}
  \end{align*}
  by \cref{ex:7.2.19}(a) we have
  \[
    J^r = \begin{pmatrix}
      \binom{r}{0} \lambda^r & \binom{r}{1} \lambda^{r - 1} & \binom{r}{2} \lambda^{r - 2} & \cdots & \binom{r}{m - 1} \lambda^{r - (m - 1)} \\
      0                      & \binom{r}{0} \lambda^r       & \binom{r}{1} \lambda^{r - 1} & \cdots & \binom{r}{m - 2} \lambda^{r - (m - 2)} \\
      \vdots                 & \vdots                       & \vdots                       &        & \vdots                                 \\
      0                      & 0                            & 0                            & \cdots & \binom{r}{0} \lambda^r
    \end{pmatrix}.
  \]
\end{proof}

\begin{proof}[\pf{ex:7.2.19}(c)]
  By \cref{5.3.1} we have
  \begin{align*}
         & \Lim_{r \to \infty} J^r \in \ms[m][m][\C]                                                                                          \\
    \iff & \forall k \in \set{1, \dots, m - 1}, \Lim_{r \to \infty} \binom{r}{k} \lambda^{r - k} \in \C                                       \\
    \iff & \forall k \in \set{1, \dots, m - 1}, \Lim_{r \to \infty} \binom{r}{k} \lambda^{r - k} = \begin{dcases}
                                                                                                     0 & \text{if } \abs{\lambda} < 1           \\
                                                                                                     1 & \text{if } (\lambda = 1) \land (m = 1)
                                                                                                   \end{dcases}.
  \end{align*}
\end{proof}

\begin{proof}[\pf{ex:7.2.19}(d)]
  Let \(A \in \ms[n][n][\C]\).
  Since the characteristic polynomial of \(A\) split, we know that \(A\) has a Jordan form \(J\).
  By \cref{7.1.10} we have \(J = Q^{-1} A Q\) for some \(Q \in \ms[n][n][\C]\).
  Thus by \cref{5.3.2} \(\Lim_{r \to \infty} A^r\) exists iff \(\Lim_{r \to \infty} J^r\) exists.
  Let \(\seq{J}{1,,k}\) be Jordan blocks such that \(J = \seq[\oplus]{J}{1,,k}\).
  By \cref{2.3.1} we have \(J^r = J_1^r \oplus \cdots J_k^r\).
  Thus we see that \(\Lim_{r \to \infty} J^r\) exists iff \(\Lim_{r \to \infty} J_i^r\) exists for all \(i \in \set{1, \dots, k}\).
  By \cref{ex:7.2.19}(c) we see that if \(1\) is an eigenvalue of \(J_i\) for some \(i \in \set{1, \dots, k}\), then \(J_i = (1)\).
  Thus by \cref{7.8} we have \(\dim(\vs{E}_1) = \dim(\vs{K}_1) = m\), where \(m\) is the multiplicity of \(1\).
  We conclude that if \(\lim_{r \to \infty} A^r\) exists and \(1\) is an eigenvalue of \(A\), then \(\dim(E_1) = m\).
\end{proof}

\begin{ex}\label{ex:7.2.20}
\end{ex}

\begin{ex}\label{ex:7.2.21}
\end{ex}
